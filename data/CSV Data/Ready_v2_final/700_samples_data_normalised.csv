paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,fieldsOfStudy/1,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,authors/5/authorId,authors/5/name,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,fieldsOfStudy/2
bad620c25920edbaba8836032459b135669171c3,https://www.semanticscholar.org/paper/bad620c25920edbaba8836032459b135669171c3,Machine Learning and Its Applications to Biology,"The term machine learning refers to a set of topics dealing with the creation and evaluation of algorithms that facilitate pattern recognition, classification, and prediction, based on models derived from existing data. Two facets of mechanization should be acknowledged when considering machine learning in broad terms. Firstly, it is intended that the classification and prediction tasks can be accomplished by a suitably programmed computing machine. That is, the product of machine learning is a classifier that can be feasibly used on available hardware. Secondly, it is intended that the creation of the classifier should itself be highly mechanized, and should not involve too much human input. This second facet is inevitably vague, but the basic objective is that the use of automatic algorithm construction methods can minimize the possibility that human biases could affect the selection and performance of the algorithm. Both the creation of the algorithm and its operation to classify objects or predict events are to be based on concrete, observable data.The history of relations between biology and the field of machine learning is long and complex. An early technique [1] for machine learning called the perceptron constituted an attempt to model actual neuronal behavior, and the field of artificial neural network (ANN) design emerged from this attempt. Early work on the analysis of translation initiation sequences [2] employed the perceptron to define criteria for start sites in Escherichia coli. Further artificial neural network architectures such as the adaptive resonance theory (ART) [3] and neocognitron [4] were inspired from the organization of the visual nervous system. In the intervening years, the flexibility of machine learning techniques has grown along with mathematical frameworks for measuring their reliability, and it is natural to hope that machine learning methods will improve the efficiency of discovery and understanding in the mounting volume and complexity of biological data. This tutorial is structured in four main components. Firstly, a brief section reviews definitions and mathematical prerequisites. Secondly, the field of supervised learning is described. Thirdly, methods of unsupervised learning are reviewed. Finally, a section reviews methods and examples as implemented in the open source data analysis and visualization language R (http://www.r-project.org).",2007.0,50,482,19,True,Computer Science,Medicine,2490889,A. Tarca,33361123.0,V. Carey,145793137.0,Xue-wen Chen,2142580259.0,R. Romero,1775019.0,S. Drăghici,,,,,,,,,,,,,,,,,,,,,,,
1bcbf2a4500d27d036e0f9d36d7af71c72f8ab61,https://www.semanticscholar.org/paper/1bcbf2a4500d27d036e0f9d36d7af71c72f8ab61,Recognizing facial expression: machine learning and application to spontaneous behavior,"We present a systematic comparison of machine learning methods applied to the problem of fully automatic recognition of facial expressions. We report results on a series of experiments comparing recognition engines, including AdaBoost, support vector machines, linear discriminant analysis. We also explored feature selection techniques, including the use of AdaBoost for feature selection prior to classification by SVM or LDA. Best results were obtained by selecting a subset of Gabor filters using AdaBoost followed by classification with support vector machines. The system operates in real-time, and obtained 93% correct generalization to novel subjects for a 7-way forced choice on the Cohn-Kanade expression dataset. The outputs of the classifiers change smoothly as a function of time and thus can be used to measure facial expression dynamics. We applied the system to to fully automated recognition of facial actions (FACS). The present system classifies 17 action units, whether they occur singly or in combination with other actions, with a mean accuracy of 94.8%. We present preliminary results for applying this system to spontaneous facial expressions.",2005.0,18,701,56,False,Computer Science,,2218905,M. Bartlett,46548046.0,G. Littlewort,145595070.0,M. Frank,2767464.0,C. Lainscsek,2039025.0,I. Fasel,1741200.0,J. Movellan,,,,,,,,,,,,,,,,,,,,,
fcb926027ba5001f8f69dc0f1de5ded7d003b6af,https://www.semanticscholar.org/paper/fcb926027ba5001f8f69dc0f1de5ded7d003b6af,A comparison of machine learning techniques for phishing detection,"There are many applications available for phishing detection. However, unlike predicting spam, there are only few studies that compare machine learning techniques in predicting phishing. The present study compares the predictive accuracy of several machine learning methods including Logistic Regression (LR), Classification and Regression Trees (CART), Bayesian Additive Regression Trees (BART), Support Vector Machines (SVM), Random Forests (RF), and Neural Networks (NNet) for predicting phishing emails. A data set of 2889 phishing and legitimate emails is used in the comparative study. In addition, 43 features are used to train and test the classifiers.",2007.0,24,398,29,False,Computer Science,,1403307248,Saeed Abu-Nimeh,2635516.0,D. Nappa,37457992.0,Xinlei Wang,145531251.0,S. Nair,,,,,,,,,,,,,,,,,,,,,,,,,
eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece,https://www.semanticscholar.org/paper/eb0c1e3d880e361b7ff61e5ac1d489cb75c55ece,Adaptive computation and machine learning,"All rights reserved. No part of this book may be reproduced in any form by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from this is the last candidate. next esc will revert to uncompleted text. he publisher. Overview Dataset shift is a challenging situation where the joint distribution of inputs and outputs differs between the training and test stages. Covariate shift is a simpler particular case of dataset shift where only the input distribution changes (covariate denotes input), while the conditional distribution of the outputs given the inputs p(y|x) remains unchanged. Dataset shift is present in most practical applications for reasons ranging from the bias introduced by experimental design, to the mere irreproducibility of the testing conditions at training time. For example, in an image classification task, training data might have been recorded under controlled laboratory conditions, whereas the test data may show different lighting conditions. In other applications, the process that generates data is in itself adaptive. Some of our authors consider the problem of spam email filtering: successful "" spammers "" will try to build spam in a form that differs from the spam the automatic filter has been built on. Dataset shift seems to have raised relatively little interest in the machine learning community until very recently. Indeed, many machine learning algorithms are based on the assumption that the training data is drawn from exactly the same distribution as the test data on which the model will later be evaluated. Semi-supervised learning and active learning, two problems that seem very similar to covariate shift have received much more attention. How do they differ from covariate shift? Semi-supervised learning is designed to take advantage of unlabeled data present at training time, but is not conceived to be robust against changes in the input distribution. In fact, one can easily construct examples of covariate shift for which common SSL strategies such as the "" cluster assumption "" will lead to disaster. In active learning the algorithm is asked to select from the available unlabeled inputs those for which obtaining the label will be most beneficial for learning. This is very relevant in contexts where labeling data is very costly, but active learning strategies 2 Contents are not specifically design for dealing with covariate shift. This book attempts to give an overview of the different recent efforts that are being …",1998.0,884,875,24,False,Computer Science,,144299726,Thomas G. Dietterich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
48234756b7cf798bfeb47328f7c5d597fd4838c2,https://www.semanticscholar.org/paper/48234756b7cf798bfeb47328f7c5d597fd4838c2,ADASYN: Adaptive synthetic sampling approach for imbalanced learning,"This paper presents a novel adaptive synthetic (ADASYN) sampling approach for learning from imbalanced data sets. The essential idea of ADASYN is to use a weighted distribution for different minority class examples according to their level of difficulty in learning, where more synthetic data is generated for minority class examples that are harder to learn compared to those minority examples that are easier to learn. As a result, the ADASYN approach improves learning with respect to the data distributions in two ways: (1) reducing the bias introduced by the class imbalance, and (2) adaptively shifting the classification decision boundary toward the difficult examples. Simulation analyses on several machine learning data sets show the effectiveness of this method across five evaluation metrics.",2008.0,34,2395,333,True,Computer Science,,2198278,Haibo He,2115297828.0,Yang Bai,2111388466.0,E. A. Garcia,2116066317.0,Shutao Li,,,,,,,,,,,,,,,,,,,,,,,,,
e541c475457a731d7d434c4302867fc45af5876f,https://www.semanticscholar.org/paper/e541c475457a731d7d434c4302867fc45af5876f,Active Learning,"The key idea behind active learning is that a machine learning algorithm can perform better with less training if it is allowed to choose the data from which it learns. An active learner may pose ""queries,"" usually in the form of unlabeled data instances to be labeled by an ""oracle"" (e.g., a human annotator) that already understands the nature of the problem. This sort of approach is well-motivated in many modern machine learning and data mining applications, where unlabeled data may be abundant or easy to come by, but training labels are difficult, time-consuming, or expensive to obtain. This book is a general introduction to active learning. It outlines several scenarios in which queries might be formulated, and details many query selection algorithms which have been organized into four broad categories, or ""query selection frameworks."" We also touch on some of the theoretical foundations of active learning, and conclude with an overview of the strengths and weaknesses of these approaches in practice, including a summary of ongoing work to address these open challenges and opportunities.",2012.0,86,790,60,False,Computer Science,,1717452,Burr Settles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3493e0285fe329a710f54be2ef82350fdaafc991,https://www.semanticscholar.org/paper/3493e0285fe329a710f54be2ef82350fdaafc991,Applications of Machine Learning to Cognitive Radio Networks,"Cognitive radio offers the promise of intelligent radios that can learn from and adapt to their environment. To date, most cognitive radio research has focused on policy-based radios that are hard-coded with a list of rules on how the radio should behave in certain scenarios. Some work has been done on radios with learning engines tailored for very specific applications. This article describes a concrete model for a generic cognitive radio to utilize a learning engine. The goal is to incorporate the results of the learning engine into a predicate calculus-based reasoning engine so that radios can remember lessons learned in the past and act quickly in the future. We also investigate the differences between reasoning and learning, and the fundamentals of when a particular application requires learning, and when simple reasoning is sufficient. The basic architecture is consistent with cognitive engines seen in AI research. The focus of this article is not to propose new machine learning algorithms, but rather to formalize their application to cognitive radio and develop a framework from within which they can be useful. We describe how our generic cognitive engine can tackle problems such as capacity maximization and dynamic spectrum access.",2007.0,17,317,11,False,Computer Science,,144047524,T. Clancy,36065037.0,Joe Hecker,1767354.0,E. P. Stuntebeck,1388350203.0,Tim O'Shea,,,,,,,,,,,,,,,,,,,,,,,,,
08c81389b3ac4b8253d718a7cebe04a5536efa78,https://www.semanticscholar.org/paper/08c81389b3ac4b8253d718a7cebe04a5536efa78,Improving Machine Learning Approaches to Coreference Resolution,"We present a noun phrase coreference system that extends the work of Soon et al. (2001) and, to our knowledge, produces the best results to date on the MUC-6 and MUC-7 coreference resolution data sets --- F-measures of 70.4 and 63.4, respectively. Improvements arise from two sources: extra-linguistic changes to the learning framework and a large-scale expansion of the feature set to include more sophisticated linguistic knowledge.",2002.0,13,774,92,True,Computer Science,,145106110,Vincent Ng,1794075.0,Claire Gardent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ab08f2a0b98fe7938d08875eb6125fa518620222,https://www.semanticscholar.org/paper/ab08f2a0b98fe7938d08875eb6125fa518620222,The Need for Open Source Software in Machine Learning,"Open source tools have recently reached a level of maturity which makes them suitable for building large-scale real-world systems. At the same time, the field of machine learning has developed a large body of powerful learning algorithms for diverse applications. However, the true potential of these methods is not used, since existing implementations are not openly shared, resulting in software with low usability, and weak interoperability. We argue that this situation can be significantly improved by increasing incentives for researchers to publish their software under an open source model. Additionally, we outline the problems authors are faced with when trying to publish algorithmic implementations of machine learning methods. We believe that a resource of peer reviewed software accompanied by short articles would be highly valuable to both the machine learning and the general scientific community.",2007.0,63,208,10,False,Computer Science,,3029782,S. Sonnenburg,2522031.0,M. Braun,1706780.0,Cheng Soon Ong,1751569.0,Samy Bengio,52184096.0,L. Bottou,144282963.0,G. Holmes,1688882.0,Yann LeCun,145034054.0,K. Müller,145366908.0,Fernando C Pereira,3472959.0,C. Rasmussen,152597562.0,Gunnar Rätsch,1707625.0,B. Schölkopf,46234526.0,Alex Smola,145467703.0,Pascal Vincent,145183709.0,J. Weston,143957317.0,R. C. Williamson,
0948365ef39ef153e61e9569ade541cf881c7c2a,https://www.semanticscholar.org/paper/0948365ef39ef153e61e9569ade541cf881c7c2a,Learning the Kernel Matrix with Semidefinite Programming,"Kernel-based learning algorithms work by embedding the data into a Euclidean space, and then searching for linear relations among the embedded data points. The embedding is performed implicitly, by specifying the inner products between each pair of points in the embedding space. This information is contained in the so-called kernel matrix, a symmetric and positive semidefinite matrix that encodes the relative positions of all points. Specifying this matrix amounts to specifying the geometry of the embedding space and inducing a notion of similarity in the input space---classical model selection problems in machine learning. In this paper we show how the kernel matrix can be learned from data via semidefinite programming (SDP) techniques. When applied to a kernel matrix associated with both training and test data this gives a powerful transductive algorithm---using the labeled part of the data one can learn an embedding also for the unlabeled part. The similarity between test points is inferred from training points and their labels. Importantly, these learning problems are convex, so we obtain a method for learning both the model class and the function without local minima. Furthermore, this approach leads directly to a convex method for learning the 2-norm soft margin parameter in support vector machines, solving an important open problem.",2002.0,35,2517,281,False,Mathematics,Computer Science,1725533,G. Lanckriet,1685083.0,N. Cristianini,1745169.0,P. Bartlett,1701847.0,L. Ghaoui,1694621.0,Michael I. Jordan,,,,,,,,,,,,,,,,,,,,,,,
60caa5b3d066e13feac496fd0736e976970eb09f,https://www.semanticscholar.org/paper/60caa5b3d066e13feac496fd0736e976970eb09f,Overview of machine learning,"The most widely studied problem in machine learning is supervised learning. We are given a labeled training set of input-output pairs, D = (xi, yi)i=1, and have to learn a way to predict the output or target ỹ for a novel test input x̃ (i.e, for x̃ 6∈ D). (We use the tilde notation to denote test cases that we have not seen before.) Some examples include: predicting if someone has cancer ỹ ∈ {0, 1} given some measured variables x̃; predicting the stock price tomorrow ỹ ∈ IR given the stock prices today x̃; etc. A common approach is to just predict one’s “best guess”, such as ŷ(x̃). However, we prefer to compute a probability distribution over the output, p(ỹ|x̃), since it is very useful to have a measure of confidence associated with one’s prediction, especially in medical and financial domains. In addition, probabilistic methods are essential for unsupervised learning, as we discuss in Section 3. If y is discrete or categorical, say y ∈ {1, 2, . . . , C}, this problem is called classification or pattern recognition. If there are C = 2 classes or labels, the problem is called binary classification (see Figure 1 for an example), otherwise it is called multi-class classification. We usually assume the classes are mutually exclusive, so y can only be in one possible state. If we want to allow multiple labels, we can represent y by a bit-vector of length C, so yj = 1 if y belongs to class j. If y is continuous, say y ∈ IR, this problem is called regression. If y is multidimensional, say y ∈ IR , we call it multivariate regression. If y is discrete, but ordered (e.g., y ∈ {low,medium,high}), the problem is called ordinal regression. A priori, our prediction might be quite poor, but we are provided with a labeled training set of input-output pairs, D = (xi, yi) n i=1, which provides a set of examples of the “right response” for a set of possible inputs. If each input",2007.0,2,173,3,False,,,2056417995,K. Murphy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56b37f88b109fd455a389642e1747f766f1be471,https://www.semanticscholar.org/paper/56b37f88b109fd455a389642e1747f766f1be471,Using machine learning to focus iterative optimization,"Iterative compiler optimization has been shown to outperform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper develops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an independent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C67I3 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magnitude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations.",2006.0,20,450,31,False,Computer Science,,3288675,F. Agakov,30561807.0,Edwin V. Bonilla,1874233.0,J. Cavazos,2644436.0,Björn Franke,2043821.0,G. Fursin,1401533251.0,M. O’Boyle,153031524.0,J. Thomson,120284556.0,M. Toussaint,145715698.0,Christopher K. I. Williams,,,,,,,,,,,,,,,
f2642db17084b14068d56f332de2f2d5a1622c5a,https://www.semanticscholar.org/paper/f2642db17084b14068d56f332de2f2d5a1622c5a,Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning,"One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.",2009.0,26,578,31,False,Computer Science,Medicine,2069968720,Guorui Feng,145678691.0,G. Huang,3026738.0,Qingping Lin,1723858.0,R. Gay,,,,,,,,,,,,,,,,,,,,,,,,,
c2c4db02486ea139e8142295888ff7b075575fba,https://www.semanticscholar.org/paper/c2c4db02486ea139e8142295888ff7b075575fba,Text Classification Using Machine Learning Techniques,"Automated text classification has been considered as a vital method to manage and process a vast amount of documents in digital forms that are widespread and continuously increasing. In general, text classification plays an important role in information extraction and summarization, text retrieval, and question- answering. This paper illustrates the text classification process using machine learning techniques. The references cited cover the major theoretical issues and guide the researcher to interesting research directions.",2005.0,36,471,24,False,Computer Science,,67128225,M. Ikonomakis,1697867.0,S. Kotsiantis,38799518.0,V. Tampakas,,,,,,,,,,,,,,,,,,,,,,,,,,,
9111e0a578ae34664dddcd76794bbe20e168a1ff,https://www.semanticscholar.org/paper/9111e0a578ae34664dddcd76794bbe20e168a1ff,Usilng Machine Learning Technliques to Identify Botnet Traffic,"To date, techniques to counter cyber-attacks have predominantly been reactive; they focus on monitoring network traffic, detecting anomalies and cyber-attack traffic patterns, and, a posteriori, combating the cyber-attacks and mitigating their effects. Contrary to such approaches, we advocate proactively detecting and identifying botnets prior to their being used as part of a cyber-attack (Strayer et al., 2006). In this paper, we present our work on using machine learning-based classification techniques to identify the command and control (C2) traffic of IRC-based botnets - compromised hosts that are collectively commanded using Internet relay chat (IRC). We split this task into two stages: (I) distinguishing between IRC and non-IRC traffic, and (II) distinguishing between botnet and real IRC traffic. For stage I, we compare the performance of J48, naive Bayes, and Bayesian network classifiers, identify the features that achieve good overall classification accuracy, and determine the classification sensitivity to the training set size. While sensitive to the training data and the attributes used to characterize communication flows, machine learning-based classifiers show promise in identifying IRC traffic. Using classification in stage II is trickier, since accurately labeling IRC traffic as botnet and non-botnet is challenging. We are currently exploring labeling flows as suspicious and non-suspicious based on telltales of hosts being compromised",2006.0,19,348,16,False,Computer Science,,32744942,C. Livadas,145963952.0,R. Walsh,17286974.0,D. Lapsley,1732393.0,W. Strayer,,,,,,,,,,,,,,,,,,,,,,,,,
c39a7d4c36efd7d08678f6ab6d97e31df91235b5,https://www.semanticscholar.org/paper/c39a7d4c36efd7d08678f6ab6d97e31df91235b5,The application of machine learning to structural health monitoring,"In broad terms, there are two approaches to damage identification. Model-driven methods establish a high-fidelity physical model of the structure, usually by finite element analysis, and then establish a comparison metric between the model and the measured data from the real structure. If the model is for a system or structure in normal (i.e. undamaged) condition, any departures indicate that the structure has deviated from normal condition and damage is inferred. Data-driven approaches also establish a model, but this is usually a statistical representation of the system, e.g. a probability density function of the normal condition. Departures from normality are then signalled by measured data appearing in regions of very low density. The algorithms that have been developed over the years for data-driven approaches are mainly drawn from the discipline of pattern recognition, or more broadly, machine learning. The object of this paper is to illustrate the utility of the data-driven approach to damage identification by means of a number of case studies.",2007.0,37,322,9,False,Mathematics,Medicine,144789804,K. Worden,145004158.0,G. Manson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
75e56ef7924972fde2ffc32d7071cd182d0f0f21,https://www.semanticscholar.org/paper/75e56ef7924972fde2ffc32d7071cd182d0f0f21,Selection of Relevant Features in Machine Learning,"In this paper, we review the problem of selecting rele- vant features for use in machine learning. We describe this problem in terms of heuristic search through a space of feature sets, and we identify four dimensions along which approaches to the problem can vary. We consider recent work on feature selection in terms of this framework, then close with some challenges for future work in the area. 1. The Problem of Irrelevant Features accuracy) to grow slowly with the number of irrele- vant attributes. Theoretical results for algorithms that search restricted hypothesis spaces are encouraging. For instance, the worst-case number of errors made by Littlestone's (1987) WINNOW method grows only logarithmically with the number of irrelevant features. Pazzani and Sarrett's (1992) average-case analysis for WHOLIST, a simple conjunctive algorithm, and Lang- ley and Iba's (1993) treatment of the naive Bayesian classifier, suggest that their sample complexities grow at most linearly with the number of irrelevant features. However, the theoretical results are less optimistic for induction methods that search a larger space of concept descriptions. For example, Langley and Iba's (1993) average-case analysis of simple nearest neighbor indicates that its sample complexity grows exponen- tially with the number of irrelevant attributes, even for conjunctive target concepts. Experimental stud- ies of nearest neighbor are consistent with this conclu- sion, and other experiments suggest that similar results hold even for induction algorithms that explicitly se- lect features. For example, the sample complexity for decision-tree methods appears to grow linearly with the number of irrelevants for conjunctive concepts, but exponentially for parity concepts, since the evaluation metric cannot distinguish relevant from irrelevant fea- tures in the latter situation (Langley & Sage, in press). Results of this sort have encouraged machine learn- ing researchers to explore more sophisticated methods for selecting relevant features. In the sections that fol- low, we present a general framework for this task, and then consider some recent examples of work on this important problem.",1994.0,21,778,24,True,Mathematics,,1713919,P. Langley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e6276780b23ce2cff6e19397b5e726842f8a24d4,https://www.semanticscholar.org/paper/e6276780b23ce2cff6e19397b5e726842f8a24d4,Ensemble Based Extreme Learning Machine,"Extreme learning machine (ELM) was proposed as a new class of learning algorithm for single-hidden layer feedforward neural network (SLFN). To achieve good generalization performance, ELM minimizes training error on the entire training data set, therefore it might suffer from overfitting as the learning model will approximate all training samples well. In this letter, an ensemble based ELM (EN-ELM) algorithm is proposed where ensemble learning and cross-validation are embedded into the training phase so as to alleviate the overtraining problem and enhance the predictive stability. Experimental results on several benchmark databases demonstrate that EN-ELM is robust and efficient for classification.",2010.0,17,217,11,False,Computer Science,,2143065963,Nan Liu,2113289095.0,Han Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
160a4786dd643d9f758b9cc0758bdd2581524941,https://www.semanticscholar.org/paper/160a4786dd643d9f758b9cc0758bdd2581524941,Machine learning for detection and diagnosis of disease.,"Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.",2006.0,152,293,7,False,Computer Science,Medicine,1812793,P. Sajda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fc82788021963ff8e318ffe955829bc68e48943a,https://www.semanticscholar.org/paper/fc82788021963ff8e318ffe955829bc68e48943a,Machine Learning of Temporal Relations,"This paper investigates a machine learning approach for temporally ordering and anchoring events in natural language texts. To address data sparseness, we used temporal reasoning as an over-sampling method to dramatically expand the amount of training data, resulting in predictive accuracy on link labeling as high as 93% using a Maximum Entropy classifier on human annotated data. This method compared favorably against a series of increasingly sophisticated baselines involving expansion of rules derived from human intuitions.",2006.0,28,274,27,True,Computer Science,,1729172,I. Mani,145539145.0,M. Verhagen,2107124.0,Ben Wellner,2495244.0,Chong Min Lee,1707726.0,J. Pustejovsky,,,,,,,,,,,,,,,,,,,,,,,
06757c457ec442eb35af6ea45d8d0e2339415178,https://www.semanticscholar.org/paper/06757c457ec442eb35af6ea45d8d0e2339415178,The Interplay of Optimization and Machine Learning Research,"The fields of machine learning and mathematical programming are increasingly intertwined. Optimization problems lie at the heart of most machine learning approaches. The Special Topic on Machine Learning and Large Scale Optimization examines this interplay. Machine learning researchers have embraced the advances in mathematical programming allowing new types of models to be pursued. The special topic includes models using quadratic, linear, second-order cone, semi-definite, and semi-infinite programs. We observe that the qualities of good optimization algorithms from the machine learning and optimization perspectives can be quite different. Mathematical programming puts a premium on accuracy, speed, and robustness. Since generalization is the bottom line in machine learning and training is normally done off-line, accuracy and small speed improvements are of little concern in machine learning. Machine learning prefers simpler algorithms that work in reasonable computational time for specific classes of problems. Reducing machine learning problems to well-explored mathematical programming classes with robust general purpose optimization codes allows machine learning researchers to rapidly develop new techniques. In turn, machine learning presents new challenges to mathematical programming. The special issue include papers from two primary themes: novel machine learning models and novel optimization approaches for existing models. Many papers blend both themes, making small changes in the underlying core mathematical program that enable the develop of effective new algorithms.",2006.0,50,142,5,False,Computer Science,,2065549778,Kristin P. Bennett,1399027884.0,E. Parrado-Hernández,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6383011826c14af5d63700b4a3dd6e33391c55d0,https://www.semanticscholar.org/paper/6383011826c14af5d63700b4a3dd6e33391c55d0,Bioinformatics: the machine learning approach,"In this book Pierre Baldi and Soren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.",2002.0,0,615,21,False,Computer Science,,144902513,P. Baldi,8169197.0,S. Brunak,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ccfaba27ca7bce5b95dde407e9df568e5902b218,https://www.semanticscholar.org/paper/ccfaba27ca7bce5b95dde407e9df568e5902b218,Learning with Submodular Functions: A Convex Optimization Perspective,"Submodular functions are relevant to machine learning for at least two reasons: (1) some problems may be expressed directly as the optimization of submodular functions and (2) the Lovsz extension of submodular functions provides a useful set of regularization functions for supervised and unsupervised learning. In Learning with Submodular Functions: A Convex Optimization Perspective, the theory of submodular functions is presented in a self-contained way from a convex analysis perspective, presenting tight links between certain polyhedra, combinatorial optimization and convex optimization problems. In particular, it describes how submodular function minimization is equivalent to solving a wide variety of convex optimization problems. This allows the derivation of new efficient algorithms for approximate and exact submodular function minimization with theoretical guarantees and good practical performance. By listing many examples of submodular functions, it reviews various applications to machine learning, such as clustering, experimental design, sensor placement, graphical model structure learning or subset selection, as well as a family of structured sparsity-inducing norms that can be derived and used from submodular functions. Learning with Submodular Functions: A Convex Optimization Perspective is an ideal reference for researchers, scientists, or engineers with an interest in applying submodular functions to machine learning problems.",2011.0,221,410,62,True,Computer Science,Mathematics,144570279,F. Bach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
403a730841c1e8e9e8062df22ff8f43537afd6ee,https://www.semanticscholar.org/paper/403a730841c1e8e9e8062df22ff8f43537afd6ee,Feature Selection for Machine Learning: Comparing a Correlation-Based Filter Approach to the Wrapper,"Feature selection is often an essential data processing step prior to applying a learning algorithm. The removal of irrelevant and redundant information often improves the performance of machine learning algorithms. There are two common approaches: a wrapper uses the intended learning algorithm itself to evaluate the usefulness of features, while a fllter evaluates features according to heuristics based on general characteristics of the data. The wrapper approach is generally considered to produce better feature subsets but runs much more slowly than a fllter. This paper describes a new fllter approach to feature selection that uses a correlation based heuristic to evaluate the worth of feature subsets When applied as a data preprocessing step for two common machine learning algorithms, the new method compares favourably with the wrapper but requires much less computation.",1999.0,17,588,56,False,Computer Science,,118860642,M. Hall,2226870.0,L. A. Smith,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fb829c5e6b406bb325fa5a02e05073df12b1772b,https://www.semanticscholar.org/paper/fb829c5e6b406bb325fa5a02e05073df12b1772b,Classes of Kernels for Machine Learning: A Statistics Perspective,"In this paper, we present classes of kernels for machine learning from a statistics perspective. Indeed, kernels are positive definite functions and thus also covariances. After discussing key properties of kernels, as well as a new formula to construct kernels, we present several important classes of kernels: anisotropic stationary kernels, isotropic stationary kernels, compactly supported kernels, locally stationary kernels, nonstationary kernels, and separable nonstationary kernels. Compactly supported kernels and separable nonstationary kernels are of prime interest because they provide a computational reduction for kernel-based methods. We describe the spectral representation of the various classes of kernels and conclude with a discussion on the characterization of nonlinear maps that reduce nonstationary kernels to either stationarity or local stationarity.",2002.0,41,629,32,False,Mathematics,Computer Science,1877479,M. Genton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29cedf9f9edd5179aa2e78654a7b81c9da45f0d0,https://www.semanticscholar.org/paper/29cedf9f9edd5179aa2e78654a7b81c9da45f0d0,Learning and optimization using the clonal selection principle,"The clonal selection principle is used to explain the basic features of an adaptive immune response to an antigenic stimulus. It establishes the idea that only those cells that recognize the antigens (Ag's) are selected to proliferate. The selected cells are subject to an affinity maturation process, which improves their affinity to the selective Ag's. This paper proposes a computational implementation of the clonal selection principle that explicitly takes into account the affinity maturation of the immune response. The general algorithm, named CLONALG, is derived primarily to perform machine learning and pattern recognition tasks, and then it is adapted to solve optimization problems, emphasizing multimodal and combinatorial optimization. Two versions of the algorithm are derived, their computational cost per iteration is presented, and a sensitivity analysis in relation to the user-defined parameters is given. CLONALG is also contrasted with evolutionary algorithms. Several benchmark problems are considered to evaluate the performance of CLONALG and it is also compared to a niching method for multimodal function optimization.",2002.0,61,2279,208,False,Computer Science,,145390561,L. Castro,7359553.0,F. V. Zuben,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d91ea30f4f9de817b29bb4ece00f43cb971822b4,https://www.semanticscholar.org/paper/d91ea30f4f9de817b29bb4ece00f43cb971822b4,Machine Learning Benchmarks and Random Forest Regression,"Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called ‘random forests’. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.",2004.0,14,452,19,False,Computer Science,,3013332,M. Segal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
49ca42e19bd1c647d099f9accd44e31e2e8e0b5a,https://www.semanticscholar.org/paper/49ca42e19bd1c647d099f9accd44e31e2e8e0b5a,Machine-Learning Research,"Machine-learning research has been making great progress in many directions. This article summarizes four of these directions and discusses some current open problems. The four directions are (1) the improvement of classification accuracy by learning ensembles of classifiers, (2) methods for scaling up supervised learning algorithms, (3) reinforcement learning, and (4) the learning of complex stochastic models.",1997.0,89,733,13,False,Computer Science,,144299726,Thomas G. Dietterich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f5e04e45416dbfb66c3e132570f897d67f13d474,https://www.semanticscholar.org/paper/f5e04e45416dbfb66c3e132570f897d67f13d474,Using GPUs for machine learning algorithms,"Using dedicated hardware to do machine learning typically ends up in disaster because of cost, obsolescence, and poor software. The popularization of graphic processing units (GPUs), which are now available on every PC, provides an attractive alternative. We propose a generic 2-layer fully connected neural network GPU implementation which yields over 3/spl times/ speedup for both training and testing with respect to a 3 GHz P4 CPU.",2005.0,12,218,12,False,Computer Science,,3265929,Dave Steinkrau,2812486.0,P. Simard,144437216.0,I. Buck,,,,,,,,,,,,,,,,,,,,,,,,,,,
8b8570a9892c487ac05dc196e2bfe23faa34d99e,https://www.semanticscholar.org/paper/8b8570a9892c487ac05dc196e2bfe23faa34d99e,Interactive machine learning,"Perceptual user interfaces (PUIs) are an important part of ubiquitous computing. Creating such interfaces is difficult because of the image and signal processing knowledge required for creating classifiers. We propose an interactive machine-learning (IML) model that allows users to train, classify/view and correct the classifications. The concept and implementation details of IML are discussed and contrasted with classical machine learning models. Evaluations of two algorithms are also presented. We also briefly describe Image Processing with Crayons (Crayons), which is a tool for creating new camera-based interfaces using a simple painting metaphor. The Crayons tool embodies our notions of interactive machine learning",2003.0,33,542,34,False,Computer Science,,3141061,Jerry Alan Fails,1733794.0,D. Olsen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8df9c71f09eb0dabf5adf17bee0f6b36190b52b2,https://www.semanticscholar.org/paper/8df9c71f09eb0dabf5adf17bee0f6b36190b52b2,Representational Learning with Extreme Learning Machine for Big Data Liyanaarachchi,"Restricted Boltzmann Machines (RBM) and auto encoders, learns to represent features in a dataset meaningfully and used as the basic building blocks to create deep networks. This paper introduces Extreme Learning Machine based Auto Encoder (ELM-AE), which learns feature representations using singular values and is used as the basic building block for Multi Layer Extreme Learning Machine (ML-ELM). ML-ELM performance is better than auto encoders based deep networks and Deep Belief Networks (DBN), while in par with Deep Boltzmann Machines (DBM) for MNIST dataset. However MLELM is significantly faster than any state−of−the−art deep networks.",,10,254,32,False,,,3276764,L. C. Kasun,2986982.0,Hongming Zhou,145678691.0,G. Huang,1807914.0,C. Vong,,,,,,,,,,,,,,,,,,,,,,,,,
22d6d9c1b7ac2738b51d93be45ac8f753f81867c,https://www.semanticscholar.org/paper/22d6d9c1b7ac2738b51d93be45ac8f753f81867c,Stacked Autoencoders for Unsupervised Feature Learning and Multiple Organ Detection in a Pilot Study Using 4D Patient Data,"Medical image analysis remains a challenging application area for artificial intelligence. When applying machine learning, obtaining ground-truth labels for supervised learning is more difficult than in many more common applications of machine learning. This is especially so for datasets with abnormalities, as tissue types and the shapes of the organs in these datasets differ widely. However, organ detection in such an abnormal dataset may have many promising potential real-world applications, such as automatic diagnosis, automated radiotherapy planning, and medical image retrieval, where new multimodal medical images provide more information about the imaged tissues for diagnosis. Here, we test the application of deep learning methods to organ identification in magnetic resonance medical images, with visual and temporal hierarchical features learned to categorize object classes from an unlabeled multimodal DCE-MRI dataset so that only a weakly supervised training is required for a classifier. A probabilistic patch-based method was employed for multiple organ detection, with the features learned from the deep learning model. This shows the potential of the deep learning model for application to medical images, despite the difficulty of obtaining libraries of correctly labeled training datasets and despite the intrinsic abnormalities present in patient datasets.",2013.0,63,453,20,False,Computer Science,Medicine,1797022,Hoo-Chang Shin,145945500.0,M. Orton,50070934.0,D. Collins,97784505.0,S. Doran,144544800.0,M. Leach,,,,,,,,,,,,,,,,,,,,,,,
9874b4cfd9e8ef89fd0b753af18c14cbc7c42744,https://www.semanticscholar.org/paper/9874b4cfd9e8ef89fd0b753af18c14cbc7c42744,What do you mean by collaborative learning,"This book arises from a series of workshops on collaborative learning, that gathered together 20 scholars from the disciplines of psychology, education and computer science. The series was part of a research program entitled 'Learning in Humans and Machines' (LHM), launched by Peter Reimann and Hans Spada, and funded by the European Science Foundation. This program aimed to develop a multidisciplinary dialogue on learning, involving mainly scholars from cognitive psychology, educational science, and artificial intelligence (including machine learning). During the preparation of the program, Agnes Blaye, Claire O'Malley, Michael Baker and I developed a theme on collaborative learning. When the program officially began, 12 members were selected to work on this theme and formed the so-called 'task force 5'. I became the coordinator of the group. This group organised two workshops, in Sitges (Spain, 1994) and Aix-en-Provence (France, 1995). In 1996, the group was enriched with new members to reach its final size. Around 20 members met in the subsequent workshops, at Samoens (France, 1996), Houthalen (Belgium, 1996) and Mannheim (Germany, 1997). Several individuals joined the group for some time but have not written a chapter. I would nevertheless like to acknowledge their contributions to our activities: George Bilchev, Stevan Harnad, Calle Jansson and Claire O'Malley.",1999.0,50,2358,204,False,,,1799133,P. Dillenbourg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1f7ed2ebc6b641e3804cf177fd42a1b8de95003b,https://www.semanticscholar.org/paper/1f7ed2ebc6b641e3804cf177fd42a1b8de95003b,Machine-learning techniques and their applications in manufacturing,"Abstract Machine learning is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Manufacturing is an area where the application of machine learning can be very fruitful. However, little has been published about the use of machine-learning techniques in the manufacturing domain. This paper evaluates several machine-learning techniques and examines applications in which they have been successfully deployed. Special attention is given to inductive learning, which is among the most mature of the machine-learning approaches currently available. Current trends and recent developments in machine-learning research are also discussed. The paper concludes with a summary of some of the key research issues in machine learning.",2005.0,157,97,3,False,Engineering,,2082214471,D. T. Pham,2002947.0,A. Afify,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2ff6fcebe5561433a2d2abeb8b30b2fbf3c0e303,https://www.semanticscholar.org/paper/2ff6fcebe5561433a2d2abeb8b30b2fbf3c0e303,Introduction to Semi-Supervised Learning,"Semi-supervised learning is a learning paradigm concerned with the study of how computers and natural systems such as humans learn in the presence of both labeled and unlabeled data. Traditionally, learning has been studied either in the unsupervised paradigm (e.g., clustering, outlier detection) where all the data is unlabeled, or in the supervised paradigm (e.g., classification, regression) where all the data is labeled.The goal of semi-supervised learning is to understand how combining labeled and unlabeled data may change the learning behavior, and design algorithms that take advantage of such a combination. Semi-supervised learning is of great interest in machine learning and data mining because it can use readily available unlabeled data to improve supervised learning tasks when the labeled data is scarce or expensive. Semi-supervised learning also shows potential as a quantitative tool to understand human category learning, where most of the input is self-evidently unlabeled. In this introductory book, we present some popular semi-supervised learning models, including self-training, mixture models, co-training and multiview learning, graph-based methods, and semi-supervised support vector machines. For each model, we discuss its basic mathematical formulation. The success of semi-supervised learning depends critically on some underlying assumptions. We emphasize the assumptions made by each model and give counterexamples when appropriate to demonstrate the limitations of the different models. In addition, we discuss semi-supervised learning for cognitive psychology. Finally, we give a computational learning theoretic perspective on semi-supervised learning, and we conclude the book with a brief discussion of open questions in the field.",2009.0,195,1849,109,True,Computer Science,,1832364,Xiaojin Zhu,2417893.0,A. Goldberg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6,https://www.semanticscholar.org/paper/ae6fdc00ec8c2299f101ddd428bfd82a0b55bac6,Practical feature subset selection for machine learning,"Machine learning algorithms automatically extract knowledge from machine readable information. Unfortunately, their success is usually dependant on the quality of the data that they operate on. If the data is inadequate, or contains extraneous and irrelevant information, machine learning algorithms may produce less accurate and less understandable results, or may fail to discover anything of use at all. Feature subset selectors are algorithms that attempt to identify and remove as much irrelevant and redundant information as possible prior to learning. Feature subset selection can result in enhanced performance, a reduced hypothesis search space, and, in some cases, reduced storage requirement. This paper describes a new feature selection algorithm that uses a correlation based heuristic to determine the “goodness” of feature subsets, and evaluates its effectiveness with three common machine learning algorithms. Experiments using a number of standard machine learning data sets are presented. Feature subset selection gave significant improvement for all three algorithms.",1998.0,16,538,41,False,Computer Science,,118860642,M. Hall,2226870.0,L. A. Smith,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07abd02f02774d178f26ca99937e5f94001a9ec9,https://www.semanticscholar.org/paper/07abd02f02774d178f26ca99937e5f94001a9ec9,Learning to link with wikipedia,"This paper describes how to automatically cross-reference documents with Wikipedia: the largest knowledge base ever known. It explains how machine learning can be used to identify significant terms within unstructured text, and enrich it with links to the appropriate Wikipedia articles. The resulting link detector and disambiguator performs very well, with recall and precision of almost 75%. This performance is constant whether the system is evaluated on Wikipedia articles or ""real world"" documents.
 This work has implications far beyond enriching documents with explanatory links. It can provide structured knowledge about any unstructured fragment of text. Any task that is currently addressed with bags of words - indexing, clustering, retrieval, and summarization to name a few - could use the techniques described here to draw on a vast network of concepts and semantics.",2008.0,21,1339,237,True,Computer Science,,1972431,David N. Milne,9419406.0,I. Witten,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e4de0f69cd867dbcae88211ac05318be17615a66,https://www.semanticscholar.org/paper/e4de0f69cd867dbcae88211ac05318be17615a66,Regularized Extreme Learning Machine,"Extreme Learning Machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called Regularized Extreme Learning Machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.",2009.0,15,356,34,False,Computer Science,,2606144,W. Deng,2817677.0,Qinghua Zheng,2118537230.0,Lin Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,
554894f70b28dba58b396c2d84080ac01051261b,https://www.semanticscholar.org/paper/554894f70b28dba58b396c2d84080ac01051261b,Gaussian Processes For Machine Learning,"Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other ""kernel machines"" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.",2004.0,90,404,14,True,Medicine,Mathematics,1680574,M. Seeger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Computer Science
bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da,https://www.semanticscholar.org/paper/bbfad4e88fd8bfbbd77ba53a56fe2886ecc147da,Applications of machine learning and rule induction,"Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domain-specific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide increasing levels of automation in the knowledge engineering process, replacing much time-consuming human activity with automatic techniques that improve accuracy or efficiency by discovering and exploiting regularities in training data. The ultimate test of machine learning is its ability to produce systems that are used regularly in industry, education, and elsewhere.",1995.0,29,559,21,False,Computer Science,,1713919,P. Langley,94059053.0,H. Simon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
467568f1777bc51a15a5100516cd4fe8de62b9ab,https://www.semanticscholar.org/paper/467568f1777bc51a15a5100516cd4fe8de62b9ab,Transfer Learning for Reinforcement Learning Domains: A Survey,"The reinforcement learning paradigm is a popular way to address problems that have only limited environmental feedback, rather than correctly labeled examples, as is common in other machine learning contexts. While significant progress has been made to improve learning in a single task, the idea of transfer learning has only recently been applied to reinforcement learning tasks. The core idea of transfer is that experience gained in learning to perform one task can help improve learning performance in a related, but different, task. In this article we present a framework that classifies transfer learning methods in terms of their capabilities and goals, and then use it to survey the existing literature, as well as to suggest future directions for transfer learning work.",2009.0,135,1584,107,False,Computer Science,,39286677,Matthew E. Taylor,144848112.0,P. Stone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724,https://www.semanticscholar.org/paper/7d8b40eb7f3eb0e03c35f066c97a2040f2f8b724,Machine Learning Approaches to Estimating Software Development Effort,"Accurate estimation of software development effort is critical in software engineering. Underestimates lead to time pressures that may compromise full functional development and thorough testing of software. In contrast, overestimates can result in noncompetitive contract bids and/or over allocation of development resources and personnel. As a result, many models for estimating software development effort have been proposed. This article describes two methods of machine learning, which we use to build estimators of software development effort from historical data. Our experiments indicate that these techniques are competitive with traditional estimators on one dataset, but also illustrate that these methods are sensitive to the data on which they are trained. This cautionary note applies to any model-construction strategy that relies on historical data. All such models for software effort estimation should be evaluated by exploring model sensitivity on a variety of historical data. >",1995.0,19,541,24,False,Computer Science,,38402081,K. Srinivasan,8993101.0,D. Fisher,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ee8a371fc5adc5469435020a52fb815f3b57a71,https://www.semanticscholar.org/paper/5ee8a371fc5adc5469435020a52fb815f3b57a71,Semi-Supervised Learning,"In the field of machine learning, semi-supervised learning (SSL) occupies the middle ground, between supervised learning (in which all training examples are labeled) and unsupervised learning (in which no label data are given). Interest in SSL has increased in recent years, particularly because of application domains in which unlabeled data are plentiful, such as images, text, and bioinformatics. This first comprehensive overview of SSL presents state-of-the-art algorithms, a taxonomy of the field, selected applications, benchmark experiments, and perspectives on ongoing and future research. Semi-Supervised Learning first presents the key assumptions and ideas underlying the field: smoothness, cluster or low-density separation, manifold structure, and transduction. The core of the book is the presentation of SSL methods, organized according to algorithmic strategies. After an examination of generative models, the book describes algorithms that implement the low-density separation assumption, graph-based methods, and algorithms that perform two-step learning. The book then discusses SSL applications and offers guidelines for SSL practitioners by analyzing the results of extensive benchmark experiments. Finally, the book looks at interesting directions for SSL research. The book closes with a discussion of the relationship between semi-supervised learning and transduction. Adaptive Computation and Machine Learning series",2006.0,474,2537,67,True,Computer Science,,1730609,O. Chapelle,66191239.0,Bernhard Schlkopf,2281542.0,A. Zien,,,,,,,,,,,,,,,,,,,,,,,,,,,
b8eb7da56dae58f77788c33a57b5b810ca930527,https://www.semanticscholar.org/paper/b8eb7da56dae58f77788c33a57b5b810ca930527,The Geometry of ROC Space: Understanding Machine Learning Metrics through ROC Isometrics,"Many different metrics are used in machine learning and data mining to build and evaluate models. However, there is no general theory of machine learning metrics, that could answer questions such as: When we simultaneously want to optimise two criteria, how can or should they be traded off? Some metrics are inherently independent of class and misclassification cost distributions, while other are not -- can this be made more precise? This paper provides a derivation of ROC space from first principles through 3D ROC space and the skew ratio, and redefines metrics in these dimensions. The paper demonstrates that the graphical depiction of machine learning metrics by means of ROC isometrics gives many useful insights into the characteristics of these metrics, and provides a foundation on which a theory of machine learning metrics can be built.",2003.0,17,298,45,False,Computer Science,,47840704,Peter A. Flach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
675b93244f917d5dc2c79b89d2936d81d077e663,https://www.semanticscholar.org/paper/675b93244f917d5dc2c79b89d2936d81d077e663,Using Machine Learning to Break Visual Human Interaction Proofs (HIPs),"Machine learning is often used to automatically solve human tasks. In this paper, we look for tasks where machine learning algorithms are not as good as humans with the hope of gaining insight into their current limitations. We studied various Human Interactive Proofs (HIPs) on the market, because they are systems designed to tell computers and humans apart by posing challenges presumably too hard for computers. We found that most HIPs are pure recognition tasks which can easily be broken using machine learning. The harder HIPs use a combination of segmentation and recognition tasks. From this observation, we found that building segmentation tasks is the most effective way to confuse machine learning algorithms. This has enabled us to build effective HIPs (which we deployed in MSN Passport), as well as design challenging segmentation tasks for machine learning algorithms.",2004.0,9,284,22,False,Computer Science,,1809791,K. Chellapilla,2812486.0,P. Simard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
50ec005395794592f6c977f6d273635ef563c241,https://www.semanticscholar.org/paper/50ec005395794592f6c977f6d273635ef563c241,Collaborative Filtering: A Machine Learning Perspective,"Collaborative Filtering: A Machine Learning Perspective Benjamin Marlin Master of Science Graduate Department of Computer Science University of Toronto 2004 Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modifications of one or more standard machine learning methods for classification, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.",2004.0,59,211,32,False,Computer Science,,1805742,Benjamin M Marlin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ca799d07d8508df7949e7bea1c353d2a987a2417,https://www.semanticscholar.org/paper/ca799d07d8508df7949e7bea1c353d2a987a2417,Machine Learning for Fast Quadrupedal Locomotion,"For a robot, the ability to get from one place to another is one of the most basic skills. However, locomotion on legged robots is a challenging multidimensional control problem. This paper presents a machine learning approach to legged locomotion, with all training done on the physical robots. The main contributions are a specification of our fully automated learning environment and a detailed empirical comparison of four different machine learning algorithms for learning quadrupedal locomotion. The resulting learned walk is considerably faster than all previously reported hand-coded walks for the same robot platform.",2004.0,40,194,18,False,Computer Science,,2686687,Nate Kohl,144848112.0,P. Stone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58e03fdbc096684dd3185ec538e082dc6cc6a464,https://www.semanticscholar.org/paper/58e03fdbc096684dd3185ec538e082dc6cc6a464,Experiments with random projections for machine learning,"Dimensionality reduction via Random Projections has attracted considerable attention in recent years. The approach has interesting theoretical underpinnings and offers computational advantages. In this paper we report a number of experiments to evaluate Random Projections in the context of inductive supervised learning. In particular, we compare Random Projections and PCA on a number of different datasets and using different machine learning methods. While we find that the random projection approach predictively underperforms PCA, its computational advantages may make it attractive for certain applications.",2003.0,19,250,16,False,Mathematics,Computer Science,1761767,Dmitriy Fradkin,1710305.0,D. Madigan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c21ce2fd906ef6b5c669d55458bd956155eda14f,https://www.semanticscholar.org/paper/c21ce2fd906ef6b5c669d55458bd956155eda14f,Machine Learning and Data Mining; Methods and Applications,"From the Publisher: Master the new computational tools to get the most out of your information system. This practical guide, the first to clearly outline the situation for the benefit of engineers and scientists, provides a straightforward introduction to basic machine learning and data mining methods, covering the analysis of numerical, text, and sound data.",1998.0,5,395,11,False,Computer Science,,2421006,R. Michalski,1725040.0,I. Bratko,66327168.0,Avan Bratko,,,,,,,,,,,,,,,,,,,,,,,,,,,
7b7222ac076d211d7fcae7d012bebcc4ea71e952,https://www.semanticscholar.org/paper/7b7222ac076d211d7fcae7d012bebcc4ea71e952,"An Empirical Comparison of Pattern Recognition, Neural Nets, and Machine Learning Classification Methods","Classification methods from statistical pattern recognition, neural nets, and machine learning were applied to four real-world data sets. Each of these data sets has been previously analyzed and reported in the statistical, medical, or machine learning literature. The data sets are characterized by statisucal uncertainty; there is no completely accurate solution to these problems. Training and testing or resampling techniques are used to estimate the true error rates of the classification methods. Detailed attention is given to the analysis of performance of the neural nets using back propagation. For these problems, which have relatively few hypotheses and features, the machine learning procedures for rule induction or tree induction clearly performed best.",1989.0,40,531,16,False,Computer Science,,145700185,S. Weiss,2092055.0,I. Kapouleas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c,https://www.semanticscholar.org/paper/5dcb588150d84ef6d1b1ed6ca96e2fd62399de2c,Readings in Machine Learning,"From the Publisher: The ability to learn is a fundamental characteristic of intelligent behavior. Consequently, machine learning has been a focus of artificial intelligence since the beginnings of AI in the 1950s. The 1980s saw tremendous growth in the field, and this growth promises to continue with valuable contributions to science, engineering, and business. Readings in Machine Learning collects the best of the published machine learning literature, including papers that address a wide range of learning tasks, and that introduce a variety of techniques for giving machines the ability to learn. The editors, in cooperation with a group of expert referees, have chosen important papers that empirically study, theoretically analyze, or psychologically justify machine learning algorithms. The papers are grouped into a dozen categories, each of which is introduced by the editors.",1991.0,0,378,8,False,Computer Science,,1734317,J. Shavlik,67136395.0,T. Deitterich,144299726.0,Thomas G. Dietterich,,,,,,,,,,,,,,,,,,,,,,,,,,,
50a869bcd6d45ec7fdb317877c3d2a047c2cfc38,https://www.semanticscholar.org/paper/50a869bcd6d45ec7fdb317877c3d2a047c2cfc38,Overfitting and undercomputing in machine learning,"A central problem in machine learning is supervised learning—that is, learning from labeled training data. For example, a learning system for medical diagnosis might be trained with examples of patients whose case records (medical tests, clinical observations) and diagnoses were known. The task of the learning system is to infer a function that predicts the diagnosis of a patient from his or her case records. The function to be learned might be represented as a set of rules, a decision tree, a Bayes network, or a neural network. Learning algorithms essentially operate by searching some space of functions (usually called the hypothesis class) for a function that fits the given data. Because there are usually exponentially many functions, this search cannot actually examine individual hypothesis functions but instead must use some more direct method of constructing the hypothesis functions from the data. This search can usually be formalized by defining an objective function (e.g., number of data points predicted incorrectly) and applying various algorithms to find a function that minimizes this objective function is NP-hard. For example, fitting the weights of a neural network or finding the smallest decision tree are both NP-complete problems [Blum and Rivest, 1989; Quinlan and Rivest 1989]. Hence, heuristic algorithms such as gradient descent (for neural networks) and greedy search (for decision trees) have been applied with great success. Of course, the suboptimality of such heuristic algorithms ~mmediately suggests a reas&able line of research: find ~lgorithms that can search the hypothesis class better. Hence, there has been extensive research in applying secondorder methods to fit neural networks and in conducting much more thorough searches in learning decision trees and rule sets. Ironically, when these algorithms were tested on real datasets, it was found that their performance was often worse than simrde szradient descent or greedy sear~h [&inlan and Cameron-Jones 1995; Weigend 1994]. In short: it appears to be bet~er not to optimize! One of the other important trends in machine-learning research has been the establishment and nurturing of connections between various previously disparate fields, including computational learning theory, connectionist learning, symbolic learning. and statistics. The . connection to statistics was crucial in resolvins$ this naradox. The-key p~oblem arises from the structure of the machine-learning task, A learning algorithm is trained on a set of training data, but then it is applied to make predictions on new data points. The goal is to maximize its predictive accuracy on the new data points—not necessarily its accuracy on the trammg data. Indeed, if we work too hard to find the very best fit to the training data, there is a risk that we will fit the noise in the data by memorizing various peculiarities",1995.0,8,465,11,False,Computer Science,,144299726,Thomas G. Dietterich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dd7cee21074ea6b346011d7463f7387ad9bfcc2a,https://www.semanticscholar.org/paper/dd7cee21074ea6b346011d7463f7387ad9bfcc2a,Information Extraction from HTML: Application of a General Machine Learning Approach,"Because the World Wide Web consists primarily of text, information extraction is central to any effort that would use the Web as a resource for knowledge discovery. We show how information extraction can be cast as a standard machine learning problem, and argue for the suitability of relational learning in solving it. The implementation of a general-purpose relational learner for information extraction, SRV, is described. In contrast with earlier learning systems for information extraction, SRV makes no assumptions about document structure and the kinds of information available for use in learning extraction patterns. Instead, structural and other information is supplied as input in the form of an extensible token-oriented feature set. We demonstrate the effectiveness of this approach by adapting SRV for use in learning extraction rules for a domain consisting of university course and research project pages sampled from the Web. Making SRV Web-ready only involves adding several simple HTML-specific features to its basic feature set.",1998.0,16,318,17,False,Computer Science,,1758106,D. Freitag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c6efa1ce0177d6990ad6580013e04b281c4a5295,https://www.semanticscholar.org/paper/c6efa1ce0177d6990ad6580013e04b281c4a5295,Machine learning for science: state of the art and future prospects.,"Recent advances in machine learning methods, along with successful applications across a wide variety of fields such as planetary science and bioinformatics, promise powerful new tools for practicing scientists. This viewpoint highlights some useful characteristics of modern machine learning methods and their relevance to scientific applications. We conclude with some speculations on near-term progress and promising directions.",2001.0,39,278,4,True,Engineering,Medicine,1705072,E. Mjolsness,1703049.0,D. DeCoste,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
184208a99018ce2f6c177b41d8786f8cda518a94,https://www.semanticscholar.org/paper/184208a99018ce2f6c177b41d8786f8cda518a94,Support vector machine learning for image retrieval,"A novel method of relevance feedback is presented based on support vector machine learning in the content-based image retrieval system. A SVM classifier can be learned from training data of relevance images and irrelevance images marked by users. Using the classifier, the system can retrieve more images relevant to the query in the database efficiently. Experiments were carried out on a large-size database of 9918 images. It shows that the interactive learning and retrieval process can find correct images increasingly. It also shows the generalization ability of SVM under the condition of limited training samples.",2001.0,6,301,28,False,Computer Science,,39089563,Lei Zhang,38222706.0,Fuzong Lin,49846744.0,Bo Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,
3efcb97c1de1c87832a7a1d99e91801992a938ec,https://www.semanticscholar.org/paper/3efcb97c1de1c87832a7a1d99e91801992a938ec,Crafting Papers on Machine Learning,"This essay gives advice to authors of papers on machine learning, although much of it carries over to other computational disciplines. The issues covered include the material that should appear in a well-balanced paper, factors that arise in diierent approaches to evaluation , and ways to improve a submission's ability to communicate ideas to its readers.",2000.0,9,252,2,False,Computer Science,,1713919,P. Langley,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07dc576e87368680e1cfafe9336853979168c2bd,https://www.semanticscholar.org/paper/07dc576e87368680e1cfafe9336853979168c2bd,An application of machine learning to network intrusion detection,"Differentiating anomalous network activity from normal network traffic is difficult and tedious. A human analyst must search through vast amounts of data to find anomalous sequences of network connections. To support the analyst's job, we built an application which enhances domain knowledge with machine learning techniques to create rules for an intrusion detection expert system. We employ genetic algorithms and decision trees to automatically generate rules for classifying network connections. This paper describes the machine learning methodology and the applications employing this methodology.",1999.0,10,280,9,True,Computer Science,,2068223893,Chris Sinclair,48817490.0,L. Pierce,39570882.0,S. Matzner,,,,,,,,,,,,,,,,,,,,,,,,,,,
14f2b886678251cdd80dc9701c889bc55de7940d,https://www.semanticscholar.org/paper/14f2b886678251cdd80dc9701c889bc55de7940d,Student Modeling and Machine Learning,"After identifying essential student modeling issues and machine learning approaches, this paper examines how machine learning techniques have been used to automate the construction of student models as well as the background knowledge necessary for student modeling. In the process, the paper sheds light on the difficulty, suitability and potential of using machine learning for student modeling processes, and, to a lesser extent, the potential of using student modeling techniques in machine learning. (http://aied.inf.ed.ac.uk/members98/archive/vol_9/sison/full.html)",1998.0,95,171,7,False,Computer Science,,1693141,Raymund C. Sison,1845202.0,M. Shimura,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
635cf1572f0194d900c11b6b34e41e3b85616863,https://www.semanticscholar.org/paper/635cf1572f0194d900c11b6b34e41e3b85616863,MLC++: a machine learning library in C++,"We present MLC++, a library of C++ classes and tools for supervised machine learning. While MLC++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC++ is can attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC++ aims to solve, the design of MLC++, and the current functionality.<<ETX>>",1994.0,37,224,9,False,Computer Science,,1726733,Ron Kohavi,34929449.0,George H. John,2057352235.0,Richard Long,2061867819.0,David Manley,30079006.0,Karl Pfleger,,,,,,,,,,,,,,,,,,,,,,,
451f86130daf5632b87cd4c0f417b245c6ebb582,https://www.semanticscholar.org/paper/451f86130daf5632b87cd4c0f417b245c6ebb582,Does Machine Learning Really Work?,"Does machine learning really work? Yes. Over the past decade, machine learning has evolved from a field of laboratory demonstrations to a field of significant commercial value. Machine-learning algorithms have now learned to detect credit card fraud by mining data on past transactions, learned to steer vehicles driving autonomously on public highways at 70 miles an hour, and learned the reading interests of many individuals to assemble personally customized electronic newsAbstracts. A new computational theory of learning is beginning to shed light on fundamental issues, such as the trade-off among the number of training examples available, the number of hypotheses considered, and the likely accuracy of the learned hypothesis. Newer research is beginning to explore issues such as long-term learning of new representations, the integration of Bayesian inference and induction, and life-long cumulative learning. This article, based on the keynote talk presented at the Thirteenth National Conference on Artificial Intelligence, samples a number of recent accomplishments in machine learning and looks at where the field might be headed. [Copyright restrictions preclude electronic publication of this article.]",1997.0,19,172,4,False,Computer Science,,40975594,Tom Michael Mitchell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
