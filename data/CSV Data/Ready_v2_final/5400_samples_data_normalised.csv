paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,fieldsOfStudy/1,fieldsOfStudy/2,authors/4/authorId,authors/4/name,authors/5/authorId,authors/5/name,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,fieldsOfStudy/3,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,authors/16/authorId,authors/16/name,authors/17/authorId,authors/17/name
93c56e3c2a95458273b2943d89db1280d066209a,https://www.semanticscholar.org/paper/93c56e3c2a95458273b2943d89db1280d066209a,Automatic Detection of Off-Task Behaviors in Intelligent Tutoring Systems with Machine Learning Techniques,"Identifying off-task behaviors in intelligent tutoring systems is a practical and challenging research topic. This paper proposes a machine learning model that can automatically detect students' off-task behaviors. The proposed model only utilizes the data available from the log files that record students' actions within the system. The model utilizes a set of time features, performance features, and mouse movement features, and is compared to 1) a model that only utilizes time features and 2) a model that uses time and performance features. Different students have different types of behaviors; therefore, personalized version of the proposed model is constructed and compared to the corresponding nonpersonalized version. In order to address data sparseness problem, a robust Ridge Regression algorithm is utilized to estimate model parameters. An extensive set of experiment results demonstrates the power of using multiple types of evidence, the personalized model, and the robust Ridge Regression algorithm.",2010,25,78,2,False,Computer Science,1778765,Suleyman Cetintas,145388187.0,Luo Si,2820030.0,Y. Xin,40374171.0,Casey Hord,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d1b53e3a977dffe016d88387e59e48d6dbdedcc6,https://www.semanticscholar.org/paper/d1b53e3a977dffe016d88387e59e48d6dbdedcc6,In-Memory Computation of a Machine-Learning Classifier in a Standard 6T SRAM Array,"This paper presents a machine-learning classifier where computations are performed in a standard 6T SRAM array, which stores the machine-learning model. Peripheral circuits implement mixed-signal weak classifiers via columns of the SRAM, and a training algorithm enables a strong classifier through boosting and also overcomes circuit nonidealities, by combining multiple columns. A prototype 128 <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> 128 SRAM array, implemented in a 130-nm CMOS process, demonstrates ten-way classification of MNIST images (using image-pixel features downsampled from 28 <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> 28 = 784 to 9 <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> 9 = 81, which yields a baseline accuracy of 90%). In SRAM mode (bit-cell read/write), the prototype operates up to 300 MHz, and in classify mode, it operates at 50 MHz, generating a classification every cycle. With accuracy equivalent to a discrete SRAM/digital-MAC system, the system achieves ten-way classification at an energy of 630 pJ per decision, 113 times lower than a discrete system with standard training algorithm and 13 times lower than a discrete system with the proposed training algorithm.",2017,23,218,28,False,Computer Science,1519068895,Jintao Zhang,2108367712.0,Zhuo Wang,145020569.0,N. Verma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4af05d0b97513e19174b759c5039b115e7ff0d4c,https://www.semanticscholar.org/paper/4af05d0b97513e19174b759c5039b115e7ff0d4c,Bayesian Generative Active Deep Learning,"© 36th International Conference on Machine Learning, ICML 2019. All rights reserved. Deep learning models have demonstrated outstanding performance in several problems, but their training process tends to require immense amounts of computational and human resources for training and labeling, constraining the types of problems that can be tackled. Therefore, the design of effective training methods that require small labeled training sets is an important research direction that will allow a more effective use of resources. Among current approaches designed to address this issue, two are particularly interesting: data augmentation and active learning. Data augmentation achieves this goal by artificially generating new training points, while active learning relies on the selection of the ""most informative"" subset of unlabeled training samples to be labelled by an oracle. Although successful in practice, data augmentation can waste computational resources because it indiscriminately generates samples that are not guaranteed to be informative, and active learning selects a small subset of informative samples (from a large un-annotated set) that may be insufficient for the training process. In this paper, we propose a Bayesian generative active deep learning approach that combines active learning with data augmentation - we provide theoretical and empirical evidence (MNIST, CIFAR-{10,100}, and SVHN) that our approach has more efficient training and better classification results than data augmentation and active learning.",2019,37,62,9,False,Computer Science,143678789,Toan Tran,3354627.0,Thanh-Toan Do,145950884.0,I. Reid,145575177.0,G. Carneiro,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dfdb5068507f64bf448857680fd6773ef08f0f21,https://www.semanticscholar.org/paper/dfdb5068507f64bf448857680fd6773ef08f0f21,Feature Learning for Activity Recognition in Ubiquitous Computing,"Feature extraction for activity recognition in context-aware ubiquitous computing applications is usually a heuristic process, informed by underlying domain knowledge. Relying on such explicit knowledge is problematic when aiming to generalize across different application domains. We investigate the potential of recent machine learning methods for discovering universal features for context-aware applications of activity recognition. We also describe an alternative data representation based on the empirical cumulative distribution function of the raw data, which effectively abstracts from absolute values. Experiments on accelerometer data from four publicly available activity recognition datasets demonstrate the significant potential of our approach to address both contemporary activity recognition tasks and next generation problems such as skill assessment and the detection of novel activities.",2011,21,322,19,False,Computer Science,2191750,T. Plötz,3128867.0,N. Hammerla,145171812.0,P. Olivier,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b70bddfad1b0787be702b28125f929c7118d5513,https://www.semanticscholar.org/paper/b70bddfad1b0787be702b28125f929c7118d5513,Data Mining: Statistics and More?,"Abstract Data mining is a new discipline lying at the interface of statistics, database technology, pattern recognition, machine learning, and other areas. It is concerned with the secondary analysis of large databases in order to find previously unsuspected relationships which are of interest or value to the database owners. New problems arise, partly as a consequence of the sheer size of the data sets involved, and partly because of issues of pattern matching. However, since statistics provides the intellectual glue underlying the effort, it is important for statisticians to become involved. There are very real opportunities for statisticians to make significant contributions.",1998,23,380,19,False,Computer Science,1781982,D. Hand,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
25e44679b20098b14208c17cd0914edec7a8a1a7,https://www.semanticscholar.org/paper/25e44679b20098b14208c17cd0914edec7a8a1a7,"Stance Classification of Ideological Debates: Data, Models, Features, and Constraints","Determining the stance expressed in a post written for a two-sided debate in an online debate forum is a relatively new and challenging problem in opinion mining. We seek to gain a better understanding of how to improve machine learning approaches to stance classification of ideological debates, specifically by examining how the performance of a learning-based stance classification system varies with the amount and quality of the training data, the complexity of the underlying model, the richness of the feature set, as well as the application of extra-linguistic constraints.",2013,25,190,19,False,Computer Science,2064634,K. Hasan,145106110.0,Vincent Ng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bf3fd0a26f7d4c4037aa0fb7fa54e947f498bca6,https://www.semanticscholar.org/paper/bf3fd0a26f7d4c4037aa0fb7fa54e947f498bca6,Lowering the barrier to applying machine learning,"Machine learning algorithms are key components in many cutting edge applications of computation. However, the full potential of machine learning has not been realized because using machine learning is hard, even for otherwise tech-savvy developers. This is because developing with machine learning is different than normal programming. My thesis is that developers applying machine learning need new general-purpose tools that provide structure for common processes and common pipelines while remaining flexible to account for variability in problems. In this paper, I describe my efforts to understanding the difficulties that developers face when applying machine learning. I then describe Gestalt, a general-purpose integrated development environment designed the application of machine learning. Finally, I describe work on developing a pattern language for building machine learning systems and creating new techniques that help developers understand the interaction between their data and learning algorithms.",2010,122,29,3,True,Computer Science,39699737,Kayur Patel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63787b1fcf5add4b5e25376f7bee155a39e958c3,https://www.semanticscholar.org/paper/63787b1fcf5add4b5e25376f7bee155a39e958c3,A Survey of Multi-View Representation Learning,"Recently, multi-view representation learning has become a rapidly growing direction in machine learning and data mining areas. This paper introduces two categories for multi-view representation learning: multi-view representation alignment and multi-view representation fusion. Consequently, we first review the representative methods and theories of multi-view representation learning based on the perspective of alignment, such as correlation-based alignment. Representative examples are canonical correlation analysis (CCA) and its several extensions. Then, from the perspective of representation fusion, we investigate the advancement of multi-view representation learning that ranges from generative methods including multi-modal topic learning, multi-view sparse coding, and multi-view latent space Markov networks, to neural network-based methods including multi-modal autoencoders, multi-view convolutional neural networks, and multi-modal recurrent neural networks. Further, we also investigate several important applications of multi-view representation learning. Overall, this survey aims to provide an insightful overview of theoretical foundation and state-of-the-art developments in the field of multi-view representation learning and to help researchers find the most appropriate tools for particular applications.",2019,207,223,11,True,Mathematics,47001714,Yingming Li,2150427580.0,Ming Yang,2118748124.0,Zhongfei Zhang,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
17b07d8bb61a1462299053b66615f9eddd8f23d5,https://www.semanticscholar.org/paper/17b07d8bb61a1462299053b66615f9eddd8f23d5,Comprehensive Decision Tree Models in Bioinformatics,"Purpose Classification is an important and widely used machine learning technique in bioinformatics. Researchers and other end-users of machine learning software often prefer to work with comprehensible models where knowledge extraction and explanation of reasoning behind the classification model are possible. Methods This paper presents an extension to an existing machine learning environment and a study on visual tuning of decision tree classifiers. The motivation for this research comes from the need to build effective and easily interpretable decision tree models by so called one-button data mining approach where no parameter tuning is needed. To avoid bias in classification, no classification performance measure is used during the tuning of the model that is constrained exclusively by the dimensions of the produced decision tree. Results The proposed visual tuning of decision trees was evaluated on 40 datasets containing classical machine learning problems and 31 datasets from the field of bioinformatics. Although we did not expected significant differences in classification performance, the results demonstrate a significant increase of accuracy in less complex visually tuned decision trees. In contrast to classical machine learning benchmarking datasets, we observe higher accuracy gains in bioinformatics datasets. Additionally, a user study was carried out to confirm the assumption that the tree tuning times are significantly lower for the proposed method in comparison to manual tuning of the decision tree. Conclusions The empirical results demonstrate that by building simple models constrained by predefined visual boundaries, one not only achieves good comprehensibility, but also very good classification performance that does not differ from usually more complex models built using default settings of the classical decision tree algorithm. In addition, our study demonstrates the suitability of visually tuned decision trees for datasets with binary class attributes and a high number of possibly redundant attributes that are very common in bioinformatics.",2012,47,82,2,False,Biology,2971162,G. Štiglic,2592642.0,Simon Kocbek,1710947.0,Igor Pernek,1716474.0,P. Kokol,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
226575b9b189989965d44538c70b710b526d818f,https://www.semanticscholar.org/paper/226575b9b189989965d44538c70b710b526d818f,A Deep Learning Scheme for Motor Imagery Classification based on Restricted Boltzmann Machines,"Motor imagery classification is an important topic in brain–computer interface (BCI) research that enables the recognition of a subject’s intension to, e.g., implement prosthesis control. The brain dynamics of motor imagery are usually measured by electroencephalography (EEG) as nonstationary time series of low signal-to-noise ratio. Although a variety of methods have been previously developed to learn EEG signal features, the deep learning idea has rarely been explored to generate new representation of EEG features and achieve further performance improvement for motor imagery classification. In this study, a novel deep learning scheme based on restricted Boltzmann machine (RBM) is proposed. Specifically, frequency domain representations of EEG signals obtained via fast Fourier transform (FFT) and wavelet package decomposition (WPD) are obtained to train three RBMs. These RBMs are then stacked up with an extra output layer to form a four-layer neural network, which is named the frequential deep belief network (FDBN). The output layer employs the softmax regression to accomplish the classification task. Also, the conjugate gradient method and backpropagation are used to fine tune the FDBN. Extensive and systematic experiments have been performed on public benchmark datasets, and the results show that the performance improvement of FDBN over other selected state-of-the-art methods is statistically significant. Also, several findings that may be of significant interest to the BCI community are presented in this article.",2017,43,273,13,False,Computer Science,145022789,N. Lu,2115646199.0,Tengfei Li,143679596.0,Xiaodong Ren,47355902.0,Hongyu Miao,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
646d73e2cdcb628d4c1569e9766a1dd9748ef819,https://www.semanticscholar.org/paper/646d73e2cdcb628d4c1569e9766a1dd9748ef819,"""The human body is a black box"": supporting clinical decision-making with deep learning","Machine learning technologies are increasingly developed for use in healthcare. While research communities have focused on creating state-of-the-art models, there has been less focus on real world implementation and the associated challenges to fairness, transparency, and accountability that come from actual, situated use. Serious questions remain underexamined regarding how to ethically build models, interpret and explain model output, recognize and account for biases, and minimize disruptions to professional expertise and work cultures. We address this gap in the literature and provide a detailed case study covering the development, implementation, and evaluation of Sepsis Watch, a machine learning-driven tool that assists hospital clinicians in the early diagnosis and treatment of sepsis. Sepsis is a severe infection that can lead to organ failure or death if not treated in time and is the leading cause of inpatient deaths in US hospitals. We, the team that developed and evaluated the tool, discuss our conceptualization of the tool not as a model deployed in the world but instead as a socio-technical system requiring integration into existing social and professional contexts. Rather than focusing solely on model interpretability to ensure fair and accountable machine learning, we point toward four key values and practices that should be considered when developing machine learning to support clinical decision-making: rigorously define the problem in context, build relationships with stakeholders, respect professional discretion, and create ongoing feedback loops with stakeholders. Our work has significant implications for future research regarding mechanisms of institutional accountability and considerations for responsibly designing machine learning systems. Our work underscores the limits of model interpretability as a solution to ensure transparency, accuracy, and accountability in practice. Instead, our work demonstrates other means and goals to achieve FATML values in design and in practice.",2019,70,83,9,False,Psychology,47057856,M. Sendak,21817215.0,M. Elish,145866390.0,M. Gao,2585470.0,Joseph D. Futoma,Computer Science,,2076928049.0,W. Ratliff,48448064.0,M. Nichols,39743396.0,A. Bedoya,12308037.0,S. Balu,1409737692.0,Cara O'Brien,,,,,,,,,,,,,,,,,,,
ee0d116972a61613bf0552cbfef1b6c463514c89,https://www.semanticscholar.org/paper/ee0d116972a61613bf0552cbfef1b6c463514c89,Machine Learning-augmented Predictive Modeling of Turbulent Separated Flows over Airfoils,"A modeling paradigm is developed to augment predictive models of turbulence by effectively utilizing limited data generated from physical experiments. The key components of our approach involve inverse modeling to infer the spatial distribution of model discrepancies, and, machine learning to reconstruct discrepancy information from a large number of inverse problems into corrective model forms. We apply the methodology to turbulent flows over airfoils involving flow separation. Model augmentations are developed for the Spalart Allmaras (SA) model using adjoint-based full field inference on experimentally measured lift coefficient data. When these model forms are reconstructed using neural networks (NN) and embedded within a standard solver, we show that much improved predictions in lift can be obtained for geometries and flow conditions that were not used to train the model. The NN-augmented SA model also predicts surface pressures extremely well. Portability of this approach is demonstrated by confirming that predictive improvements are preserved when the augmentation is embedded in a different commercial finite-element solver. The broader vision is that by incorporating data that can reveal the form of the innate model discrepancy, the applicability of data-driven turbulence models can be extended to more general flows.",2016,73,228,4,True,Computer Science,2111007498,Anand Pratap Singh,3451561.0,S. Medida,1974037.0,K. Duraisamy,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
abf77aff13ccd0301a3e693f5d1247a924791bad,https://www.semanticscholar.org/paper/abf77aff13ccd0301a3e693f5d1247a924791bad,A Unified Gradient Regularization Family for Adversarial Examples,"Adversarial examples are augmented data points generated by imperceptible perturbation of input samples. They have recently drawn much attention with the machine learning and data mining community. Being difficult to distinguish from real examples, such adversarial examples could change the prediction of many of the best learning models including the state-of-the-art deep learning models. Recent attempts have been made to build robust models that take into account adversarial examples. However, these methods can either lead to performance drops or lack mathematical motivations. In this paper, we propose a unified framework to build robust machine learning models against adversarial examples. More specifically, using the unified framework, we develop a family of gradient regularization methods that effectively penalize the gradient of loss function w.r.t. inputs. Our proposed framework is appealing in that it offers a unified view to deal with adversarial examples. It incorporates another recently-proposed perturbation based approach as a special case. In addition, we present some visual effects that reveals semantic meaning in those perturbations, and thus support our regularization method and provide another explanation for generalizability of adversarial examples. By applying this technique to Maxout networks, we conduct a series of experiments and achieve encouraging results on two benchmark datasets. In particular, we attain the best accuracy on MNIST data (without data augmentation) and competitive performance on CIFAR-10 data.",2015,23,156,6,True,Computer Science,2753561,Chunchuan Lyu,5380819.0,Kaizhu Huang,39935739.0,Hai-Ning Liang,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2c04849a3802715d5a9d89179c9f161014d6c2a,https://www.semanticscholar.org/paper/e2c04849a3802715d5a9d89179c9f161014d6c2a,Modeling pixel means and covariances using factorized third-order boltzmann machines,"Learning a generative model of natural images is a useful way of extracting features that capture interesting regularities. Previous work on learning such models has focused on methods in which the latent features are used to determine the mean and variance of each pixel independently, or on methods in which the hidden units determine the covariance matrix of a zero-mean Gaussian distribution. In this work, we propose a probabilistic model that combines these two approaches into a single framework. We represent each image using one set of binary latent features that model the image-specific covariance and a separate set that model the mean. We show that this approach provides a probabilistic framework for the widely used simple-cell complex-cell architecture, it produces very realistic samples of natural images and it extracts features that yield state-of-the-art recognition accuracy on the challenging CIFAR 10 dataset.",2010,35,261,38,False,Mathematics,1706809,Marc'Aurelio Ranzato,1695689.0,Geoffrey E. Hinton,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2a9f505e1ab148aa3d91810f509ee133272be554,https://www.semanticscholar.org/paper/2a9f505e1ab148aa3d91810f509ee133272be554,Machine-Learning Techniques for Customer Retention: A Comparative Study,"Nowadays, customers have become more interested in the quality of service (QoS) that organizations can provide them. Services provided by different vendors are not highly distinguished which increases competition between organizations to maintain and increase their QoS. Customer Relationship Management systems are used to enable organizations to acquire new customers, establish a continuous relationship with them and increase customer retention for more profitability. CRM systems use machine-learning models to analyze customers’ personal and behavioral data to give organization a competitive advantage by increasing customer retention rate. Those models can predict customers who are expected to churn and reasons of churn. Predictions are used to design targeted marketing plans and service offers. This paper tries to compare and analyze the performance of different machine-learning techniques that are used for churn prediction problem. Ten analytical techniques that belong to different categories of learning are chosen for this study. The chosen techniques include Discriminant Analysis, Decision Trees (CART), instance-based learning (k-nearest neighbors), Support Vector Machines, Logistic Regression, ensemble–based learning techniques (Random Forest, Ada Boosting trees and Stochastic Gradient Boosting), Naive Bayesian, and Multi-layer perceptron. Models were applied on a dataset of telecommunication that contains 3333 records. Results show that both random forest and ADA boost outperform all other techniques with almost the same accuracy 96%. Both Multi-layer perceptron and Support vector machine can be recommended as well with 94% accuracy. Decision tree achieved 90%, naive Bayesian 88% and finally logistic regression and Linear Discriminant Analysis (LDA) with accuracy 86.7%.",2018,67,60,4,True,Computer Science,47070331,S. Sabbeh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a17bad00f1e031e22cdf5b83369c5defeaea9d1e,https://www.semanticscholar.org/paper/a17bad00f1e031e22cdf5b83369c5defeaea9d1e,Deep Challenges Associated with Deep Learning,"Deep learning is a recent emerging field of research in data science. Deep learning is essentially a combination of artificial intelligence and machine learning. Inspired by brain neurons, this has proven greater flexibility and builds more accurate models compared to machine learning. But making theoretical designs and to perform desired experiments are quite challenging due to many aspects. In the present paper, these challenges have been discussed to provide researchers a clear vision for the futuristic research in the field of deep learning.",2019,9,23,0,False,Computer Science,51233230,Ochin Sharma,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
82bab33fc8793565697e893c19117220b496dc67,https://www.semanticscholar.org/paper/82bab33fc8793565697e893c19117220b496dc67,Machine learning for online query relaxation,"In this paper we provide a fast, data-driven solution to the failing query problem: given a query that returns an empty answer, how can one relax the query's constraints so that it returns a non-empty set of tuples? We introduce a novel algorithm, loqr, which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes. loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query.In a first step, loqr uses a small, randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute's value satisfies the constraints in the failing query; this query-driven operation is performed online for each failing query. In the second step, loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query; then it uses the attributes' values from this rule to relax the failing query's constraints. Our experiments on six application domains show that loqr is both robust and fast: it successfully relaxes more than 95% of the failing queries, and it takes under a second for processing queries that consist of up to 20 attributes (larger queries of up to 93 attributes are processed in several seconds).",2004,29,68,2,False,Mathematics,3276863,Ion Muslea,,,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b1bd8464fad64d70cef2273fefdf95877998b75b,https://www.semanticscholar.org/paper/b1bd8464fad64d70cef2273fefdf95877998b75b,Machine-Learning Quantum States in the NISQ Era,"We review the development of generative modeling techniques in machine learning for the purpose of reconstructing real, noisy, many-qubit quantum states. Motivated by its interpretability and utility, we discuss in detail the theory of the restricted Boltzmann machine. We demonstrate its practical use for state reconstruction, starting from a classical thermal distribution of Ising spins, then moving systematically through increasingly complex pure and mixed quantum states. We review recent techniques in reconstruction of a cold atom wavefunction, intended for use on experimental noisy intermediate-scale quantum (NISQ) devices. Finally, we discuss the outlook for future experimental state reconstruction using machine learning in the NISQ era and beyond.",2019,26,56,1,True,Physics,3422999,G. Torlai,3422513.0,R. Melko,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e4bcb45990a928ac06df4dc24dc07e16fcac9141,https://www.semanticscholar.org/paper/e4bcb45990a928ac06df4dc24dc07e16fcac9141,Synthesizing benchmarks for predictive modeling,"Predictive modeling using machine learning is an effective method for building compiler heuristics, but there is a shortage of benchmarks. Typical machine learning experiments outside of the compilation field train over thousands or millions of examples. In machine learning for compilers, however, there are typically only a few dozen common benchmarks available. This limits the quality of learned models, as they have very sparse training data for what are often high-dimensional feature spaces. What is needed is a way to generate an unbounded number of training programs that finely cover the feature space. At the same time the generated programs must be similar to the types of programs that human developers actually write, otherwise the learning will target the wrong parts of the feature space. We mine open source repositories for program fragments and apply deep learning techniques to automatically construct models for how humans write programs. We sample these models to generate an unbounded number of runnable training programs. The quality of the programs is such that even human developers struggle to distinguish our generated programs from hand-written code. We use our generator for OpenCL programs, CLgen, to automatically synthesize thousands of programs and show that learning over these improves the performance of a state of the art predictive model by 1.27x. In addition, the fine covering of the feature space automatically exposes weaknesses in the feature design which are invisible with the sparse training examples from existing benchmark suites. Correcting these weaknesses further increases performance by 4.30x.",2017,57,84,8,True,Computer Science,2059804383,Chris Cummins,1955185.0,Pavlos Petoumenos,,Zheng Wang,2476647.0,H. Leather,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bb4de6ce625837aae1c7b09576cd9173d3ac0729,https://www.semanticscholar.org/paper/bb4de6ce625837aae1c7b09576cd9173d3ac0729,Large‐scale data mining using genetics‐based machine learning,"In the last decade, genetics‐based machine learning methods have shown their competence in large‐scale data mining tasks because of the scalability capacity that these techniques have demonstrated. This capacity goes beyond the innate massive parallelism of evolutionary computation methods by the proposal of a variety of mechanisms specifically tailored for machine learning tasks, including knowledge representations that exploit regularities in the datasets, hardware accelerations or data‐intensive computing methods, among others. This paper reviews different classes of methods that alone or (in many cases) combined accelerate genetics‐based machine learning methods. © 2013 Wiley Periodicals, Inc.",2013,143,47,0,False,Computer Science,1697337,J. Bacardit,1822628.0,Xavier Llorà,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45b5765cd1847f6a370da01483a53e9ed9676557,https://www.semanticscholar.org/paper/45b5765cd1847f6a370da01483a53e9ed9676557,On exponential convergence of SGD in non-convex over-parametrized learning,"Large over-parametrized models learned via stochastic gradient descent (SGD) methods have become a key element in modern machine learning. Although SGD methods are very effective in practice, most theoretical analyses of SGD suggest slower convergence than what is empirically observed. In our recent work [8] we analyzed how interpolation, common in modern over-parametrized learning, results in exponential convergence of SGD with constant step size for convex loss functions. In this note, we extend those results to a much broader non-convex function class satisfying the Polyak-Lojasiewicz (PL) condition. A number of important non-convex problems in machine learning, including some classes of neural networks, have been recently shown to satisfy the PL condition. We argue that the PL condition provides a relevant and attractive setting for many machine learning problems, particularly in the over-parametrized regime.",2018,15,66,12,False,Computer Science,1679638,Raef Bassily,145520115.0,Mikhail Belkin,143791100.0,Siyuan Ma,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5d819d0949692635e73368f8879a3b2ab0462974,https://www.semanticscholar.org/paper/5d819d0949692635e73368f8879a3b2ab0462974,"Comprehensive survey of deep learning in remote sensing: theories, tools, and challenges for the community","Abstract. In recent years, deep learning (DL), a rebranding of neural networks (NNs), has risen to the top in numerous areas, namely computer vision (CV), speech recognition, and natural language processing. Whereas remote sensing (RS) possesses a number of unique challenges, primarily related to sensors and applications, inevitably RS draws from many of the same theories as CV, e.g., statistics, fusion, and machine learning, to name a few. This means that the RS community should not only be aware of advancements such as DL, but also be leading researchers in this area. Herein, we provide the most comprehensive survey of state-of-the-art RS DL research. We also review recent new developments in the DL field that can be used in DL for RS. Namely, we focus on theories, tools, and challenges for the RS community. Specifically, we focus on unsolved challenges and opportunities as they relate to (i) inadequate data sets, (ii) human-understandable solutions for modeling physical phenomena, (iii) big data, (iv) nontraditional heterogeneous data sources, (v) DL architectures and learning algorithms for spectral, spatial, and temporal data, (vi) transfer learning, (vii) an improved theoretical understanding of DL systems, (viii) high barriers to entry, and (ix) training and optimizing the DL.",2017,417,390,14,True,Computer Science,145672842,J. Ball,152900419.0,Derek T. Anderson,2863960.0,Chee Seng Chan,,,Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0c8067178c90f12df1eed68926d658c21b4e04f1,https://www.semanticscholar.org/paper/0c8067178c90f12df1eed68926d658c21b4e04f1,Convex Learning of Multiple Tasks and their Structure,"Reducing the amount of human supervision is a key problem in machine learning and a natural approach is that of exploiting the relations (structure) among different tasks. This is the idea at the core of multi-task learning. In this context a fundamental question is how to incorporate the tasks structure in the learning problem. We tackle this question by studying a general computational framework that allows to encode apriori knowledge of the tasks structure in the form of a convex penalty; in this setting a variety of previously proposed methods can be recovered as special cases, including linear and non-linear approaches. Within this framework, we show that tasks and their structure can be efficiently learned considering a convex optimization problem that can be approached by means of block coordinate methods such as alternating minimization and for which we prove convergence to the global minimum.",2015,42,63,12,False,Computer Science,7666146,C. Ciliberto,2211263.0,Youssef Mroueh,1685292.0,T. Poggio,1690976.0,L. Rosasco,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bf6d4aeeb10cb389d019e7f25fdc6dfba48036dc,https://www.semanticscholar.org/paper/bf6d4aeeb10cb389d019e7f25fdc6dfba48036dc,Quantum-Assisted Learning of Hardware-Embedded Probabilistic Graphical Models,"Mainstream machine-learning techniques such as deep learning and probabilistic programming rely heavily on sampling from generally intractable probability distributions. There is increasing interest in the potential advantages of using quantum computing technologies as sampling engines to speed up these tasks or to make them more effective. However, some pressing challenges in state-of-the-art quantum annealers have to be overcome before we can assess their actual performance. The sparse connectivity, resulting from the local interaction between quantum bits in physical hardware implementations, is considered the most severe limitation to the quality of constructing powerful generative unsupervised machine-learning models. Here we use embedding techniques to add redundancy to data sets, allowing us to increase the modeling capacity of quantum annealers. We illustrate our findings by training hardware-embedded graphical models on a binarized data set of handwritten digits and two synthetic data sets in experiments with up to 940 quantum bits. Our model can be trained in quantum hardware without full knowledge of the effective parameters specifying the corresponding quantum Gibbs-like distribution; therefore, this approach avoids the need to infer the effective temperature at each iteration, speeding up learning; it also mitigates the effect of noise in the control parameters, making it robust to deviations from the reference Gibbs distribution. Our approach demonstrates the feasibility of using quantum annealers for implementing generative models, and it provides a suitable framework for benchmarking these quantum technologies on machine-learning-related tasks.",2016,78,90,2,True,Physics,3468759,M. Benedetti,1402789590.0,John Realpe-G'omez,144637418.0,R. Biswas,1402938096.0,A. Perdomo-Ortiz,Computer Science,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0bdcf145ecc3cabd44933c530fa3e59dda8561ea,https://www.semanticscholar.org/paper/0bdcf145ecc3cabd44933c530fa3e59dda8561ea,Tag-Aware Ordinal Sparse Factor Analysis for Learning and Content Analytics,"Machine learning offers novel ways and means to design personalized learning systems wherein each student's educational experience is customized in real time depending on their background, learning goals, and performance to date. SPARse Factor Analysis (SPARFA) is a novel framework for machine learning-based learning analytics, which estimates a learner's knowledge of the concepts underlying a domain, and content analytics, which estimates the relationships among a collection of questions and those concepts. SPARFA jointly learns the associations among the questions and the concepts, learner concept knowledge profiles, and the underlying question difficulties, solely based on the correct/incorrect graded responses of a population of learners to a collection of questions. In this paper, we extend the SPARFA framework significantly to enable: (i) the analysis of graded responses on an ordinal scale (partial credit) rather than a binary scale (correct/incorrect); (ii) the exploitation of tags/labels for questions that partially describe the question{concept associations. The resulting Ordinal SPARFA-Tag framework greatly enhances the interpretability of the estimated concepts. We demonstrate using real educational data that Ordinal SPARFA-Tag outperforms both SPARFA and existing collaborative filtering techniques in predicting missing learner responses.",2013,97,133,9,False,Computer Science,1730535,Andrew S. Lan,2123358.0,Andrew E. Waters,1746575.0,Christoph Studer,144908066.0,Richard Baraniuk,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cea2a9ea8f83e3736f5efdd004a0617b5817c35c,https://www.semanticscholar.org/paper/cea2a9ea8f83e3736f5efdd004a0617b5817c35c,Statistically reinforced machine learning for nonlinear patterns and variable interactions,". Most statistical models assume linearity and few variable interactions, even though real-world ecological patterns often result from nonlinear and highly interactive processes. We here introduce a set of novel empirical modeling techniques which can address this mismatch: statistically reinforced machine learning. We demonstrate the behaviors of three techniques (conditional inference tree, model-based tree, and permutation-based random forest) by analyzing an arti ﬁ cially generated example dataset that contains patterns based on nonlinearity and variable interactions. The results show the potential of statistically reinforced machine learning algorithms to detect nonlinear relationships and higher-order interactions. Estimation reliability for any technique, however, depended on sample size. The applications of statistically reinforced machine learning approaches would be particularly bene ﬁ cial for investigating (1) novel patterns for which shapes cannot be assumed a priori, (2) higher-order interactions which are often overlooked in parametric statistics, (3) context dependency where patterns change depending on other conditions, (4) signi ﬁ cance and effect sizes of variables while taking nonlinearity and variable interactions into account, and (5) a hypothesis using parametric statistics after identifying patterns using statistically reinforced machine learning techniques.",2017,86,64,1,True,Computer Science,38057242,M. Ryo,5718291.0,M. Rillig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
233fe6feeee53f0eda5f6a8b26a3a174fce5542d,https://www.semanticscholar.org/paper/233fe6feeee53f0eda5f6a8b26a3a174fce5542d,"Discrepancy, Coresets, and Sketches in Machine Learning","This paper defines the notion of class discrepancy for families of functions. It shows that low discrepancy classes admit small offline and streaming coresets. We provide general techniques for bounding the class discrepancy of machine learning problems. As corollaries of the general technique we bound the discrepancy (and therefore coreset complexity) of logistic regression, sigmoid activation loss, matrix covariance, kernel density and any analytic function of the dot product or the squared distance. Our results prove the existence of epsilon-approximation O(sqrt{d}/epsilon) sized coresets for the above problems. This resolves the long-standing open problem regarding the coreset complexity of Gaussian kernel density estimation. We provide two more related but independent results. First, an exponential improvement of the widely used merge-and-reduce trick which gives improved streaming sketches for any low discrepancy problem. Second, an extremely simple deterministic algorithm for finding low discrepancy sequences (and therefore coresets) for any positive semi-definite kernel. This paper establishes some explicit connections between class discrepancy, coreset complexity, learnability, and streaming algorithms.",2019,32,26,3,False,Computer Science,3386660,Zohar S. Karnin,2941680.0,Edo Liberty,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b7a6b7adafd01e939c9266083dfba9edea88846c,https://www.semanticscholar.org/paper/b7a6b7adafd01e939c9266083dfba9edea88846c,On the Universality of Invariant Networks,"Constraining linear layers in neural networks to respect symmetry transformations from a group $G$ is a common design principle for invariant networks that has found many applications in machine learning. In this paper, we consider a fundamental question that has received little attention to date: Can these networks approximate any (continuous) invariant function? We tackle the rather general case where $G\leq S_n$ (an arbitrary subgroup of the symmetric group) that acts on $\mathbb{R}^n$ by permuting coordinates. This setting includes several recent popular invariant networks. We present two main results: First, $G$-invariant networks are universal if high-order tensors are allowed. Second, there are groups $G$ for which higher-order tensors are unavoidable for obtaining universality. $G$-invariant networks consisting of only first-order tensors are of special interest due to their practical value. We conclude the paper by proving a necessary condition for the universality of $G$-invariant networks that incorporate only first-order tensors.",2019,24,160,30,False,Computer Science,3416939,Haggai Maron,2645055.0,Ethan Fetaya,66870455.0,Nimrod Segol,3232072.0,Y. Lipman,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9cbf9de13b8115ba2b6019b5cd7449b2820fdc28,https://www.semanticscholar.org/paper/9cbf9de13b8115ba2b6019b5cd7449b2820fdc28,Unsupervised Machine Learning and Band Topology.,"The study of topological band structures is an active area of research in condensed matter physics and beyond. Here, we combine recent progress in this field with developments in machine learning, another rising topic of interest. Specifically, we introduce an unsupervised machine learning approach that searches for and retrieves paths of adiabatic deformations between Hamiltonians, thereby clustering them according to their topological properties. The algorithm is general, as it does not rely on a specific parametrization of the Hamiltonian and is readily applicable to any symmetry class. We demonstrate the approach using several different models in both one and two spatial dimensions and for different symmetry classes with and without crystalline symmetries. Accordingly, it is also shown how trivial and topological phases can be diagnosed upon comparing with a generally designated set of trivial atomic insulators.",2020,104,54,2,True,Physics,4238038,M. Scheurer,47926510.0,Robert-Jan Slager,,,,,Computer Science,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d481a674ccbca3110fb7d66054b4ed28de1c3120,https://www.semanticscholar.org/paper/d481a674ccbca3110fb7d66054b4ed28de1c3120,Machine-learning Algorithm to Predict Hypotension Based on High-fidelity Arterial Pressure Waveform Analysis,"What We Already Know about This Topic The ability to predict intraoperative hypotension may advance the ability to prevent hypotension-associated complications effectively The extent to which advanced waveform analysis of invasive arterial lines may provide meaningful forewarning remains unknown What This Article Tells Us That Is New A machine-learning algorithm based on thousands of arterial waveform features can identify an intraoperative hypotensive event 15 min before its occurrence with a sensitivity of 88% and specificity of 87% Further studies must evaluate the real-time value of such algorithms in a broader set of clinical conditions and patients Background: With appropriate algorithms, computers can learn to detect patterns and associations in large data sets. The authors’ goal was to apply machine learning to arterial pressure waveforms and create an algorithm to predict hypotension. The algorithm detects early alteration in waveforms that can herald the weakening of cardiovascular compensatory mechanisms affecting preload, afterload, and contractility. Methods: The algorithm was developed with two different data sources: (1) a retrospective cohort, used for training, consisting of 1,334 patients’ records with 545,959 min of arterial waveform recording and 25,461 episodes of hypotension; and (2) a prospective, local hospital cohort used for external validation, consisting of 204 patients’ records with 33,236 min of arterial waveform recording and 1,923 episodes of hypotension. The algorithm relates a large set of features calculated from the high-fidelity arterial pressure waveform to the prediction of an upcoming hypotensive event (mean arterial pressure < 65 mmHg). Receiver-operating characteristic curve analysis evaluated the algorithm’s success in predicting hypotension, defined as mean arterial pressure less than 65 mmHg. Results: Using 3,022 individual features per cardiac cycle, the algorithm predicted arterial hypotension with a sensitivity and specificity of 88% (85 to 90%) and 87% (85 to 90%) 15 min before a hypotensive event (area under the curve, 0.95 [0.94 to 0.95]); 89% (87 to 91%) and 90% (87 to 92%) 10 min before (area under the curve, 0.95 [0.95 to 0.96]); 92% (90 to 94%) and 92% (90 to 94%) 5 min before (area under the curve, 0.97 [0.97 to 0.98]). Conclusions: The results demonstrate that a machine-learning algorithm can be trained, with large data sets of high-fidelity arterial waveforms, to predict hypotension in surgical patients’ records.",2018,40,219,13,False,Medicine,10330055,F. Hatib,49711459.0,Zhongping Jian,2093538.0,S. Buddi,7497039.0,Christine Lee,,,143614098.0,J. Settels,47731010.0,K. Sibert,49521118.0,J. Rinehart,2067401564.0,M. Cannesson,,,,,,,,,,,,,,,,,,,,,
21ca9cb91e0a84e1ca49197850664c3a081840a1,https://www.semanticscholar.org/paper/21ca9cb91e0a84e1ca49197850664c3a081840a1,Keratoconus severity identification using unsupervised machine learning,"We developed an unsupervised machine learning algorithm and applied it to big corneal parameters to identify and monitor keratoconus stages. A big dataset of corneal swept source optical coherence tomography (OCT) images of 12,242 eyes acquired from SS-1000 CASIA OCT Imaging Systems in multiple centers across Japan was assembled. A total of 3,156 eyes with valid Ectasia Status Index (ESI) between zero and 100% were selected for the downstream analysis. Four hundred and twenty corneal topography, elevation, and pachymetry parameters (excluding ESI Keratoconus indices) were selected. The algorithm included three major steps. 1) Principal component analysis (PCA) was used to linearly reduce the dimensionality of the input data from 420 to eight significant principal components. 2) Manifold learning was used to further reducing the selected principal components nonlinearly to two eigen-parameters. 3) Finally, a density-based clustering was applied to the eigen-parameters to identify eyes with keratoconus. Visualization of clusters in 2-D space was used to validate the quality of learning subjectively and ESI was used to assess the accuracy of the identified clusters objectively. The proposed method identified four clusters; I: a cluster composed of mostly normal eyes (224 eyes with ESI equal to zero, 23 eyes with ESI between five and 29, and nine eyes with ESI greater than 29), II: a cluster composed of mostly healthy eyes and eyes with forme fruste keratoconus (1772 eyes with ESI equal to zero, 698 eyes with ESI between five and 29, and 117 eyes with ESI greater than 29), III: a cluster composed of mostly eyes with mild keratoconus stage (184 eyes with ESI greater than 29, 74 eyes with ESI between five and 29, and 6 eyes with ESI equal to zero), and IV: a cluster composed of eyes with mostly advanced keratoconus stage (80 eyes had ESI greater than 29 and 1 eye had ESI between five and 29). We found that keratoconus status and severity can be well identified using unsupervised machine learning algorithms along with linear and non-linear corneal data transformation. The proposed method can better identify and visualize the keratoconus stages.",2018,32,53,4,True,Mathematics,1732890,Siamak Yousefi,2083335686.0,E. Yousefi,47284941.0,Hidenori Takahashi,2152314345.0,T. Hayashi,Medicine,,4608541.0,H. Tampo,34390753.0,S. Inoda,8339280.0,Y. Arai,5880309.0,P. Asbell,,,,,,,,,,,,,,,,,,,,,
3e00c737fced2bed77fde4ca7ccf474870f89c40,https://www.semanticscholar.org/paper/3e00c737fced2bed77fde4ca7ccf474870f89c40,Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning,"We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an optimal combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units.",1999,36,225,9,False,Computer Science,1799688,V. Hatzivassiloglou,1761739.0,Judith L. Klavans,1709847.0,E. Eskin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4eaa9e9be652e9f2e7445cbbb92dbdaee478db68,https://www.semanticscholar.org/paper/4eaa9e9be652e9f2e7445cbbb92dbdaee478db68,Interpretable classification models for recidivism prediction,"We investigate a long‐debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent and interpretable to use for decision making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation to allocating preventative social services. Each case might have an objective other than classification accuracy, such as a desired true positive rate TPR or false positive rate FPR. Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (support vector machines, stochastic gradient boosting and ridge regression) produce equally accurate models along the full ROC curve. However, methods that are designed for interpretability (classification and regression trees and C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a recent method called supersparse linear integer models to produce accurate, transparent and interpretable scoring systems along the full ROC curve. These scoring systems can be used for decision making for many different use cases, since they are just as accurate as the most powerful black box machine learning models for many applications, but completely transparent, and highly interpretable.",2015,154,186,6,True,Computer Science,2641684,J. Zeng,3072590.0,Berk Ustun,48395540.0,C. Rudin,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
08510d874f5f57b75efb58d65c03f348b416fff3,https://www.semanticscholar.org/paper/08510d874f5f57b75efb58d65c03f348b416fff3,Predicting the direction of stock market prices using random forest,"Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence.",2016,17,155,10,False,Computer Science,3407635,Luckyson Khaidem,2280469.0,Snehanshu Saha,3407935.0,Sudeepa Roy Dey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
04e20517c5a82999a4fbde505eca12baed338449,https://www.semanticscholar.org/paper/04e20517c5a82999a4fbde505eca12baed338449,Machine learning in control of functional electrical stimulation systems for locomotion,"Two machine learning techniques were evaluated for automatic design of a rule-based control of functional electrical stimulation (FES) for locomotion of spinal cord injured humans. The task was to learn the invariant characteristics of the relationship between sensory information and the FES-control signal by using off-line supervised training. Sensory signals were recorded using pressure sensors installed in the insoles of a subject's shoes and goniometers attached across the joints of the affected leg. The FES-control consisted of pulses corresponding to time intervals when the subject pressed on the manual push-button to deliver the stimulation during FES-assisted ambulation. The machine learning techniques used were the adaptive logic network (ALN) and the inductive learning algorithm (IL). Results to date suggest that, given the same training data, the IL learned faster than the ALN while both performed the test rapidly. The generalization was estimated by measuring the test errors and it was better with an ALN, especially if past points were used to reflect the time dimension. Both techniques were able to predict future stimulation events. An advantage of the ALN over the IL was that ALN's can be retrained with new data without losing previously collected knowledge. The advantages of the IL over the ALN were that the IL produces small, explicit, comprehensible trees and that the relative importance of each sensory contribution can be quantified.<<ETX>>",1995,30,136,1,False,Computer Science,48878345,A. Kostov,2589572.0,B. Andrews,1751548.0,D. Popović,145962057.0,R. Stein,Medicine,,35221772.0,W. W. Armstrong,,,,,,,,,,,,,,,,,,,,,,,,,,,
b944cc4241d195b1609a7a9d87fce0e9ba1498bc,https://www.semanticscholar.org/paper/b944cc4241d195b1609a7a9d87fce0e9ba1498bc,Kernel Sparse Representation-Based Classifier,"Sparse representation-based classifier (SRC), a combined result of machine learning and compressed sensing, shows its good classification performance on face image data. However, SRC could not well classify the data with the same direction distribution. The same direction distribution means that the sample vectors belonging to different classes distribute on the same vector direction. This paper presents a new classifier, kernel sparse representation-based classifier (KSRC), based on SRC and the kernel trick which is a usual technique in machine learning. KSRC is a nonlinear extension of SRC and can remedy the drawback of SRC. To make the data in an input space separable, we implicitly map these data into a high-dimensional kernel feature space by using some nonlinear mapping associated with a kernel function. Since this kernel feature space has a very high (or possibly infinite) dimensionality, or is unknown, we have to avoid working in this space explicitly. Fortunately, we can indeed reduce the dimensionality of the kernel feature space by exploiting kernel-based dimensionality reduction methods. In the reduced subspace, we need to find sparse combination coefficients for a test sample and assign a class label to it. Similar to SRC, KSRC is also cast into an ℓ1-minimization problem or a quadratically constrained ℓ1 -minimization problem. Extensive experimental results on UCI and face data sets show KSRC improves the performance of SRC.",2012,50,307,33,False,Mathematics,2152827417,Li Zhang,49374288.0,Weida Zhou,1717197.0,P. Chang,48211068.0,Jing Liu,Computer Science,,2152531966.0,Zhe Yan,2155390331.0,Ting Wang,2314391.0,Fanzhang Li,,,,,,,,,,,,,,,,,,,,,,,
c3de2985843fa0387554cb120333848140939eee,https://www.semanticscholar.org/paper/c3de2985843fa0387554cb120333848140939eee,Integrating Plausibility Checks and Machine Learning for Misbehavior Detection in VANET,"The safety and efficiency of vehicular communications rely on the correctness of the data exchanged between vehicles. In this paper we address the issue of detecting and classifying location spoofing misbehavior using the VeReMi dataset. We propose a framework for a system that uses plausibility checks as a feature vector for machine learning models, used to detect and classify misbehavior. Using KNN and SVM, our results show we can improve the overall detection precision of the plausibility checks used in the feature vectors by over 20%, while maintaining a recall within 5%. We have also proven once a misbehavior has been detected it is possible to classify different types of known misbehavior's. Classifying the misbehavior types allows for more accurate and specific action steps to counteract the attacks, hence improving the ability to recover safety and security in the system.",2018,16,60,10,False,Computer Science,66904445,Steven So,32704690.0,Prinkle Sharma,1817032.0,J. Petit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4bd48e59590908d103e0c8b379002a64253f124a,https://www.semanticscholar.org/paper/4bd48e59590908d103e0c8b379002a64253f124a,Learning to Align from Scratch,"Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face verification. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Specifically, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the specific data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned filters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face verification compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method.",2012,47,298,39,False,Computer Science,3219900,Gary B. Huang,2985062.0,Marwan A. Mattar,1697141.0,Honglak Lee,1389846455.0,E. Learned-Miller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a4f98df6113b0cae123ad892f24d1fb9452003e9,https://www.semanticscholar.org/paper/a4f98df6113b0cae123ad892f24d1fb9452003e9,A Machine Learning Approach to Pronoun Resolution in Spoken Dialogue,We apply a decision tree based approach to pronoun resolution in spoken dialogue. Our system deals with pronouns with NP-and non-NP-antecedents. We present a set of features designed for pronoun resolution in spoken dialogue and determine the most promising features. We evaluate the system on twenty Switchboard dialogues and show that it compares well to Byron's (2002) manually tuned system.,2003,29,118,7,True,Computer Science,31380436,M. Strube,144458770.0,M. Müller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2485d98f5b00d721f9aa46a2efba75e55b5e3f1c,https://www.semanticscholar.org/paper/2485d98f5b00d721f9aa46a2efba75e55b5e3f1c,Compressed Sensing Theory And Applications,"Compressed SensingBiomedical Signal Analysis for Connected HealthcareAn Introduction to Compressed SensingVideo Processing and Computational VideoGrundlagen der digitalen InformationsübertragungNeural Networks and Statistical LearningBayesian InferenceCompressive Sensing Based Algorithms for Electronic Defence2016 4th International Workshop on Compressed Sensing Theory and Its Applications to Radar, Sonar and Remote Sensing (CoSeRa) Took Place 19-23 September 2016 in Aachen, GermanyAdapted Compressed Sensing for Effective Hardware ImplementationsVideo Capsule EndoscopyApplied Informatics and Communication, Part IIICompressive Sensing of Earth ObservationsSparse Image and Signal ProcessingAlgorithmic Learning TheoryCognitive Fusion for Target TrackingSparse Optimization Theory and MethodsCommunications, Signal Processing, and SystemsWireless Algorithms, Systems, and ApplicationsIssues in Discovery, Experimental, and Laboratory Medicine: 2011 EditionSignal Processing and Machine Learning for Biomedical Big DataAdvances in Wireless Sensor NetworksCompressed Sensing and Its ApplicationsMultimedia Signals and SystemsCompressive Sensing in HealthcareCompressed Sensing in Radar Signal ProcessingComputer Vision in Medical ImagingArtificial Intelligence Applications and InnovationsBridge Maintenance, Safety, Management, Resilience and SustainabilityInformation Computing and ApplicationsCommunications and NetworkingCompressive Sensing for Urban RadarControl Theory for PhysicistsTopics in Modal Analysis I, Volume 5Compressive Sensing for the Photonic Mixer DeviceComputational Science and Its Applications – ICCSA 2021Medical Imaging: Concepts, Methodologies, Tools, and ApplicationsHyperspectral Image Unmixing Incorporating Adjacency InformationProceedings Of The International Congress Of Mathematicians 2018 (Icm 2018) (In 4 Volumes)IoT as a Service",2011,73,969,74,False,,3068758,T. Blumensath,144113976.0,M. Davies,2068949454.0,Gabriel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3c5f17ba04676574d95fac3d0687fe642ceb1dda,https://www.semanticscholar.org/paper/3c5f17ba04676574d95fac3d0687fe642ceb1dda,QoT estimation for unestablished lighpaths using machine learning,"We investigate a machine-learning technique that predicts whether the bit-error-rate of unestablished lightpaths meets the required threshold based on traffic volume, desired route and modulation format. The system is trained and tested on synthetic data.",2017,7,79,3,False,Computer Science,1823877,L. Barletta,33354551.0,A. Giusti,2370043.0,C. Rottondi,1759410.0,M. Tornatore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
56146f629048735d52d51a88b843b0b28add558f,https://www.semanticscholar.org/paper/56146f629048735d52d51a88b843b0b28add558f,Classifying the Dimensional Variation in Additive Manufactured Parts From Laser-Scanned Three-Dimensional Point Cloud Data Using Machine Learning Approaches,"The objective of this work is to develop and apply a spectral graph theoretic approach for differentiating between (classifying) additive manufactured (AM) parts contingent on the severity of their dimensional variation from laser-scanned coordinate measurements (3D point cloud). The novelty of the approach is in invoking spectral graph Laplacian eigenvalues as an extracted feature from the laser-scanned 3D point cloud data in conjunction with various machine learning techniques. The outcome is a new method that classifies the dimensional variation of an AM part by sampling less than 5% of the 2 million 3D point cloud data acquired (per part). This is a practically important result, because it reduces the measurement burden for postprocess quality assurance in AM— parts can be laser-scanned and their dimensional variation quickly assessed on the shop floor. To realize the research objective, the procedure is as follows. Test parts are made using the fused filament fabrication (FFF) polymer AM process. The FFF process conditions are varied per a phased design of experiments plan to produce parts with distinctive dimensional variations. Subsequently, each test part is laser scanned and 3D point cloud data are acquired. To classify the dimensional variation among parts, Laplacian eigenvalues are extracted from the 3D point cloud data and used as features within different machine learning approaches. Six machine learning approaches are juxtaposed: sparse representation, k-nearest neighbors, neural network, na€ıve Bayes, support vector machine, and decision tree. Of these, the sparse representation technique provides the highest classification accuracy (F-score> 97%). [DOI: 10.1115/1.4036641]",2017,52,83,1,False,Engineering,3466678,M. S. Tootooni,100575550.0,Ashley Dsouza,2054490173.0,Ryan Donovan,145849050.0,Prahalada K. Rao,,,48189729.0,Z. Kong,3136667.0,P. Borgesen,,,,,,,,,,,,,,,,,,,,,,,,,
4cd2ffebffd60708fb958c82b5fc72813fb9a1dd,https://www.semanticscholar.org/paper/4cd2ffebffd60708fb958c82b5fc72813fb9a1dd,Incorporating prior knowledge with weighted margin support vector machines,"Like many purely data-driven machine learning methods, Support Vector Machine (SVM) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance. In some applications, there might be human knowledge available that, in principle, could compensate for the lack of data. In this paper, we propose a simple generalization of SVM: Weighted Margin SVM (WMSVMs) that permits the incorporation of prior knowledge. We show that Sequential Minimal Optimization can be used in training WMSVM. We discuss the issues of incorporating prior knowledge using this rather general formulation. The experimental results show that the proposed methods of incorporating prior knowledge is effective.",2004,24,152,20,False,Computer Science,2143690397,Xiaoyun Wu,1748081.0,R. Srihari,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9f26a986f5470f922e9dfdec15590566ea6673d0,https://www.semanticscholar.org/paper/9f26a986f5470f922e9dfdec15590566ea6673d0,Machine translation-assisted language learning: writing for beginners,"The few studies that deal with machine translation (MT) as a language learning tool focus on its use by advanced learners, never by beginners. Yet, freely available MT engines (i.e. Google Translate) and MT-related web initiatives (i.e. Gabble-on.com) position themselves to cater precisely to the needs of learners with a limited command of a second language (L2). Anecdotal classroom evidence points to beginners availing themselves of MT help, even against the advice of teachers. In order to find out whether MT could help develop learners' writing skills in L2, we ran some tests asking participants to write directly into L2 in one instance and into L1 in another, while pre-editing the L1 and post-editing the L2 within the MT's Tradukka.com interface. Analysis of the output produced has found that MT helps beginners to communicate more, particularly when they had a lesser mastery of the language. The less their mastery of the L2, the greater the difference between the number of words composed with the help of MT and the number of those written directly into L2. It also helped them to communicate better, with blind marking indicating higher quality when writing with MT mediation. Looking at the screen recordings, on the other hand, we found that writing directly into L2 requires more effort, as measured by number of pauses, and involves more engagement with the task, as measured by the number of editing interventions.",2011,32,114,14,False,Computer Science,143861007,I. Garcia,104154241.0,María Isabel Núñez Peña,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8b78f69afe420bd9efc70c7cbb256d5ce78843fd,https://www.semanticscholar.org/paper/8b78f69afe420bd9efc70c7cbb256d5ce78843fd,Fast rates in statistical and online learning,"The speed with which a learning algorithm converges as it is presented with more data is a central problem in machine learning -- a fast rate of convergence means less data is needed for the same level of performance. The pursuit of fast rates in online and statistical learning has led to the discovery of many conditions in learning theory under which fast learning is possible. We show that most of these conditions are special cases of a single, unifying condition, that comes in two forms: the central condition for 'proper' learning algorithms that always output a hypothesis in the given model, and stochastic mixability for online algorithms that may make predictions outside of the model. We show that under surprisingly weak assumptions both conditions are, in a certain sense, equivalent. The central condition has a re-interpretation in terms of convexity of a set of pseudoprobabilities, linking it to density estimation under misspecification. For bounded losses, we show how the central condition enables a direct proof of fast rates and we prove its equivalence to the Bernstein condition, itself a generalization of the Tsybakov margin condition, both of which have played a central role in obtaining fast rates in statistical learning. Yet, while the Bernstein condition is two-sided, the central condition is one-sided, making it more suitable to deal with unbounded losses. In its stochastic mixability form, our condition generalizes both a stochastic exp-concavity condition identified by Juditsky, Rigollet and Tsybakov and Vovk's notion of mixability. Our unifying conditions thus provide a substantial step towards a characterization of fast rates in statistical learning, similar to how classical mixability characterizes constant regret in the sequential prediction with expert advice setting.",2015,87,84,10,False,Computer Science,1775951,T. Erven,144729640.0,P. Grünwald,144447714.0,Nishant A. Mehta,2216712.0,M. Reid,Mathematics,,143957317.0,R. C. Williamson,,,,,,,,,,,,,,,,,,,,,,,,,,,
b8daf037e85e5724165edb2319d074aab742e4dd,https://www.semanticscholar.org/paper/b8daf037e85e5724165edb2319d074aab742e4dd,Hypernetworks: A Molecular Evolutionary Architecture for Cognitive Learning and Memory,"Recent interest in human-level intelligence suggests a rethink of the role of machine learning in computational intelligence. We argue that ""without cognitive learning the goal of achieving human-level synthetic intelligence is far from completion. Here we review the principles underlying human learning and memory, and identify three of them, i.e., continuity, glocality, and compositionality, as the most fundamental to human-level machine learning. We then propose the recently-developed hypernetwork model as a candidate architecture for cognitive learning and memory. Hypernetworks are a random hypergraph structure higher-order probabilistic relations of data by an evolutionary self-organizing process based on molecular self- assembly. The chemically-based massive interaction for information organization and processing in the molecular hypernetworks, referred to as hyperinteractionism, is contrasted ""with the symbolist, connectionist, and dynamicist approaches to mind and intelligence. We demonstrate the generative learning capability of the hypernetworks to simulate linguistic recall memory, visual imagery, and language-vision crossmodal translation based on a video corpus of movies and dramas in a multimodal memory game environment. We also offer prospects for the hyperinteractionistic molecular mind approach to a unified theory of cognitive learning.",2008,98,109,3,True,Computer Science,1692756,Byoung-Tak Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e7e4790b4622d339dcf681ab84c5804551c1b5a3,https://www.semanticscholar.org/paper/e7e4790b4622d339dcf681ab84c5804551c1b5a3,Active Learning by Learning,"Pool-based active learning is an important technique that helps reduce labeling efforts within a pool of unlabeled instances. Currently, most pool-based active learning strategies are constructed based on some human-designed philosophy; that is, they reflect what human beings assume to be “good labeling questions.” However, while such human-designed philosophies can be useful on specific data sets, it is often difficult to establish the theoretical connection of those philosophies to the true learning performance of interest. In addition, given that a single human-designed philosophy is unlikely to work on all scenarios, choosing and blending those strategies under different scenarios is an important but challenging practical task. This paper tackles this task by letting the machines adaptively “learn” from the performance of a set of given strategies on a particular data set. More specifically, we design a learning algorithm that connects active learning with the well-known multi-armed bandit problem. Further, we postulate that, given an appropriate choice for the multi-armed bandit learner, it is possible to estimate the performance of different strategies on the fly. Extensive empirical studies of the resulting ALBL algorithm confirm that it performs better than state-of-the-art strategies and a leading blending algorithm for active learning, all of which are based on human-designed philosophy.",2015,23,110,15,True,Computer Science,2957796,Wei-Ning Hsu,1798966.0,Hsuan-Tien Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2e2e9b6b88fcef57b8093becb28aa1af8f094c5e,https://www.semanticscholar.org/paper/2e2e9b6b88fcef57b8093becb28aa1af8f094c5e,An Empirical Study of Rich Subgroup Fairness for Machine Learning,"Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.",2018,17,116,6,True,Computer Science,81338045,M. Kearns,5880154.0,Seth Neel,1682008.0,Aaron Roth,1768074.0,Zhiwei Steven Wu,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
874b3a63422eeaf24c14435ee6091ed48247bff3,https://www.semanticscholar.org/paper/874b3a63422eeaf24c14435ee6091ed48247bff3,Efficient memory-based learning for robot control,"This dissertation is about the application of machine learning to robot control. A system which has no initial model of the robot/world dynamics should be able to construct such a model using data received through its sensors|an approach which is formalized here as the SAB (State-ActionBehaviour) control cycle. A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered. The experiences are stored in a manner which permits fast recall of the closest previous experience to any new situation, thus permitting very quick predictions of the e ects of proposed actions and, given a goal behaviour, permitting fast generation of a candidate action. The learning can take place in high-dimensional non-linear control spaces with real-valued ranges of variables. Furthermore, the method avoids a number of shortcomings of earlier learning methods in which the controller can become trapped in inadequate performance which does not improve. Also considered is how the system is made resistant to noisy inputs and how it adapts to environmental changes. A well founded mechanism for choosing actions is introduced which solves the experiment/perform dilemma for this domain with adequate computational e ciency, and with fast convergence to the goal behaviour. The dissertation explains in detail how the SAB control cycle can be integrated into both low and high complexity tasks. The methods and algorithms are evaluated with numerous experiments using both real and simulated robot domains. The nal experiment also illustrates how a compound learning task can be structured into a hierarchy of simple learning tasks.",1990,67,445,44,False,Computer Science,1760402,A. Moore,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6f23573e3540617c34782964b4e53fd0f7c090d7,https://www.semanticscholar.org/paper/6f23573e3540617c34782964b4e53fd0f7c090d7,Inactive learning?: difficulties employing active learning in practice,"Despite the tremendous level of adoption of machine learning techniques in real-world settings, and the large volume of research on active learning, active learning techniques have been slow to gain substantial traction in practical applications. This reluctance of adoption is contrary to active learning's promise of reduced model-development costs and increased performance on a model-development budget. This essay presents several important and under-discussed challenges to using active learning well in practice. We hope this paper can serve as a call to arms for researchers in active learning--an encouragement to focus even more attention on how practitioners might actually use active learning.",2011,40,94,6,False,Computer Science,1708597,Josh Attenberg,1752722.0,F. Provost,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c39a158f793a204567c38b38b6f57a12b958dff1,https://www.semanticscholar.org/paper/c39a158f793a204567c38b38b6f57a12b958dff1,Comparison of Machine Learning Approaches for Prediction of Advanced Liver Fibrosis in Chronic Hepatitis C Patients,"Background/Aim: Using machine learning approaches as non-invasive methods have been used recently as an alternative method in staging chronic liver diseases for avoiding the drawbacks of biopsy. This study aims to evaluate different machine learning techniques in prediction of advanced fibrosis by combining the serum bio-markers and clinical information to develop the classification models. Methods: A prospective cohort of 39,567 patients with chronic hepatitis C was divided into two sets—one categorized as mild to moderate fibrosis (F0-F2), and the other categorized as advanced fibrosis (F3-F4) according to METAVIR score. Decision tree, genetic algorithm, particle swarm optimization, and multi-linear regression models for advanced fibrosis risk prediction were developed. Receiver operating characteristic curve analysis was performed to evaluate the performance of the proposed models. Results: Age, platelet count, AST, and albumin were found to be statistically significant to advanced fibrosis. The machine learning algorithms under study were able to predict advanced fibrosis in patients with HCC with AUROC ranging between 0.73 and 0.76 and accuracy between 66.3 and 84.4 percent. Conclusions: Machine-learning approaches could be used as alternative methods in prediction of the risk of advanced liver fibrosis due to chronic hepatitis C.",2018,27,66,5,False,Computer Science,5964659,Somaya Hashem,5717956.0,G. Esmat,2089459493.0,W. Elakel,71741805.0,S. Habashy,Medicine,,6734254.0,Safaa Abdel Raouf,33873196.0,M. Elhefnawi,3029537.0,M. Eladawy,5342647.0,M. Elhefnawi,,,,,,,,,,,,,,,,,,,,,
774c45fab38c1051e68d86b8757564ce5da44911,https://www.semanticscholar.org/paper/774c45fab38c1051e68d86b8757564ce5da44911,Probability estimation with machine learning methods for dichotomous and multicategory outcome: Theory,"Probability estimation for binary and multicategory outcome using logistic and multinomial logistic regression has a long‐standing tradition in biostatistics. However, biases may occur if the model is misspecified. In contrast, outcome probabilities for individuals can be estimated consistently with machine learning approaches, including k‐nearest neighbors (k‐NN), bagged nearest neighbors (b‐NN), random forests (RF), and support vector machines (SVM). Because machine learning methods are rarely used by applied biostatisticians, the primary goal of this paper is to explain the concept of probability estimation with these methods and to summarize recent theoretical findings. Probability estimation in k‐NN, b‐NN, and RF can be embedded into the class of nonparametric regression learning machines; therefore, we start with the construction of nonparametric regression estimates and review results on consistency and rates of convergence. In SVMs, outcome probabilities for individuals are estimated consistently by repeatedly solving classification problems. For SVMs we review classification problem and then dichotomous probability estimation. Next we extend the algorithms for estimating probabilities using k‐NN, b‐NN, and RF to multicategory outcomes and discuss approaches for the multicategory probability estimation problem using SVM. In simulation studies for dichotomous and multicategory dependent variables we demonstrate the general validity of the machine learning methods and compare it with logistic regression. However, each method fails in at least one simulation scenario. We conclude with a discussion of the failures and give recommendations for selecting and tuning the methods. Applications to real data and example code are provided in a companion article (doi:10.1002/bimj.201300077).",2014,112,77,2,False,Medicine,48740962,J. Kruppa,46399637.0,Yufeng Liu,1732153.0,G. Biau,144789007.0,M. Kohler,Mathematics,,1825863.0,I. König,1753856.0,J. Malley,144071896.0,A. Ziegler,,,,,,,,,,,,,,,,,,,,,,,
f055c2a866c969b98f72e1742a9b21fe08526016,https://www.semanticscholar.org/paper/f055c2a866c969b98f72e1742a9b21fe08526016,Single-Ended Speech Quality Measurement Using Machine Learning Methods,"We describe a novel single-ended algorithm constructed from models of speech signals, including clean and degraded speech, and speech corrupted by multiplicative noise and temporal discontinuities. Machine learning methods are used to design the models, including Gaussian mixture models, support vector machines, and random forest classifiers. Estimates of the subjective mean opinion score (MOS) generated by the models are combined using hard or soft decisions generated by a classifier which has learned to match the input signal with the models. Test results show the algorithm outperforming ITU-T P.563, the current ""state-of-art"" standard single-ended algorithm. Employed in a distributed double-ended measurement configuration, the proposed algorithm is found to be more effective than P.563 in assessing the quality of noise reduction systems and can provide a functionality not available with P.862 PESQ, the current double-ended standard algorithm",2006,45,92,6,True,Computer Science,2632038,T. Falk,145106154.0,W. Chan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74aca1a6cf3d9a45c93145a318f7f1357836cf99,https://www.semanticscholar.org/paper/74aca1a6cf3d9a45c93145a318f7f1357836cf99,Machine Learning of Noise-Resilient Quantum Circuits,"Noise mitigation and reduction will be crucial for obtaining useful answers from near-term quantum computers. In this work, we present a general framework based on machine learning for reducing the impact of quantum hardware noise on quantum circuits. Our method, called noise-aware circuit learning (NACL), applies to circuits designed to compute a unitary transformation, prepare a set of quantum states, or estimate an observable of a many-qubit state. Given a task and a device model that captures information about the noise and connectivity of qubits in a device, NACL outputs an optimized circuit to accomplish this task in the presence of noise. It does so by minimizing a task-specific cost function over circuit depths and circuit structures. To demonstrate NACL, we construct circuits resilient to a fine-grained noise model derived from gate set tomography on a superconducting-circuit quantum device, for applications including quantum state overlap, quantum Fourier transform, and W-state preparation.",2020,55,57,1,True,Computer Science,49556529,L. Cincio,47368368.0,K. Rudinger,2707503.0,M. Sarovar,50721537.0,Patrick J. Coles,Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d9a53a7108ca6c715d572fcebb567895b190a4cb,https://www.semanticscholar.org/paper/d9a53a7108ca6c715d572fcebb567895b190a4cb,A comparison of numerical optimizers for logistic regression,"Logistic regression is a workhorse of statistics and is closely related to method s used in Machine Learning, including the Perceptron and the Support Vector Machine. This note compares eight different algorithms for computing the maximum a-posteriori parameter estimate. A full derivation of each algorithm is given. In particular, a new derivation of Iterative Scaling is given which applies more generally than the conventional one. A new derivation is also given for the Modified Iterative Scaling algorithm of Collins et al. (2002). Most of the alg orithms operate in the primal space, but can also work in dual space. All algorithms are compared in terms of computational complexity by experiments on large data sets. The fastest algorithms turn out to be conjugate gradient ascent and quasi-Newton algorithms, which far outstrip Iterative Scaling and its variants.",2004,12,291,12,False,Mathematics,52626911,T. Minka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2ac0d8e7b2099933fe4f56a63abf981c0269cbce,https://www.semanticscholar.org/paper/2ac0d8e7b2099933fe4f56a63abf981c0269cbce,Structural Machine Learning with Galois Lattice and Graphs,"This paper defines a formal approach to learning from examples described by labelled graphs. We propose a formal model based upon lattice theory and in particular with the use of Galois lattice. We enlarge the domain of formal concept analysis, by the use of the Galois lattice model with structural description of examples and concepts. Our implementation, called ""Graal"" (for GRAph And Learning) constructs a Galois lattice for any description language provided that the two operations of comparison and generalization are determined for that language. We prove that these operations exist in the case of labelled graphs.",1998,40,105,3,False,Computer Science,1721628,M. Liquiere,1733472.0,J. Sallantin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2a59ada796ee255d7c67e0d2eebd708e2d1a0f56,https://www.semanticscholar.org/paper/2a59ada796ee255d7c67e0d2eebd708e2d1a0f56,Overcoming Catastrophic Forgetting During Domain Adaptation of Neural Machine Translation,"Continued training is an effective method for domain adaptation in neural machine translation. However, in-domain gains from adaptation come at the expense of general-domain performance. In this work, we interpret the drop in general-domain performance as catastrophic forgetting of general-domain knowledge. To mitigate it, we adapt Elastic Weight Consolidation (EWC)—a machine learning method for learning a new task without forgetting previous tasks. Our method retains the majority of general-domain performance lost in continued training without degrading in-domain performance, outperforming the previous state-of-the-art. We also explore the full range of general-domain performance available when some in-domain degradation is acceptable.",2019,27,88,14,True,Computer Science,143992775,Brian Thompson,3456371.0,Jeremy Gwinnup,3115181.0,Huda Khayrallah,1800354.0,Kevin Duh,,,1755162.0,Philipp Koehn,,,,,,,,,,,,,,,,,,,,,,,,,,,
36936ed6697206c12679772f6d6f923b63a54cab,https://www.semanticscholar.org/paper/36936ed6697206c12679772f6d6f923b63a54cab,Active Learning for Statistical Phrase-based Machine Translation,"Statistical machine translation (SMT) models need large bilingual corpora for training, which are unavailable for some language pairs. This paper provides the first serious experimental study of active learning for SMT. We use active learning to improve the quality of a phrase-based SMT system, and show significant improvements in translation compared to a random sentence selection baseline, when test and training data are taken from the same or different domains. Experimental results are shown in a simulated setting using three language pairs, and in a realistic situation for Bangla-English, a language pair with limited translation resources.",2009,14,100,21,True,Computer Science,2561045,Gholamreza Haffari,39900014.0,Maxim Roy,3028658.0,Anoop Sarkar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1f1819af398f6aaf29265d5ebb95ccd748bc0859,https://www.semanticscholar.org/paper/1f1819af398f6aaf29265d5ebb95ccd748bc0859,Deep Learning Algorithms for Bearing Fault Diagnosticsx—A Comprehensive Review,"In this survey paper, we systematically summarize existing literature on bearing fault diagnostics with deep learning (DL) algorithms. While conventional machine learning (ML) methods, including artificial neural network, principal component analysis, support vector machines, etc., have been successfully applied to the detection and categorization of bearing faults for decades, recent developments in DL algorithms in the last five years have sparked renewed interest in both industry and academia for intelligent machine health monitoring. In this paper, we first provide a brief review of conventional ML methods, before taking a deep dive into the state-of-the-art DL algorithms for bearing fault applications. Specifically, the superiority of DL based methods are analyzed in terms of fault feature extraction and classification performances; many new functionalities enabled by DL techniques are also summarized. In addition, to obtain a more intuitive insight, a comparative study is conducted on the classification accuracy of different algorithms utilizing the open source Case Western Reserve University (CWRU) bearing dataset. Finally, to facilitate the transition on applying various DL algorithms to bearing fault diagnostics, detailed recommendations and suggestions are provided for specific application conditions. Future research directions to further enhance the performance of DL algorithms on health monitoring are also discussed.",2019,193,130,4,True,Computer Science,2180849,Shen Zhang,50202626.0,Shibo Zhang,32216262.0,Bingnan Wang,9377911.0,T. Habetler,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3cc4823ce339ba27299241c8a7d639e8722f81c1,https://www.semanticscholar.org/paper/3cc4823ce339ba27299241c8a7d639e8722f81c1,Machine Learning and Computational Mathematics,"Neural network-based machine learning is capable of approximating functions in very high dimension with unprecedented efficiency and accuracy. This has opened up many exciting new possibilities, not just in traditional areas of artificial intelligence, but also in scientific computing and computational science. At the same time, machine learning has also acquired the reputation of being a set of ""black box"" type of tricks, without fundamental principles. This has been a real obstacle for making further progress in machine learning. In this article, we try to address the following two very important questions: (1) How machine learning has already impacted and will further impact computational mathematics, scientific computing and computational science? (2) How computational mathematics, particularly numerical analysis, {can} impact machine learning? We describe some of the most important progress that has been made on these issues. Our hope is to put things into a perspective that will help to integrate machine learning with computational mathematics.",2020,40,32,2,True,Mathematics,1789499,E. Weinan,,,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
93a06eb066fe58ed7d036e46e4cee53483e16bb8,https://www.semanticscholar.org/paper/93a06eb066fe58ed7d036e46e4cee53483e16bb8,Optimus: an efficient dynamic resource scheduler for deep learning clusters,"Deep learning workloads are common in today's production clusters due to the proliferation of deep learning driven AI services (e.g., speech recognition, machine translation). A deep learning training job is resource-intensive and time-consuming. Efficient resource scheduling is the key to the maximal performance of a deep learning cluster. Existing cluster schedulers are largely not tailored to deep learning jobs, and typically specifying a fixed amount of resources for each job, prohibiting high resource efficiency and job performance. This paper proposes Optimus, a customized job scheduler for deep learning clusters, which minimizes job training time based on online resource-performance models. Optimus uses online fitting to predict model convergence during training, and sets up performance models to accurately estimate training speed as a function of allocated resources in each job. Based on the models, a simple yet effective method is designed and used for dynamically allocating resources and placing deep learning tasks to minimize job completion time. We implement Optimus on top of Kubernetes, a cluster manager for container orchestration, and experiment on a deep learning cluster with 7 CPU servers and 6 GPU servers, running 9 training jobs using the MXNet framework. Results show that Optimus outperforms representative cluster schedulers by about 139% and 63% in terms of job completion time and makespan, respectively.",2018,54,250,42,True,Computer Science,9561490,Yanghua Peng,3393850.0,Yixin Bao,40930325.0,Yangrui Chen,1726963.0,Chuan Wu,,,145207202.0,Chuanxiong Guo,,,,,,,,,,,,,,,,,,,,,,,,,,,
f58aa5d6d9261c08a98db5dc8a6b693901589a2b,https://www.semanticscholar.org/paper/f58aa5d6d9261c08a98db5dc8a6b693901589a2b,A novel deep learning-based method for damage identification of smart building structures,"In the past few years, intelligent structural damage identification algorithms based on machine learning techniques have been developed and obtained considerable attentions worldwide, due to the advantages of reliable analysis and high efficiency. However, the performances of existing machine learning–based damage identification methods are heavily dependent on the selected signatures from raw signals. This will cause the fact that the damage identification method, which is the optimal solution for a specific application, may fail to provide the similar performance on other cases. Besides, the feature extraction is a time-consuming task, which may affect the real-time performance in practical applications. To address these problems, this article proposes a novel method based on deep convolutional neural networks to identify and localise damages of building structures equipped with smart control devices. The proposed deep convolutional neural network is capable of automatically extracting high-level features from raw signals or low-level features and optimally selecting the combination of extracted features via a multi-layer fusion to satisfy any damage identification objective. To evaluate the performance of the proposed deep convolutional neural network method, a five-level benchmark building equipped with adaptive smart isolators subjected to the seismic loading is investigated. The result shows that the proposed method has outstanding generalisation capacity and higher identification accuracy than other commonly used machine learning methods. Accordingly, it is deemed as an ideal and effective method for damage identification of smart structures.",2018,54,125,1,True,Computer Science,2152845752,Yang Yu,2518211.0,Chaoyue Wang,51099962.0,X. Gu,47787431.0,Jianchun Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
06ba352fcd577d523112cbc8a418a4936d9b67ed,https://www.semanticscholar.org/paper/06ba352fcd577d523112cbc8a418a4936d9b67ed,Machine Learning Based Trust Computational Model for IoT Services,"The Internet of Things has facilitated access to a large volume of sensitive information on each participating object in an ecosystem. This imposes many threats ranging from the risks of data management to the potential discrimination enabled by data analytics over delicate information such as locations, interests, and activities. To address these issues, the concept of trust is introduced as an important role in supporting both humans and services to overcome the perception of uncertainty and risks before making any decisions. However, establishing trust in a cyber world is a challenging task due to the volume of diversified influential factors from cyber-physical-systems. Hence, it is essential to have an intelligent trust computation model that is capable of generating accurate and intuitive trust values for prospective actors. Therefore, in this paper, a quantifiable trust assessment model is proposed. Built on this model, individual trust attributes are then calculated numerically. Moreover, a novel algorithm based on machine learning principles is devised to classify the extracted trust features and combine them to produce a final trust value to be used for decision making. Finally, our model's effectiveness is verified through a simulation. The results show that our method has advantages over other aggregation methods.",2019,50,97,9,True,Computer Science,144232723,Upul Jayasinghe,1701732.0,G. Lee,144659162.0,Tai-Won Um,145839868.0,Q. Shi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c9ef00e1ce135cf6ae566c58e80a8a9f9f73e3cc,https://www.semanticscholar.org/paper/c9ef00e1ce135cf6ae566c58e80a8a9f9f73e3cc,Intrinsic Motivation Systems for Autonomous Mental Development,"Exploratory activities seem to be intrinsically rewarding for children and crucial for their cognitive development. Can a machine be endowed with such an intrinsic motivation system? This is the question we study in this paper, presenting a number of computational systems that try to capture this drive towards novel or curious situations. After discussing related research coming from developmental psychology, neuroscience, developmental robotics, and active learning, this paper presents the mechanism of Intelligent Adaptive Curiosity, an intrinsic motivation system which pushes a robot towards situations in which it maximizes its learning progress. This drive makes the robot focus on situations which are neither too predictable nor too unpredictable, thus permitting autonomous mental development. The complexity of the robot's activities autonomously increases and complex developmental sequences self-organize without being constructed in a supervised manner. Two experiments are presented illustrating the stage-like organization emerging with this mechanism. In one of them, a physical robot is placed on a baby play mat with objects that it can learn to manipulate. Experimental results show that the robot first spends time in situations which are easy to learn, then shifts its attention progressively to situations of increasing difficulty, avoiding situations in which nothing can be learned. Finally, these various results are discussed in relation to more complex forms of behavioral organization and data coming from developmental psychology",2007,99,1134,87,True,Computer Science,1720664,P. Oudeyer,143791091.0,F. Kaplan,2762516.0,V. Hafner,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90b4d439de2ffdb38cf6949804b446afe33bb57d,https://www.semanticscholar.org/paper/90b4d439de2ffdb38cf6949804b446afe33bb57d,Privacy: a machine learning view,"The problem of disseminating a data set for machine learning while controlling the disclosure of data source identity is described using a commuting diagram of functions. This formalization is used to present and analyze an optimization problem balancing privacy and data utility requirements. The analysis points to the application of a generalization mechanism for maintaining privacy in view of machine learning needs. We present new proofs of NP-hardness of the problem of minimizing information loss while satisfying a set of privacy requirements, both with and without the addition of a particular uniform coding requirement. As an initial analysis of the approximation properties of the problem, we show that the cell suppression problem with a constant number of attributes can be approximated within a constant. As a side effect, proofs of NP-hardness of the minimum k-union, maximum k-intersection, and parallel versions of these are presented. Bounded versions of these problems are also shown to be approximable within a constant.",2004,35,39,4,False,Computer Science,2711007,S. Vinterbo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5522764282c85aea422f1c4dc92ff7e0ca6987bc,https://www.semanticscholar.org/paper/5522764282c85aea422f1c4dc92ff7e0ca6987bc,A Clockwork RNN,"Sequence prediction and classification are ubiquitous and challenging problems in machine learning that can require identifying complex dependencies between temporally distant inputs. Recurrent Neural Networks (RNNs) have the ability, in theory, to cope with these temporal dependencies by virtue of the short-term memory implemented by their recurrent (feedback) connections. However, in practice they are difficult to train successfully when long-term memory is required. This paper introduces a simple, yet powerful modification to the simple RNN (SRN) architecture, the Clockwork RNN (CW-RNN), in which the hidden layer is partitioned into separate modules, each processing inputs at its own temporal granularity, making computations only at its prescribed clock rate. Rather than making the standard RNN models more complex, CW-RNN reduces the number of SRN parameters, improves the performance significantly in the tasks tested, and speeds up the network evaluation. The network is demonstrated in preliminary experiments involving three tasks: audio signal generation, TIMIT spoken word classification, where it outperforms both SRN and LSTM networks, and online handwriting recognition, where it outperforms SRNs.",2014,41,419,66,False,Computer Science,2865775,J. Koutník,3035541.0,Klaus Greff,145842938.0,Faustino J. Gomez,145341374.0,J. Schmidhuber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
86f44fcca870d8017fbe980dcd4b2da7f3cf9c2a,https://www.semanticscholar.org/paper/86f44fcca870d8017fbe980dcd4b2da7f3cf9c2a,Quantum machine learning for data scientists,"This text aims to present and explain quantum machine learning algorithms to a data scientist in an accessible and consistent way. The algorithms and equations presented are not written in rigorous mathematical fashion, instead, the pressure is put on examples and step by step explanation of difficult topics. This contribution gives an overview of selected quantum machine learning algorithms, however there is also a method of scores extraction for quantum PCA algorithm proposed as well as a new cost function in feed-forward quantum neural networks is introduced. The text is divided into four parts: the first part explains the basic quantum theory, then quantum computation and quantum computer architecture are explained in section two. The third part presents quantum algorithms which will be used as subroutines in quantum machine learning algorithms. Finally, the fourth section describes quantum machine learning algorithms with the use of knowledge accumulated in previous parts.",2018,30,43,5,False,Physics,103111459,Dawid Kopczyk,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9be84e979c2a0471f1f0c358612dd499092b0f84,https://www.semanticscholar.org/paper/9be84e979c2a0471f1f0c358612dd499092b0f84,Digital Twins of Manufacturing Systems as a Base for Machine Learning,"In the engineering phase of modern manufacturing systems, simulation-based methods and tools have been established to face the increasing demands on time-efficiency and profitability. In the application of these simulation solutions, model-based digital twins are created, as multi-domain simulation models to describe the behavior of the manufacturing system. During the production process, a data-driven digital twin arises in the context of industry 4.0 based on an increasing networking and new cloud technologies. Recent developments in machine learning of fer new possibilities in conjunction with the digital twin. These range from data-based learning of models to learning control logic of complex systems. This paper proposes a combined model-based and data-driven concept of a digital twin. It shows how to use machine learning in connection with these models, in order to archive faster development times of manufacturing systems.",2018,20,31,0,False,Computer Science,66457214,Florian Jaensch,35238264.0,A. Csiszar,48476261.0,Christian Scheifele,143993688.0,A. Verl,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
364f02eff4f10ea602d86fd8c98c8694b76f46fd,https://www.semanticscholar.org/paper/364f02eff4f10ea602d86fd8c98c8694b76f46fd,Problems with Shapley-value-based explanations as feature importance measures,"Game-theoretic formulations of feature importance have become popular as a way to ""explain"" machine learning models. These methods define a cooperative game between the features of a model and distribute influence among these input elements using some form of the game's unique Shapley values. Justification for these methods rests on two pillars: their desirable mathematical properties, and their applicability to specific motivations for explanations. We show that mathematical problems arise when Shapley values are used for feature importance and that the solutions to mitigate these necessarily induce further complexity, such as the need for causal reasoning. We also draw on additional literature to argue that Shapley values do not provide explanations which suit human-centric goals of explainability.",2020,32,156,9,False,Computer Science,102411992,Indra Elizabeth Kumar,72563021.0,S. Venkatasubramanian,1786183.0,C. Scheidegger,34597147.0,Sorelle A. Friedler,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
