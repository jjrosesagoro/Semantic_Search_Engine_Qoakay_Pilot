paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,fieldsOfStudy/1,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,fieldsOfStudy/2,fieldsOfStudy/3,authors/5/authorId,authors/5/name,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,authors/16/authorId,authors/16/name,authors/17/authorId,authors/17/name,authors/18/authorId,authors/18/name,authors/19/authorId,authors/19/name,authors/20/authorId,authors/20/name,authors/21/authorId,authors/21/name
7da323e7103245eeaed32367c46abe3f4913df86,https://www.semanticscholar.org/paper/7da323e7103245eeaed32367c46abe3f4913df86,A survey of techniques for internet traffic classification using machine learning,"The research community has begun looking for IP traffic classification techniques that do not rely on `well known¿ TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.",2008,63,1476,83,False,Computer Science,,1716282,Thuy T. T. Nguyen,145027304.0,G. Armitage,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f9ae5196908d21336ab02f5c20258dc760d125d6,https://www.semanticscholar.org/paper/f9ae5196908d21336ab02f5c20258dc760d125d6,Adversarial machine learning,"In this paper (expanded from an invited talk at AISEC 2010), we discuss an emerging field of study: adversarial machine learning---the study of effective machine learning techniques against an adversarial opponent. In this paper, we: give a taxonomy for classifying attacks against online machine learning algorithms; discuss application-specific factors that limit an adversary's capabilities; introduce two models for modeling an adversary's capabilities; explore the limits of an adversary's knowledge about the algorithm, feature space, training, and input data; explore vulnerabilities in machine learning algorithms; discuss countermeasures against attacks; introduce the evasion challenge; and discuss privacy-preserving learning techniques.",2011,204,1121,75,True,Computer Science,,1787610,J. Tygar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
be9811f7e6019d5cd59ff97829a44bb5577bab00,https://www.semanticscholar.org/paper/be9811f7e6019d5cd59ff97829a44bb5577bab00,Machine Learning in Action,"SummaryMachine Learning in Action is unique book that blends the foundational theories of machine learning with the practical realities of building tools for everyday data analysis. You'll use the flexible Python programming language to build programs that implement algorithms for data classification, forecasting, recommendations, and higher-level features like summarization and simplification. About the BookA machine is said to learn when its performance improves with experience. Learning requires algorithms and programs that capture data and ferret out the interesting or useful patterns. Once the specialized domain of analysts and mathematicians, machine learning is becoming a skill needed by many.Machine Learning in Action is a clearly written tutorial for developers. It avoids academic language and takes you straight to the techniques you'll use in your day-to-day work. Many (Python) examples present the core algorithms of statistical data processing, data analysis, and data visualization in code you can reuse. You'll understand the concepts and how they fit in with tactical tasks like classification, forecasting, recommendations, and higher-level features like summarization and simplification.Readers need no prior experience with machine learning or statistical processing. Familiarity with Python is helpful.Purchase includes free PDF, ePub, and Kindle eBooks downloadable at manning.com. What's InsideA no-nonsense introduction Examples showing common ML tasks Everyday data analysis Implementing classic algorithms like Apriori and Adaboos=================================== Table of ContentsPART 1 CLASSIFICATION Machine learning basics Classifying with k-Nearest Neighbors Splitting datasets one feature at a time: decision trees Classifying with probability theory: nave Bayes Logistic regression Support vector machines Improving classification with the AdaBoost meta algorithm PART 2 FORECASTING NUMERIC VALUES WITH REGRESSION Predicting numeric values: regression Tree-based regression PART 3 UNSUPERVISED LEARNING Grouping unlabeled items using k-means clustering Association analysis with the Apriori algorithm Efficiently finding frequent itemsets with FP-growth PART 4 ADDITIONAL TOOLS Using principal component analysis to simplify data Simplifying data with the singular value decomposition Big data and MapReduce",2012,0,466,53,False,Computer Science,,145785408,P. Harrington,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0fa45cfa88ee9ac93cb01ec159ad8713d0e32d93,https://www.semanticscholar.org/paper/0fa45cfa88ee9ac93cb01ec159ad8713d0e32d93,"Data Mining: Practical Machine Learning Tools and Techniques, 3/E","Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. 
 
 Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. 
 
 *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks—in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization",2014,0,779,132,False,Computer Science,,9419406,I. Witten,143713826.0,Eibe Frank,118860642.0,M. Hall,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90848562905d26873b57bbf3f2f98319e38e5bde,https://www.semanticscholar.org/paper/90848562905d26873b57bbf3f2f98319e38e5bde,Density Ratio Estimation in Machine Learning,"Machine learning is an interdisciplinary field of science and engineering that studies mathematical theories and practical applications of systems that learn. This book introduces theories, methods, and applications of density ratio estimation, which is a newly emerging paradigm in the machine learning community. Various machine learning problems such as non-stationarity adaptation, outlier detection, dimensionality reduction, independent component analysis, clustering, classification, and conditional density estimation can be systematically solved via the estimation of probability density ratios. The authors offer a comprehensive introduction of various density ratio estimators including methods via density estimation, moment matching, probabilistic classification, density fitting, and density ratio fitting as well as describing how these can be applied to machine learning. The book also provides mathematical theories for density ratio estimation including parametric and non-parametric convergence analysis and numerical stability analysis to complete the first and definitive treatment of the entire framework of density ratio estimation in machine learning.",2012,67,419,52,False,Computer Science,,67154907,Masashi Sugiyama,2026193813.0,Taiji Suzuki,1897617.0,T. Kanamori,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
498ca0a1f8c980586408addf7ab2919ecdb7dd3d,https://www.semanticscholar.org/paper/498ca0a1f8c980586408addf7ab2919ecdb7dd3d,Factorizing YAGO: scalable machine learning for linked data,"Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 ⋅ 1014 possible triples in the YAGO~2 core ontology.",2012,38,399,45,False,Computer Science,,1729762,Maximilian Nickel,1700754.0,Volker Tresp,1688561.0,H. Kriegel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c62043a7d2537bbf40a84b9913957452a47fdb83,https://www.semanticscholar.org/paper/c62043a7d2537bbf40a84b9913957452a47fdb83,Dataset Shift in Machine Learning,"Dataset shift is a common problem in predictive modeling that occurs when the joint distribution of inputs and outputs differs between training and test stages. Covariate shift, a particular case of dataset shift, occurs when only the input distribution changes. Dataset shift is present in most practical applications, for reasons ranging from the bias introduced by experimental design to the irreproducibility of the testing conditions at training time. (An example is -email spam filtering, which may fail to recognize spam that differs in form from the spam the automatic filter has been built on.) Despite this, and despite the attention given to the apparently similar problems of semi-supervised learning and active learning, dataset shift has received relatively little attention in the machine learning community until recently. This volume offers an overview of current efforts to deal with dataset and covariate shift. The chapters offer a mathematical and philosophical introduction to the problem, place dataset shift in relationship to transfer learning, transduction, local learning, active learning, and semi-supervised learning, provide theoretical views of dataset and covariate shift (including decision theoretic and Bayesian perspectives), and present algorithms for covariate shift. Contributors: Shai Ben-David, Steffen Bickel, Karsten Borgwardt, Michael Brckner, David Corfield, Amir Globerson, Arthur Gretton, Lars Kai Hansen, Matthias Hein, Jiayuan Huang, Takafumi Kanamori, Klaus-Robert Mller, Sam Roweis, Neil Rubens, Tobias Scheffer, Marcel Schmittfull, Bernhard Schlkopf, Hidetoshi Shimodaira, Alex Smola, Amos Storkey, Masashi Sugiyama, Choon Hui Teo Neural Information Processing series",2009,162,1338,48,True,Computer Science,,1409068337,Joaquin Quionero-Candela,67154907.0,Masashi Sugiyama,2071649.0,Anton Schwaighofer,145306271.0,Neil D. Lawrence,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bb99668d4df98a3f6ff0b9fa3402e09008f22e2c,https://www.semanticscholar.org/paper/bb99668d4df98a3f6ff0b9fa3402e09008f22e2c,Making large-scale support vector machine learning practical,"Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.",1999,12,1863,186,False,Computer Science,,1680188,T. Joachims,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d2972fa779c91162f447d1e15540fba0df4cb547,https://www.semanticscholar.org/paper/d2972fa779c91162f447d1e15540fba0df4cb547,Deploying an interactive machine learning system in an evidence-based practice center: abstrackr,"Medical researchers looking for evidence pertinent to a specific clinical question must navigate an increasingly voluminous corpus of published literature. This data deluge has motivated the development of machine learning and data mining technologies to facilitate efficient biomedical research. Despite the obvious labor-saving potential of these technologies and the concomitant academic interest therein, however, adoption of machine learning techniques by medical researchers has been relatively sluggish. One explanation for this is that while many machine learning methods have been proposed and retrospectively evaluated, they are rarely (if ever) actually made accessible to the practitioners whom they would benefit. In this work, we describe the ongoing development of an end-to-end interactive machine learning system at the Tufts Evidence-based Practice Center. More specifically, we have developed abstrackr, an online tool for the task of citation screening for systematic reviews. This tool provides an interface to our machine learning methods. The main aim of this work is to provide a case study in deploying cutting-edge machine learning methods that will actually be used by experts in a clinical research setting.",2012,15,381,18,False,Computer Science,,1912476,Byron C. Wallace,50044599.0,Kevin Small,1729374.0,C. Brodley,143917291.0,J. Lau,2947796.0,T. Trikalinos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a35d565e8f70cb6ddc09a65904a72622e16b9485,https://www.semanticscholar.org/paper/a35d565e8f70cb6ddc09a65904a72622e16b9485,Distributed Optimization and Statistical Learning via the Alternating Direction Method of Multipliers,"Many problems of recent interest in statistics and machine learning can be posed in the framework of convex optimization. Due to the explosion in size and complexity of modern datasets, it is increasingly important to be able to solve problems with a very large number of features or training examples. As a result, both the decentralized collection or storage of these datasets as well as accompanying distributed solution methods are either necessary or at least highly desirable. In this review, we argue that the alternating direction method of multipliers is well suited to distributed convex optimization, and in particular to large-scale problems arising in statistics, machine learning, and related areas. The method was developed in the 1970s, with roots in the 1950s, and is equivalent or closely related to many other algorithms, such as dual decomposition, the method of multipliers, Douglas–Rachford splitting, Spingarn's method of partial inverses, Dykstra's alternating projections, Bregman iterative algorithms for l1 problems, proximal methods, and others. After briefly surveying the theory and history of the algorithm, we discuss applications to a wide variety of statistical and machine learning problems of recent interest, including the lasso, sparse logistic regression, basis pursuit, covariance selection, support vector machines, and many others. We also discuss general distributed optimization, extensions to the nonconvex setting, and efficient implementation, including some details on distributed MPI and Hadoop MapReduce implementations.",2011,185,15556,3442,True,Computer Science,,1843103,Stephen P. Boyd,46822591.0,Neal Parikh,1737474.0,E. Chu,2698633.0,B. Peleato,3809476.0,Jonathan Eckstein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d837267b364b4dc97bb35facda235a19be5ed374,https://www.semanticscholar.org/paper/d837267b364b4dc97bb35facda235a19be5ed374,Machine Learning in Non-Stationary Environments - Introduction to Covariate Shift Adaptation,"As the power of computing has grown over the past few decades, the field of machine learning has advanced rapidly in both theory and practice. Machine learning methods are usually based on the assumption that the data generation mechanism does not change over time. Yet real-world applications of machine learning, including image recognition, natural language processing, speech recognition, robot control, and bioinformatics, often violate this common assumption. Dealing with non-stationarity is one of modern machine learning's greatest challenges. This book focuses on a specific non-stationary environment known as covariate shift, in which the distributions of inputs (queries) change but the conditional distribution of outputs (answers) is unchanged, and presents machine learning theory, algorithms, and applications to overcome this variety of non-stationarity. After reviewing the state-of-the-art research in the field, the authors discuss topics that include learning under covariate shift, model selection, importance estimation, and active learning. They describe such real world applications of covariate shift adaption as brain-computer interface, speaker identification, and age prediction from facial images. With this book, they aim to encourage future research in machine learning, statistics, and engineering that strives to create truly autonomous learning machines able to learn under non-stationarity.",2012,0,325,22,False,Computer Science,,67154907,Masashi Sugiyama,1716788.0,M. Kawanabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7ea08581dff3d8ac301b80e693ebe0e6d247f2f5,https://www.semanticscholar.org/paper/7ea08581dff3d8ac301b80e693ebe0e6d247f2f5,The master algorithm: how the quest for the ultimate learning machine will remake our world,"Nowadays, “machine learning” is present in several aspects of the current world, internet advisors, advertisements and “smart” devices that seem to know what we need in a given moment. These are some examples of the problems solved by machine learning. This book presents the past, the present and the future of the different types of machine learning algorithms. At the beginning of the book, the author takes us to the first years of the computing science, where a programmer had to do absolutely everything by himself to make an algorithm do a certain task. As time passes, there appeared the first algorithms that were capable of programming themselves learning from the available data. The author presents what he himself calls the five “tribes” of machine learning, the essence that defends each one and the kind of problems that are able to solve without problems. With a great amount of simple examples, the author depicts which advantages and disadvantages of the “master” algorithms of each “tribes” are, saying that the problem that a tribe solves perfectly well, another one cannot do it, and the other way about. The author suggests to get the best out of each “tribe” and make a unique learning algorithm able to learn without caring about the problem: the master algorithm.",2015,8,324,34,False,Computer Science,,31294642,W. Hasperué,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6c10262a5d4230c7c85fbe528b8bbd9444116bca,https://www.semanticscholar.org/paper/6c10262a5d4230c7c85fbe528b8bbd9444116bca,Finding Density Functionals with Machine Learning,"Machine learning is used to approximate density functionals. For the model problem of the kinetic energy of noninteracting fermions in 1D, mean absolute errors below 1 kcal/mol on test densities similar to the training set are reached with fewer than 100 training densities. A predictor identifies if a test density is within the interpolation region. Via principal component analysis, a projected functional derivative finds highly accurate self-consistent densities. The challenges for application of our method to real electronic structure problems are discussed.",2011,42,396,3,True,Physics,Computer Science,27796226,John C. Snyder,48041657.0,M. Rupp,39960184.0,K. Hansen,145034054.0,K. Müller,2544144.0,K. Burke,Mathematics,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28d82be56e54df4b28a17908de17480d699e1409,https://www.semanticscholar.org/paper/28d82be56e54df4b28a17908de17480d699e1409,A Machine Learning Approach to Android Malware Detection,"With the recent emergence of mobile platforms capable of executing increasingly complex software and the rising ubiquity of using mobile platforms in sensitive applications such as banking, there is a rising danger associated with malware targeted at mobile devices. The problem of detecting such malware presents unique challenges due to the limited resources avalible and limited privileges granted to the user, but also presents unique opportunity in the required metadata attached to each application. In this article, we present a machine learning-based system for the detection of malware on Android devices. Our system extracts a number of features and trains a One-Class Support Vector Machine in an offline (off-device) manner, in order to leverage the higher computing power of a server or cluster of servers.",2012,44,320,22,False,Computer Science,,2200163,Justin Sahs,145155297.0,L. Khan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9e7e1d962ab25ae00e6fb5713116967e9b64be50,https://www.semanticscholar.org/paper/9e7e1d962ab25ae00e6fb5713116967e9b64be50,IEEE Transactions on Pattern Analysis and Machine Intelligence,"Nonparametric kernel methods are widely used and proven to be successful in many statistical learning problems. Well-known examples include the kernel density estimate (KDE) for density estimation and the support vector machine (SVM) for classification. We propose a kernel classifier that optimizes the L2 or integrated squared error (ISE) of a “difference of densities”. We focus on the Gaussian kernel, although the method applies to other kernels suitable for density estimation. Like a support vector machine (SVM), the classifier is sparse and results from solving a quadratic program. We provide statistical performance guarantees for the proposed L2 kernel classifier in the form of a finite sample oracle inequality, and strong consistency in the sense of both ISE and probability of error. A special case of our analysis applies to a previously introduced ISE-based method for kernel density estimation. For dimensionality greater than 15, the basic L2 kernel classifier performs poorly in practice. Thus, we extend the method through the introduction of a natural regularization parameter, which allows it to remain competitive with the SVM in high dimensions. Simulation results for both synthetic and real-world data are presented.",2022,31,788,74,False,,,2061918,JooSeuk Kim,40531965.0,C. Scott,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8729441d734782c3ed532a7d2d9611b438c0a09a,https://www.semanticscholar.org/paper/8729441d734782c3ed532a7d2d9611b438c0a09a,ADADELTA: An Adaptive Learning Rate Method,"We present a novel per-dimension learning rate method for gradient descent called ADADELTA. The method dynamically adapts over time using only first order information and has minimal computational overhead beyond vanilla stochastic gradient descent. The method requires no manual tuning of a learning rate and appears robust to noisy gradient information, different model architecture choices, various data modalities and selection of hyperparameters. We show promising results compared to other methods on the MNIST digit classification task using a single machine and on a large scale voice dataset in a distributed cluster environment.",2012,8,5762,870,False,Computer Science,,48799969,Matthew D. Zeiler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d37fc9e9c4fedc32865b08661e7fb950df1f8fbe,https://www.semanticscholar.org/paper/d37fc9e9c4fedc32865b08661e7fb950df1f8fbe,Kernel methods in machine learning,"We review machine learning methods employing positive definite kernels. These methods formulate learning and estimation problems in a reproducing kernel Hilbert space (RKHS) of functions defined on the data domain, expanded in terms of a kernel. Working in linear spaces of function has the benefit of facilitating the construction and analysis of learning algorithms while at the same time allowing large classes of functions. The latter include nonlinear functions as well as functions defined on nonvectorial data. We cover a wide range of methods, ranging from binary classifiers to sophisticated methods for estimation with structured data.",2007,164,1292,56,False,Mathematics,,143936663,Thomas Hofmann,1707625.0,B. Schölkopf,46234526.0,Alex Smola,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7fe7e80bf59a112386211b38ef2ea0b71ae76345,https://www.semanticscholar.org/paper/7fe7e80bf59a112386211b38ef2ea0b71ae76345,Machine Learning that Matters,"Much of current machine learning (ML) research has lost its connection to problems of import to the larger world of science and society. From this perspective, there exist glaring limitations in the data sets we investigate, the metrics we employ for evaluation, and the degree to which results are communicated back to their originating domains. What changes are needed to how we conduct research to increase the impact that ML has? We present six Impact Challenges to explicitly focus the field's energy and attention, and we discuss existing obstacles that must be addressed. We aim to inspire ongoing discussion and focus on ML that matters.",2012,20,243,22,False,Computer Science,Mathematics,6541629,K. Wagstaff,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da,https://www.semanticscholar.org/paper/ce0b8b6fca7dc089548cc2e9aaac3bae82bb19da,Making machine learning models interpretable,"Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.",2012,46,284,6,False,Computer Science,,1723612,A. Vellido,84306443.0,J. Martín-Guerrero,145408620.0,P. Lisboa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3c8bf504ddc7db1829466b6e9da5251025dd48f1,https://www.semanticscholar.org/paper/3c8bf504ddc7db1829466b6e9da5251025dd48f1,Automatic analysis of malware behavior using machine learning,"Malicious software - so called malware - poses a major threat to the security of computer systems. The amount and diversity of its variants render classic security defenses ineffective, such that millions of hosts in the Internet are infected with malware in the form of computer viruses, Internet worms and Trojan horses. While obfuscation and polymorphism employed by malware largely impede detection at file level, the dynamic analysis of malware binaries during run-time provides an instrument for characterizing and defending against the threat of malicious software. 
 
In this article, we propose a framework for the automatic analysis of malware behavior using machine learning. The framework allows for automatically identifying novel classes of malware with similar behavior (clustering) and assigning unknown malware to these discovered classes (classification). Based on both, clustering and classification, we propose an incremental approach for behavior-based analysis, capable of processing the behavior of thousands of malware binaries on a daily basis. The incremental analysis significantly reduces the run-time overhead of current analysis methods, while providing accurate discovery and discrimination of novel malware variants.",2011,68,663,60,True,Computer Science,,144825749,K. Rieck,2032673.0,Philipp Trinius,2043274.0,Carsten Willems,144227650.0,Thorsten Holz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
02227c94dd41fe0b439e050d377b0beb5d427cda,https://www.semanticscholar.org/paper/02227c94dd41fe0b439e050d377b0beb5d427cda,Reading Digits in Natural Images with Unsupervised Feature Learning,"Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.",2011,28,4413,1673,False,Computer Science,,34180232,Yuval Netzer,2156632012.0,Tao Wang,144638694.0,Adam Coates,1726358.0,A. Bissacco,144397975.0,Bo Wu,,,34699434.0,A. Ng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
58209c6db7b321ea7c75395b23ddb5100cd9bf81,https://www.semanticscholar.org/paper/58209c6db7b321ea7c75395b23ddb5100cd9bf81,Machine Learning for the New York City Power Grid,"Power companies can benefit from the use of knowledge discovery methods and statistical machine learning for preventive maintenance. We introduce a general process for transforming historical electrical grid data into models that aim to predict the risk of failures for components and systems. These models can be used directly by power companies to assist with prioritization of maintenance and repair work. Specialized versions of this process are used to produce (1) feeder failure rankings, (2) cable, joint, terminator, and transformer rankings, (3) feeder Mean Time Between Failure (MTBF) estimates, and (4) manhole events vulnerability rankings. The process in its most general form can handle diverse, noisy, sources that are historical (static), semi-real-time, or real-time, incorporates state-of-the-art machine learning algorithms for prioritization (supervised ranking or MTBF), and includes an evaluation of results via cross-validation and blind test. Above and beyond the ranked lists and MTBF estimates are business management interfaces that allow the prediction capability to be integrated directly into corporate planning and decision support; such interfaces rely on several important properties of our general modeling approach: that machine learning features are meaningful to domain experts, that the processing of data is transparent, and that prediction results are accurate enough to support sound decision making. We discuss the challenges in working with historical electrical grid data that were not designed for predictive purposes. The “rawness” of these data contrasts with the accuracy of the statistical models that can be obtained from the process; these models are sufficiently accurate to assist in maintaining New York City's electrical grid.",2012,58,220,7,True,Computer Science,Medicine,48395540,C. Rudin,1788375.0,D. Waltz,50035867.0,Roger N. Anderson,21545954.0,A. Boulanger,1403192347.0,Ansaf Salleb-Aouissi,,,32350828.0,M. Chow,1720958.0,Haimonti Dutta,152889617.0,Philip Gross,40486307.0,Bert Huang,1779637.0,Steve Ierome,,,,,,,,,,,,,,,,,,,,,,,,
f04df4e20a18358ea2f689b4c129781628ef7fc1,https://www.semanticscholar.org/paper/f04df4e20a18358ea2f689b4c129781628ef7fc1,A large annotated corpus for learning natural language inference,"Understanding entailment and contradiction is fundamental to understanding natural language, and inference about entailment and contradiction is a valuable testing ground for the development of semantic representations. However, machine learning research in this area has been dramatically limited by the lack of large-scale resources. To address this, we introduce the Stanford Natural Language Inference corpus, a new, freely available collection of labeled sentence pairs, written by humans doing a novel grounded task based on image captioning. At 570K pairs, it is two orders of magnitude larger than all other resources of its type. This increase in scale allows lexicalized classifiers to outperform some sophisticated existing entailment models, and it allows a neural network-based model to perform competitively on natural language inference benchmarks for the first time.",2015,37,2847,764,True,Computer Science,,3644767,Samuel R. Bowman,32301760.0,Gabor Angeli,144922861.0,Christopher Potts,144783904.0,Christopher D. Manning,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e9a986c8ff6c2f381d026fe014f6aaa865f34da7,https://www.semanticscholar.org/paper/e9a986c8ff6c2f381d026fe014f6aaa865f34da7,Deep Learning with Differential Privacy,"Machine learning techniques based on neural networks are achieving remarkable results in a wide variety of domains. Often, the training of models requires large, representative datasets, which may be crowdsourced and contain sensitive information. The models should not expose private information in these datasets. Addressing this goal, we develop new algorithmic techniques for learning and a refined analysis of privacy costs within the framework of differential privacy. Our implementation and experiments demonstrate that we can train deep neural networks with non-convex objectives, under a modest privacy budget, and at a manageable cost in software complexity, training efficiency, and model quality.",2016,60,3070,717,True,Computer Science,Mathematics,2057642721,Martín Abadi,1396184193.0,Andy Chu,153440022.0,Ian J. Goodfellow,145057514.0,H. B. McMahan,145591745.0,Ilya Mironov,,,35210462.0,Kunal Talwar,2152832173.0,Li Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9ead7583542e55c84bb9b90260ffde7a70c88e8d,https://www.semanticscholar.org/paper/9ead7583542e55c84bb9b90260ffde7a70c88e8d,Large-scale machine learning at twitter,"The success of data-driven solutions to difficult problems, along with the dropping costs of storing and processing massive amounts of data, has led to growing interest in large-scale machine learning. This paper presents a case study of Twitter's integration of machine learning tools into its existing Hadoop-based, Pig-centric analytics platform. We begin with an overview of this platform, which handles ""traditional"" data warehousing and business intelligence tasks for the organization. The core of this work lies in recent Pig extensions to provide predictive analytics capabilities that incorporate machine learning, focused specifically on supervised classification. In particular, we have identified stochastic gradient descent techniques for online learning and ensemble methods as being highly amenable to scaling out to large amounts of data. In our deployed solution, common machine learning tasks such as data sampling, feature generation, training, and testing can be accomplished directly in Pig, via carefully crafted loaders, storage functions, and user-defined functions. This means that machine learning is just another Pig script, which allows seamless integration with existing infrastructure for data management, scheduling, and monitoring in a production environment, as well as access to rich libraries of user-defined functions and the materialized output of other scripts.",2012,56,200,5,True,Computer Science,,145580839,Jimmy J. Lin,2062921.0,A. Kolcz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9e35e0fa69a96dea383890401d44366117eb6631,https://www.semanticscholar.org/paper/9e35e0fa69a96dea383890401d44366117eb6631,Machine Learning,The guest editor discusses some recent advances in machine learning and their applications to exciting new problem areas.,2013,4,2,0,False,Computer Science,,21060755,F. Alexander,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea58af907495e97c93997119db4a59fab5cd3683,https://www.semanticscholar.org/paper/ea58af907495e97c93997119db4a59fab5cd3683,Deep Machine Learning - A New Frontier in Artificial Intelligence Research [Research Frontier],"This article provides an overview of the mainstream deep learning approaches and research directions proposed over the past decade. It is important to emphasize that each approach has strengths and ""weaknesses, depending on the application and context in ""which it is being used. Thus, this article presents a summary on the current state of the deep machine learning field and some perspective into how it may evolve. Convolutional Neural Networks (CNNs) and Deep Belief Networks (DBNs) (and their respective variations) are focused on primarily because they are well established in the deep learning field and show great promise for future work.",2010,57,1041,22,False,Computer Science,,1804314,I. Arel,2483864.0,Derek C. Rose,1970334.0,T. Karnowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0148bbc80ea2f2526ab019a317639b4fb357f399,https://www.semanticscholar.org/paper/0148bbc80ea2f2526ab019a317639b4fb357f399,A Machine Learning Approach to Twitter User Classification,"
 
 This paper addresses the task of user classification in social media, with an application to Twitter. We automatically infer the values of user attributes such as political orientation or ethnicity by leveraging observable information such as the user behavior, network structure and the linguistic content of the user’s Twitter feed. We employ a machine learning approach which relies on a comprehensive set of features derived from such user information. We report encouraging experimental results on 3 tasks with different characteristics: political affiliation detection, ethnicity identification and detecting affinity for a particular business. Finally, our analysis shows that rich linguistic features prove consistently valuable across the 3 tasks and show great promise for additional user classification needs.
 
",2011,23,597,44,True,Computer Science,,145375801,M. Pennacchiotti,36445704.0,Ana-Maria Popescu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0e779fd59353a7f1f5b559b9d65fa4bfe367890c,https://www.semanticscholar.org/paper/0e779fd59353a7f1f5b559b9d65fa4bfe367890c,Geometric Deep Learning: Going beyond Euclidean data,"Many scientific fields study data with an underlying structure that is non-Euclidean. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions) and are natural targets for machine-learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural-language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure and in cases where the invariances of these structures are built into networks used to model them.",2016,126,2238,135,True,Computer Science,,1732570,M. Bronstein,143627859.0,Joan Bruna,1688882.0,Yann LeCun,3149531.0,Arthur D. Szlam,1697397.0,P. Vandergheynst,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
79cf9462a583e1889781868cbf8c31e43b36dd2f,https://www.semanticscholar.org/paper/79cf9462a583e1889781868cbf8c31e43b36dd2f,Towards Federated Learning at Scale: System Design,"Federated Learning is a distributed machine learning approach which enables model training on a large corpus of decentralized data. We have built a scalable production system for Federated Learning in the domain of mobile devices, based on TensorFlow. In this paper, we describe the resulting high-level design, sketch some of the challenges and their solutions, and touch upon the open problems and future directions.",2019,26,1305,121,False,Computer Science,Mathematics,2039588,Keith Bonawitz,153086296.0,Hubert Eichner,1718871.0,W. Grieskamp,68973739.0,Dzmitry Huba,67147139.0,A. Ingerman,,,2072422622.0,Vladimir Ivanov,3104292.0,Chloé Kiddon,32139366.0,Jakub Konecný,2066466157.0,Stefano Mazzocchi,145057514.0,H. B. McMahan,2592207.0,Timon Van Overveldt,2131407.0,David Petrou,1878835.0,D. Ramage,68972407.0,Jason Roselander,,,,,,,,,,,,,,,,
4187caa4d0d329f47e18377a6cd31ef3f580cfcc,https://www.semanticscholar.org/paper/4187caa4d0d329f47e18377a6cd31ef3f580cfcc,GraphLab: A New Framework For Parallel Machine Learning,"Designing and implementing efficient, provably correct parallel machine learning (ML) algorithms is challenging. Existing high-level parallel abstractions like MapReduce are insufficiently expressive while low-level tools like MPI and Pthreads leave ML experts repeatedly solving the same design challenges. By targeting common patterns in ML, we developed GraphLab, which improves upon abstractions like MapReduce by compactly expressing asynchronous iterative algorithms with sparse computational dependencies while ensuring data consistency and achieving a high degree of parallel performance. We demonstrate the expressiveness of the GraphLab framework by designing and implementing parallel versions of belief propagation, Gibbs sampling, Co-EM, Lasso and Compressed Sensing. We show that using GraphLab we can achieve excellent parallel performance on large scale real-world problems.",2010,23,848,103,False,Computer Science,,1680638,Y. Low,49988044.0,Joseph E. Gonzalez,1717990.0,Aapo Kyrola,1741745.0,D. Bickson,1730156.0,Carlos Guestrin,,,1695576.0,J. Hellerstein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e3d772986d176057aca2f5e3eb783da53b559134,https://www.semanticscholar.org/paper/e3d772986d176057aca2f5e3eb783da53b559134,Unsupervised Machine Translation Using Monolingual Corpora Only,"Machine translation has recently achieved impressive performance thanks to recent advances in deep learning and the availability of large-scale parallel corpora. There have been numerous attempts to extend these successes to low-resource language pairs, yet requiring tens of thousands of parallel sentences. In this work, we take this research direction to the extreme and investigate whether it is possible to learn to translate even without any parallel data. We propose a model that takes sentences from monolingual corpora in two different languages and maps them into the same latent space. By learning to reconstruct in both languages from this shared feature space, the model effectively learns to translate without using any labeled data. We demonstrate our model on two widely used datasets and two language pairs, reporting BLEU scores of 32.8 and 15.1 on the Multi30k and WMT English-French datasets, without using even a single parallel sentence at training time.",2017,42,859,157,False,Computer Science,,1830914,Guillaume Lample,8905591.0,Ludovic Denoyer,1706809.0,Marc'Aurelio Ranzato,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
819167ace2f0caae7745d2f25a803979be5fbfae,https://www.semanticscholar.org/paper/819167ace2f0caae7745d2f25a803979be5fbfae,The Limitations of Deep Learning in Adversarial Settings,"Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.",2015,40,2872,397,True,Computer Science,Mathematics,1967156,Nicolas Papernot,144061974.0,P. Mcdaniel,1680133.0,S. Jha,2623167.0,Matt Fredrikson,144643812.0,Z. B. Celik,,,144231976.0,A. Swami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7fcb90f68529cbfab49f471b54719ded7528d0ef,https://www.semanticscholar.org/paper/7fcb90f68529cbfab49f471b54719ded7528d0ef,Federated Learning: Strategies for Improving Communication Efficiency,"Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.",2016,26,2494,222,False,Computer Science,,32139366,Jakub Konecný,145057514.0,H. B. McMahan,1815972.0,Felix X. Yu,2662221.0,Peter Richtárik,9486035.0,A. Suresh,,,36577444.0,D. Bacon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1d122a074c936fcfd95faf44608e377a9d1799c8,https://www.semanticscholar.org/paper/1d122a074c936fcfd95faf44608e377a9d1799c8,DeepFM: A Factorization-Machine based Neural Network for CTR Prediction,"Learning sophisticated feature interactions behind user behaviors is critical in maximizing CTR for recommender systems. Despite great progress, existing methods seem to have a strong bias towards low- or high-order interactions, or require expertise feature engineering. In this paper, we show that it is possible to derive an end-to-end learning model that emphasizes both low- and high-order feature interactions. The proposed model, DeepFM, combines the power of factorization machines for recommendation and deep learning for feature learning in a new neural network architecture. Compared to the latest Wide & Deep model from Google, DeepFM has a shared input to its ""wide"" and ""deep"" parts, with no need of feature engineering besides raw features. Comprehensive experiments are conducted to demonstrate the effectiveness and efficiency of DeepFM over the existing models for CTR prediction, on both benchmark data and commercial data.",2017,32,1457,273,True,Computer Science,,3339005,Huifeng Guo,2824766.0,Ruiming Tang,144782498.0,Yunming Ye,7718952.0,Zhenguo Li,1996703.0,Xiuqiang He,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f,https://www.semanticscholar.org/paper/8ccdee6826cbc97256bd4d082ebfa8cdfd2c727f,Hidden fluid mechanics: Learning velocity and pressure fields from flow visualizations,"Machine-learning fluid flow Quantifying fluid flow is relevant to disciplines ranging from geophysics to medicine. Flow can be experimentally visualized using, for example, smoke or contrast agents, but extracting velocity and pressure fields from this information is tricky. Raissi et al. developed a machine-learning approach to tackle this problem. Their method exploits the knowledge of Navier-Stokes equations, which govern the dynamics of fluid flow in many scientifically relevant situations. The authors illustrate their approach using examples such as blood flow in an aneurysm. Science, this issue p. 1026 A machine learning approach exploiting the knowledge of Navier-Stokes equations can extract detailed fluid flow information. For centuries, flow visualization has been the art of making fluid motion visible in physical and biological systems. Although such flow patterns can be, in principle, described by the Navier-Stokes equations, extracting the velocity and pressure fields directly from the images is challenging. We addressed this problem by developing hidden fluid mechanics (HFM), a physics-informed deep-learning framework capable of encoding the Navier-Stokes equations into the neural networks while being agnostic to the geometry or the initial and boundary conditions. We demonstrate HFM for several physical and biomedical problems by extracting quantitative information for which direct measurements may not be possible. HFM is robust to low resolution and substantial noise in the observation data, which is important for potential applications.",2020,30,561,19,True,Medicine,Computer Science,145401977,M. Raissi,37412357.0,A. Yazdani,1720124.0,G. Karniadakis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bb35ef89addbbc28d960bc0cab70d8a29fdf6eee,https://www.semanticscholar.org/paper/bb35ef89addbbc28d960bc0cab70d8a29fdf6eee,A Survey on Multi-Task Learning,"Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.",2017,277,990,62,True,Computer Science,Mathematics,46867608,Yu Zhang,152290618.0,Qiang Yang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b5be3165d56580b60e29ad1a4a08b124d6cb8264,https://www.semanticscholar.org/paper/b5be3165d56580b60e29ad1a4a08b124d6cb8264,Scaling up machine learning: parallel and distributed approaches,"This tutorial gives a broad view of modern approaches for scaling up machine learning and data mining methods on parallel/distributed platforms. Demand for scaling up machine learning is task-specific: for some tasks it is driven by the enormous dataset sizes, for others by model complexity or by the requirement for real-time prediction. Selecting a task-appropriate parallelization platform and algorithm requires understanding their benefits, trade-offs and constraints. This tutorial focuses on providing an integrated overview of state-of-the-art platforms and algorithm choices. These span a range of hardware options (from FPGAs and GPUs to multi-core systems and commodity clusters), programming frameworks (including CUDA, MPI, MapReduce, and DryadLINQ), and learning settings (e.g., semi-supervised and online learning). The tutorial is example-driven, covering a number of popular algorithms (e.g., boosted trees, spectral clustering, belief propagation) and diverse applications (e.g., recommender systems and object recognition in vision).
 The tutorial is based on (but not limited to) the material from our upcoming Cambridge U. Press edited book which is currently in production.
 Visit the tutorial website at http://hunch.net/~large_scale_survey/",2011,6,386,16,False,Computer Science,,1988453,R. Bekkerman,47695762.0,M. Bilenko,144162125.0,J. Langford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a25fbcbbae1e8f79c4360d26aa11a3abf1a11972,https://www.semanticscholar.org/paper/a25fbcbbae1e8f79c4360d26aa11a3abf1a11972,A Survey on Transfer Learning,"A major assumption in many machine learning and data mining algorithms is that the training and future data must be in the same feature space and have the same distribution. However, in many real-world applications, this assumption may not hold. For example, we sometimes have a classification task in one domain of interest, but we only have sufficient training data in another domain of interest, where the latter data may be in a different feature space or follow a different data distribution. In such cases, knowledge transfer, if done successfully, would greatly improve the performance of learning by avoiding much expensive data-labeling efforts. In recent years, transfer learning has emerged as a new learning framework to address this problem. This survey focuses on categorizing and reviewing the current progress on transfer learning for classification, regression, and clustering problems. In this survey, we discuss the relationship between transfer learning and other related machine learning techniques such as domain adaptation, multitask learning and sample selection bias, as well as covariate shift. We also explore some potential future issues in transfer learning research.",2010,90,15007,853,False,Computer Science,,1746914,Sinno Jialin Pan,152290618.0,Qiang Yang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e3948c28d605e0d90e88e160556cfc14fbba57c8,https://www.semanticscholar.org/paper/e3948c28d605e0d90e88e160556cfc14fbba57c8,Incremental and Decremental Support Vector Machine Learning,"An on-line recursive algorithm for training support vector machines, one vector at a time, is presented. Adiabatic increments retain the Kuhn-Tucker conditions on all previously seen training data, in a number of steps each computed analytically. The incremental procedure is reversible, and decremental ""unlearning"" offers an efficient method to exactly evaluate leave-one-out generalization performance. Interpretation of decremental unlearning in feature space sheds light on the relationship between generalization and geometry of the data.",2000,16,1316,140,False,Computer Science,Mathematics,2702388,G. Cauwenberghs,1685292.0,T. Poggio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e7e25fd534e9e024da329aea546484938df305a5,https://www.semanticscholar.org/paper/e7e25fd534e9e024da329aea546484938df305a5,Gaussian Processes for Machine Learning (GPML) Toolbox,"The GPML toolbox provides a wide range of functionality for Gaussian process (GP) inference and prediction. GPs are specified by mean and covariance functions; we offer a library of simple mean and covariance functions and mechanisms to compose more complex ones. Several likelihood functions are supported including Gaussian and heavy-tailed for regression as well as others suitable for classification. Finally, a range of inference methods is provided, including exact and variational inference, Expectation Propagation, and Laplace's method dealing with non-Gaussian likelihoods and FITC for dealing with large regression tasks.",2010,7,845,50,False,Computer Science,,3472959,C. Rasmussen,1748758.0,H. Nickisch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c526ad0ea8b4cff9a671eb8a90ea98eb64ae17a7,https://www.semanticscholar.org/paper/c526ad0ea8b4cff9a671eb8a90ea98eb64ae17a7,Sparse Bayesian Extreme Learning Machine for Multi-classification,"Extreme learning machine (ELM) has become a popular topic in machine learning in recent years. ELM is a new kind of single-hidden layer feedforward neural network with an extremely low computational cost. ELM, however, has two evident drawbacks: 1) the output weights solved by Moore-Penrose generalized inverse is a least squares minimization issue, which easily suffers from overfitting and 2) the accuracy of ELM is drastically sensitive to the number of hidden neurons so that a large model is usually generated. This brief presents a sparse Bayesian approach for learning the output weights of ELM in classification. The new model, called Sparse Bayesian ELM (SBELM), can resolve these two drawbacks by estimating the marginal likelihood of network outputs and automatically pruning most of the redundant hidden neurons during learning phase, which results in an accurate and compact model. The proposed SBELM is evaluated on wide types of benchmark classification problems, which verifies that the accuracy of SBELM model is relatively insensitive to the number of hidden neurons; and hence a much more compact model is always produced as compared with other state-of-the-art neural network classifiers.",2014,26,169,15,False,Computer Science,Medicine,2353988,Jiahua Luo,1807914.0,C. Vong,48692831.0,P. Wong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a7fce24eb350e592338ffa9f3b7f8b2a8887b0dd,https://www.semanticscholar.org/paper/a7fce24eb350e592338ffa9f3b7f8b2a8887b0dd,Human Decisions and Machine Predictions,"Can machine learning improve human decision making? Bail decisions provide a good test case. Millions of times each year, judges make jail-or-release decisions that hinge on a prediction of what a defendant would do if released. The concreteness of the prediction task combined with the volume of data available makes this a promising machine-learning application. Yet comparing the algorithm to judges proves complicated. First, the available data are generated by prior judge decisions. We only observe crime outcomes for released defendants, not for those judges detained. This makes it hard to evaluate counterfactual decision rules based on algorithmic predictions. Second, judges may have a broader set of preferences than the variable the algorithm predicts; for instance, judges may care specifically about violent crimes or about racial inequities. We deal with these problems using different econometric strategies, such as quasi-random assignment of cases to judges. Even accounting for these concerns, our results suggest potentially large welfare gains: one policy simulation shows crime reductions up to 24.7% with no change in jailing rates, or jailing rate reductions up to 41.9% with no increase in crime rates. Moreover, all categories of crime, including violent crimes, show reductions; and these gains can be achieved while simultaneously reducing racial disparities. These results suggest that while machine learning can be valuable, realizing this value requires integrating these tools into an economic framework: being clear about the link between predictions and decisions; specifying the scope of payoff functions; and constructing unbiased decision counterfactuals. JEL Codes: C10 (Econometric and statistical methods and methodology), C55 (Large datasets: Modeling and analysis), K40 (Legal procedure, the legal system, and illegal behavior).",2017,68,639,48,True,Economics,Medicine,3371403,J. Kleinberg,1892673.0,Himabindu Lakkaraju,1702139.0,J. Leskovec,50197963.0,J. Ludwig,2062143.0,S. Mullainathan,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
10bc3b9bf71c4448ea0f4bec441403c0e50a0691,https://www.semanticscholar.org/paper/10bc3b9bf71c4448ea0f4bec441403c0e50a0691,Machine Learning - An Algorithmic Perspective,"Written in an easily accessible style, this book provides the ideal blend of theory and practical, applicable knowledge. It covers neural networks, graphical models, reinforcement learning, evolutionary algorithms, dimensionality reduction methods, and the important area of optimization. It treads the fine line between adequate academic rigor and overwhelming students with equations and mathematical concepts. The author includes examples based on widely available datasets and practical and theoretical problems to test understanding and application of the material. The book describes algorithms with code examples backed up by a website that provides working implementations in Python.",2009,4,982,119,False,Computer Science,,2112255,S. Marsland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d5b8b4bffc457150310dd3713241310a7f5c6165,https://www.semanticscholar.org/paper/d5b8b4bffc457150310dd3713241310a7f5c6165,SystemML: Declarative machine learning on MapReduce,"MapReduce is emerging as a generic parallel programming paradigm for large clusters of machines. This trend combined with the growing need to run machine learning (ML) algorithms on massive datasets has led to an increased interest in implementing ML algorithms on MapReduce. However, the cost of implementing a large class of ML algorithms as low-level MapReduce jobs on varying data and machine cluster sizes can be prohibitive. In this paper, we propose SystemML in which ML algorithms are expressed in a higher-level language and are compiled and executed in a MapReduce environment. This higher-level language exposes several constructs including linear algebra primitives that constitute key building blocks for a broad class of supervised and unsupervised ML algorithms. The algorithms expressed in SystemML are compiled and optimized into a set of MapReduce jobs that can run on a cluster of machines. We describe and empirically evaluate a number of optimization strategies for efficiently executing these algorithms on Hadoop, an open-source MapReduce implementation. We report an extensive performance evaluation on three ML algorithms on varying data and cluster sizes.",2011,23,315,41,False,Computer Science,,2317316,A. Ghoting,3252167.0,R. Krishnamurthy,3208372.0,E. Pednault,1698945.0,B. Reinwald,1808676.0,V. Sindhwani,,,1947100.0,S. Tatikonda,1968180.0,Yuanyuan Tian,2066721.0,Shivakumar Vaithyanathan,,,,,,,,,,,,,,,,,,,,,,,,,,,,
90972e7394b5fc884470cf78a657aae3932a8d8a,https://www.semanticscholar.org/paper/90972e7394b5fc884470cf78a657aae3932a8d8a,Using Machine Learning to Detect Cyberbullying,"Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5% accuracy.",2011,22,334,29,False,Computer Science,,145951266,Kelly Reynolds,1713751.0,April Kontostathis,38000101.0,Lynne Edwards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2a4ba0c1699965381fb2ba802157a89edd217943,https://www.semanticscholar.org/paper/2a4ba0c1699965381fb2ba802157a89edd217943,Data Mining and Machine Learning in Cybersecurity,"With the rapid advancement of information discovery techniques, machine learning and data mining continue to play a significant role in cybersecurity. Although several conferences, workshops, and journals focus on the fragmented research topics in this area, there has been no single interdisciplinary resource on past and current works and possible paths for future research in this area. This book fills this need. From basic concepts in machine learning and data mining to advanced problems in the machine learning domain, Data Mining and Machine Learning in Cybersecurity provides a unified reference for specific machine learning solutions to cybersecurity problems. It supplies a foundation in cybersecurity fundamentals and surveys contemporary challengesdetailing cutting-edge machine learning and data mining techniques. It also: Unveils cutting-edge techniques for detectingnew attacks Contains in-depth discussions of machine learning solutions to detection problems Categorizes methods for detecting, scanning, and profiling intrusions and anomalies Surveys contemporary cybersecurity problems and unveils state-of-the-art machine learning and data mining solutions Details privacy-preserving data mining methods This interdisciplinary resource includes technique review tables that allow for speedy access to common cybersecurity problems and associated data mining methods. Numerous illustrative figures help readers visualize the workflow of complex techniques and more than forty case studies provide a clear understanding of the design and application of data mining and machine learning techniques in cybersecurity.",2011,0,324,22,False,Computer Science,,145201255,S. Dua,48505005.0,Xian Du,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0ecf8c56300c20622f317e1e6cefdeeb85c513e2,https://www.semanticscholar.org/paper/0ecf8c56300c20622f317e1e6cefdeeb85c513e2,Improving propensity score weighting using machine learning,"Machine learning techniques such as classification and regression trees (CART) have been suggested as promising alternatives to logistic regression for the estimation of propensity scores. The authors examined the performance of various CART‐based propensity score models using simulated data. Hypothetical studies of varying sample sizes (n=500, 1000, 2000) with a binary exposure, continuous outcome, and 10 covariates were simulated under seven scenarios differing by degree of non‐linear and non‐additive associations between covariates and the exposure. Propensity score weights were estimated using logistic regression (all main effects), CART, pruned CART, and the ensemble methods of bagged CART, random forests, and boosted CART. Performance metrics included covariate balance, standard error, per cent absolute bias, and 95 per cent confidence interval (CI) coverage. All methods displayed generally acceptable performance under conditions of either non‐linearity or non‐additivity alone. However, under conditions of both moderate non‐additivity and moderate non‐linearity, logistic regression had subpar performance, whereas ensemble methods provided substantially better bias reduction and more consistent 95 per cent CI coverage. The results suggest that ensemble methods, especially boosted CART, may be useful for propensity score weighting. Copyright © 2009 John Wiley & Sons, Ltd.",2010,51,668,44,True,Computer Science,Medicine,16170202,Brian K. Lee,3020850.0,J. Lessler,145130892.0,E. Stuart,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f308f4305a7295d89ca6d287c351ce117952a710,https://www.semanticscholar.org/paper/f308f4305a7295d89ca6d287c351ce117952a710,Machine Learning: The ingredients of machine learning,"MACHINE LEARNING IS ALL ABOUT using the right features to build the right models that achieve the right tasks – this is the slogan, visualised in Figure 3 on p.11, with which we ended the Prologue. In essence, features define a ‘language’ in which we describe the relevant objects in our domain, be they e-mails or complex organic molecules. We should not normally have to go back to the domain objects themselves once we have a suitable feature representation, which is why features play such an important role in machine learning. We will take a closer look at them in Section 1.3. A task is an abstract representation of a problem we want to solve regarding those domain objects: the most common form of these is classifying them into two or more classes, but we shall encounter other tasks throughout the book. Many of these tasks can be represented as a mapping from data points to outputs. This mapping or model is itself produced as the output of a machine learning algorithm applied to training data; there is a wide variety of models to choose from, as we shall see in Section 1.2. We start this chapter by discussing tasks, the problems that can be solved with machine learning. No matter what variety of machine learning models you may encounter, you will find that they are designed to solve one of only a small number of tasks and use only a few different types of features.",2012,0,1,0,False,Mathematics,,144474086,P. Flach,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23bd1da0ad4d2ec956346655f0bb0206e13556b8,https://www.semanticscholar.org/paper/23bd1da0ad4d2ec956346655f0bb0206e13556b8,Learning to Control a Brain–Machine Interface for Reaching and Grasping by Primates,"Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain–machine interface (BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations.",2003,45,1737,122,True,Biology,Medicine,2016191,J. Carmena,2057713.0,M. Lebedev,1826610.0,R. Crist,1397219405.0,J. E. O’Doherty,39819116.0,David M. Santucci,,,38928339.0,D. Dimitrov,8004223.0,P. Patil,1845021.0,C. Henriquez,144052463.0,M. Nicolelis,,,,,,,,,,,,,,,,,,,,,,,,,,
1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0,https://www.semanticscholar.org/paper/1aad5969ca023d0aefe61d83a3cf6a3cb4d100e0,PANFIS: A Novel Incremental Learning Machine,"Most of the dynamics in real-world systems are compiled by shifts and drifts, which are uneasy to be overcome by omnipresent neuro-fuzzy systems. Nonetheless, learning in nonstationary environment entails a system owning high degree of flexibility capable of assembling its rule base autonomously according to the degree of nonlinearity contained in the system. In practice, the rule growing and pruning are carried out merely benefiting from a small snapshot of the complete training data to truncate the computational load and memory demand to the low level. An exposure of a novel algorithm, namely parsimonious network based on fuzzy inference system (PANFIS), is to this end presented herein. PANFIS can commence its learning process from scratch with an empty rule base. The fuzzy rules can be stitched up and expelled by virtue of statistical contributions of the fuzzy rules and injected datum afterward. Identical fuzzy sets may be alluded and blended to be one fuzzy set as a pursuit of a transparent rule base escalating human's interpretability. The learning and modeling performances of the proposed PANFIS are numerically validated using several benchmark problems from real-world or synthetic datasets. The validation includes comparisons with state-of-the-art evolving neuro-fuzzy methods and showcases that our new method can compete and in some cases even outperform these approaches in terms of predictive fidelity and model complexity.",2014,66,222,9,False,Computer Science,Medicine,144497394,Mahardhika Pratama,143997049.0,S. Anavatti,1719855.0,P. Angelov,1689446.0,E. Lughofer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4f6487d61ba6c2afa44be0e870599bb292e27638,https://www.semanticscholar.org/paper/4f6487d61ba6c2afa44be0e870599bb292e27638,Uncovering social spammers: social honeypots + machine learning,"Web-based social systems enable new community-based opportunities for participants to engage, share, and interact. This community value and related services like search and advertising are threatened by spammers, content polluters, and malware disseminators. In an effort to preserve community value and ensure longterm success, we propose and evaluate a honeypot-based approach for uncovering social spammers in online social systems. Two of the key components of the proposed approach are: (1) The deployment of social honeypots for harvesting deceptive spam profiles from social networking communities; and (2) Statistical analysis of the properties of these spam profiles for creating spam classifiers to actively filter out existing and new spammers. We describe the conceptual framework and design considerations of the proposed approach, and we present concrete observations from the deployment of social honeypots in MySpace and Twitter. We find that the deployed social honeypots identify social spammers with low false positive rates and that the harvested spam data contains signals that are strongly correlated with observable profile features (e.g., content, friend information, posting patterns, etc.). Based on these profile features, we develop machine learning based classifiers for identifying previously unknown spammers with high precision and a low rate of false positives.",2010,29,722,50,False,Computer Science,,2848353,Kyumin Lee,1697232.0,James Caverlee,2068783178.0,Steve Webb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da,https://www.semanticscholar.org/paper/e2e0e226f1f74ff65c0de3e5ad565bcd8b9710da,Adaptive Federated Learning in Resource Constrained Edge Computing Systems,"Emerging technologies and applications including Internet of Things, social networking, and crowd-sourcing generate large amounts of data at the network edge. Machine learning models are often built from the collected data, to enable the detection, classification, and prediction of future events. Due to bandwidth, storage, and privacy concerns, it is often impractical to send all the data to a centralized location. In this paper, we consider the problem of learning model parameters from data distributed across multiple edge nodes, without sending raw data to a centralized place. Our focus is on a generic class of machine learning models that are trained using gradient-descent-based approaches. We analyze the convergence bound of distributed gradient descent from a theoretical point of view, based on which we propose a control algorithm that determines the best tradeoff between local update and global parameter aggregation to minimize the loss function under a given resource budget. The performance of the proposed algorithm is evaluated via extensive experiments with real datasets, both on a networked prototype system and in a larger-scale simulated environment. The experimentation results show that our proposed approach performs near to the optimum with various machine learning models and different data distributions.",2018,48,849,70,True,Computer Science,Mathematics,50695457,Shiqiang Wang,40917131.0,Tiffany Tuor,2522394.0,Theodoros Salonidis,145353889.0,K. Leung,1702283.0,C. Makaya,,,145299837.0,T. He,46998035.0,K. Chan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
37259f7132fc9dc604ab8211b85a20cf5bb559fe,https://www.semanticscholar.org/paper/37259f7132fc9dc604ab8211b85a20cf5bb559fe,A First Course in Machine Learning,"A First Course in Machine Learning covers the core mathematical and statistical techniques needed to understand some of the most popular machine learning algorithms. The algorithms presented span the main problem areas within machine learning: classification, clustering and projection. The text gives detailed descriptions and derivations for a small number of algorithms rather than cover many algorithms in less detail. Referenced throughout the text and available on a supporting website (http://bit.ly/firstcourseml), an extensive collection of MATLAB/Octave scripts enables students to recreate plots that appear in the book and investigate changing model specifications and parameter values. By experimenting with the various algorithms and concepts, students see how an abstract set of equations can be used to solve real problems. Requiring minimal mathematical prerequisites, the classroom-tested material in this text offers a concise, accessible introduction to machine learning. It provides students with the knowledge and confidence to explore the machine learning literature and research specific methods in more detail.",2011,0,173,14,False,Computer Science,,145129912,S. Rogers,50859190.0,M. Girolami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3,https://www.semanticscholar.org/paper/c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3,Generalizing from a Few Examples: A Survey on Few-Shot Learning,"Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.",2019,193,824,60,False,Computer Science,,2115793087,Yaqing Wang,3259992.0,Quanming Yao,145193332.0,J. Kwok,1726587.0,L. Ni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ce70030c9d4e2ce2280cc15f50da42ea755d37d3,https://www.semanticscholar.org/paper/ce70030c9d4e2ce2280cc15f50da42ea755d37d3,Neural Networks and Learning Machines,"For graduate-level neural network courses offered in the departments of Computer Engineering, Electrical Engineering, and Computer Science. Neural Networks and Learning Machines, Third Edition is renowned for its thoroughness and readability. This well-organized and completely upto-date text remains the most comprehensive treatment of neural networks from an engineering perspective. This is ideal for professional engineers and research scientists. Matlab codes used for the computer experiments in the text are available for download at: http://www.pearsonhighered.com/haykin/ Refocused, revised and renamed to reflect the duality of neural networks and learning machines, this edition recognizes that the subject matter is richer when these topics are studied together. Ideas drawn from neural networks and machine learning are hybridized to perform improved learning tasks beyond the capability of either independently.",2010,14,5028,514,False,Computer Science,,1735300,S. Haykin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
208cd4b25768f0096fb2e80e7690473da0e2a563,https://www.semanticscholar.org/paper/208cd4b25768f0096fb2e80e7690473da0e2a563,Meta-learning with differentiable closed-form solvers,"Adapting deep networks to new concepts from a few examples is challenging, due to the high computational requirements of standard fine-tuning procedures. Most work on few-shot learning has thus focused on simple learning techniques for adaptation, such as nearest neighbours or gradient descent. Nonetheless, the machine learning literature contains a wealth of methods that learn non-deep models very efficiently. In this paper, we propose to use these fast convergent methods as the main adaptation mechanism for few-shot learning. The main idea is to teach a deep network to use standard machine learning tools, such as ridge regression, as part of its own internal model, enabling it to quickly adapt to novel data. This requires back-propagating errors through the solver steps. While normally the cost of the matrix operations involved in such a process would be significant, by using the Woodbury identity we can make the small number of examples work to our advantage. We propose both closed-form and iterative solvers, based on ridge regression and logistic regression components. Our methods constitute a simple and novel approach to the problem of few-shot learning and achieve performance competitive with or superior to the state of the art on three benchmarks.",2018,73,548,161,False,Computer Science,Mathematics,2271057,Luca Bertinetto,143848064.0,João F. Henriques,143635540.0,Philip H. S. Torr,1687524.0,A. Vedaldi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
642c1b4a9da95ea4239708afc5929a5007a1870d,https://www.semanticscholar.org/paper/642c1b4a9da95ea4239708afc5929a5007a1870d,Tensor2Tensor for Neural Machine Translation,Tensor2Tensor is a library for deep learning models that is well-suited for neural machine translation and includes the reference implementation of the state-of-the-art Transformer model.,2018,20,393,42,False,Computer Science,Mathematics,40348417,Ashish Vaswani,1751569.0,Samy Bengio,2445241.0,E. Brevdo,1565641737.0,François Chollet,19177000.0,Aidan N. Gomez,,,2776283.0,Stephan Gouws,145024664.0,Llion Jones,40527594.0,Lukasz Kaiser,2583391.0,Nal Kalchbrenner,3877127.0,Niki Parmar,35474601.0,Ryan Sepassi,1846258.0,Noam M. Shazeer,39328010.0,Jakob Uszkoreit,,,,,,,,,,,,,,,,,,
3bff76c25f7c416834655ba664553b14eb67a11c,https://www.semanticscholar.org/paper/3bff76c25f7c416834655ba664553b14eb67a11c,Sparse Bayesian Learning and the Relevance Vector Machine,"This paper introduces a general Bayesian framework for obtaining sparse solutions to regression and classification tasks utilising models linear in the parameters. Although this framework is fully general, we illustrate our approach with a particular specialisation that we denote the 'relevance vector machine' (RVM), a model of identical functional form to the popular and state-of-the-art 'support vector machine' (SVM). We demonstrate that by exploiting a probabilistic Bayesian learning framework, we can derive accurate prediction models which typically utilise dramatically fewer basis functions than a comparable SVM while offering a number of additional advantages. These include the benefits of probabilistic predictions, automatic estimation of 'nuisance' parameters, and the facility to utilise arbitrary basis functions (e.g. non-'Mercer' kernels). We detail the Bayesian framework and associated learning algorithm for the RVM, and give some illustrative examples of its application along with some comparative benchmarks. We offer some explanation for the exceptional degree of sparsity obtained, and discuss and demonstrate some of the advantageous features, and potential extensions, of Bayesian relevance learning.",2001,38,1621,145,False,Computer Science,,2831141,Michael E. Tipping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f466157848d1a7772fb6d02cdac9a7a5e7ef982e,https://www.semanticscholar.org/paper/f466157848d1a7772fb6d02cdac9a7a5e7ef982e,Neural Discrete Representation Learning,"Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of ""posterior collapse"" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.",2017,42,1496,316,False,Computer Science,Mathematics,3422336,Aäron van den Oord,1689108.0,Oriol Vinyals,2645384.0,K. Kavukcuoglu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63861fbeb7ec41986b85965b9780b428d919919e,https://www.semanticscholar.org/paper/63861fbeb7ec41986b85965b9780b428d919919e,Support vector machine active learning for image retrieval,"Relevance feedback is often a critical component when designing image databases. With these databases it is difficult to specify queries directly and explicitly. Relevance feedback interactively determinines a user's desired output or query concept by asking the user whether certain proposed images are relevant or not. For a relevance feedback algorithm to be effective, it must grasp a user's query concept accurately and quickly, while also only asking the user to label a small number of images. We propose the use of a support vector machine active learning algorithm for conducting effective relevance feedback for image retrieval. The algorithm selects the most informative images to query a user and quickly learns a boundary that separates the images that satisfy the user's query concept from the rest of the dataset. Experimental results show that our algorithm achieves significantly higher search accuracy than traditional query refinement schemes after just three to four rounds of relevance feedback.",2001,80,1525,146,True,Computer Science,,2058177533,Simon Tong,33794424.0,E. Chang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5a9e85af0bc45472e9c14e956819f1324085aaa1,https://www.semanticscholar.org/paper/5a9e85af0bc45472e9c14e956819f1324085aaa1,A Review of Machine Learning Algorithms for Text-Documents Classification,"With the increasing availability of electronic documents and the rapid growth of the World Wide Web, the task of automatic categorization of documents became the key method for organizing the information and know- ledge discovery. Proper classification of e-documents, online news, blogs, e-mails and digital libraries need text mining, machine learning and natural language processing tech- niques to get meaningful knowledge. The aim of this paper is to highlight the important techniques and methodologies that are employed in text documents classification, while at the same time making awareness of some of the interesting challenges that remain to be solved, focused mainly on text representation and machine learning techniques. This paper provides a review of the theory and methods of document classification and text mining, focusing on the existing litera- ture.",2010,120,564,29,False,Computer Science,,144828657,B. Baharudin,1704695.0,Lam Hong Lee,39488669.0,Khairullah Khan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ca011427853d34ce4ec9ccafde8a70c9eacc3e21,https://www.semanticscholar.org/paper/ca011427853d34ce4ec9ccafde8a70c9eacc3e21,Deep Learning for Computer Vision: A Brief Review,"Over the last years deep learning methods have been shown to outperform previous state-of-the-art machine learning techniques in several fields, with computer vision being one of the most prominent cases. This review paper provides a brief overview of some of the most significant deep learning schemes used in computer vision problems, that is, Convolutional Neural Networks, Deep Boltzmann Machines and Deep Belief Networks, and Stacked Denoising Autoencoders. A brief account of their history, structure, advantages, and limitations is given, followed by a description of their applications in various computer vision tasks, such as object detection, face recognition, action and activity recognition, and human pose estimation. Finally, a brief overview is given of future directions in designing deep learning schemes for computer vision problems and the challenges involved therein.",2018,110,1455,22,True,Medicine,Computer Science,2594647,A. Voulodimos,120205775.0,N. Doulamis,1746705.0,A. Doulamis,1806369.0,E. Protopapadakis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ecf6c42d84351f34e1625a6a2e4cc6526da45c74,https://www.semanticscholar.org/paper/ecf6c42d84351f34e1625a6a2e4cc6526da45c74,Representation Learning on Graphs: Methods and Applications,"Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.",2017,65,1447,156,False,Computer Science,,49437682,William L. Hamilton,83539859.0,Rex Ying,1702139.0,J. Leskovec,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78989616eeeac55b202e3e4205225e7135054185,https://www.semanticscholar.org/paper/78989616eeeac55b202e3e4205225e7135054185,An Introduction to Deep Learning for the Physical Layer,"We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.",2017,80,1435,140,True,Computer Science,Mathematics,1388350203,Tim O'Shea,1749686.0,J. Hoydis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6d431f835c06afdea45dff6b24486bf301ebdef0,https://www.semanticscholar.org/paper/6d431f835c06afdea45dff6b24486bf301ebdef0,An Overview of Multi-Task Learning in Deep Neural Networks,"Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.",2017,58,1749,112,False,Computer Science,Mathematics,2884561,Sebastian Ruder,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
57e4afe9ca74414fa02f2e0a929b64dc9a03334d,https://www.semanticscholar.org/paper/57e4afe9ca74414fa02f2e0a929b64dc9a03334d,Application of Machine Learning To Epileptic Seizure Detection,"We present and evaluate a machine learning approach to constructing patient-specific classifiers that detect the onset of an epileptic seizure through analysis of the scalp EEG, a non-invasive measure of the brain's electrical activity. This problem is challenging because the brain's electrical activity is composed of numerous classes with overlapping characteristics. The key steps involved in realizing a high performance algorithm included shaping the problem into an appropriate machine learning framework, and identifying the features critical to separating seizure from other types of brain activity. When trained on 2 or more seizures per patient and tested on 916 hours of continuous EEG from 24 patients, our algorithm detected 96% of 173 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. We also provide information about how to download the CHB-MIT database, which contains the data used in this study.",2010,12,433,64,False,Computer Science,,2614750,A. Shoeb,1724429.0,J. Guttag,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
818826f356444f3daa3447755bf63f171f39ec47,https://www.semanticscholar.org/paper/818826f356444f3daa3447755bf63f171f39ec47,Active Learning Literature Survey,"The key idea behind active learning is that a machine learning algorithm can achieve greater accuracy with fewer training labels if it is allowed to choose the data from which it learns. An active learner may pose queries, usually in the form of unlabeled data instances to be labeled by an oracle (e.g., a human annotator). Active learning is well-motivated in many modern machine learning problems, where unlabeled data may be abundant or easily obtained, but labels are difficult, time-consuming, or expensive to obtain. This report provides a general introduction to active learning and a survey of the literature. This includes a discussion of the scenarios in which queries can be formulated, and an overview of the query strategy frameworks proposed in the literature to date. An analysis of the empirical and theoretical evidence for successful active learning, a summary of problem setting variants and practical issues, and a discussion of related topics in machine learning research are also presented.",2009,169,4995,541,False,Computer Science,,1717452,Burr Settles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98,https://www.semanticscholar.org/paper/2b91a2cbcd9cce902cbc8da78fec5f18f4bffc98,Deep learning for sentiment analysis: A survey,"Deep learning has emerged as a powerful machine learning technique that learns multiple layers of representations or features of the data and produces state‐of‐the‐art prediction results. Along with the success of deep learning in many application domains, deep learning is also used in sentiment analysis in recent years. This paper gives an overview of deep learning and then provides a comprehensive survey of its current applications in sentiment analysis.",2018,151,946,41,True,Computer Science,Mathematics,50081327,Lei Zhang,1717480.0,Shuai Wang,145321667.0,B. Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d64a0840bea331caf9b1a61611b8eb8d15ec4f9f,https://www.semanticscholar.org/paper/d64a0840bea331caf9b1a61611b8eb8d15ec4f9f,Consumer Credit Risk Models Via Machine-Learning Algorithms,"We apply machine-learning techniques to construct nonlinear nonparametric forecasting models of consumer credit risk. By combining customer transactions and credit bureau data from January 2005 to April 2009 for a sample of a major commercial bank's customers, we are able to construct out-of-sample forecasts that significantly improve the classification rates of credit-card-holder delinquencies and defaults, with linear regression R2's of forecasted/realized delinquencies of 85%. Using conservative assumptions for the costs and benefits of cutting credit lines based on machine-learning forecasts, we estimate the cost savings to range from 6% to 25% of total losses. Moreover, the time-series patterns of estimated delinquency rates from this model over the course of the recent financial crisis suggest that aggregated consumer credit-risk analytics may have important applications in forecasting systemic risk.",2010,46,481,17,True,Economics,,5083642,A. Khandani,10800713.0,A. Kim,118887609.0,A. Lo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b87c0cf95208caacb025bf87d9ba451a87aacaca,https://www.semanticscholar.org/paper/b87c0cf95208caacb025bf87d9ba451a87aacaca,Machine Health Monitoring Using Local Feature-Based Gated Recurrent Unit Networks,"In modern industries, machine health monitoring systems (MHMS) have been applied wildly with the goal of realizing predictive maintenance including failures tracking, downtime reduction, and assets preservation. In the era of big machinery data, data-driven MHMS have achieved remarkable results in the detection of faults after the occurrence of certain failures (diagnosis) and prediction of the future working conditions and the remaining useful life (prognosis). The numerical representation for raw sensory data is the key stone for various successful MHMS. Conventional methods are the labor-extensive as they usually depend on handcrafted features, which require expert knowledge. Inspired by the success of deep learning methods that redefine representation learning from raw data, we propose local feature-based gated recurrent unit (LFGRU) networks. It is a hybrid approach that combines handcrafted feature design with automatic feature learning for machine health monitoring. First, features from windows of input time series are extracted. Then, an enhanced bidirectional GRU network is designed and applied on the generated sequence of local features to learn the representation. A supervised learning layer is finally trained to predict machine condition. Experiments on three machine health monitoring tasks: tool wear prediction, gearbox fault diagnosis, and incipient bearing fault detection verify the effectiveness and generalization of the proposed LFGRU.",2018,41,412,23,False,Computer Science,Engineering,49832912,Rui Zhao,2047932486.0,Dongzhe Wang,35374692.0,Ruqiang Yan,144067957.0,K. Mao,40592209.0,Fei Shen,,,49605588.0,Jinjiang Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fd62a4d907ff9a98cd69926b7dd72cb980713715,https://www.semanticscholar.org/paper/fd62a4d907ff9a98cd69926b7dd72cb980713715,Deep Learning in Remote Sensing: A Comprehensive Review and List of Resources,"Central to the looming paradigm shift toward data-intensive science, machine-learning techniques are becoming increasingly important. In particular, deep learning has proven to be both a major breakthrough and an extremely powerful tool in many fields. Shall we embrace deep learning as the key to everything? Or should we resist a black-box solution? These are controversial issues within the remote-sensing community. In this article, we analyze the challenges of using deep learning for remote-sensing data analysis, review recent advances, and provide resources we hope will make deep learning in remote sensing seem ridiculously simple. More importantly, we encourage remote-sensing scientists to bring their expertise into deep learning and use it as an implicit general model to tackle unprecedented, large-scale, influential challenges, such as climate change and urbanization.",2017,216,1272,42,True,Computer Science,,46875441,Xiaoxiang Zhu,2977931.0,D. Tuia,35041003.0,Lichao Mou,39943835.0,Gui-Song Xia,9802604.0,Liang-pei Zhang,,,2152480620.0,Feng Xu,2033171.0,F. Fraundorfer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
