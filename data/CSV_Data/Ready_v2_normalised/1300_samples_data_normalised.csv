paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,fieldsOfStudy/1,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,fieldsOfStudy/2,fieldsOfStudy/3,authors/5/authorId,authors/5/name,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,authors/16/authorId,authors/16/name,authors/17/authorId,authors/17/name,authors/18/authorId,authors/18/name,authors/19/authorId,authors/19/name,authors/20/authorId,authors/20/name,authors/21/authorId,authors/21/name,authors/22/authorId,authors/22/name,authors/23/authorId,authors/23/name,authors/24/authorId,authors/24/name,authors/25/authorId,authors/25/name,authors/26/authorId,authors/26/name,authors/27/authorId,authors/27/name,authors/28/authorId,authors/28/name,authors/29/authorId,authors/29/name,authors/30/authorId,authors/30/name,authors/31/authorId,authors/31/name,authors/32/authorId,authors/32/name,authors/33/authorId,authors/33/name,authors/34/authorId,authors/34/name,authors/35/authorId,authors/35/name
ab1258b286c729e2c0cd2546ec0f79ad21bd3883,https://www.semanticscholar.org/paper/ab1258b286c729e2c0cd2546ec0f79ad21bd3883,Quantum Boltzmann Machine,"Inspired by the success of Boltzmann Machines based on classical Boltzmann distribution, we propose a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian. Due to the non-commutative nature of quantum mechanics, the training process of the Quantum Boltzmann Machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors like D-Wave for QBM training and application.",2016,41,331,17,True,Physics,Computer Science,143707843,M. Amin,8381113.0,E. Andriyash,34965728.0,J. Rolfe,103330749.0,B. Kulchytskyy,3422513.0,R. Melko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
127c1cb96b73399f429de553d315561504cc7cd4,https://www.semanticscholar.org/paper/127c1cb96b73399f429de553d315561504cc7cd4,On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection,"Humans are the final decision makers in critical tasks that involve ethical and legal concerns, ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although machine learning models can sometimes achieve impressive performance in these tasks, these tasks are not amenable to full automation. To realize the potential of machine learning for improving human decisions, it is important to understand how assistance from machine learning models affects human performance and human agency. In this paper, we use deception detection as a testbed and investigate how we can harness explanations and predictions of machine learning models to improve human performance while retaining human agency. We propose a spectrum between full human agency and full automation, and develop varying levels of machine assistance along the spectrum that gradually increase the influence of machine predictions. We find that without showing predicted labels, explanations alone slightly improve human performance in the end task. In comparison, human performance is greatly improved by showing predicted labels (>20% relative improvement) and can be further improved by explicitly suggesting strong machine performance. Interestingly, when predicted labels are shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement of strong machine performance. Our results demonstrate a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff.",2018,82,159,18,True,Computer Science,Physics,120801533,Vivian Lai,40348583.0,Chenhao Tan,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a16e484824b2580e092c985aa659e8680aeda5ee,https://www.semanticscholar.org/paper/a16e484824b2580e092c985aa659e8680aeda5ee,Shallow Semantic Parsing using Support Vector Machines,"In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.",2004,22,447,52,False,Computer Science,,1735131,Sameer Pradhan,1866226.0,Wayne H. Ward,2483422.0,K. Hacioglu,10796472.0,James H. Martin,1746807.0,Dan Jurafsky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7c7ab73469c25437332f5c1c1c5cb67c7b2f0855,https://www.semanticscholar.org/paper/7c7ab73469c25437332f5c1c1c5cb67c7b2f0855,Low-Shot Visual Recognition by Shrinking and Hallucinating Features,"Low-shot visual learning–the ability to recognize novel object categories from very few examples–is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose (1) representation regularization techniques, and (2) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3× on the challenging ImageNet dataset.",2016,51,569,60,True,Computer Science,,1790580,Bharath Hariharan,2983898.0,Ross B. Girshick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d76c07211479e233f7c6a6f32d5346c983c5598f,https://www.semanticscholar.org/paper/d76c07211479e233f7c6a6f32d5346c983c5598f,Multi-task Sequence to Sequence Learning,"Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.",2015,34,714,71,False,Computer Science,Mathematics,1707242,Minh-Thang Luong,2827616.0,Quoc V. Le,1701686.0,Ilya Sutskever,1689108.0,Oriol Vinyals,40527594.0,Lukasz Kaiser,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1f7ed2ebc6b641e3804cf177fd42a1b8de95003b,https://www.semanticscholar.org/paper/1f7ed2ebc6b641e3804cf177fd42a1b8de95003b,Machine-learning techniques and their applications in manufacturing,"Abstract Machine learning is concerned with enabling computer programs automatically to improve their performance at some tasks through experience. Manufacturing is an area where the application of machine learning can be very fruitful. However, little has been published about the use of machine-learning techniques in the manufacturing domain. This paper evaluates several machine-learning techniques and examines applications in which they have been successfully deployed. Special attention is given to inductive learning, which is among the most mature of the machine-learning approaches currently available. Current trends and recent developments in machine-learning research are also discussed. The paper concludes with a summary of some of the key research issues in machine learning.",2005,157,97,3,False,Engineering,,2082214471,D. T. Pham,2002947.0,A. Afify,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4f71ab367eb37cfd145d41327f7bb14077e5e7c5,https://www.semanticscholar.org/paper/4f71ab367eb37cfd145d41327f7bb14077e5e7c5,Deep Learning for Hyperspectral Image Classification: An Overview,"Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral–spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.",2019,118,484,17,True,Engineering,Computer Science,2116066317,Shutao Li,145273596.0,Weiwei Song,38140728.0,Leyuan Fang,2597809.0,Yushi Chen,2370080.0,Pedram Ghamisi,,,1682001.0,J. Benediktsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
33fd991b4e05becfdec93585d25b6369a0519133,https://www.semanticscholar.org/paper/33fd991b4e05becfdec93585d25b6369a0519133,Trading convexity for scalability,"Convex learning algorithms, such as Support Vector Machines (SVMs), are often seen as highly desirable because they offer strong practical properties and are amenable to theoretical analysis. However, in this work we show how non-convexity can provide scalability advantages over convexity. We show how concave-convex programming can be applied to produce (i) faster SVMs where training errors are no longer support vectors, and (ii) much faster Transductive SVMs.",2006,54,380,47,True,Computer Science,Mathematics,2939803,Ronan Collobert,50095217.0,Fabian H Sinz,145183709.0,J. Weston,52184096.0,L. Bottou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f6154535699c65633243c482d2b97d4b66036633,https://www.semanticscholar.org/paper/f6154535699c65633243c482d2b97d4b66036633,Learning Features of Music from Scratch,"This paper introduces a new large-scale music dataset, MusicNet, to serve as a source of supervision and evaluation of machine learning methods for music research. MusicNet consists of hundreds of freely-licensed classical music recordings by 10 composers, written for 11 instruments, together with instrument/note annotations resulting in over 1 million temporal labels on 34 hours of chamber music performances under various studio and microphone conditions. 
The paper defines a multi-label classification task to predict notes in musical recordings, along with an evaluation protocol, and benchmarks several machine learning architectures for this task: i) learning from spectrogram features; ii) end-to-end learning with a neural net; iii) end-to-end learning with a convolutional neural net. These experiments show that end-to-end models trained for note prediction learn frequency selective filters as a low-level representation of audio.",2016,39,138,27,False,Computer Science,Mathematics,50343904,John Thickstun,1753355.0,Z. Harchaoui,144695232.0,S. Kakade,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3fc9cff6ad55986de180204b98613af42f8ac37d,https://www.semanticscholar.org/paper/3fc9cff6ad55986de180204b98613af42f8ac37d,Towards CRISP-ML(Q): A Machine Learning Process Model with Quality Assurance Methodology,"Machine learning is an established and frequently used technique in industry and academia, but a standard process model to improve success and efficiency of machine learning applications is still missing. Project organizations and machine learning practitioners face manifold challenges and risks when developing machine learning applications and have a need for guidance to meet business expectations. This paper therefore proposes a process model for the development of machine learning applications, covering six phases from defining the scope to maintaining the deployed machine learning application. Business and data understanding are executed simultaneously in the first phase, as both have considerable impact on the feasibility of the project. The next phases are comprised of data preparation, modeling, evaluation, and deployment. Special focus is applied to the last phase, as a model running in changing real-time environments requires close monitoring and maintenance to reduce the risk of performance degradation over time. With each task of the process, this work proposes quality assurance methodology that is suitable to address challenges in machine learning development that are identified in the form of risks. The methodology is drawn from practical experience and scientific literature, and has proven to be general and stable. The process model expands on CRISP-DM, a data mining process model that enjoys strong industry support, but fails to address machine learning specific tasks. The presented work proposes an industry- and application-neutral process model tailored for machine learning applications with a focus on technical tasks for quality assurance.",2020,181,47,5,True,Computer Science,Mathematics,2066100848,Stefan Studer,4667865.0,T. Bui,40117299.0,C. Drescher,3332452.0,A. Hanuschkin,2065341843.0,Ludwig Winkler,,,2050675924.0,S. Peters,2113612432.0,Klaus-Robert Müller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3893783a815a5d46a26f154683115d4195b1b004,https://www.semanticscholar.org/paper/3893783a815a5d46a26f154683115d4195b1b004,Preparing Medical Imaging Data for Machine Learning.,"Artificial intelligence (AI) continues to garner substantial interest in medical imaging. The potential applications are vast and include the entirety of the medical imaging life cycle from image creation to diagnosis to outcome prediction. The chief obstacles to development and clinical implementation of AI algorithms include availability of sufficiently large, curated, and representative training data that includes expert labeling (eg, annotations). Current supervised AI methods require a curation process for data to optimally train, validate, and test algorithms. Currently, most research groups and industry have limited data access based on small sample sizes from small geographic areas. In addition, the preparation of data is a costly and time-intensive process, the results of which are algorithms with limited utility and poor generalization. In this article, the authors describe fundamental steps for preparing medical imaging data in AI algorithm development, explain current limitations to data curation, and explore new approaches to address the problem of data availability.",2020,132,280,7,True,Medicine,,4127337,M. Willemink,70244729.0,Wojciech A Koszek,1498054949.0,Cailin Hardell,2118432480.0,Jie Wu,1733946.0,D. Fleischmann,,,144953202.0,H. Harvey,2684053.0,L. Folio,144838131.0,R. Summers,143648587.0,D. Rubin,4204731.0,M. Lungren,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
160a4786dd643d9f758b9cc0758bdd2581524941,https://www.semanticscholar.org/paper/160a4786dd643d9f758b9cc0758bdd2581524941,Machine learning for detection and diagnosis of disease.,"Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.",2006,152,293,7,False,Computer Science,Medicine,1812793,P. Sajda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
18cb13754640a0bbdef07cf8efcfc762a27afa6a,https://www.semanticscholar.org/paper/18cb13754640a0bbdef07cf8efcfc762a27afa6a,Intelligent Hybrid Vehicle Power Control—Part I: Machine Learning of Optimal Vehicle Power,"In this series of two papers, we present our research on intelligent energy management for hybrid electric vehicles (HEVs). These two papers cover the modeling of power flow in HEVs, the mathematical background of optimization in energy management in HEVs, a machine learning framework that combines dynamic programming (DP) with machine learning to learn about roadway-type- and traffic-congestion-level-specific energy optimization, machine learning algorithms, and real-time quasi-optimal control of energy flow in an HEV. This first paper presents our research on machine learning for optimal energy management in HEVs. We will present a machine learning framework ML_EMO_HEV developed for the optimization of energy management in an HEV, machine learning algorithms for predicting driving environments, and the generation of an optimal power split for a given driving environment. Experiments are conducted based on a simulated Ford Escape Hybrid vehicle model provided by Argonne National Laboratory's Powertrain Systems Analysis Toolkit (PSAT). Based on the experimental results on the test data, we can conclude that the neural networks trained under the ML_EMO_HEV framework are effective in predicting roadway type and traffic congestion levels, predicting driving trends, and learning optimal engine speed and optimal battery power from DP.",2012,26,152,3,False,Engineering,Computer Science,9491649,Y. Murphey,2087950.0,Jungme Park,2109331148.0,ZhiHang Chen,2157249.0,M. Kuang,144347027.0,M. Masrur,,,31639858.0,A. Phillips,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2550d4f922f9652c4755695c822e492161524bce,https://www.semanticscholar.org/paper/2550d4f922f9652c4755695c822e492161524bce,DeepDefense: Identifying DDoS Attack via Deep Learning,"Distributed Denial of Service (DDoS) attacks grow rapidly and become one of the fatal threats to the Internet. Automatically detecting DDoS attack packets is one of the main defense mechanisms. Conventional solutions monitor network traffic and identify attack activities from legitimate network traffic based on statistical divergence. Machine learning is another method to improve identifying performance based on statistical features. However, conventional machine learning techniques are limited by the shallow representation models. In this paper, we propose a deep learning based DDoS attack detection approach (DeepDefense). Deep learning approach can automatically extract high-level features from low-level ones and gain powerful representation and inference. We design a recurrent deep neural network to learn patterns from sequences of network traffic and trace network attack activities. The experimental results demonstrate a better performance of our model compared with conventional machine learning models. We reduce the error rate from 7.517% to 2.103% compared with conventional machine learning method in the larger data set.",2017,37,178,19,False,Computer Science,,2115844647,Xiaoyong Yuan,1838103.0,Chuanhuang Li,2108672978.0,Xiaolin Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d4599b177559dd5ede4dda9d6d96aa149fc71942,https://www.semanticscholar.org/paper/d4599b177559dd5ede4dda9d6d96aa149fc71942,An Efficient Learning Procedure for Deep Boltzmann Machines,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.",2012,72,416,46,True,Medicine,Computer Science,145124475,R. Salakhutdinov,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
13b3a1e7e0bd80bf38b444ce1568213ebe98d0df,https://www.semanticscholar.org/paper/13b3a1e7e0bd80bf38b444ce1568213ebe98d0df,Large Scale Online Learning of Image Similarity Through Ranking,"Learning a measure of similarity between pairs of objects is an important generic problem in machine learning. It is particularly useful in large scale applications like searching for an image that is similar to a given image or finding videos that are relevant to a given video. In these tasks, users look for objects that are not only visually similar but also semantically related to a given object. Unfortunately, the approaches that exist today for learning such semantic similarity do not scale to large data sets. This is both because typically their CPU and storage requirements grow quadratically with the sample size, and because many methods impose complex positivity constraints on the space of learned similarity functions. The current paper presents OASIS, an Online Algorithm for Scalable Image Similarity learning that learns a bilinear similarity measure over sparse representations. OASIS is an online dual approach using the passive-aggressive family of learning algorithms with a large margin criterion and an efficient hinge loss cost. Our experiments show that OASIS is both fast and accurate at a wide range of scales: for a data set with thousands of images, it achieves better results than existing state-of-the-art methods, while being an order of magnitude faster. For large, web scale, data sets, OASIS can be trained on more than two million images from 150K text queries within 3 days on a single CPU. On this large scale data set, human evaluations showed that 35% of the ten nearest neighbors of a given test image, as found by OASIS, were semantically relevant to that image. This suggests that query independent similarity could be accurately learned even for large scale data sets that could not be handled before.",2009,41,710,80,True,Computer Science,,1732280,Gal Chechik,2112448321.0,Varun Sharma,2304764.0,Uri Shalit,1751569.0,Samy Bengio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b516eeb62d112f5dffd60336f28cfaf5200a29bf,https://www.semanticscholar.org/paper/b516eeb62d112f5dffd60336f28cfaf5200a29bf,Machine learning in autistic spectrum disorder behavioral research: A review and ways forward,"ABSTRACT Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals.",2019,62,153,6,False,Psychology,Medicine,1692763,F. Thabtah,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52d97890dbc290108136739ec2afe0f2b6c4f570,https://www.semanticscholar.org/paper/52d97890dbc290108136739ec2afe0f2b6c4f570,Freeze-Thaw Bayesian Optimization,"In this paper we develop a dynamic form of Bayesian optimization for machine learning models with the goal of rapidly finding good hyperparameter settings. Our method uses the partial information gained during the training of a machine learning model in order to decide whether to pause training and start a new model, or resume the training of a previously-considered model. We specifically tailor our method to machine learning problems by developing a novel positive-definite covariance kernel to capture a variety of training curves. Furthermore, we develop a Gaussian process prior that scales gracefully with additional temporal observations. Finally, we provide an information-theoretic framework to automate the decision process. Experiments on several common machine learning models show that our approach is extremely effective in practice.",2014,28,211,26,False,Computer Science,Mathematics,1754860,Kevin Swersky,144108062.0,Jasper Snoek,1722180.0,Ryan P. Adams,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d480f90ec1d48309850c1b92b2053990a60522a8,https://www.semanticscholar.org/paper/d480f90ec1d48309850c1b92b2053990a60522a8,MLI: An API for Distributed Machine Learning,"MLI is an Application Programming Interface designed to address the challenges of building Machine Learning algorithms in a distributed setting based on data-centric computing. Its primary goal is to simplify the development of high-performance, scalable, distributed algorithms. Our initial results show that, relative to existing systems, this interface can be used to build distributed implementations of a wide variety of common Machine Learning algorithms with minimal complexity and highly competitive performance and scalability.",2013,30,194,23,True,Computer Science,Mathematics,144752747,Evan R. Sparks,145532827.0,Ameet S. Talwalkar,145260024.0,Virginia Smith,2983690.0,Jey Kottalam,40201504.0,Xinghao Pan,,,49988044.0,Joseph E. Gonzalez,143666627.0,M. Franklin,1694621.0,Michael I. Jordan,1746961.0,Tim Kraska,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
df2a7756382540e92895f10703cec32d50c4f316,https://www.semanticscholar.org/paper/df2a7756382540e92895f10703cec32d50c4f316,Fast and accurate modeling of molecular atomization energies with machine learning.,"We introduce a machine learning model to predict atomization energies of a diverse set of organic molecules, based on nuclear charges and atomic positions only. The problem of solving the molecular Schrödinger equation is mapped onto a nonlinear statistical regression problem of reduced complexity. Regression models are trained on and compared to atomization energies computed with hybrid density-functional theory. Cross validation over more than seven thousand organic molecules yields a mean absolute error of ∼10  kcal/mol. Applicability is demonstrated for the prediction of molecular atomization potential energy curves.",2011,23,1191,26,True,Physics,Mathematics,48041657,M. Rupp,2462983.0,A. Tkatchenko,145034054.0,K. Müller,7847508.0,O. A. von Lilienfeld,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5091316bb1c6db6c6a813f4391911a5c311fdfe0,https://www.semanticscholar.org/paper/5091316bb1c6db6c6a813f4391911a5c311fdfe0,“Why Should I Trust You?”: Explaining the Predictions of Any Classifier,"Despite widespread adoption, machine learning models remain mostly black boxes. Understanding the reasons behind predictions is, however, quite important in assessing trust, which is fundamental if one plans to take action based on a prediction, or when choosing whether to deploy a new model. Such understanding also provides insights into the model, which can be used to transform an untrustworthy model or prediction into a trustworthy one. In this work, we propose LIME, a novel explanation technique that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable model locally varound the prediction. We also propose a method to explain models by presenting representative individual predictions and their explanations in a non-redundant way, framing the task as a submodular optimization problem. We demonstrate the flexibility of these methods by explaining different models for text (e.g. random forests) and image classification (e.g. neural networks). We show the utility of explanations via novel experiments, both simulated and with human subjects, on various scenarios that require trust: deciding if one should trust a prediction, choosing between models, improving an untrustworthy classifier, and identifying why a classifier should not be trusted.",2016,41,8857,1299,True,Computer Science,Mathematics,78846919,Marco Tulio Ribeiro,34650964.0,Sameer Singh,1730156.0,Carlos Guestrin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2a04511d1f95e8663429bcadb46083e10d3458eb,https://www.semanticscholar.org/paper/2a04511d1f95e8663429bcadb46083e10d3458eb,Integrating machine learning and workflow management to support acquisition and adaptation of workflow models,"Current workflow management systems (WFMS) offer little aid for the acquisition of workflow models and their adaptation to changing requirements. To support these activities we propose to integrate machine learning and workflow management. This enables an inductive approach to workflow acquisition and adaptation by processing traces of manually enacted workflows. We present a machine learning component that combines two different machine learning algorithms. In this paper we focus mainly on the first one, which induces the structure of the workflow, based on the induction of hidden markov models. The second algorithm, a standard decision rule induction algorithm, induces transition conditions. The main concepts have been implemented in a prototype, which we have validated using artificial process traces. The induced workflow models can be imported by the business process management system ADONIS.",1998,34,239,14,False,Computer Science,,2325415,J. Herbst,145328956.0,D. Karagiannis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
792d90c2ed5ebbe050bd80b9c865dc416b574c09,https://www.semanticscholar.org/paper/792d90c2ed5ebbe050bd80b9c865dc416b574c09,Machine Learning–Based Model for Prediction of Outcomes in Acute Stroke,"Background and Purpose— The prediction of long-term outcomes in ischemic stroke patients may be useful in treatment decisions. Machine learning techniques are being increasingly adapted for use in the medical field because of their high accuracy. This study investigated the applicability of machine learning techniques to predict long-term outcomes in ischemic stroke patients. Methods— This was a retrospective study using a prospective cohort that enrolled patients with acute ischemic stroke. Favorable outcome was defined as modified Rankin Scale score 0, 1, or 2 at 3 months. We developed 3 machine learning models (deep neural network, random forest, and logistic regression) and compared their predictability. To evaluate the accuracy of the machine learning models, we also compared them to the Acute Stroke Registry and Analysis of Lausanne (ASTRAL) score. Results— A total of 2604 patients were included in this study, and 2043 (78%) of them had favorable outcomes. The area under the curve for the deep neural network model was significantly higher than that of the ASTRAL score (0.888 versus 0.839; P<0.001), while the areas under the curves of the random forest (0.857; P=0.136) and logistic regression (0.849; P=0.413) models were not significantly higher than that of the ASTRAL score. Using only the 6 variables that are used for the ASTRAL score, the performance of the machine learning models did not significantly differ from that of the ASTRAL score. Conclusions— Machine learning algorithms, particularly the deep neural network, can improve the prediction of long-term outcomes in ischemic stroke patients.",2019,9,154,3,True,Medicine,,5980849,Joonnyung Heo,32745548.0,Jihoon G Yoon,4994655.0,Hyungjong Park,34471989.0,Young Dae Kim,2185414.0,H. S. Nam,,,34508977.0,J. Heo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3eaf21ba64b0775b68ebadbafe064bb619b75ca6,https://www.semanticscholar.org/paper/3eaf21ba64b0775b68ebadbafe064bb619b75ca6,Adaptive Sparseness for Supervised Learning,"The goal of supervised learning is to infer a functional mapping based on a set of training examples. To achieve good generalization, it is necessary to control the ""complexity"" of the learned function. In Bayesian approaches, this is done by adopting a prior for the parameters of the function being learned. We propose a Bayesian approach to supervised learning, which leads to sparse solutions; that is, in which irrelevant parameters are automatically set exactly to zero. Other ways to obtain sparse classifiers (such as Laplacian priors, support vector machines) involve (hyper)parameters which control the degree of sparseness of the resulting classifiers; these parameters have to be somehow adjusted/estimated from the training data. In contrast, our approach does not involve any (hyper)parameters to be adjusted or estimated. This is achieved by a hierarchical-Bayes interpretation of the Laplacian prior, which is then modified by the adoption of a Jeffreys' noninformative hyperprior. Implementation is carried out by an expectation-maximization (EM) algorithm. Experiments with several benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms SVMs and performs competitively with the best alternative techniques, although it involves no tuning or adjustment of sparseness-controlling hyperparameters.",2003,40,565,54,True,Computer Science,,34659351,Mário A. T. Figueiredo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6d0fb3a5ad66c83e9f2ef066d83f0ae23180da41,https://www.semanticscholar.org/paper/6d0fb3a5ad66c83e9f2ef066d83f0ae23180da41,Distributed GraphLab : A Framework for Machine Learning and Data Mining in the Cloud,"While high-level data parallel frameworks, like MapReduce, simplify the design and implementation of large-scale data processing systems, they do not naturally or efficiently support many important data mining and machine learning algorithms and can lead to inefficient learning systems. To help fill this critical void, we introduced the GraphLab abstraction which naturally expresses asynchronous, dynamic, graph-parallel computation while ensuring data consistency and achieving a high degree of parallel performance in the shared-memory setting. In this paper, we extend the GraphLab framework to the substantially more challenging distributed setting while preserving strong data consistency guarantees. We develop graph based extensions to pipelined locking and data versioning to reduce network congestion and mitigate the effect of network latency. We also introduce fault tolerance to the GraphLab abstraction using the classic Chandy-Lamport snapshot algorithm and demonstrate how it can be easily implemented by exploiting the GraphLab abstraction itself. Finally, we evaluate our distributed implementation of the GraphLab abstraction on a large Amazon EC2 deployment and show 1-2 orders of magnitude performance gains over Hadoop-based implementations.",2012,40,1292,208,False,,,1680638,Y. Low,2119113835.0,Joseph Gonzalez,1717990.0,Aapo Kyrola,1741745.0,D. Bickson,1730156.0,Carlos Guestrin,,,1695576.0,J. Hellerstein,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84534c6fa2dab4768617205b302bfbce9b3bb376,https://www.semanticscholar.org/paper/84534c6fa2dab4768617205b302bfbce9b3bb376,A Review of Ensemble Methods in Bioinformatics,"Ensemble learning is an intensively studies technique in machine learning and pattern recognition. Recent work in computational biology has seen an increasing use of ensemble learning methods due to their unique advantages in dealing with small sample size, high-dimensionality, and complexity data structures. The aim of this article is two-fold. First, it is to provide a review of the most widely used ensemble learning methods and their application in various bioinformatics problems, including the main topics of gene expression, mass spectrometry-based proteomics, gene-gene interaction identification from genome-wide association studies, and prediction of regulatory elements from DNA and protein sequences. Second, we try to identify and summarize future trends of ensemble methods in bioinformatics. Promising directions such as ensemble of support vector machine, meta-ensemble, and ensemble based feature selection are discussed.",2010,110,404,12,False,Computer Science,,1745108,Pengyi Yang,119607780.0,Y. H. Yang,1776472.0,B. Zhou,9392149.0,Albert Y. Zomaya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ccf6a69a7f33bcf052aa7def176d3b9de495beb7,https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7,Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",2016,47,1842,279,False,Computer Science,Mathematics,2843215,Tolga Bolukbasi,2782886.0,Kai-Wei Chang,145085305.0,James Y. Zou,1699322.0,Venkatesh Saligrama,2186481.0,A. Kalai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751,https://www.semanticscholar.org/paper/f2ac2a3fd7b341f2b1be752b4dd46ed9abcf0751,Deep Reinforcement Learning,"We discuss deep reinforcement learning in an overview style. We draw a big picture, filled with details. We discuss six core elements, six important mechanisms, and twelve applications, focusing on contemporary work, and in historical contexts. We start with background of artificial intelligence, machine learning, deep learning, and reinforcement learning (RL), with resources. Next we discuss RL core elements, including value function, policy, reward, model, exploration vs. exploitation, and representation. Then we discuss important mechanisms for RL, including attention and memory, unsupervised learning, hierarchical RL, multi-agent RL, relational RL, and learning to learn. After that, we discuss RL applications, including games, robotics, natural language processing (NLP), computer vision, finance, business management, healthcare, education, energy, transportation, computer systems, and, science, engineering, and art. Finally we summarize briefly, discuss challenges and opportunities, and close with an epilogue.",2018,890,261,17,True,Computer Science,Mathematics,2276894,Yuxi Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3b9f6a6901db3bc39be9e147f6efe66e40f6a8f7,https://www.semanticscholar.org/paper/3b9f6a6901db3bc39be9e147f6efe66e40f6a8f7,The practice on using machine learning for network anomaly intrusion detection,"Machine learning is regarded as an effective tool utilized by intrusion detection system (IDS) to detect abnormal activities from network traffic. In particular, neural networks, support vector machines (SVM) and decision trees are three significant and popular schemes borrowed from the machine learning community into intrusion detection in recent academic research. However, these machine learning schemes are rarely employed in large-scale practical settings. In this paper, we implement and compare machine learning schemes of neural networks, SVM and decision trees in a uniform environment with the purpose of exploring the practice and issues of using these approaches in detecting abnormal behaviors. With the analysis of experimental results, we claim that the real performance of machine learning algorithms depends heavily on practical context. Therefore, the machine learning approaches are supposed to be applied in an appropriate way in terms of the actual settings.",2011,22,51,0,False,Computer Science,,3592010,Yuxin Meng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6989e13df80edfc6e638e8d8502cb0739d494ca6,https://www.semanticscholar.org/paper/6989e13df80edfc6e638e8d8502cb0739d494ca6,Machine Learning in Compiler Optimization,"In the last decade, machine-learning-based compilation has moved from an obscure research niche to a mainstream activity. In this paper, we describe the relationship between machine learning and compiler optimization and introduce the main concepts of features, models, training, and deployment. We then provide a comprehensive survey and provide a road map for the wide variety of different research areas. We conclude with a discussion on open issues in the area and potential research directions. This paper provides both an accessible introduction to the fast moving area of machine-learning-based compilation and a detailed bibliography of its main achievements.",2018,200,128,8,True,Computer Science,,12230182,Zheng Wang,1401533251.0,M. O’Boyle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c48e85a7a4ed7895096ae074a6c544415e6c4b85,https://www.semanticscholar.org/paper/c48e85a7a4ed7895096ae074a6c544415e6c4b85,RAPID: Rating Pictorial Aesthetics using Deep Learning,"Effective visual features are essential for computational aesthetic quality rating systems. Existing methods used machine learning and statistical modeling techniques on handcrafted features or generic image descriptors. A recently-published large-scale dataset, the AVA dataset, has further empowered machine learning based approaches. We present the RAPID (RAting PIctorial aesthetics using Deep learning) system, which adopts a novel deep neural network approach to enable automatic feature learning. The central idea is to incorporate heterogeneous inputs generated from the image, which include a global view and a local view, and to unify the feature learning and classifier training using a double-column deep convolutional neural network. In addition, we utilize the style attributes of images to help improve the aesthetic quality categorization accuracy. Experimental results show that our approach significantly outperforms the state of the art on the AVA dataset.",2014,34,325,50,False,Computer Science,,145574672,Xin Lu,145527707.0,Zhe L. Lin,41151701.0,Hailin Jin,1706007.0,Jianchao Yang,2116439116.0,J. Z. Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
400081ba358663ca24365044e5e818638c439e1d,https://www.semanticscholar.org/paper/400081ba358663ca24365044e5e818638c439e1d,Machine Learning DDoS Detection for Consumer Internet of Things Devices,"An increasing number of Internet of Things (IoT) devices are connecting to the Internet, yet many of these devices are fundamentally insecure, exposing the Internet to a variety of attacks. Botnets such as Mirai have used insecure consumer IoT devices to conduct distributed denial of service (DDoS) attacks on critical Internet infrastructure. This motivates the development of new techniques to automatically detect consumer IoT attack traffic. In this paper, we demonstrate that using IoT-specific network behaviors (e.g., limited number of endpoints and regular time intervals between packets) to inform feature selection can result in high accuracy DDoS detection in IoT network traffic with a variety of machine learning algorithms, including neural networks. These results indicate that home gateway routers or other network middleboxes could automatically detect local IoT device sources of DDoS attacks using low-cost machine learning algorithms and traffic data that is flow-based and protocol-agnostic.",2018,23,368,32,True,Computer Science,,145488660,Rohan Doshi,11669550.0,Noah J. Apthorpe,1800154.0,N. Feamster,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bb35ef89addbbc28d960bc0cab70d8a29fdf6eee,https://www.semanticscholar.org/paper/bb35ef89addbbc28d960bc0cab70d8a29fdf6eee,A Survey on Multi-Task Learning,"Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.",2017,277,993,62,True,Computer Science,Mathematics,46867608,Yu Zhang,152290618.0,Qiang Yang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dc0c84b7c5e6521216da789f8171544709120cf0,https://www.semanticscholar.org/paper/dc0c84b7c5e6521216da789f8171544709120cf0,Opportunities and obstacles for deep learning in biology and medicine,"Deep learning, which describes a class of machine learning algorithms, has recently showed impressive results across a variety of domains. Biology and medicine are data rich, but the data are complex and often ill-understood. Problems of this nature may be particularly well-suited to deep learning techniques. We examine applications of deep learning to a variety of biomedical problems -- patient classification, fundamental biological processes, and treatment of patients -- to predict whether deep learning will transform these tasks or if the biomedical sphere poses unique challenges. We find that deep learning has yet to revolutionize or definitively resolve any of these problems, but promising advances have been made on the prior state of the art. Even when improvement over a previous baseline has been modest, we have seen signs that deep learning methods may speed or aid human investigation. More work is needed to address concerns related to interpretability and how to best model each problem. Furthermore, the limited amount of labeled data for training presents problems in some domains, as can legal and privacy constraints on work with sensitive health records. Nonetheless, we foresee deep learning powering changes at the bench and bedside with the potential to transform several areas of biology and medicine.",2017,628,1219,18,True,Biology,Computer Science,5926615,T. Ching,1868440.0,Daniel S. Himmelstein,1402651503.0,Brett K. Beaulieu-Jones,32985843.0,Alexandr A Kalinin,48154546.0,Brian T Do,Medicine,,3738725.0,Gregory P. Way,137381074.0,E. Ferrero,2236448.0,P. Agapow,40862218.0,M. Zietz,35290352.0,M. M. Hoffman,1993128574.0,W. Xie,6997856.0,G. Rosen,6580840.0,Benjamin J. Lengerich,39246361.0,Johnny Israeli,3369052.0,Jack Lanchantin,8230326.0,S. Woloszynek,1721012.0,Anne E Carpenter,3407268.0,Avanti Shrikumar,1807918.0,Jinbo Xu,30469485.0,Evan M. Cofer,39291675.0,C. A. Lavender,3178417.0,Srinivas C. Turaga,35786407.0,Amr M. Alexandari,144202084.0,Zhiyong Lu,145192391.0,David J Harris,1973239.0,D. DeCaprio,121817403.0,Yanjun Qi,2844479.0,A. Kundaje,2699239.0,Yifan Peng,3116087.0,L. K. Wiley,3451383.0,Marwin H. S. Segler,2831521.0,S. Boca,7946353.0,S. Joshua Joshua Swamidass,119855034.0,Austin Huang,2041407.0,A. Gitter,2104940.0,C. Greene
6db308779954a4006d87ce68148ff2aaa176dfb2,https://www.semanticscholar.org/paper/6db308779954a4006d87ce68148ff2aaa176dfb2,A Study of K-Nearest Neighbour as an Imputation Method,"Data quality is a major concern in Machine Learning and other correlated areas such as Knowledge Discovery from Databases (KDD). As most Machine Learning algorithms induce knowledge strictly from data, the quality of the knowledge extracted is largely determined by the quality of the underlying data. One relevant problem in data quality is the presence of missing data. Despite the frequent occurrence of missing data, many Machine Learning algorithms handle missing data in a rather naive way. Missing data treatment should be carefully thought, otherwise bias might be introduced into the knowledge induced. In this work, we analyse the use of the k-nearest neighbour as an imputation method. Imputation is a term that denotes a procedure that replaces the missing values in a data set by some plausible values. Our analysis indicates that missing data imputation based on the k-nearest neighbour algorithm can outperform the internal methods used by C4.5 and CN2 to treat missing data.",2002,16,307,19,False,Computer Science,,145666101,Gustavo E. A. P. A. Batista,1737677.0,M. C. Monard,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
02505d330224375113f2a905efdd867591c0d435,https://www.semanticscholar.org/paper/02505d330224375113f2a905efdd867591c0d435,Music Plus One and Machine Learning,"A system for musical accompaniment is presented in which a computer-driven orchestra follows and learns from a soloist in a concerto-like setting. The system is decomposed into three modules: the first computes a real-time score match using a hidden Markov model; the second generates the output audio by phase-vocoding a preexisting audio recording; the third provides a link between these two, by predicting future timing evolution using a Kalman filter-like model. Several examples are presented showing the system in action in diverse musical settings. Connections with machine learning are highlighted, showing current weaknesses and new possible directions.",2010,12,74,8,False,Computer Science,,145087434,C. Raphael,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f10e071292d593fef939e6ef4a59baf0bb3a6c2b,https://www.semanticscholar.org/paper/f10e071292d593fef939e6ef4a59baf0bb3a6c2b,Reinforcement Learning Neural Turing Machines,"The expressive power of a machine learning model is closely related to the number of sequential computational steps it can learn. For example, Deep Neural Networks have been more successful than shallow networks because they can perform a greater number of sequential computational steps (each highly parallel). The Neural Turing Machine (NTM) [8] is a model that can compactly express an even greater number of sequential computational steps, so it is even more powerful than a DNN. Its memory addressing operations are designed to be differentiable; thus the NTM can be trained with backpropagation. While differentiable memory is relatively easy to implement and train, it necessitates accessing the entire memory content at each computational step. This makes it difficult to implement a fast NTM. In this work, we use the Re inforce algorithm to learn where to access the memory, while using backpropagation to learn what to write to the memory. We call this model the RL-NTM. Reinforce allows our model to access a constant number of memory cells at each computational step, so its implementation can be faster. The RL-NTM is the first mo del that can, in principle, learn programs of unbounded running time. We successfully trained the RL-NTM to solve a number of algorithmic tasks that are simpler than the ones solvable by the fully differentiable NTM. As the RL-NTM is a fairly intricate model, we needed a method for verifying the correctness of our implementation. To do so, we developed a simple technique for numerically checking arbitrary implementations of models that use Reinforce, which may be of independent interest.",2015,28,170,9,False,Computer Science,,2563432,Wojciech Zaremba,1701686.0,Ilya Sutskever,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2de37e9d1046f18ab739202a1dfb7541fccadd7a,https://www.semanticscholar.org/paper/2de37e9d1046f18ab739202a1dfb7541fccadd7a,Beyond PageRank: machine learning for static ranking,"Since the publication of Brin and Page's paper on PageRank, many in the Web community have depended on PageRank for the static (query-independent) ordering of Web pages. We show that we can significantly outperform PageRank using features that are independent of the link structure of the Web. We gain a further boost in accuracy by using data on the frequency at which users visit Web pages. We use RankNet, a ranking machine learning algorithm, to combine these and other static features based on anchor text and domain characteristics. The resulting model achieves a static ranking pairwise accuracy of 67.3% (vs. 56.7% for PageRank or 50% for random).",2006,37,207,9,False,Computer Science,,144422314,Matthew Richardson,2054035504.0,Amit Prakash,145022783.0,E. Brill,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ad32f5295c2e0f7d077fee722aff2998e2af72e7,https://www.semanticscholar.org/paper/ad32f5295c2e0f7d077fee722aff2998e2af72e7,An Extensive Empirical Study of Feature Selection Metrics for Text Classification,"Machine learning for text classification is the cornerstone of document categorization, news filtering, document routing, and personalization. In text domains, effective feature selection is essential to make the learning task efficient and more accurate. This paper presents an empirical comparison of twelve feature selection methods (e.g. Information Gain) evaluated on a benchmark of 229 text classification problem instances that were gathered from Reuters, TREC, OHSUMED, etc. The results are analyzed from multiple goal perspectives-accuracy, F-measure, precision, and recall-since each is appropriate in different situations. The results reveal that a new feature selection metric we call 'Bi-Normal Separation' (BNS), outperformed the others by a substantial margin in most situations. This margin widened in tasks with high class skew, which is rampant in text classification problems and is particularly challenging for induction algorithms. A new evaluation methodology is offered that focuses on the needs of the data mining practitioner faced with a single dataset who seeks to choose one (or a pair of) metrics that are most likely to yield the best performance. From this perspective, BNS was the top single choice for all goals except precision, for which Information Gain yielded the best result most often. This analysis also revealed, for example, that Information Gain and Chi-Squared have correlated failures, and so they work poorly together. When choosing optimal pairs of metrics for each of the four performance goals, BNS is consistently a member of the pair---e.g., for greatest recall, the pair BNS + F1-measure yielded the best performance on the greatest number of tasks by a considerable margin.",2003,20,2795,213,False,Computer Science,,3332330,George Forman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5d5593a7b368fa4b3f3a8abf111ff17c261313c1,https://www.semanticscholar.org/paper/5d5593a7b368fa4b3f3a8abf111ff17c261313c1,Toward harnessing user feedback for machine learning,"There has been little research into how end users might be able to communicate advice to machine learning systems. If this resource--the users themselves--could somehow work hand-in-hand with machine learning systems, the accuracy of learning systems could be improved and the users' understanding and trust of the system could improve as well. We conducted a think-aloud study to see how willing users were to provide feedback and to understand what kinds of feedback users could give. Users were shown explanations of machine learning predictions and asked to provide feedback to improve the predictions. We found that users had no difficulty providing generous amounts of feedback. The kinds of feedback ranged from suggestions for reweighting of features to proposals for new features, feature combinations, relational features, and wholesale changes to the learning algorithm. The results show that user feedback has the potential to significantly improve machine learning systems, but that learning algorithms need to be extended in several ways to be able to assimilate this feedback.",2007,30,153,3,True,Computer Science,,2121662,S. Stumpf,40051508.0,Vidya Rajaram,2107914294.0,Lida Li,1737204.0,M. Burnett,144299726.0,Thomas G. Dietterich,,,2054441880.0,Erin Sullivan,2070070437.0,Russell Drummond,2658798.0,Jonathan L. Herlocker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4a7eea3ec3080ecb277bfe466afce4822a1071d7,https://www.semanticscholar.org/paper/4a7eea3ec3080ecb277bfe466afce4822a1071d7,Quantum embeddings for machine learning,"Quantum classifiers are trainable quantum circuits used as machine learning models. The first part of the circuit implements a quantum feature map that encodes classical inputs into quantum states, embedding the data in a high-dimensional Hilbert space; the second part of the circuit executes a quantum measurement interpreted as the output of the model. Usually, the measurement is trained to distinguish quantum-embedded data. We propose to instead train the first part of the circuit---the embedding---with the objective of maximally separating data classes in Hilbert space, a strategy we call quantum metric learning. As a result, the measurement minimizing a linear classification loss is already known and depends on the metric used: for embeddings separating data using the l1 or trace distance, this is the Helstrom measurement, while for the l2 or Hilbert-Schmidt distance, it is a simple overlap measurement. This approach provides a powerful analytic framework for quantum machine learning and eliminates a major component in current models, freeing up more precious resources to best leverage the capabilities of near-term quantum information processors.",2020,21,136,16,False,Computer Science,Physics,145762777,S. Lloyd,3048564.0,M. Schuld,35323164.0,Aroosa Ijaz,2070140.0,J. Izaac,3399181.0,N. Killoran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0cc6dbfd929bc816d507527993f55f9b4e88615d,https://www.semanticscholar.org/paper/0cc6dbfd929bc816d507527993f55f9b4e88615d,Machine learning & artificial intelligence in the quantum domain: a review of recent progress,"Quantum information technologies, on the one hand, and intelligent learning systems, on the other, are both emergent technologies that are likely to have a transformative impact on our society in the future. The respective underlying fields of basic research—quantum information versus machine learning (ML) and artificial intelligence (AI)—have their own specific questions and challenges, which have hitherto been investigated largely independently. However, in a growing body of recent work, researchers have been probing the question of the extent to which these fields can indeed learn and benefit from each other. Quantum ML explores the interaction between quantum computing and ML, investigating how results and techniques from one field can be used to solve the problems of the other. Recently we have witnessed significant breakthroughs in both directions of influence. For instance, quantum computing is finding a vital application in providing speed-ups for ML problems, critical in our ‘big data’ world. Conversely, ML already permeates many cutting-edge technologies and may become instrumental in advanced quantum technologies. Aside from quantum speed-up in data analysis, or classical ML optimization used in quantum experiments, quantum enhancements have also been (theoretically) demonstrated for interactive learning tasks, highlighting the potential of quantum-enhanced learning agents. Finally, works exploring the use of AI for the very design of quantum experiments and for performing parts of genuine research autonomously, have reported their first successes. Beyond the topics of mutual enhancement—exploring what ML/AI can do for quantum physics and vice versa—researchers have also broached the fundamental issue of quantum generalizations of learning and AI concepts. This deals with questions of the very meaning of learning and intelligence in a world that is fully described by quantum mechanics. In this review, we describe the main ideas, recent developments and progress in a broad spectrum of research investigating ML and AI in the quantum domain.",2017,375,511,7,True,Physics,Computer Science,2878563,V. Dunjko,32534184.0,H. Briegel,,,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b1059d25d092e0e872a1d2db01b24c73eb869ad9,https://www.semanticscholar.org/paper/b1059d25d092e0e872a1d2db01b24c73eb869ad9,Machine Learning for Neural Decoding,"Abstract Despite rapid advances in machine learning tools, the majority of neural decoding approaches still use traditional methods. Modern machine learning tools, which are versatile and easy to use, have the potential to significantly improve decoding performance. This tutorial describes how to effectively apply these algorithms for typical decoding problems. We provide descriptions, best practices, and code for applying common machine learning methods, including neural networks and gradient boosting. We also provide detailed comparisons of the performance of various methods at the task of decoding spiking activity in motor cortex, somatosensory cortex, and hippocampus. Modern methods, particularly neural networks and ensembles, significantly outperform traditional approaches, such as Wiener and Kalman filters. Improving the performance of neural decoding algorithms allows neuroscientists to better understand the information contained in a neural population and can help to advance engineering applications such as brain–machine interfaces. Our code package is available at github.com/kordinglab/neural_decoding.",2017,104,128,11,True,Biology,Computer Science,2937772,Joshua I. Glaser,38308858.0,Raeed H. Chowdhury,5228640.0,M. Perich,32776555.0,L. Miller,3282030.0,Konrad Paul Kording,Mathematics,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f2642db17084b14068d56f332de2f2d5a1622c5a,https://www.semanticscholar.org/paper/f2642db17084b14068d56f332de2f2d5a1622c5a,Error Minimized Extreme Learning Machine With Growth of Hidden Nodes and Incremental Learning,"One of the open problems in neural network research is how to automatically determine network architectures for given applications. In this brief, we propose a simple and efficient approach to automatically determine the number of hidden nodes in generalized single-hidden-layer feedforward networks (SLFNs) which need not be neural alike. This approach referred to as error minimized extreme learning machine (EM-ELM) can add random hidden nodes to SLFNs one by one or group by group (with varying group size). During the growth of the networks, the output weights are updated incrementally. The convergence of this approach is proved in this brief as well. Simulation results demonstrate and verify that our new approach is much faster than other sequential/incremental/growing algorithms with good generalization performance.",2009,26,578,31,False,Computer Science,Medicine,2069968720,Guorui Feng,145678691.0,G. Huang,3026738.0,Qingping Lin,1723858.0,R. Gay,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23bd1da0ad4d2ec956346655f0bb0206e13556b8,https://www.semanticscholar.org/paper/23bd1da0ad4d2ec956346655f0bb0206e13556b8,Learning to Control a Brain–Machine Interface for Reaching and Grasping by Primates,"Reaching and grasping in primates depend on the coordination of neural activity in large frontoparietal ensembles. Here we demonstrate that primates can learn to reach and grasp virtual objects by controlling a robot arm through a closed-loop brain–machine interface (BMIc) that uses multiple mathematical models to extract several motor parameters (i.e., hand position, velocity, gripping force, and the EMGs of multiple arm muscles) from the electrical activity of frontoparietal neuronal ensembles. As single neurons typically contribute to the encoding of several motor parameters, we observed that high BMIc accuracy required recording from large neuronal ensembles. Continuous BMIc operation by monkeys led to significant improvements in both model predictions and behavioral performance. Using visual feedback, monkeys succeeded in producing robot reach-and-grasp movements even when their arms did not move. Learning to operate the BMIc was paralleled by functional reorganization in multiple cortical areas, suggesting that the dynamic properties of the BMIc were incorporated into motor and sensory cortical representations.",2003,45,1739,122,True,Biology,Medicine,2016191,J. Carmena,2057713.0,M. Lebedev,1826610.0,R. Crist,1397219405.0,J. E. O’Doherty,39819116.0,David M. Santucci,,,38928339.0,D. Dimitrov,8004223.0,P. Patil,1845021.0,C. Henriquez,144052463.0,M. Nicolelis,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f0cdd1488734abe45854b96d76f797188005656b,https://www.semanticscholar.org/paper/f0cdd1488734abe45854b96d76f797188005656b,"A Survey on Federated Learning Systems: Vision, Hype and Reality for Data Privacy and Protection","Federated learning has been a hot research topic in enabling the collaborative training of machine learning models among different organizations under the privacy restrictions. As researchers try to support more machine learning models with different privacy-preserving approaches, there is a requirement in developing systems and infrastructures to ease the development of various federated learning algorithms. Similar to deep learning systems such as PyTorch and TensorFlow that boost the development of deep learning, federated learning systems (FLSs) are equivalently important, and face challenges from various aspects such as effectiveness, efficiency, and privacy. In this survey, we conduct a comprehensive review on federated learning systems. To achieve smooth flow and guide future research, we introduce the definition of federated learning systems and analyze the system components. Moreover, we provide a thorough categorization for federated learning systems according to six different aspects, including data distribution, machine learning model, privacy mechanism, communication architecture, scale of federation and motivation of federation. The categorization can help the design of federated learning systems as shown in our case studies. By systematically summarizing the existing federated learning systems, we present the design factors, case studies, and future research opportunities.",2019,313,239,19,True,Computer Science,Mathematics,92621060,Q. Li,39003902.0,Zeyi Wen,143824511.0,Bingsheng He,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2eee70568be6f780eb98178568df5dd204b0064e,https://www.semanticscholar.org/paper/2eee70568be6f780eb98178568df5dd204b0064e,A Machine Learning Strategy to Assist Turbulence Model Development,"Turbulence modeling in a Reynolds Averaged Navier–Stokes (RANS) setting has traditionally evolved through a combination of theory, mathematics, and empiricism. The problem of closure, resulting from the averaging process, requires an infusion of information into the various models that is often managed in an ad-hoc way or that is focused on particular classes of problems, thus diminishing the predictive capabilities of a model in other flow contexts. In this work, a proof-of-concept of a new data-driven approach of turbulence model development is presented. The key idea in the proposed framework is to use supervised learning algorithms to build a representation of turbulence modeling closure terms. The learned terms are then inserted into a Computational Fluid Dynamics (CFD) numerical simulation with the aim of offering a better representation of turbulence physics. But while the basic idea is attractive, modeling unknown terms by increasingly large amounts of data from higher-fidelity simulations (LES, DNS, etc) or even experiment, the details of how to make the approach viable are not at all obvious. In this work, we investigate the feasibility of such an approach by attempting to reproduce, through a machine learning methodology, the results obtained with the well-established SpalartAllmaras model. In other words, the key question that we seek to answer is the following: Given a number of observations of CFD solutions using the Spalart-Allmaras model (our truth model), can we reproduce those solutions using machine-learning techniques without knowledge of the structure, functional form, and coefficients of the actual model? We discuss the challenges of applying machine learning techniques in a fluid dynamic setting and possible successful approaches. We also explore the potential for machine learning as an enhancement to or replacement for traditional turbulence models. Our results highlight the potential and viability of machine learning approaches to aid turbulence model development.",2015,31,219,4,True,,,143613254,Brendan D. Tracey,1974037.0,K. Duraisamy,145409784.0,J. Alonso,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6988851089eaa95d1f76526a3e3841534313cd98,https://www.semanticscholar.org/paper/6988851089eaa95d1f76526a3e3841534313cd98,The State of the Art in Integrating Machine Learning into Visual Analytics,"Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under‐explored. This state‐of‐the‐art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions.",2017,190,166,10,True,Computer Science,Mathematics,3200296,A. Endert,49907592.0,W. Ribarsky,2427554.0,C. Turkay,144459506.0,B. Wong,3263426.0,I. Nabney,,,1729209.0,Ignacio Díaz Blanco,2052876402.0,Fabrice Rossi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1439e05971a053c2368e6dee6d484b43c833d43c,https://www.semanticscholar.org/paper/1439e05971a053c2368e6dee6d484b43c833d43c,Crafting adversarial input sequences for recurrent neural networks,"Machine learning models are frequently used to solve complex security problems, as well as to make decisions in sensitive situations like guiding autonomous vehicles or predicting financial market behaviors. Previous efforts have shown that numerous machine learning models are vulnerable to adversarial manipulations of their inputs taking the form of adversarial samples. Such inputs are crafted by adding carefully selected perturbations to legitimate inputs so as to force the machine learning model to misbehave, for instance by outputting a wrong class if the machine learning task of interest is classification. In fact, to the best of our knowledge, all previous work on adversarial samples crafting for neural networks considered models used to solve classification tasks, most frequently in computer vision applications. In this paper, we investigate adversarial input sequences for recurrent neural networks processing sequential data. We show that the classes of algorithms introduced previously to craft adversarial samples misclassified by feed-forward neural networks can be adapted to recurrent neural networks. In a experiment, we show that adversaries can craft adversarial sequences misleading both categorical and sequential recurrent neural networks.",2016,25,342,46,True,Computer Science,,1967156,Nicolas Papernot,144061974.0,P. Mcdaniel,144231976.0,A. Swami,2584869.0,Richard E. Harang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0228810a988f6b8f06337e14f564e2fd3f6e1056,https://www.semanticscholar.org/paper/0228810a988f6b8f06337e14f564e2fd3f6e1056,The Recurrent Temporal Restricted Boltzmann Machine,"The Temporal Restricted Boltzmann Machine (TRBM) is a probabilistic model for sequences that is able to successfully model (i.e., generate nice-looking samples of) several very high dimensional sequences, such as motion capture data and the pixels of low resolution videos of balls bouncing in a box. The major disadvantage of the TRBM is that exact inference is extremely hard, since even computing a Gibbs update for a single variable of the posterior is exponentially expensive. This difficulty has necessitated the use of a heuristic inference procedure, that nonetheless was accurate enough for successful learning. In this paper we introduce the Recurrent TRBM, which is a very slight modification of the TRBM for which exact inference is very easy and exact gradient learning is almost tractable. We demonstrate that the RTRBM is better than an analogous TRBM at generating motion capture and videos of bouncing balls.",2008,44,406,52,False,Mathematics,Computer Science,1701686,Ilya Sutskever,1695689.0,Geoffrey E. Hinton,144639556.0,Graham W. Taylor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f7d3f3a23c1b284a17adc93a922c56be38d221df,https://www.semanticscholar.org/paper/f7d3f3a23c1b284a17adc93a922c56be38d221df,"On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products","Machine learning algorithms increasingly influence our decisions and interact with us in all parts of our daily lives. Therefore, just as we consider the safety of power plants, highways, and a variety of other engineered socio-technical systems, we must also take into account the safety of systems involving machine learning. Heretofore, the definition of safety has not been formalized in a machine learning context. In this article, we do so by defining machine learning safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. We then use this definition to examine safety in all sorts of applications in cyber-physical systems, decision sciences, and data products. We find that the foundational principle of modern statistical machine learning, empirical risk minimization, is not always a sufficient objective. We discuss how four different categories of strategies for achieving safety in engineering, including inherently safe design, safety reserves, safe fail, and procedural safeguards can be mapped to a machine learning context. We then discuss example techniques that can be adopted in each category, such as considering interpretability and causality of predictive models, objective functions beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software and open data.",2016,61,153,5,True,Computer Science,Medicine,1712865,K. Varshney,2571534.0,H. Alemzadeh,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f83ca18f3834d45a70e9b54578e2c33870dde67d,https://www.semanticscholar.org/paper/f83ca18f3834d45a70e9b54578e2c33870dde67d,Machine Teaching: An Inverse Problem to Machine Learning and an Approach Toward Optimal Education,"I draw the reader's attention to machine teaching, the problem of finding an optimal training set given a machine learning algorithm and a target model. In addition to generating fascinating mathematical questions for computer scientists to ponder, machine teaching holds the promise of enhancing education and personnel training. The Socratic dialogue style aims to stimulate critical thinking.",2015,38,223,8,True,Computer Science,,1832364,Xiaojin Zhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
af05afea7a7e99b8b20a743a49d5b33efc041100,https://www.semanticscholar.org/paper/af05afea7a7e99b8b20a743a49d5b33efc041100,Adversarial Robustness Toolbox v1.0.0,"Adversarial Robustness Toolbox (ART) is a Python library supporting developers and researchers in defending Machine Learning models (Deep Neural Networks, Gradient Boosted Decision Trees, Support Vector Machines, Random Forests, Logistic Regression, Gaussian Processes, Decision Trees, Scikit-learn Pipelines, etc.) against adversarial threats and helps making AI systems more secure and trustworthy. Machine Learning models are vulnerable to adversarial examples, which are inputs (images, texts, tabular data, etc.) deliberately modified to produce a desired response by the Machine Learning model. ART provides the tools to build and deploy defences and test them with adversarial attacks. Defending Machine Learning models involves certifying and verifying model robustness and model hardening with approaches such as pre-processing inputs, augmenting training data with adversarial samples, and leveraging runtime detection methods to flag any inputs that might have been modified by an adversary. The attacks implemented in ART allow creating adversarial attacks against Machine Learning models which is required to test defenses with state-of-the-art threat models. Supported Machine Learning Libraries include TensorFlow (v1 and v2), Keras, PyTorch, MXNet, Scikit-learn, XGBoost, LightGBM, CatBoost, and GPy. The source code of ART is released with MIT license at https://github.com/IBM/adversarial-robustness-toolbox. The release includes code examples, notebooks with tutorials and documentation (this http URL).",2018,54,173,17,False,Computer Science,Mathematics,2378427,Maria-Irina Nicolae,144418887.0,M. Sinn,1653295564.0,Minh-Ngoc Tran,1474544597.0,Beat Buesser,22261698.0,Ambrish Rawat,,,2794970.0,Martin Wistuba,3393449.0,Valentina Zantedeschi,2478882.0,Nathalie Baracaldo,2108342611.0,Bryant Chen,144854947.0,Heiko Ludwig,2816941.0,Ian Molloy,1700532317.0,Ben Edwards,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3a73fa8758fe8072f5d8d412b9fdc52e04b3da9d,https://www.semanticscholar.org/paper/3a73fa8758fe8072f5d8d412b9fdc52e04b3da9d,Advances in Machine Learning and Data Mining for Astronomy,"Advances in Machine Learning and Data Mining for Astronomy documents numerous successful collaborations among computer scientists, statisticians, and astronomers who illustrate the application of state-of-the-art machine learning and data mining techniques in astronomy. Due to the massive amount and complexity of data in most scientific disciplines, the material discussed in this text transcends traditional boundaries between various areas in the sciences and computer science. The books introductory part provides context to issues in the astronomical sciences that are also important to health, social, and physical sciences, particularly probabilistic and statistical aspects of classification and cluster analysis. The next part describes a number of astrophysics case studies that leverage a range of machine learning and data mining technologies. In the last part, developers of algorithms and practitioners of machine learning and data mining show how these tools and techniques are used in astronomical applications. With contributions from leading astronomers and computer scientists, this book is a practical guide to many of the most important developments in machine learning, data mining, and statistics. It explores how these advances can solve current and future problems in astronomy and looks at how they could lead to the creation of entirely new algorithms within the data mining community.",2012,320,179,9,False,Computer Science,,12493773,M. Way,2234072.0,J. Scargle,2065740168.0,K. Ali,1827853.0,A. Srivastava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
648d23c86ab158060604724648deb391e453c200,https://www.semanticscholar.org/paper/648d23c86ab158060604724648deb391e453c200,Quantum Loop Topography for Machine Learning.,"Despite rapidly growing interest in harnessing machine learning in the study of quantum many-body systems, training neural networks to identify quantum phases is a nontrivial challenge. The key challenge is in efficiently extracting essential information from the many-body Hamiltonian or wave function and turning the information into an image that can be fed into a neural network. When targeting topological phases, this task becomes particularly challenging as topological phases are defined in terms of nonlocal properties. Here, we introduce quantum loop topography (QLT): a procedure of constructing a multidimensional image from the ""sample"" Hamiltonian or wave function by evaluating two-point operators that form loops at independent Monte Carlo steps. The loop configuration is guided by the characteristic response for defining the phase, which is Hall conductivity for the cases at hand. Feeding QLT to a fully connected neural network with a single hidden layer, we demonstrate that the architecture can be effectively trained to distinguish the Chern insulator and the fractional Chern insulator from trivial insulators with high fidelity. In addition to establishing the first case of obtaining a phase diagram with a topological quantum phase transition with machine learning, the perspective of bridging traditional condensed matter theory with machine learning will be broadly valuable.",2016,0,189,1,True,Computer Science,Medicine,49889859,Yi Zhang,144302105.0,Eun-Ah Kim,,,,,,,Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5d2f5c2dc11c18c0d45203e2b980fe375a56d774,https://www.semanticscholar.org/paper/5d2f5c2dc11c18c0d45203e2b980fe375a56d774,Emergence of Grounded Compositional Language in Multi-Agent Populations,"By capturing statistical patterns in large corpora, machine learning has enabled significant advances in natural language processing, including in machine translation, question answering, and sentiment analysis. However, for agents to intelligently interact with humans, simply capturing the statistical patterns is insufficient. In this paper we investigate if, and how, grounded compositional language can emerge as a means to achieve goals in multi-agent populations. Towards this end, we propose a multi-agent learning environment and learning methods that bring about emergence of a basic compositional language. This language is represented as streams of abstract discrete symbols uttered by agents over time, but nonetheless has a coherent structure that possesses a defined vocabulary and syntax. We also observe emergence of non-verbal communication such as pointing and guiding when language communication is unavailable.",2017,38,485,42,True,Computer Science,,2080746,Igor Mordatch,1689992.0,P. Abbeel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5002aa2c446f50cdf8e0c17bc833e302bfc100b3,https://www.semanticscholar.org/paper/5002aa2c446f50cdf8e0c17bc833e302bfc100b3,Recent trends in machine learning for human activity recognition—A survey,"There has been an upsurge recently in investigating machine learning techniques for activity recognition (AR) problems as they have been very effective in extracting and learning knowledge from the activity datasets. The technique ranges from heuristically derived hand‐crafted feature‐based traditional machine learning algorithms to the recently developed hierarchically self‐evolving feature‐based deep learning algorithms. AR continues to remain a challenging problem in uncontrolled smart environments despite the amount of work contributed by the researcher in this field. The complex, volatile, and chaotic nature of the activity data presents numerous challenges that influence the performance of the AR systems in the wild. In this article, we present a comprehensive overview of recent machine learning and data mining techniques generally employed for AR and the underpinning problems and challenges associated with the existing systems. We also articulate the recent advances and state‐of‐the‐art techniques in this domain in an attempt to identify the possible directions for future AR research.",2018,84,173,3,True,Computer Science,,46187563,S. R. Ramamurthy,47705715.0,N. Roy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fbf9812f29156024ec693b4633a21303eead309d,https://www.semanticscholar.org/paper/fbf9812f29156024ec693b4633a21303eead309d,Machine learning algorithm validation with a limited sample size,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.",2019,31,387,7,True,Medicine,Computer Science,8452323,A. Vabalas,2283643.0,E. Gowen,2910758.0,E. Poliakoff,1807736.0,A. Casson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
04c348d4fd08fddf5912a6ed946109a297708789,https://www.semanticscholar.org/paper/04c348d4fd08fddf5912a6ed946109a297708789,Nonconvex Online Support Vector Machines,"In this paper, we propose a nonconvex online Support Vector Machine (SVM) algorithm (LASVM-NC) based on the Ramp Loss, which has the strong ability of suppressing the influence of outliers. Then, again in the online learning setting, we propose an outlier filtering mechanism (LASVM-I) based on approximating nonconvex behavior in convex optimization. These two algorithms are built upon another novel SVM algorithm (LASVM-G) that is capable of generating accurate intermediate models in its iterative steps by leveraging the duality gap. We present experimental results that demonstrate the merit of our frameworks in achieving significant robustness to outliers in noisy data classification where mislabeled training instances are in abundance. Experimental evaluation shows that the proposed approaches yield a more scalable online SVM algorithm with sparser models and less computational running time, both in the training and recognition phases, without sacrificing generalization performance. We also point out the relation between nonconvex optimization and min-margin active learning.",2011,26,130,8,True,Mathematics,Computer Science,1769472,S. Ertekin,52184096.0,L. Bottou,145157784.0,C. Lee Giles,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e9862146f11fb22df97231b3cccf0e95e8fc2749,https://www.semanticscholar.org/paper/e9862146f11fb22df97231b3cccf0e95e8fc2749,Tutelage and socially guided robot learning,"We view the problem of machine learning as a collaboration between the human and the machine. Inspired by human-style tutelage, we situate the learning problem within a dialog in which social interaction structures the learning experience, providing instruction, directing attention, and controlling the complexity of the task. We present a learning mechanism, implemented on a humanoid robot, to demonstrate that a collaborative dialog framework allows a robot to efficiently learn a task from a human, generalize this ability to a new task configuration, and show commitment to the overall goal of the learned task. We also compare this approach to traditional machine learning approaches.",2004,37,168,12,False,Computer Science,,1682788,A. Thomaz,1711777.0,C. Breazeal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4a554da55fd9ff76c99e25d2ce937b225dc1100c,https://www.semanticscholar.org/paper/4a554da55fd9ff76c99e25d2ce937b225dc1100c,A survey of named entity recognition and classification,"This survey covers fifteen years of research in the Named Entity Recognition and Classification (NERC) field, from 1991 to 2006. We report observations about languages, named entity types, domains and textual genres studied in the literature. From the start, NERC systems have been developed using hand-made rules, but now machine learning techniques are widely used. These techniques are surveyed along with other critical aspects of NERC such as features and evaluation methods. Features are word-level, dictionary-level and corpus-level representations of words in a document. Evaluation techniques, ranging from intuitive exact match to very complex matching techniques with adjustable cost of errors, are an indisputable key to progress.",2007,84,2439,184,False,Computer Science,,40421028,David Nadeau,1714612.0,S. Sekine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ae20c630759847cd88655631d0041b0be127fd0c,https://www.semanticscholar.org/paper/ae20c630759847cd88655631d0041b0be127fd0c,Transfer Learning for Visual Categorization: A Survey,"Regular machine learning and data mining techniques study the training data for future inferences under a major assumption that the future data are within the same feature space or have the same distribution as the training data. However, due to the limited availability of human labeled training data, training data that stay in the same feature space or have the same distribution as the future data cannot be guaranteed to be sufficient enough to avoid the over-fitting problem. In real-world applications, apart from data in the target domain, related data in a different domain can also be included to expand the availability of our prior knowledge about the target future data. Transfer learning addresses such cross-domain learning problems by extracting useful information from data in a related domain and transferring them for being used in target tasks. In recent years, with transfer learning being applied to visual categorization, some typical problems, e.g., view divergence in action recognition tasks and concept drifting in image classification tasks, can be efficiently solved. In this paper, we survey state-of-the-art transfer learning algorithms in visual categorization applications, such as object recognition, image classification, and human action recognition.",2015,99,583,16,False,Computer Science,Medicine,144082425,L. Shao,152506137.0,F. Zhu,67180560.0,Xuelong Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
29b9ff8f4a26acc90e6182e1e749f15f688bc7cf,https://www.semanticscholar.org/paper/29b9ff8f4a26acc90e6182e1e749f15f688bc7cf,Machine learning for quantum mechanics in a nutshell,"Models that combine quantum mechanics (QM) with machine learning (ML) promise to deliver the accuracy of QM at the speed of ML. This hands-on tutorial introduces the reader to QM/ML models based on kernel learning, an elegant, systematically nonlinear form of ML. Pseudocode and a reference implementation are provided, enabling the reader to reproduce results from recent publications where atomization energies of small organic molecules are predicted using kernel ridge regression. © 2015 Wiley Periodicals, Inc.",2015,148,271,5,True,Computer Science,,48041657,M. Rupp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1ed1cf769e540d9ad5ce5cdc17d69bf738993257,https://www.semanticscholar.org/paper/1ed1cf769e540d9ad5ce5cdc17d69bf738993257,TensorFlow Lite Micro: Embedded Machine Learning on TinyML Systems,"Deep learning inference on embedded devices is a burgeoning field with myriad applications because tiny embedded devices are omnipresent. But we must overcome major challenges before we can benefit from this opportunity. Embedded processors are severely resource constrained. Their nearest mobile counterparts exhibit at least a 100---1,000x difference in compute capability, memory availability, and power consumption. As a result, the machine-learning (ML) models and associated ML inference framework must not only execute efficiently but also operate in a few kilobytes of memory. Also, the embedded devices' ecosystem is heavily fragmented. To maximize efficiency, system vendors often omit many features that commonly appear in mainstream systems, including dynamic memory allocation and virtual memory, that allow for cross-platform interoperability. The hardware comes in many flavors (e.g., instruction-set architecture and FPU support, or lack thereof). We introduce TensorFlow Lite Micro (TF Micro), an open-source ML inference framework for running deep-learning models on embedded systems. TF Micro tackles the efficiency requirements imposed by embedded-system resource constraints and the fragmentation challenges that make cross-platform interoperability nearly impossible. The framework adopts a unique interpreter-based approach that provides flexibility while overcoming these challenges. This paper explains the design decisions behind TF Micro and describes its implementation details. Also, we present an evaluation to demonstrate its low resource requirement and minimal run-time performance overhead.",2020,37,134,15,False,Computer Science,,2061545932,R. David,41174628.0,Jared Duke,2499238.0,Advait Jain,1805668.0,V. Reddi,1999279410.0,Nat Jeffries,,,71003878.0,Jian Li,66420904.0,Nick Kreeger,1999299009.0,Ian Nappier,3362619.0,Meghna Natraj,1999106981.0,Shlomi Regev,147961415.0,Rocky Rhodes,2116564880.0,Tiezhen Wang,47941411.0,P. Warden,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
38bdbf7cf0572732bd21b299bdbaf2aab8da959d,https://www.semanticscholar.org/paper/38bdbf7cf0572732bd21b299bdbaf2aab8da959d,PAC-Bayesian Generalisation Error Bounds for Gaussian Process Classification,"Approximate Bayesian Gaussian process (GP) classification techniques are powerful non-parametric learning methods, similar in appearance and performance to support vector machines. Based on simple probabilistic models, they render interpretable results and can be embedded in Bayesian frameworks for model selection, feature selection, etc. In this paper, by applying the PAC-Bayesian theorem of McAllester (1999a), we prove distribution-free generalisation error bounds for a wide range of approximate Bayesian GP classification techniques. We also provide a new and much simplified proof for this powerful theorem, making use of the concept of convex duality which is a backbone of many machine learning techniques. We instantiate and test our bounds for two particular GPC techniques, including a recent sparse method which circumvents the unfavourable scaling of standard GP algorithms. As is shown in experiments on a real-world task, the bounds can be very tight for moderate training sample sizes. To the best of our knowledge, these results provide the tightest known distribution-free error bounds for approximate Bayesian GPC methods, giving a strong learning-theoretical justification for the use of these techniques.",2003,78,275,29,False,Mathematics,Computer Science,1680574,M. Seeger,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6e7891253247b01b346e92fb81c2ba589ca8e428,https://www.semanticscholar.org/paper/6e7891253247b01b346e92fb81c2ba589ca8e428,Machine Learning Applications for Data Center Optimization,"Jim Gao, Google   Abstract  The modern data center (DC) is a complex interaction of multiple mechanical, electrical and controls systems. The sheer number of possible operating configurations and nonlinear interdependencies make it difficult to understand and optimize energy efficiency. We develop a neural network framework that learns from actual operations data to model plant performance and predict PUE within a range of 0.004 +/­ 0.005 (mean absolute error +/­ 1 standard deviation), or 0.4% error for a PUE of 1.1. The model has been extensively tested and validated at Google DCs. The results demonstrate that machine learning is an effective way of leveraging existing sensor data to model DC performance and improve energy efficiency.",2014,16,202,14,False,Computer Science,,72208571,J. Gao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bf79c966b293dbc5551de9785a696c099dff355b,https://www.semanticscholar.org/paper/bf79c966b293dbc5551de9785a696c099dff355b,Inductive Principles for Restricted Boltzmann Machine Learning,"Recent research has seen the proposal of several new inductive principles designed specifically to avoid the problems associated with maximum likelihood learning in models with intractable partition functions. In this paper, we study learning methods for binary restricted Boltzmann machines (RBMs) based on ratio matching and generalized score matching. We compare these new RBM learning methods to a range of existing learning methods including stochastic maximum likelihood, contrastive divergence, and pseudo-likelihood. We perform an extensive empirical evaluation across multiple tasks and data sets.",2010,24,170,34,False,Mathematics,Computer Science,1805742,Benjamin M Marlin,1754860.0,Kevin Swersky,,Bo Chen,1737568.0,N. D. Freitas,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b6139e7f568f0cbd8bcfc7dfd5885dec474270d6,https://www.semanticscholar.org/paper/b6139e7f568f0cbd8bcfc7dfd5885dec474270d6,Distributed learning in wireless sensor networks,"This paper discusses nonparametric distributed learning. After reviewing the classical learning model and highlighting the success of machine learning in centralized settings, the challenges that wireless sensor networks (WSN) pose for distributed learning are discussed, and research aimed at addressing these challenges is surveyed.",2005,131,293,10,True,Computer Science,Mathematics,2498294,J. Predd,1697413.0,S. Kulkarni,145967056.0,H. Poor,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb,https://www.semanticscholar.org/paper/857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb,Learning perturbation sets for robust machine learning,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.",2020,64,59,2,False,Computer Science,Mathematics,51026953,Eric Wong,145116464.0,J. Z. Kolter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e8b30ebe3351680c3b039555ae0a8d0865ad829b,https://www.semanticscholar.org/paper/e8b30ebe3351680c3b039555ae0a8d0865ad829b,Neural networks and machine learning,"In recent years neural computing has emerged as a practical technology, with successful applications in many fields. The majority of these applications are concerned with problems in pattern recognition, and make use of feedforward network architectures such as the multilayer perceptron and the radial basis function network. Also, it has become widely acknowledged that successful applications of neural computing require a principled, rather than ad hoc, approach. (From the preface to ""Neural Networks for Pattern Recognition"" by C.M. Bishop, Oxford Univ Press 1995.) This NATO volume, based on a 1997 workshop, presents a coordinated series of tutorial articles covering recent developments in the field of neural computing. It is ideally suited to graduate students and researchers.",1998,0,166,6,False,Computer Science,,2113861474,C. Bishop,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78f3f9eebdde8854e5a0e91bc71994f294140b56,https://www.semanticscholar.org/paper/78f3f9eebdde8854e5a0e91bc71994f294140b56,Probabilistic Forecasting of Wind Power Generation Using Extreme Learning Machine,"Accurate and reliable forecast of wind power is essential to power system operation and control. However, due to the nonstationarity of wind power series, traditional point forecasting can hardly be accurate, leading to increased uncertainties and risks for system operation. This paper proposes an extreme learning machine (ELM)-based probabilistic forecasting method for wind power generation. To account for the uncertainties in the forecasting results, several bootstrap methods have been compared for modeling the regression uncertainty, based on which the pairs bootstrap method is identified with the best performance. Consequently, a new method for prediction intervals formulation based on the ELM and the pairs bootstrap is developed. Wind power forecasting has been conducted in different seasons using the proposed approach with the historical wind power time series as the inputs alone. The results demonstrate that the proposed method is effective for probabilistic forecasting of wind power generation with a high potential for practical applications in power systems.",2014,46,499,20,True,Engineering,,143891278,C. Wan,145160042.0,Zhao Xu,2182322.0,P. Pinson,144402926.0,Z. Dong,144568755.0,K. Wong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4e75a145f59400a8d646db770cc396de87d6b9d8,https://www.semanticscholar.org/paper/4e75a145f59400a8d646db770cc396de87d6b9d8,Image Reconstruction is a New Frontier of Machine Learning,"Over past several years, machine learning, or more generally artificial intelligence, has generated overwhelming research interest and attracted unprecedented public attention. As tomographic imaging researchers, we share the excitement from our imaging perspective [item 1) in the Appendix], and organized this special issue dedicated to the theme of “Machine learning for image reconstruction.” This special issue is a sister issue of the special issue published in May 2016 of this journal with the theme “Deep learning in medical imaging” [item 2) in the Appendix]. While the previous special issue targeted medical image processing/analysis, this special issue focuses on data-driven tomographic reconstruction. These two special issues are highly complementary, since image reconstruction and image analysis are two of the main pillars for medical imaging. Together we cover the whole workflow of medical imaging: from tomographic raw data/features to reconstructed images and then extracted diagnostic features/readings.",2018,78,280,1,False,Computer Science,Medicine,2108295460,Ge Wang,2998762.0,J. C. Ye,3367666.0,K. Mueller,1711572.0,J. Fessler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
