paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,authors/5/authorId,authors/5/name,fieldsOfStudy/1,fieldsOfStudy/2,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,fieldsOfStudy/3,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,authors/16/authorId,authors/16/name,authors/17/authorId,authors/17/name,authors/18/authorId,authors/18/name,authors/19/authorId,authors/19/name,authors/20/authorId,authors/20/name,authors/21/authorId,authors/21/name,authors/22/authorId,authors/22/name,authors/23/authorId,authors/23/name,authors/24/authorId,authors/24/name,authors/25/authorId,authors/25/name,authors/26/authorId,authors/26/name,authors/27/authorId,authors/27/name,authors/28/authorId,authors/28/name,authors/29/authorId,authors/29/name,authors/30/authorId,authors/30/name,authors/31/authorId,authors/31/name,authors/32/authorId,authors/32/name,authors/33/authorId,authors/33/name,authors/34/authorId,authors/34/name,authors/35/authorId,authors/35/name,authors/36/authorId,authors/36/name,authors/37/authorId,authors/37/name,authors/38/authorId,authors/38/name,authors/39/authorId,authors/39/name
9d72c286f24a51e24ae9e80e957d1f999baee17b,https://www.semanticscholar.org/paper/9d72c286f24a51e24ae9e80e957d1f999baee17b,Adding vs. Averaging in Distributed Primal-Dual Optimization,"Distributed optimization methods for large-scale machine learning suffer from a communication bottleneck. It is difficult to reduce this bottleneck while still efficiently and accurately aggregating partial work from different machines. In this paper, we present a novel generalization of the recent communication-efficient primal-dual framework (COCOA) for distributed optimization. Our framework, COCOA+, allows for additive combination of local updates to the global parameters at each iteration, whereas previous schemes with convergence guarantees only allow conservative averaging. We give stronger (primal-dual) convergence rate guarantees for both COCOA as well as our new variants, and generalize the theory for both methods to cover non-smooth convex loss functions. We provide an extensive experimental comparison that shows the markedly improved performance of COCOA+ on several real-world distributed datasets, especially when scaling up the number of machines.",2015,45,155,28,False,Computer Science,2008023,Chenxin Ma,145260024.0,Virginia Smith,2456863.0,Martin Jaggi,1694621.0,Michael I. Jordan,2662221.0,Peter Richtárik,144696183.0,Martin Takác,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1dae4d61cd74cc919ecc638bde6b7125728ea97b,https://www.semanticscholar.org/paper/1dae4d61cd74cc919ecc638bde6b7125728ea97b,Domain Adaptation via Transfer Component Analysis,"Domain adaptation allows knowledge from a source domain to be transferred to a different but related target domain. Intuitively, discovering a good feature representation across domains is crucial. In this paper, we first propose to find such a representation through a new learning method, transfer component analysis (TCA), for domain adaptation. TCA tries to learn some transfer components across domains in a reproducing kernel Hilbert space using maximum mean miscrepancy. In the subspace spanned by these transfer components, data properties are preserved and data distributions in different domains are close to each other. As a result, with the new representations in this subspace, we can apply standard machine learning methods to train classifiers or regression models in the source domain for use in the target domain. Furthermore, in order to uncover the knowledge hidden in the relations between the data labels from the source and target domains, we extend TCA in a semisupervised learning setting, which encodes label information into transfer components learning. We call this extension semisupervised TCA. The main contribution of our work is that we propose a novel dimensionality reduction framework for reducing the distance between domains in a latent space for domain adaptation. We propose both unsupervised and semisupervised feature extraction approaches, which can dramatically reduce the distance between domain distributions by projecting data onto the learned transfer components. Finally, our approach can handle large datasets and naturally lead to out-of-sample generalization. The effectiveness and efficiency of our approach are verified by experiments on five toy datasets and two real-world applications: cross-domain indoor WiFi localization and cross-domain text classification.",2009,52,2857,579,True,Computer Science,1746914,Sinno Jialin Pan,1807998.0,I. Tsang,145193332.0,J. Kwok,152290618.0,Qiang Yang,,,,,Mathematics,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3614e6b9313b0093bda33b719c28708637237323,https://www.semanticscholar.org/paper/3614e6b9313b0093bda33b719c28708637237323,A Comparison of Supervised Machine Learning Techniques for Predicting Short-Term In-Hospital Length of Stay among Diabetic Patients,"Diabetes is a life-altering medical condition that affects millions of people and results in many hospitalizations per year. Consequently, predicting the length of stay of in-hospital diabetic patients has become increasingly important for staffing and resource planning. Although statistical methods have been used to predict length of stay in hospitalized patients, many powerful machine learning techniques have not yet been explored. In this paper, we compare and discuss the performance of various supervised machine learning algorithms (i.e., Multiple linear regression, support vector machines, multi-task learning, and random forests) for predicting long versus short-term length of stay of hospitalized diabetic patients.",2014,26,51,5,False,Computer Science,38569173,A. Morton,1492038511.0,Eman N. Marzban,46599451.0,G. Giannoulis,27875247.0,Ayush Patel,2154233.0,R. Aparasu,1706204.0,I. Kakadiaris,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07b5093aace8e485e7d23b83edb6351618138127,https://www.semanticscholar.org/paper/07b5093aace8e485e7d23b83edb6351618138127,Does Distributionally Robust Supervised Learning Give Robust Classifiers?,"Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems. When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data. DRSL with f-divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss. In this paper, we analyze this DRSL, focusing on the classification scenario. Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions. However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic. This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide. Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.",2016,37,161,22,False,Computer Science,48594758,Weihua Hu,47537639.0,Gang Niu,73355331.0,Issei Sato,67154907.0,Masashi Sugiyama,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,https://www.semanticscholar.org/paper/7eb733c8ac1b3d1dd8b50e066ddae10769e3b46e,CrypTen: Secure Multi-Party Computation Meets Machine Learning,"Secure multi-party computation (MPC) allows parties to perform computations on data while keeping that data private. This capability has great potential for machine-learning applications: it facilitates training of machine-learning models on private data sets owned by different parties, evaluation of one party’s private model using another party’s private data, etc. Although a range of studies implement machine-learning models via secure MPC, such implementations are not yet mainstream. Adoption of secure MPC is hampered by the absence of ﬂexible software frameworks that “speak the language” of machine-learning researchers and engineers. To foster adoption of secure MPC in machine learning, we present C RYP T EN : a software framework that exposes popular secure MPC primitives via abstractions that are common in modern machine-learning frameworks, such as tensor computations, automatic differentiation, and modular neural networks. This paper describes the design of C RYP T EN and measure its performance on state-of-the-art models for text classiﬁcation, speech recognition, and image classiﬁcation. Our benchmarks show that C RYP T EN ’s GPU support and high-performance communication between (an arbitrary number of) parties allows it to perform efﬁcient private evaluation of modern machine-learning models under a semi-honest threat model. For example, two parties using C RYP T EN can securely predict phonemes in speech recordings using Wav2Letter [18] faster than real-time. We hope that C RYP T EN will spur adoption of secure MPC in the machine-learning community.",2021,80,77,18,False,Computer Science,2713842,Brian Knott,2262405.0,Shobha Venkataraman,144479015.0,Awni Y. Hannun,2264597.0,Shubho Sengupta,3407874.0,Mark Ibrahim,1803520.0,L. V. D. Maaten,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cf5dfc4a9f7a82b32640128ca10832eace55880e,https://www.semanticscholar.org/paper/cf5dfc4a9f7a82b32640128ca10832eace55880e,Quantum machine learning models are kernel methods,"With near-term quantum devices available and the race for fault-tolerant quantum computers in full swing, researchers became interested in the question of what happens if we replace a machine learning model with a quantum circuit. While such “quantum models” are sometimes called “quantum neural networks”, it has been repeatedly noted that their mathematical structure is actually much more closely related to kernel methods: they analyse data in high-dimensional Hilbert spaces to which we only have access through inner products revealed by measurements. This technical manuscript summarises, formalises and extends the link by systematically rephrasing quantum models as a kernel method. It shows that most near-term and fault-tolerant quantum models can be replaced by a general support vector machine whose kernel computes distances between data-encoding quantum states. In particular, kernel-based training is guaranteed to find better or equally good quantum models than variational circuit training. Overall, the kernel perspective of quantum machine learning tells us that the way that data is encoded into quantum states is the main ingredient that can potentially set quantum models apart from classical machine learning models.",2021,34,124,12,False,Physics,3048564,M. Schuld,,,,,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45e21cb575c6d1de13c22bfd3e09072c9354e5d7,https://www.semanticscholar.org/paper/45e21cb575c6d1de13c22bfd3e09072c9354e5d7,From Theories to Queries: Active Learning in Practice,"This article surveys recent work in active learning aimed at making it more practical for real-world use. In general, active learning systems aim to make machine learning more economical, since they can participate in the acquisition of their own training data. An active learner might iteratively select informative query instances to be labeled by an oracle, for example. Work over the last two decades has shown that such approaches are eective at maintaining accuracy while reducing training set size in many machine learning applications. However, as we begin to deploy active learning in real ongoing learning systems and data annotation projects, we are encountering unexpected problems|due in part to practical realities that violate the basic assumptions of earlier foundational work. I review some of these issues, and discuss recent work being done to address the challenges.",2011,66,192,18,False,Computer Science,1717452,Burr Settles,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a2b5cb0285d593dde7c5a1b844ab0361aebfc85d,https://www.semanticscholar.org/paper/a2b5cb0285d593dde7c5a1b844ab0361aebfc85d,Comparative Experiments on Disambiguating Word Senses: An Illustration of the Role of Bias in Machine Learning,"This paper describes an experimental comparison of seven different learning algorithms on the problem of learning to disambiguate the meaning of a word from context. The algorithms tested include statistical, neural-network, decision-tree, rule-based, and case-based classification techniques. The specific problem tested involves disambiguating six senses of the word ``line'' using the words in the current and proceeding sentence as context. The statistical and neural-network methods perform the best on this particular problem and we discuss a potential reason for this observed difference. We also discuss the role of bias in machine learning and its importance in explaining performance differences observed on specific problems.",1996,73,256,18,False,Computer Science,1797655,R. Mooney,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7293d7440d89fc038d7e9ee8d1460cc9b3759478,https://www.semanticscholar.org/paper/7293d7440d89fc038d7e9ee8d1460cc9b3759478,"Modern Multivariate Statistical Techniques: Regression, Classification, and Manifold Learning","Remarkable advances in computation and data storage and the ready availability of huge data sets have been the keys to the growth of the new disciplines of data mining and machine learning, while the enormous success of the Human Genome Project has opened up the field of bioinformatics. These exciting developments, which led to the introduction of many innovative statistical tools for high-dimensional data analysis, are described here in detail. The author takes a broad perspective; for the first time in a book on multivariate analysis, nonlinear methods are discussed in detail as well as linear methods. Techniques covered range from traditional multivariate methods, such as multiple regression, principal components, canonical variates, linear discriminant analysis, factor analysis, clustering, multidimensional scaling, and correspondence analysis, to the newer methods of density estimation, projection pursuit, neural networks, multivariate reduced-rank regression, nonlinear manifold learning, bagging, boosting, random forests, independent component analysis, support vector machines, and classification and regression trees. Another unique feature of this book is the discussion of database management systems. This book is appropriate for advanced undergraduate students, graduate students, and researchers in statistics, computer science, artificial intelligence, psychology, cognitive sciences, business, medicine, bioinformatics, and engineering. Familiarity with multivariable calculus, linear algebra, and probability and statistics is required. The book presents a carefully-integrated mixture of theory and applications, and of classical and modern multivariate statistical techniques, including Bayesian methods. There are over 60 interesting data sets used as examples in the book, over 200 exercises, and many color illustrations and photographs.",2008,504,627,56,False,Computer Science,47209015,A. Izenman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
95d8ab48f213e01f33c3af0b34a0cc0e4fba9349,https://www.semanticscholar.org/paper/95d8ab48f213e01f33c3af0b34a0cc0e4fba9349,"Drug-target interaction prediction: databases, web servers and computational models","Identification of drug-target interactions is an important process in drug discovery. Although high-throughput screening and other biological assays are becoming available, experimental methods for drug-target interaction identification remain to be extremely costly, time-consuming and challenging even nowadays. Therefore, various computational models have been developed to predict potential drug-target associations on a large scale. In this review, databases and web servers involved in drug-target identification and drug discovery are summarized. In addition, we mainly introduced some state-of-the-art computational models for drug-target interactions prediction, including network-based method, machine learning-based method and so on. Specially, for the machine learning-based method, much attention was paid to supervised and semi-supervised models, which have essential difference in the adoption of negative samples. Although significant improvements for drug-target interaction prediction have been obtained by many effective computational models, both network-based and machine learning-based methods have their disadvantages, respectively. Furthermore, we discuss the future directions of the network-based drug discovery and network approach for personalized drug discovery based on personalized medicine, genome sequencing, tumor clone-based network and cancer hallmark-based network. Finally, we discussed the new evaluation validation framework and the formulation of drug-target interactions prediction problem by more realistic regression formulation based on quantitative bioactivity data.",2016,89,361,8,True,Computer Science,48283613,Xing Chen,7590116.0,C. Yan,2130057502.0,Xiaotian Zhang,2115462781.0,Xu Zhang,152130996.0,Feng Dai,2111611501.0,Jian Yin,Medicine,,1699819.0,Yongdong Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c95b487d304e9e56b2d10e26c4f86ec2defc447d,https://www.semanticscholar.org/paper/c95b487d304e9e56b2d10e26c4f86ec2defc447d,Engineering safety in machine learning,"Machine learning algorithms are increasingly influencing our decisions and interacting with us in all parts of our daily lives. Therefore, just like for power plants, highways, and myriad other engineered sociotechnical systems, we must consider the safety of systems involving machine learning. In this paper, we first discuss the definition of safety in terms of risk, epistemic uncertainty, and the harm incurred by unwanted outcomes. Then we examine dimensions, such as the choice of cost function and the appropriateness of minimizing the empirical average training cost, along which certain real-world applications may not be completely amenable to the foundational principle of modern statistical machine learning: empirical risk minimization. In particular, we note an emerging dichotomy of applications: ones in which safety is important and risk minimization is not the complete story (we name these Type A applications), and ones in which safety is not so critical and risk minimization is sufficient (we name these Type B applications). Finally, we discuss how four different strategies for achieving safety in engineering (inherently safe design, safety reserves, safe fail, and procedural safeguards) can be mapped to the machine learning context through inter-pretability and causality of predictive models, objectives beyond expected prediction accuracy, human involvement for labeling difficult or rare examples, and user experience design of software.",2016,22,84,4,True,Computer Science,1712865,K. Varshney,,,,,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
006b577653e0e91e1cfd37c32c6d1cfb198a730e,https://www.semanticscholar.org/paper/006b577653e0e91e1cfd37c32c6d1cfb198a730e,Partially Supervised Classification of Text Documents,"We investigate the following problem: Given a set of documents of a particular topic or class P , and a large set M of mixed documents that contains documents from class P and other types of documents, identify the documents from class P in M . The key feature of this problem is that there is no labeled nonP document, which makes traditional machine learning techniques inapplicable, as they all need labeled documents of both classes. We call this problem partially supervised classification. In this paper, we show that this problem can be posed as a constrained optimization problem and that under appropriate conditions, solutions to the constrained optimization problem will give good solutions to the partially supervised classification problem. We present a novel technique to solve the problem and demonstrate the effectiveness of the technique through extensive experimentation.",2002,34,579,60,False,Computer Science,145321667,B. Liu,1740222.0,Wee Sun Lee,144019071.0,Philip S. Yu,39952499.0,Xiaoli Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cfc1d089990f9b8652dade0dc8e221589c6430ce,https://www.semanticscholar.org/paper/cfc1d089990f9b8652dade0dc8e221589c6430ce,"Bioinspired hierarchical composite design using machine learning: simulation, additive manufacturing, and experiment","Biomimicry, adapting and implementing nature's designs provides an adequate first-order solution to achieving superior mechanical properties. However, the design space is too vast even using biomimetic designs as prototypes for optimization. Here, we propose a new approach to design hierarchical materials using machine learning, trained with a database of hundreds of thousands of structures from finite element analysis, together with a self-learning algorithm for discovering high-performing materials where inferior designs are phased out for superior candidates. Results show that our approach can create microstructural patterns that lead to tougher and stronger materials, which are validated through additive manufacturing and testing. We further show that machine learning can be used as an alternative method of coarse-graining – analyzing and designing materials without the use of full microstructural data. This novel paradigm of smart additive manufacturing can aid in the discovery and fabrication of new material designs boasting orders of magnitude increase in computational efficacy over conventional methods.",2018,40,211,1,True,Computer Science,11329466,Grace X. Gu,153246325.0,Chun‐Teh Chen,2121259897.0,Deon J. Richmond,2273480.0,M. Buehler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
65d3e7fa7b452611aa729ac789d4373d6ee505b0,https://www.semanticscholar.org/paper/65d3e7fa7b452611aa729ac789d4373d6ee505b0,fastMRI: An Open Dataset and Benchmarks for Accelerated MRI,"Accelerating Magnetic Resonance Imaging (MRI) by taking fewer measurements has the potential to reduce medical costs, minimize stress to patients and make MRI possible in applications where it is currently prohibitively slow or expensive. We introduce the fastMRI dataset, a large-scale collection of both raw MR measurements and clinical MR images, that can be used for training and evaluation of machine-learning approaches to MR image reconstruction. By introducing standardized evaluation criteria and a freely-accessible dataset, our goal is to help the community make rapid advances in the state of the art for MR image reconstruction. We also provide a self-contained introduction to MRI for machine learning researchers with no medical imaging background.",2018,63,402,65,False,Computer Science,3105120,Jure Zbontar,3597472.0,F. Knoll,8401284.0,Anuroop Sriram,2954796.0,Matthew Muckley,145219594.0,M. Bruno,34597877.0,Aaron Defazio,Engineering,Physics,48674611.0,Marc Parente,2376144.0,Krzysztof J Geras,51983841.0,Joe Katsnelson,2382287.0,H. Chandarana,2476328.0,Zizhao Zhang,3325894.0,M. Drozdzal,Mathematics,144290131.0,Adriana Romero,2066127975.0,Michael G. Rabbat,145467703.0,Pascal Vincent,2064937515.0,James Pinkerton,2111247556.0,Duo Wang,6584461.0,N. Yakubova,2057049036.0,Erich Owens,1699161.0,C. L. Zitnick,5677770.0,M. Recht,2827060.0,D. Sodickson,2376478.0,Y. Lui,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
516475a7cc96c86a4a4b31682bb0c3a48b41cb95,https://www.semanticscholar.org/paper/516475a7cc96c86a4a4b31682bb0c3a48b41cb95,Machine Learning in Financial Crisis Prediction: A Survey,"For financial institutions, the ability to predict or forecast business failures is crucial, as incorrect decisions can have direct financial consequences. Bankruptcy prediction and credit scoring are the two major research problems in the accounting and finance domain. In the literature, a number of models have been developed to predict whether borrowers are in danger of bankruptcy and whether they should be considered a good or bad credit risk. Since the 1990s, machine-learning techniques, such as neural networks and decision trees, have been studied extensively as tools for bankruptcy prediction and credit score modeling. This paper reviews 130 related journal papers from the period between 1995 and 2010, focusing on the development of state-of-the-art machine-learning techniques, including hybrid and ensemble classifiers. Related studies are compared in terms of classifier design, datasets, baselines, and other experimental factors. This paper presents the current achievements and limitations associated with the development of bankruptcy-prediction and credit-scoring models employing machine learning. We also provide suggestions for future research.",2012,168,225,9,False,Computer Science,1682393,Wei-Yang Lin,1802964.0,Ya-Han Hu,1707262.0,Chih-Fong Tsai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1f9f29020556a574fe3de44e417deb367c13c426,https://www.semanticscholar.org/paper/1f9f29020556a574fe3de44e417deb367c13c426,Predicting Dark Triad Personality Traits from Twitter Usage and a Linguistic Analysis of Tweets,"Social media sites are now the most popular destination for Internet users, providing social scientists with a great opportunity to understand online behaviour. There are a growing number of research papers related to social media, a small number of which focus on personality prediction. To date, studies have typically focused on the Big Five traits of personality, but one area which is relatively unexplored is that of the anti-social traits of narcissism, Machiavellians and psychopathy, commonly referred to as the Dark Triad. This study explored the extent to which it is possible to determine anti-social personality traits based on Twitter use. This was performed by comparing the Dark Triad and Big Five personality traits of 2,927 Twitter users with their profile attributes and use of language. Analysis shows that there are some statistically significant relationships between these variables. Through the use of crowd sourced machine learning algorithms, we show that machine learning provides useful prediction rates, but is imperfect in predicting an individual's Dark Triad traits from Twitter activity. While predictive models may be unsuitable for predicting an individual's personality, they may still be of practical importance when models are applied to large groups of people, such as gaining the ability to see whether anti-social traits are increasing or decreasing over a population. Our results raise important questions related to the unregulated use of social media analysis for screening purposes. It is important that the practical and ethical implications of drawing conclusions about personal information embedded in social media sites are better understood.",2012,45,248,32,False,Computer Science,38216109,Chris Sumner,34227452.0,A. Byers,3050127.0,Rachel Boochever,32102400.0,Gregory J. Park,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3e7f5455e0ed81de16f9327ffc12093ec8245cf7,https://www.semanticscholar.org/paper/3e7f5455e0ed81de16f9327ffc12093ec8245cf7,Multiview Privileged Support Vector Machines,"Multiview learning (MVL), by exploiting the complementary information among multiple feature sets, can improve the performance of many existing learning tasks. Support vector machine (SVM)-based models have been frequently used for MVL. A typical SVM-based MVL model is SVM-2K, which extends SVM for MVL by using the distance minimization version of kernel canonical correlation analysis. However, SVM-2K cannot fully unleash the power of the complementary information among different feature views. Recently, a framework of learning using privileged information (LUPI) has been proposed to model data with complementary information. Motivated by LUPI, we propose a new multiview privileged SVM model, multi-view privileged SVM model (PSVM-2V), for MVL. This brings a new perspective that extends LUPI to MVL. The optimization of PSVM-2V can be solved by the classical quadratic programming solver. We theoretically analyze the performance of PSVM-2V from the viewpoints of the consensus principle, the generalization error bound, and the SVM-2K learning model. Experimental results on 95 binary data sets demonstrate the effectiveness of the proposed method.",2018,49,73,4,False,Computer Science,48252042,Jingjing Tang,143790318.0,Ying-jie Tian,2151332283.0,Peng Zhang,87180246.0,Xiaohui Liu,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
94187ef33e34af2cdb42502083c6f9b4c3f5ba6b,https://www.semanticscholar.org/paper/94187ef33e34af2cdb42502083c6f9b4c3f5ba6b,Practical Black-Box Attacks against Deep Learning Systems using Adversarial Examples,"Advances in deep learning have led to the broad adoption of Deep Neural Networks (DNNs) to a range of important machine learning problems, e.g., guiding autonomous vehicles, speech recognition, malware detection. Yet, machine learning models, including DNNs, were shown to be vulnerable to adversarial samples-subtly (and often humanly indistinguishably) modified malicious inputs crafted to compromise the integrity of their outputs. Adversarial examples thus enable adversaries to manipulate system behaviors. Potential attacks include attempts to control the behavior of vehicles, have spam content identified as legitimate content, or have malware identified as legitimate software. Adversarial examples are known to transfer from one model to another, even if the second model has a different architecture or was trained on a different set. We introduce the first practical demonstration that this cross-model transfer phenomenon enables attackers to control a remotely hosted DNN with no access to the model, its parameters, or its training data. In our demonstration, we only assume that the adversary can observe outputs from the target DNN given inputs chosen by the adversary. We introduce the attack strategy of fitting a substitute model to the input-output pairs in this manner, then crafting adversarial examples based on this auxiliary model. We evaluate the approach on existing DNN datasets and real-world settings. In one experiment, we force a DNN supported by MetaMind (one of the online APIs for DNN classifiers) to mis-classify inputs at a rate of 84.24%. We conclude with experiments exploring why adversarial samples transfer between DNNs, and a discussion on the applicability of our attack when targeting machine learning algorithms distinct from DNNs.",2016,47,442,46,False,Computer Science,1967156,Nicolas Papernot,144061974.0,P. Mcdaniel,153440022.0,Ian J. Goodfellow,1680133.0,S. Jha,144643812.0,Z. B. Celik,144231976.0,A. Swami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3ff2ba8bd2658817c780b450028040ba78a10681,https://www.semanticscholar.org/paper/3ff2ba8bd2658817c780b450028040ba78a10681,Hardware for machine learning: Challenges and opportunities,"Machine learning plays a critical role in extracting meaningful information out of the zetabytes of sensor data collected every day. For some applications, the goal is to analyze and understand the data to identify trends (e.g., surveillance, portable/wearable electronics); in other applications, the goal is to take immediate action based the data (e.g., robotics/drones, self-driving cars, smart Internet of Things). For many of these applications, local embedded processing near the sensor is preferred over the cloud due to privacy or latency concerns, or hmitations in the communication bandwidth. However, at the sensor there are often stringent constraints on energy consumption and cost in addition to throughput and accuracy requirements. Furthermore, flexibility is often required such that the processing can be adapted for different applications or environments (e.g., update the weights and model in the classifier). In many applications, machine learning often involves transforming the input data into a higher dimensional space, which, along with programmable weights, increases data movement and consequently energy consumption. In this paper, we will discuss how these challenges can be addressed at various levels of hardware design ranging from architecture, hardware-friendly algorithms, mixed-signal circuits, and advanced technologies (including memories and sensors).",2016,84,137,10,True,Computer Science,1691305,V. Sze,50579876.0,Yu-hsin Chen,1399426866.0,Joel Einer,2071741.0,Amr Suleiman,48805472.0,Zhengdong Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4afa7d8e2de43b0b67366b1bce8768f5a246d153,https://www.semanticscholar.org/paper/4afa7d8e2de43b0b67366b1bce8768f5a246d153,Bayesian Optimization is Superior to Random Search for Machine Learning Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020,"This paper presents the results and insights from the black-box optimization (BBO) challenge at NeurIPS 2020 which ran from July–October, 2020. The challenge emphasized the importance of evaluating derivative-free optimizers for tuning the hyperparameters of machine learning models. This was the first black-box optimization challenge with a machine learning emphasis. It was based on tuning (validation set) performance of standard machine learning models on real datasets. This competition has widespread impact as black-box optimization (e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost every machine learning project as well as many applications outside of machine learning. The final leaderboard was determined using the optimization performance on held-out (hidden) objective functions, where the optimizers ran without human intervention. Baselines were set using the default settings of several open-source black-box optimization packages as well as random search.",2021,79,89,0,False,Computer Science,2072787126,Ryan Turner,152534486.0,David Eriksson,145849642.0,M. McCourt,2080026426.0,J. Kiili,2054108513.0,Eero Laaksonen,1819682268.0,Zhen Xu,Mathematics,,1743797.0,I. Guyon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
352056cf4cf7e44fb8ebf863c4b632b759a39344,https://www.semanticscholar.org/paper/352056cf4cf7e44fb8ebf863c4b632b759a39344,Robust Wasserstein profile inference and applications to machine learning,"We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.",2016,58,229,26,True,Mathematics,50411298,J. Blanchet,143855291.0,Yang Kang,1399018387.0,M. KarthyekRajhaaA.,,,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cef4d6ec6505d3d3dcbc9365802947dda107dba2,https://www.semanticscholar.org/paper/cef4d6ec6505d3d3dcbc9365802947dda107dba2,Top-down induction of decision trees classifiers - a survey,"Decision trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and data mining considered the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in a top-down manner. The paper suggests a unified algorithmic framework for presenting these algorithms and describes the various splitting criteria and pruning methodologies.",2005,89,727,32,True,Computer Science,1732091,L. Rokach,1770487.0,O. Maimon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fb6e85d35fc7b10c16d465fc71748d819672ad24,https://www.semanticscholar.org/paper/fb6e85d35fc7b10c16d465fc71748d819672ad24,MILEPOST GCC: machine learning based research compiler,"Tuning hardwired compiler optimizations for rapidly evolving hardware makes porting an optimizing compiler for each new platform extremely challenging. Our radical approach is to develop a modular, extensible, self-optimizing compiler that automatically learns the best optimization heuristics based on the behavior of the platform. In this paper we describe MILEPOST 1 GCC, a machine-learning-based compiler that automatically adjusts its optimization heuristics to improve the execution time, code size, or compilation time of specific programs on different architectures. Our preliminary experimental results show that it is possible to considerably reduce execution time of the MiBench benchmark suite on a range of platforms entirely automatically.",2008,39,129,12,False,Computer Science,2043821,G. Fursin,2052818.0,Cupertino Miranda,1731764.0,O. Temam,3305063.0,Mircea Namolaru,1388775854.0,E. Yom-Tov,38737579.0,A. Zaks,,,1795886.0,Bilha Mendelson,30561807.0,Edwin V. Bonilla,153031524.0,J. Thomson,2476647.0,H. Leather,145715698.0,Christopher K. I. Williams,1401533251.0,M. O’Boyle,,38759952.0,Phil Barnard,40580165.0,Elton Ashton,2630186.0,E. Courtois,1697641.0,F. Bodin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0b96c12fd4e25b949dea4ff57df033bf16b2d100,https://www.semanticscholar.org/paper/0b96c12fd4e25b949dea4ff57df033bf16b2d100,Blockchained On-Device Federated Learning,"By leveraging blockchain, this letter proposes a blockchained federated learning (BlockFL) architecture where local learning model updates are exchanged and verified. This enables on-device machine learning without any centralized training data or coordination by utilizing a consensus mechanism in blockchain. Moreover, we analyze an end-to-end latency model of BlockFL and characterize the optimal block generation rate by considering communication, computation, and consensus delays.",2018,16,277,32,True,Computer Science,145063546,Hyesung Kim,48490823.0,Jihong Park,1702172.0,M. Bennis,1692004.0,Seong-Lyun Kim,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ce2347d8fbeb3689a989ee13e7e0b4da50a6994a,https://www.semanticscholar.org/paper/ce2347d8fbeb3689a989ee13e7e0b4da50a6994a,Boosted Wrapper Induction,"Recent work in machine learning for information extraction has focused on two distinct sub-problems: the conventional problem of filling template slots from natural language text, and the problem of wrapper induction, learning simple extraction procedures (“wrappers”) for highly structured text such as Web pages produced by CGI scripts. For suitably regular domains, existing wrapper induction algorithms can efficiently learn wrappers that are simple and highly accurate, but the regularity bias of these algorithms makes them unsuitable for most conventional information extraction tasks. Boosting is a technique for improving the performance of a simple machine learning algorithm by repeatedly applying it to the training set with different example weightings. We describe an algorithm that learns simple, low-coverage wrapper-like extraction patterns, which we then apply to conventional information extraction problems using boosting. The result is BWI, a trainable information extraction system with a strong precision bias and F1 performance better than state-of-the-art techniques in many domains.",2000,16,293,30,False,Computer Science,1758106,D. Freitag,8551365.0,N. Kushmerick,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
71a85e735a3686bef8cce3725ae5ba82e2cabb1b,https://www.semanticscholar.org/paper/71a85e735a3686bef8cce3725ae5ba82e2cabb1b,Underspecification Presents Challenges for Credibility in Modern Machine Learning,"ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.",2020,149,319,23,False,Computer Science,1396841807,A. D'Amour,145993598.0,K. Heller,40497400.0,D. Moldovan,1874006.0,Ben Adlam,48401711.0,B. Alipanahi,2638246.0,Alex Beutel,Mathematics,,2110195795.0,Christina Chen,1695378.0,Jonathan Deaton,144154709.0,Jacob Eisenstein,28552618.0,M. Hoffman,2420527.0,F. Hormozdiari,2815290.0,N. Houlsby,,3108448.0,Shaobo Hou,3451901.0,Ghassen Jerfel,6413143.0,A. Karthikesalingam,34302129.0,Mario Lucic,2146275249.0,Yi-An Ma,6322777.0,C. McLean,2007712128.0,Diana Mincu,4836115.0,A. Mitani,145071265.0,A. Montanari,81408931.0,Zachary Nado,144223091.0,Vivek Natarajan,2065412355.0,Christopher Nielson,25897803.0,Thomas F. Osborne,2035210.0,R. Raman,88478180.0,K. Ramasamy,144042306.0,R. Sayres,3212089.0,J. Schrouff,6454443.0,Martin G. Seneviratne,2007741250.0,Shannon Sequeira,46537606.0,Harini Suresh,2974320.0,Victor Veitch,3316311.0,Max Vladymyrov,1524732527.0,Xuezhi Wang,20825661.0,Kellie Webster,2856607.0,S. Yadlowsky,2520251.0,Taedong Yun,2743563.0,Xiaohua Zhai,1733143.0,D. Sculley
10eec18a20954f9ac4d8f463524e5f160e23895f,https://www.semanticscholar.org/paper/10eec18a20954f9ac4d8f463524e5f160e23895f,Quantum generative adversarial networks,"Quantum machine learning is expected to be one of the first potential general-purpose applications of near-term quantum devices. A major recent breakthrough in classical machine learning is the notion of generative adversarial training, where the gradients of a discriminator model are used to train a separate generative model. In this work and a companion paper, we extend adversarial training to the quantum domain and show how to construct generative adversarial networks using quantum circuits. Furthermore, we also show how to compute gradients -- a key element in generative adversarial network training -- using another quantum circuit. We give an example of a simple practical circuit ansatz to parametrize quantum machine learning models and perform a simple numerical experiment to demonstrate that quantum generative adversarial networks can be trained successfully.",2018,34,187,10,True,Computer Science,1403778850,Pierre-Luc Dallaire-Demers,3399181.0,N. Killoran,,,,,,,,,Physics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6911114841189946f8ebf30998a60743c4a0ac06,https://www.semanticscholar.org/paper/6911114841189946f8ebf30998a60743c4a0ac06,Machine Learning in DNA Microarray Analysis for Cancer Classification,"The development of microarray technology has supplied a large volume of data to many fields. In particular, it has been applied to prediction and diagnosis of cancer, so that it expectedly helps us to exactly predict and diagnose cancer. To precisely classify cancer we have to select genes related to cancer because extracted genes from microarray have many noises. In this paper, we attempt to explore many features and classifiers using three benchmark datasets to systematically evaluate the performances of the feature selection methods and machine learning classifiers. Three benchmark datasets are Leukemia cancer dataset, Colon cancer dataset and Lymphoma cancer data set. Pearson's and Spearman's correlation coefficients, Euclidean distance, cosine coefficient, information gain, mutual information and signal to noise ratio have been used for feature selection. Multi-layer perceptron, k-nearest neighbour, support vector machine and structure adaptive self-organizing map have been used for classification. Also, we have combined the classifiers to improve the performance of classification. Experimental results show that the ensemble with several basis classifiers produces the best recognition rate on the benchmark dataset.",2003,37,254,14,False,Computer Science,1723666,Sung-Bae Cho,1763535.0,H. Won,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
07dffd27ccea93113e8af99f1dfd1996ba179ad3,https://www.semanticscholar.org/paper/07dffd27ccea93113e8af99f1dfd1996ba179ad3,Notional machines and introductory programming education,"This article brings together, summarizes, and comments on several threads of research that have contributed to our understanding of the challenges that novice programmers face when learning about the runtime dynamics of programs and the role of the computer in program execution. More specifically, the review covers the literature on programming misconceptions, the cognitive theory of mental models, constructivist theory of knowledge and learning, phenomenographic research on experiencing programming, and the theory of threshold concepts. These bodies of work are examined in relation to the concept of a “notional machine”—an abstract computer for executing programs of a particular kind. As a whole, the literature points to notional machines as a major challenge in introductory programming education. It is argued that instructors should acknowledge the notional machine as an explicit learning objective and address it in teaching. Teaching within some programming paradigms, such as object-oriented programming, may benefit from using multiple notional machines at different levels of abstraction. Pointers to some promising pedagogical techniques are provided.",2013,171,223,22,False,Computer Science,2936871,Juha Sorva,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
36652428740cd30d245d55889f01a7fb04a91c93,https://www.semanticscholar.org/paper/36652428740cd30d245d55889f01a7fb04a91c93,Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning,"Many interesting problems in machine learning are being revisited with new deep learning tools. For graph-based semi-supervised learning, a recent important development is graph convolutional networks (GCNs), which nicely integrate local vertex features and graph topology in the convolutional layers. Although the GCN model compares favorably with other state-of-the-art methods, its mechanisms are not clear and it still requires considerable amount of labeled data for validation and model selection. In this paper, we develop deeper insights into the GCN model and address its fundamental limits. First, we show that the graph convolution of the GCN model is actually a special form of Laplacian smoothing, which is the key reason why GCNs work, but it also brings potential concerns of over-smoothing with many convolutional layers. Second, to overcome the limits of the GCN model with shallow architectures, we propose both co-training and self-training approaches to train GCNs. Our approaches significantly improve GCNs in learning with very few labels, and exempt them from requiring additional labels for validation. Extensive experiments on benchmarks have verified our theory and proposals.",2018,33,1387,151,True,Computer Science,35692225,Qimai Li,40592359.0,Zhichao Han,19195265.0,Xiao-Ming Wu,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24b662202fbcfc9c6b2593d749273bc134012c17,https://www.semanticscholar.org/paper/24b662202fbcfc9c6b2593d749273bc134012c17,Electric Machines And Drives,"Market_Desc: · Electrical Engineering Students · Electrical Engineering Instructors· Power Electronics Engineers Special Features: · Easy to follow step-by-step in depth treatment of all the theory.· Computer simulation chapter describes the role of computer simulations in power electronics. Examples and problems based on Pspice and MATLAB are included.· Introductory chapter offers a review of basic electrical and magnetic circuit concepts.· A new CD-ROM contains the following:· Over 100 of new problems of varying degrees of difficulty for homework assignments and self-learning.· PSpice-based simulation examples, which illustrate basic concepts and help in design of converters.· A newly-developed magnetic component design program that demonstrates design trade-offs.· PowerPoint-based slides, which will improve the learning experience and the ease of using the book About The Book: The text includes cohesive presentation of power electronics fundamentals for applications and design in the power range of 500 kW or less. It describes a variety of practical and emerging power electronic converters made feasible by the new generation of power semiconductor devices. Topics included in this book are an expanded discussion of diode rectifiers and thyristor converters as well as chapters on heat sinks, magnetic components which present a step-by-step design approach and a computer simulation of power electronics which introduces numerical techniques and commonly used simulation packages such as PSpice, MATLAB and EMTP.",1992,0,247,9,False,Computer Science,34833461,G. Slemon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2776dcecb444e6c02a56b06480b8346e00e78353,https://www.semanticscholar.org/paper/2776dcecb444e6c02a56b06480b8346e00e78353,Wasserstein Propagation for Semi-Supervised Learning,"Probability distributions and histograms are natural representations for product ratings, traffic measurements, and other data considered in many machine learning applications. Thus, this paper introduces a technique for graph-based semi-supervised learning of histograms, derived from the theory of optimal transportation. Our method has several properties making it suitable for this application; in particular, its behavior can be characterized by the moments and shapes of the histograms at the labeled nodes. In addition, it can be used for histograms on non-standard domains like circles, revealing a strategy for manifold-valued semi-supervised learning. We also extend this technique to related problems such as smoothing distributions on graph nodes.",2014,26,113,5,False,Mathematics,1932072,J. Solomon,2559604.0,R. Rustamov,1744254.0,L. Guibas,2983668.0,Adrian Butscher,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
afbacf32916c0fb1f18eed07c078e05eb48e5a52,https://www.semanticscholar.org/paper/afbacf32916c0fb1f18eed07c078e05eb48e5a52,Survey on Feature Selection,"Feature selection plays an important role in the data mining process. It is needed to deal with the excessive number of features, which can become a computational burden on the learning algorithms. It is also necessary, even when computational resources are not scarce, since it improves the accuracy of the machine learning tasks, as we will see in the upcoming sections. In this review, we discuss the different feature selection approaches, and the relation between them and the various machine learning algorithms.",2015,41,150,12,False,Computer Science,32738445,T. Abdallah,8402134.0,B. D. L. Iglesia,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3888f7c3069fbe827175e8e69ed9e44c0b1c423d,https://www.semanticscholar.org/paper/3888f7c3069fbe827175e8e69ed9e44c0b1c423d,Chameleon: A Hybrid Secure Computation Framework for Machine Learning Applications,"We present Chameleon, a novel hybrid (mixed-protocol) framework for secure function evaluation (SFE) which enables two parties to jointly compute a function without disclosing their private inputs. Chameleon combines the best aspects of generic SFE protocols with the ones that are based upon additive secret sharing. In particular, the framework performs linear operations in the ring $\mathbbZ _2^l $ using additively secret shared values and nonlinear operations using Yao's Garbled Circuits or the Goldreich-Micali-Wigderson protocol. Chameleon departs from the common assumption of additive or linear secret sharing models where three or more parties need to communicate in the online phase: the framework allows two parties with private inputs to communicate in the online phase under the assumption of a third node generating correlated randomness in an offline phase. Almost all of the heavy cryptographic operations are precomputed in an offline phase which substantially reduces the communication overhead. Chameleon is both scalable and significantly more efficient than the ABY framework (NDSS'15) it is based on. Our framework supports signed fixed-point numbers. In particular, Chameleon's vector dot product of signed fixed-point numbers improves the efficiency of mining and classification of encrypted data for algorithms based upon heavy matrix multiplications. Our evaluation of Chameleon on a 5 layer convolutional deep neural network shows 133x and 4.2x faster executions than Microsoft CryptoNets (ICML'16) and MiniONN (CCS'17), respectively.",2018,94,316,37,True,Computer Science,143788448,M. Riazi,33811081.0,Christian Weinert,145799071.0,Oleksandr Tkachenko,2714003.0,Ebrahim M. Songhori,145139628.0,T. Schneider,3018662.0,F. Koushanfar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3993b1005796fe45d4423daa973d3fe58ae7f798,https://www.semanticscholar.org/paper/3993b1005796fe45d4423daa973d3fe58ae7f798,Graph Laplacians and their Convergence on Random Neighborhood Graphs,"Given a sample from a probability measure with support on a submanifold in Euclidean space one can construct a neighborhood graph which can be seen as an approximation of the submanifold. The graph Laplacian of such a graph is used in several machine learning methods like semi-supervised learning, dimensionality reduction and clustering. In this paper we determine the pointwise limit of three different graph Laplacians used in the literature as the sample size increases and the neighborhood size approaches zero. We show that for a uniform measure on the submanifold all graph Laplacians have the same limit up to constants. However in the case of a non-uniform measure on the submanifold only the so called random walk graph Laplacian converges to the weighted Laplace-Beltrami operator.",2006,48,255,24,False,Mathematics,143610806,Matthias Hein,3015507.0,Jean-Yves Audibert,1728654.0,U. V. Luxburg,,,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c636a2dd242908fe2e598a1077c0c57bfdea8633,https://www.semanticscholar.org/paper/c636a2dd242908fe2e598a1077c0c57bfdea8633,ReasoNet: Learning to Stop Reading in Machine Comprehension,"Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.",2016,38,295,24,True,Computer Science,1752875,Yelong Shen,2421691.0,Po-Sen Huang,1800422.0,Jianfeng Gao,2109136147.0,Weizhu Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
45c9f19b1eb46095e61f3c1a9970a6161c13a861,https://www.semanticscholar.org/paper/45c9f19b1eb46095e61f3c1a9970a6161c13a861,ARTMAP: supervised real-time learning and classification of nonstationary data by a self-organizing neural network,"Summary form only given. The authors introduced a neural network architecture, called ARTMAP, that autonomously learns to classify arbitrarily many, arbitrarily ordered vectors into recognition categories based on predictive success. This supervised learning system is built up from a pair of adaptive resonance theory modules (ART/sub a/ and ART/sub b/) that are capable of self-organizing stable recognition categories in response to arbitrary sequences of input patterns. Tested on a benchmark machine learning database in both online and offline simulations, the ARTMAP system learns orders of magnitude more quickly, efficiently, and accurately than alternative algorithms, and achieves 100% accuracy after training on less than half of the input patterns in the database.<<ETX>>",1991,30,1087,79,False,Computer Science,143809344,G. Carpenter,1682174.0,S. Grossberg,3098745.0,J. Reynolds,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c2280e7b4391748ef6decdb16b07cfe68a68d685,https://www.semanticscholar.org/paper/c2280e7b4391748ef6decdb16b07cfe68a68d685,Learning Discrete Representations via Information Maximizing Self-Augmented Training,"Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation. The task includes clustering and hash learning as special cases. Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets. However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest. To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT). In IMSAT, we use data augmentation to impose the invari-ance on discrete representations. More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion. At the same time, we maximize the information-theoretic dependency between data and their predicted discrete representations. Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.",2017,67,286,52,False,Computer Science,48594758,Weihua Hu,3213400.0,Takeru Miyato,3117618.0,Seiya Tokui,8252749.0,Eiichi Matsumoto,67154907.0,Masashi Sugiyama,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4a358d2af408326f6f1e4f6023d606a9a981c74a,https://www.semanticscholar.org/paper/4a358d2af408326f6f1e4f6023d606a9a981c74a,Software Vulnerability Analysis and Discovery Using Machine-Learning and Data-Mining Techniques,"Software security vulnerabilities are one of the critical issues in the realm of computer security. Due to their potential high severity impacts, many different approaches have been proposed in the past decades to mitigate the damages of software vulnerabilities. Machine-learning and data-mining techniques are also among the many approaches to address this issue. In this article, we provide an extensive review of the many different works in the field of software vulnerability analysis and discovery that utilize machine-learning and data-mining techniques. We review different categories of works in this domain, discuss both advantages and shortcomings, and point out challenges and some uncharted territories in the field.",2017,107,190,11,False,Computer Science,9153077,Seyed Mohammad Ghaffarian,9072622.0,H. Shahriari,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1302319324d2ad831a5da553d03aba2c9259fc21,https://www.semanticscholar.org/paper/1302319324d2ad831a5da553d03aba2c9259fc21,A Digital Liquid State Machine With Biologically Inspired Learning and Its Application to Speech Recognition,"This paper presents a bioinspired digital liquid-state machine (LSM) for low-power very-large-scale-integration (VLSI)-based machine learning applications. To the best of the authors' knowledge, this is the first work that employs a bioinspired spike-based learning algorithm for the LSM. With the proposed online learning, the LSM extracts information from input patterns on the fly without needing intermediate data storage as required in offline learning methods such as ridge regression. The proposed learning rule is local such that each synaptic weight update is based only upon the firing activities of the corresponding presynaptic and postsynaptic neurons without incurring global communications across the neural network. Compared with the backpropagation-based learning, the locality of computation in the proposed approach lends itself to efficient parallel VLSI implementation. We use subsets of the TI46 speech corpus to benchmark the bioinspired digital LSM. To reduce the complexity of the spiking neural network model without performance degradation for speech recognition, we study the impacts of synaptic models on the fading memory of the reservoir and hence the network performance. Moreover, we examine the tradeoffs between synaptic weight resolution, reservoir size, and recognition performance and present techniques to further reduce the overhead of hardware implementation. Our simulation results show that in terms of isolated word recognition evaluated using the TI46 speech corpus, the proposed digital LSM rivals the state-of-the-art hidden Markov-model-based recognizer Sphinx-4 and outperforms all other reported recognizers including the ones that are based upon the LSM or neural networks.",2015,63,115,9,True,Computer Science,2144289286,Yong Zhang,2149247466.0,Peng Li,3021143.0,Yingyezhe Jin,1744412.0,Yoonsuck Choe,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c6bbfb4fcaecc779c899af4bb52083870f4b996a,https://www.semanticscholar.org/paper/c6bbfb4fcaecc779c899af4bb52083870f4b996a,"Future Internet: The Internet of Things Architecture, Possible Applications and Key Challenges","The Internet is continuously changing and evolving. The main communication form of present Internet is human-human. The Internet of Things (IoT) can be considered as the future evaluation of the Internet that realizes machine-to-machine (M2M) learning. Thus, IoT provides connectivity for everyone and everything. The IoT embeds some intelligence in Internet-connected objects to communicate, exchange information, take decisions, invoke actions and provide amazing services. This paper addresses the existing development trends, the generic architecture of IoT, its distinguishing features and possible future applications. This paper also forecast the key challenges associated with the development of IoT. The IoT is getting increasing popularity for academia, industry as well as government that has the potential to bring significant personal, professional and economic benefits.",2012,14,1104,66,True,Computer Science,1874286,R. Khan,1727042.0,Sarmad Ullah Khan,33498709.0,Rifaqat Zaheer,2111266186.0,Shahid Khan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5a391667242b4a631acdd5917681b16a86523987,https://www.semanticscholar.org/paper/5a391667242b4a631acdd5917681b16a86523987,Deep Recurrent Neural Networks for Hyperspectral Image Classification,"In recent years, vector-based machine learning algorithms, such as random forests, support vector machines, and 1-D convolutional neural networks, have shown promising results in hyperspectral image classification. Such methodologies, nevertheless, can lead to information loss in representing hyperspectral pixels, which intrinsically have a sequence-based data structure. A recurrent neural network (RNN), an important branch of the deep learning family, is mainly designed to handle sequential data. Can sequence-based RNN be an effective method of hyperspectral image classification? In this paper, we propose a novel RNN model that can effectively analyze hyperspectral pixels as sequential data and then determine information categories via network reasoning. As far as we know, this is the first time that an RNN framework has been proposed for hyperspectral image classification. Specifically, our RNN makes use of a newly proposed activation function, parametric rectified tanh (PRetanh), for hyperspectral sequential data analysis instead of the popular tanh or rectified linear unit. The proposed activation function makes it possible to use fairly high learning rates without the risk of divergence during the training procedure. Moreover, a modified gated recurrent unit, which uses PRetanh for hidden representation, is adopted to construct the recurrent layer in our network to efficiently process hyperspectral data and reduce the total number of parameters. Experimental results on three airborne hyperspectral images suggest competitive performance in the proposed mode. In addition, the proposed network architecture opens a new window for future research, showcasing the huge potential of deep recurrent networks for hyperspectral data analysis.",2017,47,629,38,True,Computer Science,35041003,Lichao Mou,2370080.0,Pedram Ghamisi,46875441.0,Xiaoxiang Zhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
bdef7eb9b62e2a12b870957879f7a097b41f6012,https://www.semanticscholar.org/paper/bdef7eb9b62e2a12b870957879f7a097b41f6012,Cost-Sensitive Learning with Neural Networks,"In the usual setting of Machine Learning, classifiers are typically evaluated by estimating their error rate (or equi valently, the classification accuracy) on the test data. However, this mak es sense only if all errors have equal (uniform) costs. When the costs of er- rors differ between each other, the classifiers should be eva luated by comparing the total costs of the errors. Classifiers are typically designed to minimize the number of errors (incorrect classifications) made. When misclassification c osts vary between classes, this approach is not suitable. In this case the total misclassification cost should be minimized. In Machine Learning, only little work for dealing with non- uniform misclassification costs has been done. This paper pr esents a few different approaches for cost-sensitive modifications of the back- propagation learning algorithm for multilayered feedforw ard neural networks . The described approaches are thoroughly tested and eval- uated on several standard benchmark domains.",1998,13,266,13,False,Computer Science,1719813,M. Kukar,143986204.0,I. Kononenko,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9142637aab30d925a18881b79e0d3b8cdd1be1d4,https://www.semanticscholar.org/paper/9142637aab30d925a18881b79e0d3b8cdd1be1d4,A learning method for system identification,"A method for system identification is proposed which is based on the error-correcting training procedure in learning machines, and is referred to as ""learning identification."" This learning identification is nondisturbing, is applicable to cases where the input signal is random and nonstationary, and can be completed within a short time, so that it may be used to identify linear quasi-time-invariant systems in which some parameters vary slowly in comparison with the time required for identification. This merit also makes it possible to eliminate noise disturbances by means of the moving average method. Computer simulation of the learning identification was carried out and the times required for identification were obtained for various cases. Some modifications of the learning identification were also investigated together with their computer simulations.",1967,1,522,38,False,Computer Science,2101765170,J. Nagumo,52641565.0,A. Noda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9f63cc53122173598c80c367d03014d1ed49be9f,https://www.semanticscholar.org/paper/9f63cc53122173598c80c367d03014d1ed49be9f,From signals to knowledge: A conceptual model for multimodal learning analytics,"Multimodality in learning analytics and learning science is under the spotlight. The landscape of sensors and wearable trackers that can be used for learning support is evolving rapidly, as well as data collection and analysis methods. Multimodal data can now be collected and processed in real time at an unprecedented scale. With sensors, it is possible to capture observable events of the learning process such as learner's behaviour and the learning context. The learning process, however, consists also of latent attributes, such as the learner's cognitions or emotions. These attributes are unobservable to sensors and need to be elicited by human‐driven interpretations. We conducted a literature survey of experiments using multimodal data to frame the young research field of multimodal learning analytics. The survey explored the multimodal data used in related studies (the input space) and the learning theories selected (the hypothesis space). The survey led to the formulation of the Multimodal Learning Analytics Model whose main objectives are of (O1) mapping the use of multimodal data to enhance the feedback in a learning context; (O2) showing how to combine machine learning with multimodal data; and (O3) aligning the terminology used in the field of machine learning and learning science",2018,58,88,5,True,Computer Science,3413100,D. D. Mitri,40405611.0,J. Schneider,143989442.0,M. Specht,1714952.0,H. Drachsler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b48dc12387cdfde97d778c0e792a3638003544ad,https://www.semanticscholar.org/paper/b48dc12387cdfde97d778c0e792a3638003544ad,A review on machine learning principles for multi-view biological data integration,"Driven by high-throughput sequencing techniques, modern genomic and clinical studies are in a strong need of integrative machine learning models for better use of vast volumes of heterogeneous information in the deep understanding of biological systems and the development of predictive models. How data from multiple sources (called multi-view data) are incorporated in a learning system is a key step for successful analysis. In this article, we provide a comprehensive review on omics and clinical data integration techniques, from a machine learning perspective, for various analyses such as prediction, clustering, dimension reduction and association. We shall show that Bayesian models are able to use prior information and model measurements with various distributions; tree-based methods can either build a tree with all features or collectively make a final decision based on trees learned from each view; kernel methods fuse the similarity matrices learned from individual views together for a final similarity matrix or learning model; network-based fusion methods are capable of inferring direct and indirect associations in a heterogeneous network; matrix factorization models have potential to learn interactions among features from different views; and a range of deep neural networks can be integrated in multi-modal learning for capturing the complex mechanism of biological systems.",2016,194,274,7,True,Computer Science,2128128939,Yifeng Li,145391153.0,Fang-Xiang Wu,1723419.0,A. Ngom,,,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
470966e6967c1a0932fa388c7ff7e7b07875aacd,https://www.semanticscholar.org/paper/470966e6967c1a0932fa388c7ff7e7b07875aacd,"Machine Learning for Information Retrieval: Neural Networks, Symbolic Learning, and Genetic Algorithms","Information retrieval using probabilistic techniques has attracted significant attention on the part of researchers in information and computer science over the past few decades. In the 1980s, knowledge-based techniques also made an impressive contribution to “intelligent” information retrieval and indexing. More recently, information science researchers have turned to other newer artificial-intelligence-based inductive learning techniques including neural networks, symbolic learning, and genetic algorithms. These newer techniques, which are grounded on diverse paradigms, have provided great opportunities for researchers to enhance the information processing and retrieval capabilities of current information storage and retrieval systems. In this article, we first provide an overview of these newer techniques and their use in information science research. To familiarize readers with these techniques, we present three popular methods: the connectionist Hopfield network; the symbolic ID3/ID5R; and evolution-based genetic algorithms. We discuss their knowledge representations and algorithms in the context of information retrieval. Sample implementation and testing results from our own research are also provided for each technique. We believe these techniques are promising in their ability to analyze user queries, identify users' information needs, and suggest alternatives for search. With proper user-system interactions, these methods can greatly complement the prevailing full-text, keyword-based, probabilistic, and knowledge-based techniques. © 1995 John Wiley & Sons, Inc.",1995,112,306,20,True,Computer Science,47666658,Hsinchun Chen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6a0d253053ce8646e49904efe0e062bcc30d8257,https://www.semanticscholar.org/paper/6a0d253053ce8646e49904efe0e062bcc30d8257,Learning to Remember Rare Events,"Despite recent advances, memory-augmented deep neural networks are still limited when it comes to life-long and one-shot learning, especially in remembering rare events. We present a large-scale life-long memory module for use in deep learning. The module exploits fast nearest-neighbor algorithms for efficiency and thus scales to large memory sizes. Except for the nearest-neighbor query, the module is fully differentiable and trained end-to-end with no extra supervision. It operates in a life-long manner, i.e., without the need to reset it during training. Our memory module can be easily added to any part of a supervised neural network. To show its versatility we add it to a number of networks, from simple convolutional ones tested on image classification to deep sequence-to-sequence and recurrent-convolutional models. In all cases, the enhanced network gains the ability to remember and do life-long one-shot learning. Our module remembers training examples shown many thousands of steps in the past and it can successfully generalize from them. We set new state-of-the-art for one-shot learning on the Omniglot dataset and demonstrate, for the first time, life-long one-shot learning in recurrent neural networks on a large-scale machine translation task.",2017,31,292,41,False,Computer Science,40527594,Lukasz Kaiser,7624658.0,Ofir Nachum,39788470.0,Aurko Roy,1751569.0,Samy Bengio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
264efad57d38a3146c93ea243f377650f2ed31c1,https://www.semanticscholar.org/paper/264efad57d38a3146c93ea243f377650f2ed31c1,Source localization in an ocean waveguide using supervised machine learning,"Source localization in ocean acoustics is posed as a machine learning problem in which data-driven methods learn source ranges directly from observed acoustic data. The pressure received by a vertical linear array is preprocessed by constructing a normalized sample covariance matrix and used as the input for three machine learning methods: feed-forward neural networks (FNN), support vector machines (SVM), and random forests (RF). The range estimation problem is solved both as a classification problem and as a regression problem by these three machine learning algorithms. The results of range estimation for the Noise09 experiment are compared for FNN, SVM, RF, and conventional matched-field processing and demonstrate the potential of machine learning for underwater source localization.",2017,57,89,7,True,Computer Science,8792988,Haiqiang Niu,1929240.0,P. Gerstoft,145245006.0,Emma Reeves,,,,,,,Physics,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
398c296d0cc7f9d180f84969f8937e6d3a413796,https://www.semanticscholar.org/paper/398c296d0cc7f9d180f84969f8937e6d3a413796,Multi-column deep neural networks for image classification,"Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.",2012,42,3568,169,True,Computer Science,1895356,D. Ciresan,2514691.0,U. Meier,145341374.0,J. Schmidhuber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
96c9f11fd9901f2edeaab8cf6bbff2590cea93c4,https://www.semanticscholar.org/paper/96c9f11fd9901f2edeaab8cf6bbff2590cea93c4,Deep Learning using Support Vector Machines,"Recently, fully-connected and convolutional neural networks have been trained to reach state-of-the-art performance on a wide variety of tasks such as speech recognition, image classification, natural language processing, and bioinformatics data. For classification tasks, much of these “deep learning” models employ the softmax activation functions to learn output labels in 1-of-K format. In this paper, we demonstrate a small but consistent advantage of replacing softmax layer with a linear support vector machine. Learning minimizes a margin-based loss instead of the cross-entropy loss. In almost all of the previous works, hidden representation of deep networks are first learned using supervised or unsupervised techniques, and then are fed into SVMs as inputs. In contrast to those models, we are proposing to train all layers of the deep networks by backpropagating gradients through the top level SVM, learning features of all layers. Our experiments show that simply replacing softmax with linear SVMs gives significant gains on datasets MNIST, CIFAR-10, and the ICML 2013 Representation Learning Workshop’s face expression recognition challenge.",2013,17,153,20,False,Computer Science,34312504,Yichuan Tang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62d4aaaf562df94c4bdb116ee1e5cc2843c88bec,https://www.semanticscholar.org/paper/62d4aaaf562df94c4bdb116ee1e5cc2843c88bec,What Role Does Hydrological Science Play in the Age of Machine Learning?,"This paper is derived from a keynote talk given at the Google's 2020 Flood Forecasting Meets Machine Learning Workshop. Recent experiments applying deep learning to rainfall‐runoff simulation indicate that there is significantly more information in large‐scale hydrological data sets than hydrologists have been able to translate into theory or models. While there is a growing interest in machine learning in the hydrological sciences community, in many ways, our community still holds deeply subjective and nonevidence‐based preferences for models based on a certain type of “process understanding” that has historically not translated into accurate theory, models, or predictions. This commentary is a call to action for the hydrology community to focus on developing a quantitative understanding of where and when hydrological process understanding is valuable in a modeling discipline increasingly dominated by machine learning. We offer some potential perspectives and preliminary examples about how this might be accomplished.",2020,129,129,6,True,Computer Science,8451065,G. Nearing,67249817.0,Frederik Kratzert,1579255098.0,A. Sampson,34983296.0,C. Pelissier,40270530.0,D. Klotz,1492153172.0,J. Frame,,,1750699488.0,C. Prieto,2055614078.0,H. Gupta,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4b752ee2c09426abfd79f6556014ef6599d19d0d,https://www.semanticscholar.org/paper/4b752ee2c09426abfd79f6556014ef6599d19d0d,Towards Making Unlabeled Data Never Hurt,"It is usually expected that learning performance can be improved by exploiting unlabeled data, particularly when the number of labeled data is limited. However, it has been reported that, in some cases existing semi-supervised learning approaches perform even worse than supervised ones which only use labeled data. For this reason, it is desirable to develop safe semi-supervised learning approaches that will not significantly reduce learning performance when unlabeled data are used. This paper focuses on improving the safeness of semi-supervised support vector machines (S3VMs). First, the S3VM-us approach is proposed. It employs a conservative strategy and uses only the unlabeled instances that are very likely to be helpful, while avoiding the use of highly risky ones. This approach improves safeness but its performance improvement using unlabeled data is often much smaller than S3VMs. In order to develop a safe and well-performing approach, we examine the fundamental assumption of S3VMs, i.e., low-density separation. Based on the observation that multiple good candidate low-density separators may be identified from training data, safe semi-supervised support vector machines (S4VMs) are here proposed. This approach uses multiple low-density separators to approximate the ground-truth decision boundary and maximizes the improvement in performance of inductive SVMs for any candidate separator. Under the assumption employed by S3VMs, it is here shown that S4VMs are provably safe and that the performance improvement using unlabeled data can be maximized. An out-of-sample extension of S4VMs is also presented. This extension allows S4VMs to make predictions on unseen instances. Our empirical study on a broad range of data shows that the overall performance of S4VMs is highly competitive with S3VMs, whereas in contrast to S3VMs which hurt performance significantly in many cases, S4VMs rarely perform worse than inductive SVMs.",2011,62,298,41,True,Computer Science,2110463675,Yu-Feng Li,145624000.0,Zhi-Hua Zhou,,,,,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
055e2b142026cadd9ebb7b740e1641d6fafa962b,https://www.semanticscholar.org/paper/055e2b142026cadd9ebb7b740e1641d6fafa962b,Machine Unlearning,"Once users have shared their data online, it is generally difficult for them to revoke access and ask for the data to be deleted. Machine learning (ML) exacerbates this problem because any model trained with said data may have memorized it, putting users at risk of a successful privacy attack exposing their information. Yet, having models unlearn is notoriously difficult.We introduce SISA training, a framework that expedites the unlearning process by strategically limiting the influence of a data point in the training procedure. While our framework is applicable to any learning algorithm, it is designed to achieve the largest improvements for stateful algorithms like stochastic gradient descent for deep neural networks. SISA training reduces the computational overhead associated with unlearning, even in the worst-case setting where unlearning requests are made uniformly across the training set. In some cases, the service provider may have a prior on the distribution of unlearning requests that will be issued by users. We may take this prior into account to partition and order data accordingly, and further decrease overhead from unlearning.Our evaluation spans several datasets from different domains, with corresponding motivations for unlearning. Under no distributional assumptions, for simple learning tasks, we observe that SISA training improves time to unlearn points from the Purchase dataset by 4.63×, and 2.45× for the SVHN dataset, over retraining from scratch. SISA training also provides a speed-up of 1.36× in retraining for complex learning tasks such as ImageNet classification; aided by transfer learning, this results in a small degradation in accuracy. Our work contributes to practical data governance in machine unlearning.",2019,65,103,21,False,Computer Science,1452678444,Lucas Bourtoule,143754359.0,Varun Chandrasekaran,1415982317.0,Christopher A. Choquette-Choo,120074583.0,Hengrui Jia,1452679273.0,Adelin Travers,23696685.0,Baiwu Zhang,,,47412202.0,D. Lie,1967156.0,Nicolas Papernot,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4cc7507032779128bfc1b648a7892214c4eb6c43,https://www.semanticscholar.org/paper/4cc7507032779128bfc1b648a7892214c4eb6c43,Optimization with First-Order Surrogate Functions,"In this paper, we study optimization methods consisting of iteratively minimizing surrogates of an objective function. By proposing several algorithmic variants and simple convergence analyses, we make two main contributions. First, we provide a unified viewpoint for several first-order optimization techniques such as accelerated proximal gradient, block coordinate descent, or Frank-Wolfe algorithms. Second, we introduce a new incremental scheme that experimentally matches or outperforms state-of-the-art solvers for large-scale optimization problems typically arising in machine learning.",2013,41,182,21,False,Mathematics,2599292,J. Mairal,,,,,,,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
597a73c173b921b6adf17835ace73bfdb2de7111,https://www.semanticscholar.org/paper/597a73c173b921b6adf17835ace73bfdb2de7111,Modeling virtualized applications using machine learning techniques,"With the growing adoption of virtualized datacenters and cloud hosting services, the allocation and sizing of resources such as CPU, memory, and I/O bandwidth for virtual machines (VMs) is becoming increasingly important. Accurate performance modeling of an application would help users in better VM sizing, thus reducing costs. It can also benefit cloud service providers who can offer a new charging model based on the VMs' performance instead of their configured sizes. In this paper, we present techniques to model the performance of a VM-hosted application as a function of the resources allocated to the VM and the resource contention it experiences. To address this multi-dimensional modeling problem, we propose and refine the use of two machine learning techniques: artificial neural network (ANN) and support vector machine (SVM). We evaluate these modeling techniques using five virtualized applications from the RUBiS and Filebench suite of benchmarks and demonstrate that their median and 90th percentile prediction errors are within 4.36% and 29.17% respectively. These results are substantially better than regression based approaches as well as direct applications of machine learning techniques without our refinements. We also present a simple and effective approach to VM sizing and empirically demonstrate that it can deliver optimal results for 65% of the sizing problems that we studied and produces close-to-optimal sizes for the remaining 35%.",2012,40,128,14,False,Computer Science,3070722,Sajib Kundu,1709495.0,R. Rangaswami,2063996022.0,Ajay Gulati,145914105.0,Ming Zhao,145058524.0,K. Dutta,,,Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8b6e8d71d17a1fb5a7172c5d44ebba1cf06a7dd4,https://www.semanticscholar.org/paper/8b6e8d71d17a1fb5a7172c5d44ebba1cf06a7dd4,Shallow and Deep Networks Intrusion Detection System: A Taxonomy and Survey,"Intrusion detection has attracted a considerable interest from researchers and industries. The community, after many years of research, still faces the problem of building reliable and efficient IDS that are capable of handling large quantities of data, with changing patterns in real time situations. The work presented in this manuscript classifies intrusion detection systems (IDS). Moreover, a taxonomy and survey of shallow and deep networks intrusion detection systems is presented based on previous and current works. This taxonomy and survey reviews machine learning techniques and their performance in detecting anomalies. Feature selection which influences the effectiveness of machine learning (ML) IDS is discussed to explain the role of feature selection in the classification and training phase of ML IDS. Finally, a discussion of the false and true positive alarm rates is presented to help researchers model reliable and efficient machine learning based intrusion detection systems.",2017,164,184,11,False,Computer Science,8801229,Elike Hodo,2778050.0,X. Bellekens,153072596.0,Andrew W. Hamilton,2954675.0,C. Tachtatzis,153890027.0,Robert C. Atkinson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
006c846c72e77cb913be4b2c76664967e9e01ee0,https://www.semanticscholar.org/paper/006c846c72e77cb913be4b2c76664967e9e01ee0,AI^2: Training a Big Data Machine to Defend,"We present AI2, an analyst-in-the-loop security system where Analyst Intuition (AI) is put together with state-of-the-art machine learning to build a complete end-to-end Artificially Intelligent solution (AI). The system presents four key features: a big data behavioral analytics platform, an outlier detection system, a mechanism to obtain feedback from security analysts, and a supervised learning module. We validate our system with a real-world data set consisting of 3.6 billion log lines and 70.2 million entities. The results show that the system is capable of learning to defend against unseen attacks. With respect to unsupervised outlier analysis, our system improves the detection rate in 2.92× and reduces false positives by more than 5×.",2016,20,231,11,False,Computer Science,1803567,K. Veeramachaneni,2205099.0,Ignacio Arnaldo,2066975057.0,Vamsi Korrapati,115264590.0,Constantinos Bassias,69486668.0,Kuan-Ching Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9e18f56bfee171cd53702ad028a1be957683eeaa,https://www.semanticscholar.org/paper/9e18f56bfee171cd53702ad028a1be957683eeaa,Asynchronous Byzantine Machine Learning (the case of SGD),"Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than $f/n$, where $f$ is the number of Byzantine failures tolerated and $n$ the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.",2018,36,81,10,False,Computer Science,11008382,Georgios Damaskinos,9623412.0,El Mahdi El Mhamdi,1727558.0,R. Guerraoui,40051618.0,Rhicheek Patra,3035974.0,Mahsa Taziki,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b20e117d5d05cba6dd070fe50afbb8a71d9a4618,https://www.semanticscholar.org/paper/b20e117d5d05cba6dd070fe50afbb8a71d9a4618,Deep learning for neuroimaging: a validation study,"Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. These methods include deep belief networks and their building block the restricted Boltzmann machine. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.",2013,48,497,13,True,Computer Science,2122479,S. Plis,40482726.0,R. Devon Hjelm,145124475.0,R. Salakhutdinov,144048760.0,V. Calhoun,,,,,Mathematics,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a4e088a3d92f9299b6a8060c21a1082ecb251902,https://www.semanticscholar.org/paper/a4e088a3d92f9299b6a8060c21a1082ecb251902,Hyperbolic Entailment Cones for Learning Hierarchical Embeddings,"Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces. Moreover, they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.",2018,37,164,26,False,Mathematics,1882451,O. Ganea,9940262.0,Gary Bécigneul,143936663.0,Thomas Hofmann,,,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9e9d1a8ac0307b66c19c42eef07910309c04c57f,https://www.semanticscholar.org/paper/9e9d1a8ac0307b66c19c42eef07910309c04c57f,Acceleration of saddle-point searches with machine learning.,"In atomistic simulations, the location of the saddle point on the potential-energy surface (PES) gives important information on transitions between local minima, for example, via transition-state theory. However, the search for saddle points often involves hundreds or thousands of ab initio force calls, which are typically all done at full accuracy. This results in the vast majority of the computational effort being spent calculating the electronic structure of states not important to the researcher, and very little time performing the calculation of the saddle point state itself. In this work, we describe how machine learning (ML) can reduce the number of intermediate ab initio calculations needed to locate saddle points. Since machine-learning models can learn from, and thus mimic, atomistic simulations, the saddle-point search can be conducted rapidly in the machine-learning representation. The saddle-point prediction can then be verified by an ab initio calculation; if it is incorrect, this strategically has identified regions of the PES where the machine-learning representation has insufficient training data. When these training data are used to improve the machine-learning model, the estimates greatly improve. This approach can be systematized, and in two simple example problems we demonstrate a dramatic reduction in the number of ab initio force calls. We expect that this approach and future refinements will greatly accelerate searches for saddle points, as well as other searches on the potential energy surface, as machine-learning methods see greater adoption by the atomistics community.",2016,18,95,1,False,Computer Science,34641730,A. Peterson,,,,,,,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ffd42963b10315d79fce48e8aa8e185e9a109152,https://www.semanticscholar.org/paper/ffd42963b10315d79fce48e8aa8e185e9a109152,Evaluating Different Machine Learning Methods for Upscaling Evapotranspiration from Flux Towers to the Regional Scale,"Evapotranspiration (ET) is a vital variable for land‐atmosphere interactions that links surface energy balance, water, and carbon cycles. The in situ techniques can measure ET accurately but the observations have limited spatial and temporal coverage. Modeling approaches have been used to estimate ET at broad spatial and temporal scales, while accurately simulating ET at regional scales remains a major challenge. In this study, we upscale ET from eddy covariance flux tower sites to the regional scale with machine learning algorithms. Five machine learning algorithms are employed for ET upscaling including artificial neural network, Cubist, deep belief network, random forest, and support vector machine. The machine learning methods are trained and tested at 36 flux towers sites (65 site years) across the Heihe River Basin and are then applied to estimate ET for each grid cell (1 km × 1 km) within the watershed and for each day over the period 2012–2016. The artificial neural network, Cubist, random forest, and support vector machine algorithms have almost identical performance in estimating ET and have slightly lower root‐mean‐square error than deep belief network at the site scale. The random forest algorithm has slightly lower relative uncertainty at the regional scale than other methods based on three‐cornered hat method. Additionally, the machine learning methods perform better over densely vegetated conditions than barren land or sparsely vegetated conditions. The regional ET generated from the machine learning approaches captured the spatial and temporal patterns of ET at the regional scale.",2018,82,104,2,True,Environmental Science,8739781,Tongren Xu,49694840.0,Zhixia Guo,83386822.0,Shaomin Liu,2167426766.0,Xinlei He,113243901.0,Yangfanyu Meng,50070098.0,Ziwei Xu,,,40684078.0,Youlong Xia,2786708.0,Jingfeng Xiao,104755339.0,Y. Zhang,48521450.0,Yanfei Ma,2634361.0,Lisheng Song,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b65aaa2038d25387a565babd16bfa7193e3a0268,https://www.semanticscholar.org/paper/b65aaa2038d25387a565babd16bfa7193e3a0268,Adversarial Neural Machine Translation,"In this paper, we study a new learning paradigm for Neural Machine Translation (NMT). Instead of maximizing the likelihood of the human translation as in previous works, we minimize the distinction between human translation and the translation given by an NMT model. To achieve this goal, inspired by the recent success of generative adversarial networks (GANs), we employ an adversarial training architecture and name it as Adversarial-NMT. In Adversarial-NMT, the training of the NMT model is assisted by an adversary, which is an elaborately designed Convolutional Neural Network (CNN). The goal of the adversary is to differentiate the translation result generated by the NMT model from that by human. The goal of the NMT model is to produce high quality translations so as to cheat the adversary. A policy gradient method is leveraged to co-train the NMT model and the adversary. Experimental results on English$\rightarrow$French and German$\rightarrow$English translation tasks show that Adversarial-NMT can achieve significantly better translation quality than several strong baselines.",2017,47,121,15,False,Computer Science,47767791,Lijun Wu,2794096.0,Yingce Xia,2004786514.0,Li Zhao,143853336.0,Fei Tian,143826491.0,Tao Qin,66117656.0,J. Lai,Mathematics,,2110264337.0,Tie-Yan Liu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8dff4207615846c2b65d79383363001fa1d60c8c,https://www.semanticscholar.org/paper/8dff4207615846c2b65d79383363001fa1d60c8c,Unachievable Region in Precision-Recall Space and Its Effect on Empirical Evaluation,"Precision-recall (PR) curves and the areas under them are widely used to summarize machine learning results, especially for data sets exhibiting class skew. They are often used analogously to ROC curves and the area under ROC curves. It is known that PR curves vary as class skew changes. What was not recognized before this paper is that there is a region of PR space that is completely unachievable, and the size of this region depends only on the skew. This paper precisely characterizes the size of that region and discusses its implications for empirical evaluation methodology in machine learning.",2012,20,81,11,False,Medicine,35817698,Kendrick Boyd,144815446.0,Jesse Davis,144334798.0,David Page,1706989.0,V. S. Costa,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cae9d90524cccac5081666985d5d055b71697cee,https://www.semanticscholar.org/paper/cae9d90524cccac5081666985d5d055b71697cee,Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions,"Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions offering both imperative and declarative styles, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]",2018,96,313,42,False,Computer Science,1800919,Nicolas Vasilache,2054833547.0,O. Zinenko,2097631216.0,Theodoros Theodoridis,47316088.0,Priya Goyal,2375710.0,Zach DeVito,16521420.0,William S. Moses,,,1772079.0,Sven Verdoolaege,144187067.0,Andrew Adams,145509576.0,Albert Cohen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4e4c56efc132ec1506cf7edb1077608b2483354e,https://www.semanticscholar.org/paper/4e4c56efc132ec1506cf7edb1077608b2483354e,On breast cancer detection: an application of machine learning algorithms on the wisconsin diagnostic dataset,"This paper presents a comparison of six machine learning (ML) algorithms: GRU-SVM[1], Linear Regression, Multilayer Perceptron (MLP), Nearest Neighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on the Wisconsin Diagnostic Breast Cancer (WDBC) dataset[2] by measuring their classification test accuracy, and their sensitivity and specificity values. The said dataset consists of features which were computed from digitized images of FNA tests on a breast mass[2]. For the implementation of the ML algorithms, the dataset was partitioned in the following fashion: 70% for training phase, and 30% for the testing phase. The hyper-parameters used for all the classifiers were manually assigned. Results show that all the presented ML algorithms performed well (all exceeded 90% test accuracy) on the classification task. The MLP algorithm stands out among the implemented algorithms with a test accuracy of ≈99.04%.",2017,20,140,5,True,Computer Science,26412983,Abien Fred Agarap,,,,,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1f6b9766374d14d81c225c2ced5bb02fe0bccd43,https://www.semanticscholar.org/paper/1f6b9766374d14d81c225c2ced5bb02fe0bccd43,The What-If Tool: Interactive Probing of Machine Learning Models,"A key challenge in developing and deploying Machine Learning (ML) systems is understanding their performance across a wide range of inputs. To address this challenge, we created the What-If Tool, an open-source application that allows practitioners to probe, visualize, and analyze ML systems, with minimal coding. The What-If Tool lets practitioners test performance in hypothetical situations, analyze the importance of different data features, and visualize model behavior across multiple models and subsets of input data. It also lets practitioners measure systems according to multiple ML fairness metrics. We describe the design of the tool, and report on real-life usage at different organizations.",2019,30,225,14,True,Medicine,49556437,James Wexler,51478016.0,Mahima Pushkarna,2843215.0,Tolga Bolukbasi,145233583.0,M. Wattenberg,1765169.0,F. Viégas,2109357255.0,Jimbo Wilson,Computer Science,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7313c770c84af50aa8e34ec21c8e50413f99c89f,https://www.semanticscholar.org/paper/7313c770c84af50aa8e34ec21c8e50413f99c89f,"A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches","Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.",2012,121,1981,142,False,Computer Science,1924061,M. Galar,143741583.0,Alberto Fernández,1695535.0,E. Tartas,1715030.0,H. Bustince,1699069.0,F. Herrera,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060,https://www.semanticscholar.org/paper/ab68ddbdd8d0b61d9f9c8fa500a4c13d06158060,Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning,"The mutual information is a core statistical quantity that has applications in all areas of machine learning, whether this is in training of density models over multiple data modalities, in maximising the efficiency of noisy transmission channels, or when learning behaviour policies for exploration by artificial agents. Most learning algorithms that involve optimisation of the mutual information rely on the Blahut-Arimoto algorithm --- an enumerative algorithm with exponential complexity that is not suitable for modern machine learning applications. This paper provides a new approach for scalable optimisation of the mutual information by merging techniques from variational inference and deep learning. We develop our approach by focusing on the problem of intrinsically-motivated learning, where the mutual information forms the definition of a well-known internal drive known as empowerment. Using a variational lower bound on the mutual information, combined with convolutional networks for handling visual input streams, we develop a stochastic optimisation algorithm that allows for scalable information maximisation and empowerment-based reasoning directly from pixels to actions.",2015,32,324,16,False,Computer Science,14594344,S. Mohamed,1748523.0,Danilo Jimenez Rezende,,,,,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a65b0ade2d7b4624230ba8a919235208c84b4831,https://www.semanticscholar.org/paper/a65b0ade2d7b4624230ba8a919235208c84b4831,Home,"The team makes fundamental and applied research in the analysis of complex natural signals using paradigms and methods from Statistical Physics such as: scale invariance, predictabilit y , universal ity classes We study the parameters related to common statistical organization in different complex signals and systems, we derive new types of sparse and compact representations , and machine learning approaches . We are also developing tools for the analysis of complex signals that better match the statistical and geometrical organisation inside these data: as a typical example, we cite the evaluation of cascading properties of physical variables inside complex signals.",2016,15,1314,108,True,,2068514638,A. Doria,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2f648f983164871656c579c5e043a3dd251f61f7,https://www.semanticscholar.org/paper/2f648f983164871656c579c5e043a3dd251f61f7,Evaluation of Machine-Learning Algorithms for Predicting Opioid Overdose Risk Among Medicare Beneficiaries With Opioid Prescriptions,"Key Points Question Can machine-learning approaches predict opioid overdose risk among fee-for-service Medicare beneficiaries? Findings In this prognostic study of the administrative claims data of 560 057 Medicare beneficiaries, the deep neural network and gradient boosting machine models outperformed other methods for identifying risk, although positive predictive values were low given the low prevalence of overdose episodes. Meaning Machine-learning algorithms using administrative data appear to be a valuable and feasible tool for more accurate identification of opioid overdose risk.",2019,88,111,0,False,Medicine,1397210927,W. Lo-Ciganic,2110302871.0,James L. Huang,1399987315.0,Hao Helen Zhang,1898068.0,Jeremy C. Weiss,2115859099.0,Yonghui Wu,1380127072.0,C. Kwoh,,,144175602.0,J. Donohue,38091515.0,Gerald T. Cochran,144907004.0,A. Gordon,1753795.0,D. Malone,11543058.0,Courtney C. Kuza,4208860.0,W. Gellad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
