paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,fieldsOfStudy/1,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,authors/5/authorId,authors/5/name,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,fieldsOfStudy/2
95aef2d99334420731593bb392e6397c48d722a0,https://www.semanticscholar.org/paper/95aef2d99334420731593bb392e6397c48d722a0,Prediction of MicroRNA-Disease Associations Based on Social Network Analysis Methods,"MicroRNAs constitute an important class of noncoding, single-stranded, ~22 nucleotide long RNA molecules encoded by endogenous genes. They play an important role in regulating gene transcription and the regulation of normal development. MicroRNAs can be associated with disease; however, only a few microRNA-disease associations have been confirmed by traditional experimental approaches. We introduce two methods to predict microRNA-disease association. The first method, KATZ, focuses on integrating the social network analysis method with machine learning and is based on networks derived from known microRNA-disease associations, disease-disease associations, and microRNA-microRNA associations. The other method, CATAPULT, is a supervised machine learning method. We applied the two methods to 242 known microRNA-disease associations and evaluated their performance using leave-one-out cross-validation and 3-fold cross-validation. Experiments proved that our methods outperformed the state-of-the-art methods.",2015,67,144,2,True,Biology,Medicine,144268946,Q. Zou,2108952705.0,Jinjin Li,40300673.0,Qingqi Hong,144906697.0,Ziyu Lin,2108408362.0,Yun Wu,2108701882.0,Hua Shi,144250293.0,Y. Ju,,,,,,,,,,,
8cf7496c3303fbc4b74ccc6ccba135be3b94aca2,https://www.semanticscholar.org/paper/8cf7496c3303fbc4b74ccc6ccba135be3b94aca2,Classifying Single-Trial EEG During Motor Imagery by Iterative Spatio-Spectral Patterns Learning (ISSPL),"In most current motor-imagery-based brain-computer interfaces (BCIs), machine learning is carried out in two consecutive stages: feature extraction and feature classification. Feature extraction has focused on automatic learning of spatial filters, with little or no attention being paid to optimization of parameters for temporal filters that still require time-consuming, ad hoc manual tuning. In this paper, we present a new algorithm termed iterative spatio-spectral patterns learning (ISSPL) that employs statistical learning theory to perform automatic learning of spatio-spectral filters. In ISSPL, spectral filters and the classifier are simultaneously parameterized for optimization to achieve good generalization performance. A detailed derivation and theoretical analysis of ISSPL are given. Experimental results on two datasets show that the proposed algorithm can correctly identify the discriminative frequency bands, demonstrating the algorithm's superiority over contemporary approaches in classification performance.",2008,32,164,16,False,Computer Science,Medicine,39533001,Wei Wu,1782783.0,Xiaorong Gao,145031714.0,Bo Hong,145834126.0,Shangkai Gao,,,,,,,,,,,,,,,,,
6a060a59e0595ca487d89f70281dcc8a40fd36f6,https://www.semanticscholar.org/paper/6a060a59e0595ca487d89f70281dcc8a40fd36f6,The promise of machine learning in predicting treatment outcomes in psychiatry,"For many years, psychiatrists have tried to understand factors involved in response to medications or psychotherapies, in order to personalize their treatment choices. There is now a broad and growing interest in the idea that we can develop models to personalize treatment decisions using new statistical approaches from the field of machine learning and applying them to larger volumes of data. In this pursuit, there has been a paradigm shift away from experimental studies to confirm or refute specific hypotheses towards a focus on the overall explanatory power of a predictive model when tested on new, unseen datasets. In this paper, we review key studies using machine learning to predict treatment outcomes in psychiatry, ranging from medications and psychotherapies to digital interventions and neurobiological treatments. Next, we focus on some new sources of data that are being used for the development of predictive models based on machine learning, such as electronic health records, smartphone and social media data, and on the potential utility of data from genetics, electrophysiology, neuroimaging and cognitive testing. Finally, we discuss how far the field has come towards implementing prediction tools in real‐world clinical practice. Relatively few retrospective studies to‐date include appropriate external validation procedures, and there are even fewer prospective studies testing the clinical feasibility and effectiveness of predictive models. Applications of machine learning in psychiatry face some of the same ethical challenges posed by these techniques in other areas of medicine or computer science, which we discuss here. In short, machine learning is a nascent but important approach to improve the effectiveness of mental health care, and several prospective clinical studies suggest that it may be working already.",2021,239,68,2,False,Medicine,,6609752,A. Chekroud,82934036.0,Julia Bondar,145171115.0,Jaime Delgadillo,2659843.0,Gavin Doherty,83024519.0,Akash R. Wasil,38565406.0,M. Fokkema,34221966.0,Z. Cohen,145763736.0,D. Belgrave,4423584.0,R. DeRubeis,2046648.0,R. Iniesta,5672681.0,D. Dwyer,6047002.0,Karmel W. Choi,
6bb476f85f913786220afd3a34f56b3282396270,https://www.semanticscholar.org/paper/6bb476f85f913786220afd3a34f56b3282396270,Low-Power Circuits for Brain–Machine Interfaces,"This paper presents work on ultra-low-power circuits for brain–machine interfaces with applications for paralysis prosthetics, stroke, Parkinson's disease, epilepsy, prosthetics for the blind, and experimental neuroscience systems. The circuits include a micropower neural amplifier with adaptive power biasing for use in multi-electrode arrays; an analog linear decoding and learning architecture for data compression; low-power radio-frequency (RF) impedance-modulation circuits for data telemetry that minimize power consumption of implanted systems in the body; a wireless link for efficient power transfer; mixed-signal system integration for efficiency, robustness, and programmability; and circuits for wireless stimulation of neurons with power-conserving sleep modes and awake modes. Experimental results from chips that have stimulated and recorded from neurons in the zebra finch brain and results from RF power-link, RF data-link, electrode-recording and electrode-stimulating systems are presented. Simulations of analog learning circuits that have successfully decoded prerecorded neural signals from a monkey brain are also presented.",2008,35,98,6,True,Computer Science,Medicine,1994245,R. Sarpeshkar,2719728.0,W. Wattanapanitch,2489215.0,Scott K. Arfin,2542919.0,B. Rapoport,2432757.0,S. Mandal,48564101.0,M. Baker,2200355.0,M. Fee,145299648.0,S. Musallam,2189155.0,R. Andersen,,,,,,,
e46429c5522d6a598b1f3e6c29a87fbeaf118538,https://www.semanticscholar.org/paper/e46429c5522d6a598b1f3e6c29a87fbeaf118538,Machine Learning Based DDoS Attack Detection from Source Side in Cloud,"Denial of service (DOS) attacks are a serious threat to network security. These attacks are often sourced from virtual machines in the cloud, rather than from the attacker's own machine, to achieve anonymity and higher network bandwidth. Past research focused on analyzing traffic on the destination (victim's) side with predefined thresholds. These approaches have significant disadvantages. They are only passive defenses after the attack, they cannot use the outbound statistical features of attacks, and it is hard to trace back to the attacker with these approaches. In this paper, we propose a DOS attack detection system on the source side in the cloud, based on machine learning techniques. This system leverages statistical information from both the cloud server's hypervisor and the virtual machines, to prevent network packages from being sent out to the outside network. We evaluate nine machine learning algorithms and carefully compare their performance. Our experimental results show that more than 99.7% of four kinds of DOS attacks are successfully detected. Our approach does not degrade performance and can be easily extended to broader DOS attacks.",2017,16,95,10,False,Computer Science,,21145493,Zecheng He,2146331573.0,Tianwei Zhang,144220460.0,R. Lee,,,,,,,,,,,,,,,,,,,
73e1644ad4aab9e11c1e46eea513141c4930602d,https://www.semanticscholar.org/paper/73e1644ad4aab9e11c1e46eea513141c4930602d,A Literature Survey on Domain Adaptation of Statistical Classifiers,"The domain adaptation problem, especially domain adaptation in natural language processing, started gaining much attention very recently [Daumé III and Marcu, 2006, Blitzer et al., 2006, Ben-David et al., 2007, Daumé III, 2007, Satpal and Sarawagi, 2007]. However, some special kinds of domain adaptation problems have been studied before under different names such as class imbalance [Japkowicz and Stephen, 2002], covariate shift [Shimodaira, 2000], and sample selection bias [Heckman, 1979]. There are also some well-studied machine learning problems that are closely related but not equivalent to domain adaptation, including multi-task learning [Caruana, 1997] and semi-supervised learning [Chapelle et al., 2006]. In this literature survey, we review existing work in both the machine learning and the natural language processing communities related to domain adaptation. Because this relatively new topic is constantly attracting attention, our survey is necessarily incomplete. Nevertheless, we try to cover the major lines of work that we are aware of up to the date this survey is written. This survey will also be constantly updated. The goal of this literature survey is twofold. First, existing studies on domain adaptation seem very different from each other, and different terms are used to refer to the problem. There has not been any survey that connects these different studies. This survey thus tries to organize the existing work in a systematic way and draw a big picture of the domain adaptation problem with its possible solutions. Second, a systematic literature survey shows the limitations of current work and points out promising directions that should be explored.",2007,40,257,13,False,Computer Science,,143610475,James J. Jiang,,,,,,,,,,,,,,,,,,,,,,,
2e2e9b6b88fcef57b8093becb28aa1af8f094c5e,https://www.semanticscholar.org/paper/2e2e9b6b88fcef57b8093becb28aa1af8f094c5e,An Empirical Study of Rich Subgroup Fairness for Machine Learning,"Kearns, Neel, Roth, and Wu [ICML 2018] recently proposed a notion of rich subgroup fairness intended to bridge the gap between statistical and individual notions of fairness. Rich subgroup fairness picks a statistical fairness constraint (say, equalizing false positive rates across protected groups), but then asks that this constraint hold over an exponentially or infinitely large collection of subgroups defined by a class of functions with bounded VC dimension. They give an algorithm guaranteed to learn subject to this constraint, under the condition that it has access to oracles for perfectly learning absent a fairness constraint. In this paper, we undertake an extensive empirical evaluation of the algorithm of Kearns et al. On four real datasets for which fairness is a concern, we investigate the basic convergence of the algorithm when instantiated with fast heuristics in place of learning oracles, measure the tradeoffs between fairness and accuracy, and compare this approach with the recent algorithm of Agarwal, Beygelzeimer, Dudik, Langford, and Wallach [ICML 2018], which implements weaker and more traditional marginal fairness constraints defined by individual protected attributes. We find that in general, the Kearns et al. algorithm converges quickly, large gains in fairness can be obtained with mild costs to accuracy, and that optimizing accuracy subject only to marginal fairness leads to classifiers with substantial subgroup unfairness. We also provide a number of analyses and visualizations of the dynamics and behavior of the Kearns et al. algorithm. Overall we find this algorithm to be effective on real data, and rich subgroup fairness to be a viable notion in practice.",2018,17,116,6,True,Computer Science,Mathematics,81338045,M. Kearns,5880154.0,Seth Neel,1682008.0,Aaron Roth,1768074.0,Zhiwei Steven Wu,,,,,,,,,,,,,,,,,
d89945f77470b6a1dabd1f224f10b7d096fd9435,https://www.semanticscholar.org/paper/d89945f77470b6a1dabd1f224f10b7d096fd9435,A machine learning based approach for table detection on the web,"Table is a commonly used presentation scheme, especially for describing relational information. However, table understanding remains an open problem. In this paper, we consider the problem of table detection in web documents. Its potential applications include web mining, knowledge management, and web content summarization and delivery to narrow-bandwidth devices. We describe a machine learning based approach to classify each given table entity as either genuine or non-genuine. Various features reflecting the layout as well as content characteristics of tables are studied.In order to facilitate the training and evaluation of our table classifier, we designed a novel web document table ground truthing protocol and used it to build a large table ground truth database. The database consists of 1,393 HTML files collected from hundreds of different web sites and contains 11,477 leaf TABLE elements, out of which 1,740 are genuine tables. Experiments were conducted using the cross validation method and an F-measure of 95.89% was achieved.",2002,24,232,25,False,Computer Science,,2115625054,Yalin Wang,47756656.0,Jianying Hu,,,,,,,,,,,,,,,,,,,,,
aef0161883cfd49bbe26826cf2e40f8195ce59cf,https://www.semanticscholar.org/paper/aef0161883cfd49bbe26826cf2e40f8195ce59cf,Fast approximation of matrix coherence and statistical leverage,"The statistical leverage scores of a data matrix are the squared row-norms of any matrix whose columns are obtained by orthogonalizing the columns of the data matrix; and, the coherence is the largest leverage score. These quantities play an important role in several machine learning algorithms because they capture the key structural nonuniformity of the data matrix that must be dealt with in developing efficient randomized algorithms. Our main result is a randomized algorithm that takes as input an arbitrary n × d matrix A, with n ≫ d, and returns, as output, relative-error approximations to all n of the statistical leverage scores. The proposed algorithm runs in O(nd log n) time, as opposed to the O(nd2) time required by the naive algorithm that involves computing an orthogonal basis for the range of A. This resolves an open question from (Drineas et al., 2006) and (Mohri & Talwalkar, 2011); and our result leads to immediate improvements in coreset-based l2-regression, the estimation of the coherence of a matrix, and several related low-rank matrix problems. Interestingly, to achieve our result we judiciously apply random projections on both sides of A.",2011,54,441,39,False,Computer Science,Mathematics,1738441,P. Drineas,1399249722.0,M. Magdon-Ismail,143884206.0,Michael W. Mahoney,143982862.0,David P. Woodruff,,,,,,,,,,,,,,,,,
70b138c71256416c2b881b27e5d082b98382e4de,https://www.semanticscholar.org/paper/70b138c71256416c2b881b27e5d082b98382e4de,Comparison of Machine Learning Methods With National Cardiovascular Data Registry Models for Prediction of Risk of Bleeding After Percutaneous Coronary Intervention,"Key Points Question Can machine learning techniques, bolstered by better selection of variables, improve prediction of major bleeding after percutaneous coronary intervention (PCI)? Findings In this comparative effectiveness study that modeled more than 3 million PCI procedures, machine learning techniques improved the prediction of post-PCI major bleeding to a C statistic of 0.82 compared with a C statistic of 0.78 from the existing model. Machine learning techniques improved the identification of an additional 3.7% of bleeding cases and 1.0% of nonbleeding cases. Meaning By leveraging more complex, raw variables, machine learning techniques are better able to identify patients at risk for major bleeding and who can benefit from bleeding avoidance therapies.",2019,20,41,0,True,Medicine,,2476394,B. Mortazavi,1883747.0,E. Bucholz,144766923.0,N. Desai,78686870.0,Chenxi Huang,1911669.0,J. Curtis,4248092.0,F. Masoudi,144489052.0,R. Shaw,145537739.0,S. Negahban,2467795.0,H. Krumholz,,,,,,,
629ae15be0be0d64d5d5cd1ab57cfcd1f1a08c1c,https://www.semanticscholar.org/paper/629ae15be0be0d64d5d5cd1ab57cfcd1f1a08c1c,Machine Learning for Healthcare: On the Verge of a Major Shift in Healthcare Epidemiology,"The increasing availability of electronic health data presents a major opportunity in healthcare for both discovery and practical applications to improve healthcare. However, for healthcare epidemiologists to best use these data, computational techniques that can handle large complex datasets are required. Machine learning (ML), the study of tools and methods for identifying patterns in data, can help. The appropriate application of ML to these data promises to transform patient risk stratification broadly in the field of medicine and especially in infectious diseases. This, in turn, could lead to targeted interventions that reduce the spread of healthcare-associated pathogens. In this review, we begin with an introduction to the basics of ML. We then move on to discuss how ML can transform healthcare epidemiology, providing examples of successful applications. Finally, we present special considerations for those healthcare epidemiologists who want to use and apply ML.",2018,39,234,7,True,Medicine,,38556322,J. Wiens,6742715.0,E. Shenoy,,,,,,,,,,,,,,,,,,,,,
b3ee60a093591ff1c5002df76a2905b729a3b3c1,https://www.semanticscholar.org/paper/b3ee60a093591ff1c5002df76a2905b729a3b3c1,TerpreT: A Probabilistic Programming Language for Program Induction,"We study machine learning formulations of inductive program synthesis; given input-output examples, we try to synthesize source code that maps inputs to corresponding outputs. Our aims are to develop new machine learning approaches based on neural networks and graphical models, and to understand the capabilities of machine learning techniques relative to traditional alternatives, such as those based on constraint solving from the programming languages community. Our key contribution is the proposal of TerpreT, a domain-specific language for expressing program synthesis problems. TerpreT is similar to a probabilistic programming language: a model is composed of a specification of a program representation (declarations of random variables) and an interpreter describing how programs map inputs to outputs (a model connecting unknowns to observations). The inference task is to observe a set of input-output examples and infer the underlying program. TerpreT has two main benefits. First, it enables rapid exploration of a range of domains, program representations, and interpreter models. Second, it separates the model specification from the inference algorithm, allowing like-to-like comparisons between different approaches to inference. From a single TerpreT specification we automatically perform inference using four different back-ends. These are based on gradient descent, linear program (LP) relaxations for graphical models, discrete satisfiability solving, and the Sketch program synthesis system. We illustrate the value of TerpreT by developing several interpreter models and performing an empirical comparison between alternative inference algorithms. Our key empirical finding is that constraint solvers dominate the gradient descent and LP-based formulations. We conclude with suggestions for the machine learning community to make progress on program synthesis.",2016,71,116,6,False,Computer Science,,35058304,Alexander L. Gaunt,2107692.0,Marc Brockschmidt,50631599.0,Rishabh Singh,1684887.0,Nate Kushman,143967473.0,Pushmeet Kohli,2110315975.0,Jonathan Taylor,1725299.0,Daniel Tarlow,,,,,,,,,,,
0501505416977d9d97e4df1f1e296c15362033aa,https://www.semanticscholar.org/paper/0501505416977d9d97e4df1f1e296c15362033aa,Neuromorphic accelerators: A comparison between neuroscience and machine-learning approaches,"A vast array of devices, ranging from industrial robots to self-driven cars or smartphones, require increasingly sophisticated processing of real-world input data (image, voice, radio, …). Interestingly, hardware neural network accelerators are emerging again as attractive candidate architectures for such tasks. The neural network algorithms considered come from two, largely separate, domains: machine-learning and neuroscience. These neural networks have very different characteristics, so it is unclear which approach should be favored for hardware implementation. Yet, few studies compare them from a hardware perspective. We implement both types of networks down to the layout, and we compare the relative merit of each approach in terms of energy, speed, area cost, accuracy and functionality. Within the limit of our study (current SNN and machine learning NN algorithms, current best effort at hardware implementation efforts, and workloads used in this study), our analysis helps dispel the notion that hardware neural network accelerators inspired from neuroscience, such as SNN+STDP, are currently a competitive alternative to hardware neural networks accelerators inspired from machine-learning, such as MLP+BP: not only in terms of accuracy, but also in terms of hardware cost for realistic implementations, which is less expected. However, we also outline that SNN+STDP carry potential for reduced hardware cost compared to machine-learning networks at very large scales, if accuracy issues can be controlled (or for applications where they are less important). We also identify the key sources of inaccuracy of SNN+STDP which are less related to the loss of information due to spike coding than to the nature of the STDP learning algorithm. Finally, we outline that for the category of applications which require permanent online learning and moderate accuracy, SNN+STDP hardware accelerators could be a very cost-efficient solution.",2015,68,69,9,False,Computer Science,,1678776,Zidong Du,31851334.0,D. B. Rubin,7377735.0,Yunji Chen,37167270.0,Liqiang He,144049725.0,Tianshi Chen,2152827560.0,Lei Zhang,7514065.0,Chengyong Wu,1731764.0,O. Temam,,,,,,,,,
ddb6d9a09f2b06235e650ff7da8f7028d8128167,https://www.semanticscholar.org/paper/ddb6d9a09f2b06235e650ff7da8f7028d8128167,Classification of Breast Cancer Based on Histology Images Using Convolutional Neural Networks,"In recent years, the classification of breast cancer has been the topic of interest in the field of Healthcare informatics, because it is the second main cause of cancer-related deaths in women. Breast cancer can be identified using a biopsy where tissue is removed and studied under microscope. The diagnosis is based on the qualification of the histopathologist, who will look for abnormal cells. However, if the histopathologist is not well-trained, this may lead to wrong diagnosis. With the recent advances in image processing and machine learning, there is an interest in attempting to develop a reliable pattern recognition based systems to improve the quality of diagnosis. In this paper, we compare two machine learning approaches for the automatic classification of breast cancer histology images into benign and malignant and into benign and malignant sub-classes. The first approach is based on the extraction of a set of handcrafted features encoded by two coding models (bag of words and locality constrained linear coding) and trained by support vector machines, while the second approach is based on the design of convolutional neural networks. We have also experimentally tested dataset augmentation techniques to enhance the accuracy of the convolutional neural network as well as “handcrafted features + convolutional neural network” and “ convolutional neural network features + classifier” configurations. The results show convolutional neural networks outperformed the handcrafted feature based classifier, where we achieved accuracy between 96.15% and 98.33% for the binary classification and 83.31% and 88.23% for the multi-class classification.",2018,44,190,5,False,Computer Science,,41036423,Dalal Bardou,2119016624.0,Kun Zhang,48456818.0,Sayed Mohammad Ahmad,,,,,,,,,,,,,,,,,,,
269d91e79049092bdf0651241d0d66830aa9fafc,https://www.semanticscholar.org/paper/269d91e79049092bdf0651241d0d66830aa9fafc,Machine Learning Algorithms for Opinion Mining and Sentiment Classification,"With the evolution of web technology, there is a huge amount of data present in the web for the internet users. These users not only use the available resources in the web, but also give their feedback, thus generating additional useful information. Due to overwhelming amount of user's opinions, views, feedback and suggestions available through the web resources, it's very much essential to explore, analyse and organize their views for better decision making. Opinion Mining or Sentiment Analysis is a Natural Language Processing and Information Extraction task that identifies the user's views or opinions explained in the form of positive, negative or neutral comments and quotes underlying the text. Various supervised or data-driven techniques to Sentiment analysis like Naive Byes, Maximum Entropy and SVM.For classification use support vector machine (SVM), it performs the sentiment classification task also consider sentiment classification accuracy.",2013,17,110,7,False,Computer Science,,70269607,J. Khairnar,9414124.0,M. Kinikar,,,,,,,,,,,,,,,,,,,,,
e95b069306122e24317500dc6a4a3a6f98d9a938,https://www.semanticscholar.org/paper/e95b069306122e24317500dc6a4a3a6f98d9a938,Plant disease and pest detection using deep learning-based features,"The timely and accurate diagnosis of plant diseases plays an important role in preventing the loss of productivity and loss or reduced quantity of agricultural products. In order to solve such problems, methods based on machine learning can be used. In recent years, deep learning, which is especially widely used in image processing, offers many new applications related to precision agriculture. In this study, we evaluated the performance results using different approaches of nine powerful architectures of deep neural networks for plant disease detection. Transfer learning and deep feature extraction methods are used, which adapt these deep learning models to the problem at hand. The utilized pretrained deep models are considered in the presented work for feature extraction and for further fine-tuning. The obtained features using deep feature extraction are then classified by support vector machine (SVM), extreme learning machine (ELM), and K-nearest neighbor (KNN) methods. The experiments are carried out using data consisting of real disease and pest images from Turkey. The accuracy, sensitivity, specificity, and F1-score are all calculated for performance evaluation. The evaluation results show that deep feature extraction and SVM/ELM classification produced better results than transfer learning. In addition, the fc6 layers of the AlexNet, VGG16, and VGG19 models produced better accuracy scores when compared to the other layers.",2019,35,91,3,True,Computer Science,,1948182772,Muammer Turkoglu,2641969.0,D. Hanbay,,,,,,,,,,,,,,,,,,,,,
d98d0d1900b13b87aa4ffd6b69c046beb63f0434,https://www.semanticscholar.org/paper/d98d0d1900b13b87aa4ffd6b69c046beb63f0434,"Graphical Models, Exponential Families, and Variational Inference","The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide variety of algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.",2008,279,4061,405,True,Mathematics,Computer Science,1721860,M. Wainwright,123333909.0,M.I. Jordan,,,,,,,,,,,,,,,,,,,,,
60cf49038406fa732a48404d70f596103faac8f4,https://www.semanticscholar.org/paper/60cf49038406fa732a48404d70f596103faac8f4,Data classification with deep learning using Tensorflow,"Deep learning is a subfield of machine learning which uses artificial neural networks that is inspired by the structure and function of the human brain. Despite being a very new approach, it has become very popular recently. Deep learning has achieved much higher success in many applications where machine learning has been successful at certain rates. In particular It is preferred in the classification of big data sets because it can provide fast and efficient results. In this study, we used Tensorflow, one of the most popular deep learning libraries to classify MNIST dataset, which is frequently used in data analysis studies. Using Tensorflow, which is an open source artificial intelligence library developed by Google, we have studied and compared the effects of multiple activation functions on classification results. The functions used are Rectified Linear Unit (ReLu), Hyperbolic Tangent (tanH), Exponential Linear Unit (eLu), sigmoid, softplus and softsign. In this Study, Convolutional Neural Network (CNN) and SoftMax classifier are used as deep learning artificial neural network. The results show that the most accurate classification rate is obtained using the ReLu activation function.",2017,16,123,5,False,Computer Science,,9428487,F. Ertam,34705643.0,G. Aydin,,,,,,,,,,,,,,,,,,,,,
fb10d1a92790c4190ebf9c8acb8ada829e3620fa,https://www.semanticscholar.org/paper/fb10d1a92790c4190ebf9c8acb8ada829e3620fa,Secure Kernel Machines against Evasion Attacks,"Machine learning is widely used in security-sensitive settings like spam and malware detection, although it has been shown that malicious data can be carefully modified at test time to evade detection. To overcome this limitation, adversary-aware learning algorithms have been developed, exploiting robust optimization and game-theoretical models to incorporate knowledge of potential adversarial data manipulations into the learning algorithm. Despite these techniques have been shown to be effective in some adversarial learning tasks, their adoption in practice is hindered by different factors, including the difficulty of meeting specific theoretical requirements, the complexity of implementation, and scalability issues, in terms of computational time and space required during training. In this work, we aim to develop secure kernel machines against evasion attacks that are not computationally more demanding than their non-secure counterparts. In particular, leveraging recent work on robustness and regularization, we show that the security of a linear classifier can be drastically improved by selecting a proper regularizer, depending on the kind of evasion attack, as well as unbalancing the cost of classification errors. We then discuss the security of nonlinear kernel machines, and show that a proper choice of the kernel function is crucial. We also show that unbalancing the cost of classification errors and varying some kernel parameters can further improve classifier security, yielding decision functions that better enclose the legitimate data. Our results on spam and PDF malware detection corroborate our analysis.",2016,36,72,0,False,Computer Science,,1788142,P. Russu,2272441.0,Ambra Demontis,1684175.0,B. Biggio,1716261.0,G. Fumera,1710171.0,F. Roli,,,,,,,,,,,,,,,
4cd2ffebffd60708fb958c82b5fc72813fb9a1dd,https://www.semanticscholar.org/paper/4cd2ffebffd60708fb958c82b5fc72813fb9a1dd,Incorporating prior knowledge with weighted margin support vector machines,"Like many purely data-driven machine learning methods, Support Vector Machine (SVM) classifiers are learned exclusively from the evidence presented in the training dataset; thus a larger training dataset is required for better performance. In some applications, there might be human knowledge available that, in principle, could compensate for the lack of data. In this paper, we propose a simple generalization of SVM: Weighted Margin SVM (WMSVMs) that permits the incorporation of prior knowledge. We show that Sequential Minimal Optimization can be used in training WMSVM. We discuss the issues of incorporating prior knowledge using this rather general formulation. The experimental results show that the proposed methods of incorporating prior knowledge is effective.",2004,24,152,20,False,Computer Science,,2143690397,Xiaoyun Wu,1748081.0,R. Srihari,,,,,,,,,,,,,,,,,,,,,
d696f22560388393c170cf5c6f214466c921f2ce,https://www.semanticscholar.org/paper/d696f22560388393c170cf5c6f214466c921f2ce,Glaucoma Progression Detection Using Structural Retinal Nerve Fiber Layer Measurements and Functional Visual Field Points,"Machine learning classifiers were employed to detect glaucomatous progression using longitudinal series of structural data extracted from retinal nerve fiber layer thickness measurements and visual functional data recorded from standard automated perimetry tests. Using the collected data, a longitudinal feature vector was created for each patient's eye by computing the norm 1 difference vector of the data at the baseline and at each follow-up visit. The longitudinal features from each patient's eye were then fed to the machine learning classifier to classify each eye as stable or progressed over time. This study was performed using several machine learning classifiers including Bayesian, Lazy, Meta, and Tree, composing different families. Combinations of structural and functional features were selected and ranked to determine the relative effectiveness of each feature. Finally, the outcomes of the classifiers were assessed by several performance metrics and the effectiveness of structural and functional features were analyzed.",2014,47,79,1,True,Computer Science,Medicine,1732890,Siamak Yousefi,32618175.0,M. Goldbaum,2841631.0,M. Balasubramanian,144437435.0,T. Jung,2418684.0,R. Weinreb,2688841.0,F. Medeiros,2306091.0,L. Zangwill,2912106.0,J. Liebmann,2389101.0,C. Girkin,2375070.0,C. Bowd,,,,,
9b6f975cfdaefc6df7b92c8dc74dc084ca62140b,https://www.semanticscholar.org/paper/9b6f975cfdaefc6df7b92c8dc74dc084ca62140b,Machine Learning in PET: From Photon Detection to Quantitative Image Reconstruction,"Machine learning has found unique applications in nuclear medicine from photon detection to quantitative image reconstruction. Although there have been impressive strides in detector development for time-of-flight positron emission tomography (PET), most detectors still make use of simple signal processing methods to extract the time and position information from the detector signals. Now, with the availability of fast waveform digitizers, machine learning techniques have been applied to estimate the position and arrival time of high-energy photons. In quantitative image reconstruction, machine learning has been used to estimate various corrections factors, including scattered events and attenuation images, as well as to reduce statistical noise in reconstructed images. Here, machine learning either provides a faster alternative to an existing time-consuming computation, such as in the case of scatter estimation, or creates a data-driven approach to map an implicitly defined function, such as in the case of estimating the attenuation map for PET/MR scans. In this article, we will review the above-mentioned applications of machine learning in nuclear medicine.",2020,164,53,2,False,Computer Science,,6165576,K. Gong,145853615.0,E. Berg,2158695.0,S. Cherry,38379220.0,J. Qi,,,,,,,,,,,,,,,,,
5f3b369eb088a62e2f7e27632973cf8a7a347c8f,https://www.semanticscholar.org/paper/5f3b369eb088a62e2f7e27632973cf8a7a347c8f,10 Challenging Problems in Data Mining Research,"In October 2005, we took an initiative to identify 10 challenging problems in data mining research, by consulting some of the most active researchers in data mining and machine learning for their opinions on what are considered important and worthy topics for future research in data mining. We hope their insights will inspire new research efforts, and give young researchers (including PhD students) a high-level guideline as to where the hot problems are located in data mining.Due to the limited amount of time, we were only able to send out our survey requests to the organizers of the IEEE ICDM and ACM KDD conferences, and we received an overwhelming response. We are very grateful for the contributions provided by these researchers despite their busy schedules. This short article serves to summarize the 10 most challenging problems of the 14 responses we have received from this survey. The order of the listing does not reflect their level of importance.",2006,0,890,15,False,Computer Science,,152290618,Qiang Yang,1748808.0,Xindong Wu,,,,,,,,,,,,,,,,,,,,,
fbdbeb5d6d249041034d6357a431115145b3d7af,https://www.semanticscholar.org/paper/fbdbeb5d6d249041034d6357a431115145b3d7af,Deep convolutional neural networks for interpretable analysis of EEG sleep stage scoring,"Sleep studies are important for diagnosing sleep disorders such as insomnia, narcolepsy or sleep apnea. They rely on manual scoring of sleep stages from raw polisomnography signals, which is a tedious visual task requiring the workload of highly trained professionals. Consequently, research efforts to purse for an automatic stage scoring based on machine learning techniques have been carried out over the last years. In this work, we resort to multitaper spectral analysis to create visually interpretable images of sleep patterns from EEG signals as inputs to a deep convolutional network trained to solve visual recognition tasks. As a working example of transfer learning, a system able to accurately classify sleep stages in new unseen patients is presented. Evaluations in a widely-used publicly available dataset favourably compare to state-of-the-art results, while providing a framework for visual interpretation of outcomes.",2017,30,105,8,True,Computer Science,Mathematics,36438375,A. Vilamala,2482540.0,Kristoffer Hougaard Madsen,145579972.0,L. K. Hansen,,,,,,,,,,,,,,,,,,,
148209fa0c9279bb40322794833aa075769de95b,https://www.semanticscholar.org/paper/148209fa0c9279bb40322794833aa075769de95b,Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers,"To construct interpretable explanations that are consistent with the original ML model, counterfactual examples---showing how the model's output changes with small perturbations to the input---have been proposed. This paper extends the work in counterfactual explanations by addressing the challenge of feasibility of such examples. For explanations of ML models in critical domains such as healthcare and finance, counterfactual examples are useful for an end-user only to the extent that perturbation of feature inputs is feasible in the real world. We formulate the problem of feasibility as preserving causal relationships among input features and present a method that uses (partial) structural causal models to generate actionable counterfactuals. When feasibility constraints cannot be easily expressed, we consider an alternative mechanism where people can label generated CF examples on feasibility: whether it is feasible to intervene and realize the candidate CF example from the original input. To learn from this labelled feasibility data, we propose a modified variational auto encoder loss for generating CF examples that optimizes for feasibility as people interact with its output. Our experiments on Bayesian networks and the widely used ''Adult-Income'' dataset show that our proposed methods can generate counterfactual explanations that better satisfy feasibility constraints than existing methods.. Code repository can be accessed here: \textit{this https URL}",2019,28,108,19,False,Computer Science,Mathematics,133841722,Divyat Mahajan,40348583.0,Chenhao Tan,144676398.0,Amit Sharma,,,,,,,,,,,,,,,,,,,
4bd48e59590908d103e0c8b379002a64253f124a,https://www.semanticscholar.org/paper/4bd48e59590908d103e0c8b379002a64253f124a,Learning to Align from Scratch,"Unsupervised joint alignment of images has been demonstrated to improve performance on recognition tasks such as face verification. Such alignment reduces undesired variability due to factors such as pose, while only requiring weak supervision in the form of poorly aligned examples. However, prior work on unsupervised alignment of complex, real-world images has required the careful selection of feature representation based on hand-crafted image descriptors, in order to achieve an appropriate, smooth optimization landscape. In this paper, we instead propose a novel combination of unsupervised joint alignment with unsupervised feature learning. Specifically, we incorporate deep learning into the congealing alignment framework. Through deep learning, we obtain features that can represent the image at differing resolutions based on network depth, and that are tuned to the statistics of the specific data being aligned. In addition, we modify the learning algorithm for the restricted Boltzmann machine by incorporating a group sparsity penalty, leading to a topographic organization of the learned filters and improving subsequent alignment results. We apply our method to the Labeled Faces in the Wild database (LFW). Using the aligned images produced by our proposed unsupervised algorithm, we achieve higher accuracy in face verification compared to prior work in both unsupervised and supervised alignment. We also match the accuracy for the best available commercial method.",2012,47,298,39,False,Computer Science,,3219900,Gary B. Huang,2985062.0,Marwan A. Mattar,1697141.0,Honglak Lee,1389846455.0,E. Learned-Miller,,,,,,,,,,,,,,,,,
4f44a807ee185937c6c3caa14274b11de0b8b05c,https://www.semanticscholar.org/paper/4f44a807ee185937c6c3caa14274b11de0b8b05c,Machine Learning Capabilities of a Simulated Cerebellum,"This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional–integral–derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator’s inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.",2017,50,22,1,False,Computer Science,Medicine,3308897,Matthew J. Hausknecht,35507898.0,Wen-Ke Li,3002354.0,M. Mauk,144848112.0,P. Stone,,,,,,,,,,,,,,,,,
74aca1a6cf3d9a45c93145a318f7f1357836cf99,https://www.semanticscholar.org/paper/74aca1a6cf3d9a45c93145a318f7f1357836cf99,Machine Learning of Noise-Resilient Quantum Circuits,"Noise mitigation and reduction will be crucial for obtaining useful answers from near-term quantum computers. In this work, we present a general framework based on machine learning for reducing the impact of quantum hardware noise on quantum circuits. Our method, called noise-aware circuit learning (NACL), applies to circuits designed to compute a unitary transformation, prepare a set of quantum states, or estimate an observable of a many-qubit state. Given a task and a device model that captures information about the noise and connectivity of qubits in a device, NACL outputs an optimized circuit to accomplish this task in the presence of noise. It does so by minimizing a task-specific cost function over circuit depths and circuit structures. To demonstrate NACL, we construct circuits resilient to a fine-grained noise model derived from gate set tomography on a superconducting-circuit quantum device, for applications including quantum state overlap, quantum Fourier transform, and W-state preparation.",2020,55,57,1,True,Computer Science,Physics,49556529,L. Cincio,47368368.0,K. Rudinger,2707503.0,M. Sarovar,50721537.0,Patrick J. Coles,,,,,,,,,,,,,,,,,
68c9b3ec8384a18957a21a3fddaecf8a27e464c4,https://www.semanticscholar.org/paper/68c9b3ec8384a18957a21a3fddaecf8a27e464c4,Comparing and Validating Machine Learning Models for Mycobacterium tuberculosis Drug Discovery.,"Tuberculosis is a global health dilemma. In 2016, the WHO reported 10.4 million incidences and 1.7 million deaths. The need to develop new treatments for those infected with Mycobacterium tuberculosis ( Mtb) has led to many large-scale phenotypic screens and many thousands of new active compounds identified in vitro. However, with limited funding, efforts to discover new active molecules against Mtb needs to be more efficient. Several computational machine learning approaches have been shown to have good enrichment and hit rates. We have curated small molecule Mtb data and developed new models with a total of 18,886 molecules with activity cutoffs of 10 μM, 1 μM, and 100 nM. These data sets were used to evaluate different machine learning methods (including deep learning) and metrics and to generate predictions for additional molecules published in 2017. One Mtb model, a combined in vitro and in vivo data Bayesian model at a 100 nM activity yielded the following metrics for 5-fold cross validation: accuracy = 0.88, precision = 0.22, recall = 0.91, specificity = 0.88, kappa = 0.31, and MCC = 0.41. We have also curated an evaluation set ( n = 153 compounds) published in 2017, and when used to test our model, it showed the comparable statistics (accuracy = 0.83, precision = 0.27, recall = 1.00, specificity = 0.81, kappa = 0.36, and MCC = 0.47). We have also compared these models with additional machine learning algorithms showing Bayesian machine learning models constructed with literature Mtb data generated by different laboratories generally were equivalent to or outperformed deep neural networks with external test sets. Finally, we have also compared our training and test sets to show they were suitably diverse and different in order to represent useful evaluation sets. Such Mtb machine learning models could help prioritize compounds for testing in vitro and in vivo.",2018,83,62,2,True,Medicine,Computer Science,50556284,T. Lane,8903363.0,Daniel P. Russo,67164018.0,Kimberley M. Zorn,143961624.0,A. Clark,46927010.0,A. Korotcov,145694312.0,Valery Tkachenko,2184787.0,R. Reynolds,1846797.0,A. Perryman,2566048.0,Joel S. Freundlich,1887610.0,S. Ekins,,,,,
a596f03145285cd05a6ca57a4e25418b23b24976,https://www.semanticscholar.org/paper/a596f03145285cd05a6ca57a4e25418b23b24976,Learning to Prove Theorems via Interacting with Proof Assistants,"Humans prove theorems by relying on substantial high-level reasoning and problem-specific insights. Proof assistants offer a formalism that resembles human mathematical reasoning, representing theorems in higher-order logic and proofs as high-level tactics. However, human experts have to construct proofs manually by entering tactics into the proof assistant. In this paper, we study the problem of using machine learning to automate the interaction with proof assistants. We construct CoqGym, a large-scale dataset and learning environment containing 71K human-written proofs from 123 projects developed with the Coq proof assistant. We develop ASTactic, a deep learning-based model that generates tactics as programs in the form of abstract syntax trees (ASTs). Experiments show that ASTactic trained on CoqGym can generate effective tactics and can be used to prove new theorems not previously provable by automated methods. Code is available at this https URL.",2019,58,64,8,False,Computer Science,Mathematics,34284131,Kaiyu Yang,153302678.0,Jia Deng,,,,,,,,,,,,,,,,,,,,,
125cf56f5d6b1da3289246b6b5a51c83aaa910fa,https://www.semanticscholar.org/paper/125cf56f5d6b1da3289246b6b5a51c83aaa910fa,A 128-Channel Extreme Learning Machine-Based Neural Decoder for Brain Machine Interfaces,"Currently, state-of-the-art motor intention decoding algorithms in brain-machine interfaces are mostly implemented on a PC and consume significant amount of power. A machine learning coprocessor in 0.35- μm CMOS for the motor intention decoding in the brain-machine interfaces is presented in this paper. Using Extreme Learning Machine algorithm and low-power analog processing, it achieves an energy efficiency of 3.45 pJ/MAC at a classification rate of 50 Hz. The learning in second stage and corresponding digitally stored coefficients are used to increase robustness of the core analog processor. The chip is verified with neural data recorded in monkey finger movements experiment, achieving a decoding accuracy of 99.3% for movement type. The same coprocessor is also used to decode time of movement from asynchronous neural spikes. With time-delayed feature dimension enhancement, the classification accuracy can be increased by 5% with limited number of input channels. Further, a sparsity promoting training scheme enables reduction of number of programmable weights by ≈ 2X.",2015,48,67,3,True,Computer Science,Medicine,2118047696,Yi Chen,3297627.0,Enyi Yao,1824852.0,A. Basu,,,,,,,,,,,,,,,,,,,
1fc23d88f1274abc285d97c1232bc5b72c7985c1,https://www.semanticscholar.org/paper/1fc23d88f1274abc285d97c1232bc5b72c7985c1,Co-Creative Level Design via Machine Learning,"Procedural Level Generation via Machine Learning (PLGML), the study of generating game levels with machine learning, has received a large amount of recent academic attention. For certain measures these approaches have shown success at replicating the quality of existing game levels. However, it is unclear the extent to which they might benefit human designers. In this paper we present a framework for co-creative level design with a PLGML agent. In support of this framework we present results from a user study and results from a comparative study of PLGML approaches.",2018,16,52,3,False,Computer Science,,144599741,Matthew J. Guzdial,20985870.0,N. Liao,2757194.0,Mark O. Riedl,,,,,,,,,,,,,,,,,,,
82bab33fc8793565697e893c19117220b496dc67,https://www.semanticscholar.org/paper/82bab33fc8793565697e893c19117220b496dc67,Machine learning for online query relaxation,"In this paper we provide a fast, data-driven solution to the failing query problem: given a query that returns an empty answer, how can one relax the query's constraints so that it returns a non-empty set of tuples? We introduce a novel algorithm, loqr, which is designed to relax queries that are in the disjunctive normal form and contain a mixture of discrete and continuous attributes. loqr discovers the implicit relationships that exist among the various domain attributes and then uses this knowledge to relax the constraints from the failing query.In a first step, loqr uses a small, randomly-chosen subset of the target database to learn a set of decision rules that predict whether an attribute's value satisfies the constraints in the failing query; this query-driven operation is performed online for each failing query. In the second step, loqr uses nearest-neighbor techniques to find the learned rule that is the most similar to the failing query; then it uses the attributes' values from this rule to relax the failing query's constraints. Our experiments on six application domains show that loqr is both robust and fast: it successfully relaxes more than 95% of the failing queries, and it takes under a second for processing queries that consist of up to 20 attributes (larger queries of up to 93 attributes are processed in several seconds).",2004,29,68,2,False,Mathematics,Computer Science,3276863,Ion Muslea,,,,,,,,,,,,,,,,,,,,,,,
73bc87477e45d49f7254fccd5f34d23b4ae5f254,https://www.semanticscholar.org/paper/73bc87477e45d49f7254fccd5f34d23b4ae5f254,Feature selection using linear classifier weights: interaction with classification models,"This paper explores feature scoring and selection based on weights from linear classification models. It investigates how these methods combine with various learning models. Our comparative analysis includes three learning algorithms: Naïve Bayes, Perceptron, and Support Vector Machines (SVM) in combination with three feature weighting methods: Odds Ratio, Information Gain, and weights from linear models, the linear SVM and Perceptron. Experiments show that feature selection using weights from linear SVMs yields better classification performance than other feature weighting methods when combined with the three explored learning algorithms. The results support the conjecture that it is the sophistication of the feature weighting method rather than its apparent compatibility with the learning algorithm that improves classification performance.",2004,16,209,12,False,Computer Science,,1764321,D. Mladenic,1795846.0,J. Brank,1775954.0,M. Grobelnik,1398136050.0,Natasa Milic-Frayling,,,,,,,,,,,,,,,,,
87a3676647d1bd553ceb08cf83439b1833f1036f,https://www.semanticscholar.org/paper/87a3676647d1bd553ceb08cf83439b1833f1036f,CXPlain: Causal Explanations for Model Interpretation under Uncertainty,"Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.",2019,59,107,13,False,Computer Science,Mathematics,144861285,Patrick Schwab,144660470.0,W. Karlen,,,,,,,,,,,,,,,,,,,,,
447c91fafe70f82f259b01440b620140b6dd019f,https://www.semanticscholar.org/paper/447c91fafe70f82f259b01440b620140b6dd019f,Network Traffic Classification techniques and comparative analysis using Machine Learning algorithms,"Network Traffic Classification is a central topic nowadays in the field of computer science. It is a very essential task for internet service providers (ISPs) to know which types of network applications flow in a network. Network Traffic Classification is the first step to analyze and identify different types of applications flowing in a network. Through this technique, internet service providers or network operators can manage the overall performance of a network. There are many methods traditional technique to classify internet traffic like Port Based, Pay Load Based and Machine Learning Based technique. The most common technique used these days is Machine Learning (ML) technique. Which is used by many researchers and got very effective accuracy results. In this paper, we discuss network traffic classification techniques step by step and real time internet data set is develop using network traffic capture tool, after that feature extraction tool is use to extract features from the capture traffic and then four machine learning classifiers Support Vector Machine, C4.5 decision tree, Naïve Bays and Bayes Net classifiers are applied. Experimental analysis shows that C4.5 classifiers gives very good accuracy result as compare to other classifies.",2016,11,81,5,False,Computer Science,,2066491573,M. Shafiq,7933066.0,Xiangzhan Yu,7928685.0,A. Laghari,2106356341.0,Lu Yao,3471067.0,N. K. Karn,8810703.0,Foudil Abdessamia,,,,,,,,,,,,,
1080ba3f963eb940427e72d405bb822b3a543b73,https://www.semanticscholar.org/paper/1080ba3f963eb940427e72d405bb822b3a543b73,Challenges in the Deployment and Operation of Machine Learning in Practice,"Machine learning has recently emerged as a powerful technique to increase operational efficiency or to develop new value propositions. However, the translation of a prediction algorithm into an operationally usable machine learning model is a time-consuming and in various ways challenging task. In this work, we target to systematically elicit the challenges in deployment and operation to enable broader practical dissemination of machine learning applications. To this end, we first identify relevant challenges with a structured literature analysis. Subsequently, we conduct an interview study with machine learning practitioners across various industries, perform a qualitative content analysis, and identify challenges organized along three distinct categories as well as six overarching clusters. Eventually, results from both literature and interviews are evaluated with a comparative analysis. Key issues identified include auto- mated strategies for data drift detection and handling, standardization of machine learning infrastructure, and appropriate communication and expectation management.",2019,54,40,2,False,Computer Science,,38148198,Lucas Baier,133959836.0,Fabian Jöhren,9610652.0,S. Seebacher,,,,,,,,,,,,,,,,,,,
b75a15b1acc09c533134f9d91ea73ded61c1c616,https://www.semanticscholar.org/paper/b75a15b1acc09c533134f9d91ea73ded61c1c616,Robust Adaptive Dynamic Programming and Feedback Stabilization of Nonlinear Systems,"This paper studies the robust optimal control design for a class of uncertain nonlinear systems from a perspective of robust adaptive dynamic programming (RADP). The objective is to fill up a gap in the past literature of adaptive dynamic programming (ADP) where dynamic uncertainties or unmodeled dynamics are not addressed. A key strategy is to integrate tools from modern nonlinear control theory, such as the robust redesign and the backstepping techniques as well as the nonlinear small-gain theorem, with the theory of ADP. The proposed RADP methodology can be viewed as an extension of ADP to uncertain nonlinear systems. Practical learning algorithms are developed in this paper, and have been applied to the controller design problems for a jet engine and a one-machine power system.",2014,56,276,17,False,Computer Science,Medicine,145615652,Yu Jiang,144073377.0,Zhong-Ping Jiang,,,,,,,,,,,,,,,,,,,,,
43572a7cc087e388f7f312a0f2e17915682ff27c,https://www.semanticscholar.org/paper/43572a7cc087e388f7f312a0f2e17915682ff27c,Broadband Analog Aggregation for Low-Latency Federated Edge Learning,"To leverage rich data distributed at the network edge, a new machine-learning paradigm, called edge learning, has emerged where learning algorithms are deployed at the edge for providing intelligent services to mobile users. While computing speeds are advancing rapidly, the communication latency is becoming the bottleneck of fast edge learning. To address this issue, this work is focused on designing a low-latency multi-access scheme for edge learning. To this end, we consider a popular privacy-preserving framework, federated edge learning (FEEL), where a global AI-model at an edge-server is updated by aggregating (averaging) local models trained at edge devices. It is proposed that the updates simultaneously transmitted by devices over broadband channels should be analog aggregated “over-the-air” by exploiting the waveform-superposition property of a multi-access channel. Such broadband analog aggregation (BAA) results in dramatical communication-latency reduction compared with the conventional orthogonal access (i.e., OFDMA). In this work, the effects of BAA on learning performance are quantified targeting a single-cell random network. First, we derive two tradeoffs between communication-and-learning metrics, which are useful for network planning and optimization. The power control (“truncated channel inversion”) required for BAA results in a tradeoff between the update-reliability [as measured by the receive signal-to-noise ratio (SNR)] and the expected update-truncation ratio. Consider the scheduling of cell-interior devices to constrain path loss. This gives rise to the other tradeoff between the receive SNR and fraction of data exploited in learning. Next, the latency-reduction ratio of the proposed BAA with respect to the traditional OFDMA scheme is proved to scale almost linearly with the device population. Experiments based on a neural network and a real dataset are conducted for corroborating the theoretical results.",2018,44,322,39,False,Computer Science,Mathematics,2957831,Guangxu Zhu,1683878.0,Yong Wang,1697582.0,Kaibin Huang,,,,,,,,,,,,,,,,,,,
c57012ca87d6656c37921fde5107ee5b50a22ed2,https://www.semanticscholar.org/paper/c57012ca87d6656c37921fde5107ee5b50a22ed2,Genome-Scale Identification of Legionella pneumophila Effectors Using a Machine Learning Approach,"A large number of highly pathogenic bacteria utilize secretion systems to translocate effector proteins into host cells. Using these effectors, the bacteria subvert host cell processes during infection. Legionella pneumophila translocates effectors via the Icm/Dot type-IV secretion system and to date, approximately 100 effectors have been identified by various experimental and computational techniques. Effector identification is a critical first step towards the understanding of the pathogenesis system in L. pneumophila as well as in other bacterial pathogens. Here, we formulate the task of effector identification as a classification problem: each L. pneumophila open reading frame (ORF) was classified as either effector or not. We computationally defined a set of features that best distinguish effectors from non-effectors. These features cover a wide range of characteristics including taxonomical dispersion, regulatory data, genomic organization, similarity to eukaryotic proteomes and more. Machine learning algorithms utilizing these features were then applied to classify all the ORFs within the L. pneumophila genome. Using this approach we were able to predict and experimentally validate 40 new effectors, reaching a success rate of above 90%. Increasing the number of validated effectors to around 140, we were able to gain novel insights into their characteristics. Effectors were found to have low G+C content, supporting the hypothesis that a large number of effectors originate via horizontal gene transfer, probably from their protozoan host. In addition, effectors were found to cluster in specific genomic regions. Finally, we were able to provide a novel description of the C-terminal translocation signal required for effector translocation by the Icm/Dot secretion system. To conclude, we have discovered 40 novel L. pneumophila effectors, predicted over a hundred additional highly probable effectors, and shown the applicability of machine learning algorithms for the identification and characterization of bacterial pathogenesis determinants.",2009,52,224,15,True,Biology,Medicine,48688893,D. Burstein,6866451.0,T. Zusman,4507464.0,E. Degtyar,122381632.0,R. Viner,144093763.0,G. Segal,1963380.0,T. Pupko,,,,,,,,,,,,,
3284cce500a3138fac2a0b9802325589a6a75984,https://www.semanticscholar.org/paper/3284cce500a3138fac2a0b9802325589a6a75984,Calibration of Machine Learning Models,"One of the main goals of machine learning methods is to build a model or hypothesis from a set of data (also called evidence). After this learning process, the quality of the hypothesis must be evaluated as precisely as possible. For instance, if prediction errors have negative consequences in a certain application domain of a model (for example, detection of carcinogenic cells), it is important to know the exact ABStrAct",2012,45,40,2,True,Computer Science,,144256945,Antonio Bella,1785006.0,C. Ferri,1398777358.0,J. Hernández-Orallo,1401690994.0,M. J. Ramírez-Quintana,,,,,,,,,,,,,,,,,
b944cc4241d195b1609a7a9d87fce0e9ba1498bc,https://www.semanticscholar.org/paper/b944cc4241d195b1609a7a9d87fce0e9ba1498bc,Kernel Sparse Representation-Based Classifier,"Sparse representation-based classifier (SRC), a combined result of machine learning and compressed sensing, shows its good classification performance on face image data. However, SRC could not well classify the data with the same direction distribution. The same direction distribution means that the sample vectors belonging to different classes distribute on the same vector direction. This paper presents a new classifier, kernel sparse representation-based classifier (KSRC), based on SRC and the kernel trick which is a usual technique in machine learning. KSRC is a nonlinear extension of SRC and can remedy the drawback of SRC. To make the data in an input space separable, we implicitly map these data into a high-dimensional kernel feature space by using some nonlinear mapping associated with a kernel function. Since this kernel feature space has a very high (or possibly infinite) dimensionality, or is unknown, we have to avoid working in this space explicitly. Fortunately, we can indeed reduce the dimensionality of the kernel feature space by exploiting kernel-based dimensionality reduction methods. In the reduced subspace, we need to find sparse combination coefficients for a test sample and assign a class label to it. Similar to SRC, KSRC is also cast into an ℓ1-minimization problem or a quadratically constrained ℓ1 -minimization problem. Extensive experimental results on UCI and face data sets show KSRC improves the performance of SRC.",2012,50,307,33,False,Mathematics,Computer Science,2152827417,Li Zhang,49374288.0,Weida Zhou,1717197.0,P. Chang,48211068.0,Jing Liu,2152531966.0,Zhe Yan,2155390331.0,Ting Wang,2314391.0,Fanzhang Li,,,,,,,,,,,
8f74f5623c4e5c5931641a264cfd7c02097e1e22,https://www.semanticscholar.org/paper/8f74f5623c4e5c5931641a264cfd7c02097e1e22,A Brief Survey of Text Mining,"The enormous amount of information stored in unstructured texts cannot simply be used for further processing by computers, which typically handle text as simple sequences of character strings. Therefore, specific (pre-)processing methods and algorithms are required in order to extract useful patterns. Text mining refers generally to the process of extracting interesting information and knowledge from unstructured text. In this article, we discuss text mining as a young and interdisciplinary field in the intersection of the related areas information retrieval, machine learning, statistics, computational linguistics and especially data mining. We describe the main analysis tasks preprocessing, classification, clustering, information extraction and visualization. In addition, we briefly discuss a number of successful applications of text mining.",2005,123,872,38,False,Computer Science,,1792623,A. Hotho,1759689.0,A. Nürnberger,1683459.0,G. Paass,,,,,,,,,,,,,,,,,,,
0a22e2181ee71ae744cbaa840b86bd64abd566f3,https://www.semanticscholar.org/paper/0a22e2181ee71ae744cbaa840b86bd64abd566f3,"Machine Learning for Security and the Internet of Things: The Good, the Bad, and the Ugly","The advancement of the Internet of Things (IoT) has allowed for unprecedented data collection, automation, and remote sensing and actuation, transforming autonomous systems and bringing smart command and control into numerous cyber physical systems (CPS) that our daily lives depend on. Simultaneously, dramatic improvements in machine learning and deep neural network architectures have enabled unprecedented analytical capabilities, which we see in increasingly common applications and production technologies, such as self-driving vehicles and intelligent mobile applications. Predictably, these technologies have seen rapid adoption, which has left many implementations vulnerable to threats unforeseen or undefended against. Moreover, such technologies can be used by malicious actors, and the potential for cyber threats, attacks, intrusions, and obfuscation that are only just being considered, applied, and countered. In this paper, we consider the good, the bad, and the ugly use of machine learning for cybersecurity and CPS/IoT. In detail, we consider the numerous benefits (good use) that machine learning has brought, both in general, and specifically for security and CPS/IoT, such as the improvement of intrusion detection mechanisms and decision accuracy in CPS/IoT. More pressing, we consider the vulnerabilities of machine learning (bad use) from the perspectives of security and CPS/IoT, including the ways in which machine learning systems can be compromised, misled, and subverted at all stages of the machine learning life-cycle (data collection, pre-processing, training, validation, implementation, etc.). Finally, the most concerning, a growing trend has been the utilization of machine learning in the execution of cyberattacks and intrusions (ugly use). Thus, we consider existing mechanisms with the potential to improve target acquisition and existing threat patterns, as well as those that can enable novel attacks yet to be seen.",2019,185,48,1,True,Computer Science,,72900649,Fan Liang,3433838.0,W. G. Hatcher,2652462.0,Weixian Liao,8361046.0,Weichao Gao,144323096.0,Wei Yu,,,,,,,,,,,,,,,
347d3cb1fc7a10a2cae84b8735f5030245606855,https://www.semanticscholar.org/paper/347d3cb1fc7a10a2cae84b8735f5030245606855,Multi-objective differential evolution based random forest for e-health applications,"Many machine learning techniques have been used in past few decades for various medical applications. However, these techniques suffer from parameter tuning issue. Therefore, an efficient tuning of these parameters has an ability to improve the performance of existing machine learning techniques. Therefore, in this work, a novel multi-objective differential evolution based random forest technique is proposed. The proposed technique is able to tune the parameters of random forest in an efficient manner. Extensive experiments are carried out by considering the proposed and the existing competitive machine learning techniques on various medical applications. It is observed that the proposed technique outperforms existing techniques in terms of accuracy, f-measure, sensitivity and specificity.",2019,37,57,0,False,Computer Science,,49650872,Manjit Kaur,31346552.0,Hemant Kumar Gianey,39425248.0,Dilbag Singh,33190741.0,Munish Sabharwal,,,,,,,,,,,,,,,,,
d1b53e3a977dffe016d88387e59e48d6dbdedcc6,https://www.semanticscholar.org/paper/d1b53e3a977dffe016d88387e59e48d6dbdedcc6,In-Memory Computation of a Machine-Learning Classifier in a Standard 6T SRAM Array,"This paper presents a machine-learning classifier where computations are performed in a standard 6T SRAM array, which stores the machine-learning model. Peripheral circuits implement mixed-signal weak classifiers via columns of the SRAM, and a training algorithm enables a strong classifier through boosting and also overcomes circuit nonidealities, by combining multiple columns. A prototype 128 <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> 128 SRAM array, implemented in a 130-nm CMOS process, demonstrates ten-way classification of MNIST images (using image-pixel features downsampled from 28 <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> 28 = 784 to 9 <inline-formula> <tex-math notation=""LaTeX"">$\times $ </tex-math></inline-formula> 9 = 81, which yields a baseline accuracy of 90%). In SRAM mode (bit-cell read/write), the prototype operates up to 300 MHz, and in classify mode, it operates at 50 MHz, generating a classification every cycle. With accuracy equivalent to a discrete SRAM/digital-MAC system, the system achieves ten-way classification at an energy of 630 pJ per decision, 113 times lower than a discrete system with standard training algorithm and 13 times lower than a discrete system with the proposed training algorithm.",2017,23,218,28,False,Computer Science,,1519068895,Jintao Zhang,2108367712.0,Zhuo Wang,145020569.0,N. Verma,,,,,,,,,,,,,,,,,,,
ae76094ae93b98c3c02580ec4f5a19f89b4d2429,https://www.semanticscholar.org/paper/ae76094ae93b98c3c02580ec4f5a19f89b4d2429,Machine-Learning-Assisted Many-Body Entanglement Measurement.,"Entanglement not only plays a crucial role in quantum technologies, but is key to our understanding of quantum correlations in many-body systems. However, in an experiment, the only way of measuring entanglement in a generic mixed state is through reconstructive quantum tomography, requiring an exponential number of measurements in the system size. Here, we propose a machine-learning-assisted scheme to measure the entanglement between arbitrary subsystems of size N_{A} and N_{B}, with O(N_{A}+N_{B}) measurements, and without any prior knowledge of the state. The method exploits a neural network to learn the unknown, nonlinear function relating certain measurable moments and the logarithmic negativity. Our procedure will allow entanglement measurements in a wide variety of systems, including strongly interacting many-body systems in both equilibrium and nonequilibrium regimes.",2017,88,54,1,True,Physics,Medicine,51482641,Johnnie Gray,144299779.0,L. Banchi,2106406.0,A. Bayat,32606321.0,S. Bose,,,,,,,,,,,,,,,,,
c9b68a2486a135b90b577f7fb0bfc08d8991c6b1,https://www.semanticscholar.org/paper/c9b68a2486a135b90b577f7fb0bfc08d8991c6b1,How the brain might work: a hierarchical and temporal model for learning and recognition,"The brains of mammals are very efficient learning machines. Many aspects of mammalian learning are yet to be incorporated into machine learning algorithms. For instance, vision is typically considered to be a spatial problem in which a learning system needs to be trained with labeled examples of object images. Yet, mammals learn with continuously flowing unlabeled data. It is also generally accepted that the visual cortex in mammals is organized as a hierarchy and that many aspects of visual perception can be modeled using Bayesian computations. This dissertation introduces algorithms and networks that combine hierarchical and temporal learning with Bayesian inference for pattern recognition. These algorithms and networks, collectively called Hierarchical Temporal Memory (HTM), can be used to learn hierarchical-temporal models of data. Temporal continuity is used to learn multiple levels of the hierarchy without supervision. The HTM algorithms, when applied to a visual pattern recognition problem, exhibit invariant recognition, robustness to noise, and generalization. Inference in the hierarchy is performed using Bayesian belief propagation equations that are adapted to this problem setting. In order to understand the generalization properties of HTMs, a generative model for HTMs is developed. This model enables the generation of synthetic data from HTM networks. These data are used to analyze and characterize learning and generalization in hierarchical-temporal systems. Two existing hierarchical pattern recognition models are mapped to HTMs to explain the source of generalization in those models. Finally, the HTM Bayesian belief propagation equations are used to suggest a mathematical model for cortical microcircuits. The microcircuit model is derived by combining known anatomical constraints with the computational specifications of HTM belief propagation. The proposed model has a laminar and columnar organization that matches many known anatomical features. The proposed circuits are then used in the modeling of two well known physiological phenomena.",2008,107,186,26,False,Computer Science,,50021619,D. George,,,,,,,,,,,,,,,,,,,,,,,
62759909cd689b42a959f40dfdc9aa6dc2860911,https://www.semanticscholar.org/paper/62759909cd689b42a959f40dfdc9aa6dc2860911,Deep learning and radiomics in precision medicine,"ABSTRACT Introduction: The radiological reading room is undergoing a paradigm shift to a symbiosis of computer science and radiology using artificial intelligence integrated with machine and deep learning with radiomics to better define tissue characteristics. The goal is to use integrated deep learning and radiomics with radiological parameters to produce a personalized diagnosis for a patient. Areas covered: This review provides an overview of historical and current deep learning and radiomics methods in the context of precision medicine in radiology. A literature search for ‘Deep Learning’, ‘Radiomics’, ‘Machine learning’, ‘Artificial Intelligence’, ‘Convolutional Neural Network’, ‘Generative Adversarial Network’, ‘Autoencoders’, Deep Belief Networks”, Reinforcement Learning”, and ‘Multiparametric MRI’ was performed in PubMed, ArXiv, Scopus, CVPR, SPIE, IEEE Xplore, and NIPS to identify articles of interest. Expert opinion: In conclusion, both deep learning and radiomics are two rapidly advancing technologies that will unite in the future to produce a single unified framework for clinical decision support with a potential to completely revolutionize the field of precision medicine.",2019,138,113,4,True,Computer Science,Medicine,3416812,V. Parekh,145581165.0,M. Jacobs,,,,,,,,,,,,,,,,,,,,,
345796dd6038042697749cef024464ffa4df62a8,https://www.semanticscholar.org/paper/345796dd6038042697749cef024464ffa4df62a8,"Machine Learning for E-mail Spam Filtering: Review, Techniques and Trends","We present a comprehensive review of the most effective content-based e-mail spam filtering techniques. We focus primarily on Machine Learning-based spam filters and their variants, and report on a broad review ranging from surveying the relevant ideas, efforts, effectiveness, and the current progress. The initial exposition of the background examines the basics of e-mail spam filtering, the evolving nature of spam, spammers playing cat-and-mouse with e-mail service providers (ESPs), and the Machine Learning front in fighting spam. We conclude by measuring the impact of Machine Learning-based filters and explore the promising offshoots of latest developments.",2016,145,49,6,False,Computer Science,Mathematics,2785110,Alexy Bhowmick,2350548.0,S. Hazarika,,,,,,,,,,,,,,,,,,,,,
53f0982422af3901346159d6ab11523c248f08e0,https://www.semanticscholar.org/paper/53f0982422af3901346159d6ab11523c248f08e0,Robust Boltzmann Machines for recognition and denoising,"While Boltzmann Machines have been successful at unsupervised learning and density modeling of images and speech data, they can be very sensitive to noise in the data. In this paper, we introduce a novel model, the Robust Boltzmann Machine (RoBM), which allows Boltzmann Machines to be robust to corruptions. In the domain of visual recognition, the RoBM is able to accurately deal with occlusions and noise by using multiplicative gating to induce a scale mixture of Gaussians over pixels. Image denoising and in-painting correspond to posterior inference in the RoBM. Our model is trained in an unsupervised fashion with unlabeled noisy data and can learn the spatial structure of the occluders. Compared to standard algorithms, the RoBM is significantly better at recognition and denoising on several face databases.",2012,30,165,7,True,Computer Science,,34312504,Yichuan Tang,145124475.0,R. Salakhutdinov,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,
655f7db2f45309cab8523792b76e2bca04f72b27,https://www.semanticscholar.org/paper/655f7db2f45309cab8523792b76e2bca04f72b27,Some machine learning schemes for high-dimensional nonlinear PDEs,"We propose new machine learning schemes for solving high dimensional nonlinear partial differential equations (PDEs). Relying on the classical backward stochastic differential equation (BSDE) representation of PDEs, our algorithms estimate simultaneously the solution and its gradient by deep neural networks. These approximations are performed at each time step from the minimization of loss functions defined recursively by backward induction. The methodology is extended to variational inequalities arising in optimal stopping problems. We analyze the convergence of the deep learning schemes and provide error estimates in terms of the universal approximation of neural networks. Numerical results show that our algorithms give very good results till dimension 50 (and certainly above), for both PDEs and variational inequalities problems. For the PDEs resolution, our results are very similar to those obtained by the recent method in \cite{weinan2017deep} when the latter converges to the right solution or does not diverge. Numerical tests indicate that the proposed methods are not stuck in poor local minima as it can be the case with the algorithm designed in \cite{weinan2017deep}, and no divergence is experienced. The only limitation seems to be due to the inability of the considered deep neural networks to represent a solution with a too complex structure in high dimension.",2019,32,98,9,True,Computer Science,Mathematics,71758596,Côme Huré,145204321.0,H. Pham,3215934.0,X. Warin,,,,,,,,,,,,,,,,,,,
08510d874f5f57b75efb58d65c03f348b416fff3,https://www.semanticscholar.org/paper/08510d874f5f57b75efb58d65c03f348b416fff3,Predicting the direction of stock market prices using random forest,"Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence.",2016,17,155,10,False,Computer Science,,3407635,Luckyson Khaidem,2280469.0,Snehanshu Saha,3407935.0,Sudeepa Roy Dey,,,,,,,,,,,,,,,,,,,
bac4daa0d7bc5ed9506cb7480dccd303fd6b797e,https://www.semanticscholar.org/paper/bac4daa0d7bc5ed9506cb7480dccd303fd6b797e,On Symplectic Optimization,"Accelerated gradient methods have had significant impact in machine learning -- in particular the theoretical side of machine learning -- due to their ability to achieve oracle lower bounds. But their heuristic construction has hindered their full integration into the practical machine-learning algorithmic toolbox, and has limited their scope. In this paper we build on recent work which casts acceleration as a phenomenon best explained in continuous time, and we augment that picture by providing a systematic methodology for converting continuous-time dynamics into discrete-time algorithms while retaining oracle rates. Our framework is based on ideas from Hamiltonian dynamical systems and symplectic integration. These ideas have had major impact in many areas in applied mathematics, but have not yet been seen to have a relationship with optimization.",2018,13,75,11,False,Mathematics,,144227038,M. Betancourt,1694621.0,Michael I. Jordan,144102853.0,Ashia C. Wilson,,,,,,,,,,,,,,,,,,,
640baa58f3d0c6b7b9b74a4271bcc424c3904dbb,https://www.semanticscholar.org/paper/640baa58f3d0c6b7b9b74a4271bcc424c3904dbb,Intrusion Detection System for NSL-KDD Dataset Using Convolutional Neural Networks,"With the increment of cyber traffic, there is a growing demand for cyber security. How to accurately detect cyber intrusions is the hotspot of recent research. Traditional Intrusion Detection Systems (IDS), based on traditional machine learning methods, lacks reliability and accuracy. In this paper, we build an IDS model with deep learning methodology. Instead of the traditional machine learning used in previous researches, we think deep learning has the potential to perform better in extracting features of massive data considering the massive cyber traffic in real life. Therefore, we propose to train an IDS model based on Convolution Neural Networks (CNN), a typical deep learning method, using entire NSL-KDD dataset. We study the performance of the model using multi class classification to compare with the performance of traditional machine learning methods including Random Forest (RF) and Support Vector Machine (SVM), and deep learning methods including Deep Belief Network (DBN) and Long Short Term Memory (LSTM). The experimental results show that the performance of our IDS model is superior to the performance of models based on traditional machine learning methods and novel deep learning methods in multi-class classification. Our model improves the accuracy of the intrusion detection and provides a new research direction for intrusion detection.",2018,23,51,4,False,Computer Science,,2110669254,Yalei Ding,2015827.0,Yuqing Zhai,,,,,,,,,,,,,,,,,,,,,
ba6ebad92ffc213af75fedccf137cd9ab2b36483,https://www.semanticscholar.org/paper/ba6ebad92ffc213af75fedccf137cd9ab2b36483,Prediction of Heart Disease Using Machine Learning Algorithms,"The successful experiment of data mining in highly visible fields like marketing, e-business, and retail has led to its application in other sectors and industries. Healthcare is being discovered among these areas. There is an opulence of data available within the healthcare systems. However, there is a scarcity of useful analysis tool to find hidden relationships in data. This research intends to provide a detailed description of Naive Bayes and decision tree classifier that are applied in our research particularly in the prediction of Heart Disease. Some experiment has been conducted to compare the execution of predictive data mining technique on the same dataset, and the consequence reveals that Decision Tree outperforms over Bayesian classification.",2016,30,119,6,False,Computer Science,,145537478,S. Nikhar,70159515.0,A. Karandikar,,,,,,,,,,,,,,,,,,,,,
b3a6e5a38bb5984a27823ecc040526cde10eb730,https://www.semanticscholar.org/paper/b3a6e5a38bb5984a27823ecc040526cde10eb730,On Learning Meaningful Code Changes Via Neural Machine Translation,"Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.",2019,49,113,22,True,Computer Science,,40626221,Michele Tufano,10382838.0,Jevgenija Pantiuchina,47543605.0,Cody Watson,1801330.0,G. Bavota,1697757.0,D. Poshyvanyk,,,,,,,,,,,,,,,
4eaa9e9be652e9f2e7445cbbb92dbdaee478db68,https://www.semanticscholar.org/paper/4eaa9e9be652e9f2e7445cbbb92dbdaee478db68,Interpretable classification models for recidivism prediction,"We investigate a long‐debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent and interpretable to use for decision making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation to allocating preventative social services. Each case might have an objective other than classification accuracy, such as a desired true positive rate TPR or false positive rate FPR. Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (support vector machines, stochastic gradient boosting and ridge regression) produce equally accurate models along the full ROC curve. However, methods that are designed for interpretability (classification and regression trees and C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a recent method called supersparse linear integer models to produce accurate, transparent and interpretable scoring systems along the full ROC curve. These scoring systems can be used for decision making for many different use cases, since they are just as accurate as the most powerful black box machine learning models for many applications, but completely transparent, and highly interpretable.",2015,154,186,6,True,Computer Science,Mathematics,2641684,J. Zeng,3072590.0,Berk Ustun,48395540.0,C. Rudin,,,,,,,,,,,,,,,,,,,
cdcf84a6bb5581ae7fc25680be1083344871029e,https://www.semanticscholar.org/paper/cdcf84a6bb5581ae7fc25680be1083344871029e,A Framework for Medical Image Retrieval Using Machine Learning and Statistical Similarity Matching Techniques With Relevance Feedback,"A content-based image retrieval (CBIR) framework for diverse collection of medical images of different imaging modalities, anatomic regions with different orientations and biological systems is proposed. Organization of images in such a database (DB) is well defined with predefined semantic categories; hence, it can be useful for category-specific searching. The proposed framework consists of machine learning methods for image prefiltering, similarity matching using statistical distance measures, and a relevance feedback (RF) scheme. To narrow down the semantic gap and increase the retrieval efficiency, we investigate both supervised and unsupervised learning techniques to associate low-level global image features (e.g., color, texture, and edge) in the projected PCA-based eigenspace with their high-level semantic and visual categories. Specially, we explore the use of a probabilistic multiclass support vector machine (SVM) and fuzzy c-mean (FCM) clustering for categorization and prefiltering of images to reduce the search space. A category-specific statistical similarity matching is proposed in a finer level on the prefiltered images. To incorporate a better perception subjectivity, an RF mechanism is also added to update the query parameters dynamically and adjust the proposed matching functions. Experiments are based on a ground-truth DB consisting of 5000 diverse medical images of 20 predefined categories. Analysis of results based on cross-validation (CV) accuracy and precision-recall for image categorization and retrieval is reported. It demonstrates the improvement, effectiveness, and efficiency achieved by the proposed framework",2007,43,234,15,False,Computer Science,Medicine,1776408,Md. Mahmudur Rahman,144420948.0,P. Bhattacharya,1693149.0,B. C. Desai,,,,,,,,,,,,,,,,,,,
8c44e22d63a9adcfae6fd3682e87029e7655718c,https://www.semanticscholar.org/paper/8c44e22d63a9adcfae6fd3682e87029e7655718c,"Comments on ""The Extreme Learning Machine""","This comment letter points out that the essence of the ""extreme learning machine (ELM)"" recently appeared has been proposed earlier by Broomhead and Lowe and Pao , and discussed by other authors. Hence, it is not necessary to introduce a new name ""ELM.""",2008,43,66,2,False,Computer Science,Medicine,2153824356,L. P. Wang,1578467790.0,C. Wan,,,,,,,,,,,,,,,,,,,,,
45237a82a831ae898ad273bb615ead2068f2c61d,https://www.semanticscholar.org/paper/45237a82a831ae898ad273bb615ead2068f2c61d,Optimal quantum learning of a unitary transformation,"We address the problem of learning an unknown unitary transformation from a finite number of examples. The problem consists in finding the learning machine that optimally emulates the examples, thus reproducing the unknown unitary with maximum fidelity. Learning a unitary is equivalent to storing it in the state of a quantum memory (the memory of the learning machine) and subsequently retrieving it. We prove that, whenever the unknown unitary is drawn from a group, the optimal strategy consists in a parallel call of the available uses followed by a 'measure-and-rotate' retrieving. Differing from the case of quantum cloning, where the incoherent 'measure-and-prepare' strategies are typically suboptimal, in the case of learning the 'measure-and-rotate' strategy is optimal even when the learning machine is asked to reproduce a single copy of the unknown unitary. We finally address the problem of the optimal inversion of an unknown unitary evolution, showing also in this case the optimality of the 'measure-and-rotate' strategies and applying our result to the optimal approximate realignment of reference frames for quantum communication.",2009,8,91,4,True,Physics,,50534336,A. Bisio,2130499.0,G. Chiribella,152238093.0,G. D’Ariano,39660784.0,S. Facchini,2424822.0,P. Perinotti,,,,,,,,,,,,,,,
ebb3d5c70bedf2287f9b26ac0031004f8f617b97,https://www.semanticscholar.org/paper/ebb3d5c70bedf2287f9b26ac0031004f8f617b97,"Deep Learning for Understanding Faces: Machines May Be Just as Good, or Better, than Humans","Recent developments in deep convolutional neural networks (DCNNs) have shown impressive performance improvements on various object detection/recognition problems. This has been made possible due to the availability of large annotated data and a better understanding of the nonlinear mapping between images and class labels, as well as the affordability of powerful graphics processing units (GPUs). These developments in deep learning have also improved the capabilities of machines in understanding faces and automatically executing the tasks of face detection, pose estimation, landmark localization, and face recognition from unconstrained images and videos. In this article, we provide an overview of deep-learning methods used for face recognition. We discuss different modules involved in designing an automatic face recognition system and the role of deep learning for each of them. Some open issues regarding DCNNs for face recognition problems are then discussed. This article should prove valuable to scientists, engineers, and end users working in the fields of face recognition, security, visual surveillance, and biometrics.",2018,109,144,6,False,Computer Science,,48467498,Rajeev Ranjan,2716670.0,S. Sankaranarayanan,2068427.0,Ankan Bansal,9541177.0,Navaneeth Bodla,36407236.0,Jun-Cheng Chen,1741177.0,Vishal M. Patel,145586343.0,Carlos D. Castillo,9215658.0,R. Chellappa,,,,,,,,,
cea2a9ea8f83e3736f5efdd004a0617b5817c35c,https://www.semanticscholar.org/paper/cea2a9ea8f83e3736f5efdd004a0617b5817c35c,Statistically reinforced machine learning for nonlinear patterns and variable interactions,". Most statistical models assume linearity and few variable interactions, even though real-world ecological patterns often result from nonlinear and highly interactive processes. We here introduce a set of novel empirical modeling techniques which can address this mismatch: statistically reinforced machine learning. We demonstrate the behaviors of three techniques (conditional inference tree, model-based tree, and permutation-based random forest) by analyzing an arti ﬁ cially generated example dataset that contains patterns based on nonlinearity and variable interactions. The results show the potential of statistically reinforced machine learning algorithms to detect nonlinear relationships and higher-order interactions. Estimation reliability for any technique, however, depended on sample size. The applications of statistically reinforced machine learning approaches would be particularly bene ﬁ cial for investigating (1) novel patterns for which shapes cannot be assumed a priori, (2) higher-order interactions which are often overlooked in parametric statistics, (3) context dependency where patterns change depending on other conditions, (4) signi ﬁ cance and effect sizes of variables while taking nonlinearity and variable interactions into account, and (5) a hypothesis using parametric statistics after identifying patterns using statistically reinforced machine learning techniques.",2017,86,64,1,True,Computer Science,,38057242,M. Ryo,5718291.0,M. Rillig,,,,,,,,,,,,,,,,,,,,,
bb4de6ce625837aae1c7b09576cd9173d3ac0729,https://www.semanticscholar.org/paper/bb4de6ce625837aae1c7b09576cd9173d3ac0729,Large‐scale data mining using genetics‐based machine learning,"In the last decade, genetics‐based machine learning methods have shown their competence in large‐scale data mining tasks because of the scalability capacity that these techniques have demonstrated. This capacity goes beyond the innate massive parallelism of evolutionary computation methods by the proposal of a variety of mechanisms specifically tailored for machine learning tasks, including knowledge representations that exploit regularities in the datasets, hardware accelerations or data‐intensive computing methods, among others. This paper reviews different classes of methods that alone or (in many cases) combined accelerate genetics‐based machine learning methods. © 2013 Wiley Periodicals, Inc.",2013,143,47,0,False,Computer Science,,1697337,J. Bacardit,1822628.0,Xavier Llorà,,,,,,,,,,,,,,,,,,,,,
