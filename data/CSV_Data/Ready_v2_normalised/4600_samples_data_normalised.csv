paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,fieldsOfStudy/1,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,authors/5/authorId,authors/5/name,fieldsOfStudy/2,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,authors/16/authorId,authors/16/name,authors/17/authorId,authors/17/name,authors/18/authorId,authors/18/name
7db91eae688399cd12e9b67885a0a684dfddd9de,https://www.semanticscholar.org/paper/7db91eae688399cd12e9b67885a0a684dfddd9de,A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning,"In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.",2018,32,48,2,False,Computer Science,3228458,Sina Mohseni,1777991.0,E. Ragan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a9440546c4d61b295482eef1d5580e10c26ab195,https://www.semanticscholar.org/paper/a9440546c4d61b295482eef1d5580e10c26ab195,Robust Federated Learning in a Heterogeneous Environment,"We study a recently proposed large-scale distributed learning paradigm, namely Federated Learning, where the worker machines are end users' own devices. Statistical and computational challenges arise in Federated Learning particularly in the presence of heterogeneous data distribution (i.e., data points on different devices belong to different distributions signifying different clusters) and Byzantine machines (i.e., machines that may behave abnormally, or even exhibit arbitrary and potentially adversarial behavior). To address the aforementioned challenges, first we propose a general statistical model for this problem which takes both the cluster structure of the users and the Byzantine machines into account. Then, leveraging the statistical model, we solve the robust heterogeneous Federated Learning problem \emph{optimally}; in particular our algorithm matches the lower bound on the estimation error in dimension and the number of data points. Furthermore, as a by-product, we prove statistical guarantees for an outlier-robust clustering algorithm, which can be considered as the Lloyd algorithm with robust estimation. Finally, we show via synthetic as well as real data experiments that the estimation error obtained by our proposed algorithm is significantly better than the non-Byzantine-robust algorithms; in particular, we gain at least by 53\% and 33\% for synthetic and real data experiments, respectively, in typical settings.",2019,45,102,4,False,Computer Science,2528309,Avishek Ghosh,32712046.0,Justin Hong,Mathematics,50559902.0,Dong Yin,144161012.0,K. Ramchandran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6324f2693d3a180d7d942dfc2e182b677a54e5da,https://www.semanticscholar.org/paper/6324f2693d3a180d7d942dfc2e182b677a54e5da,Machine learning techniques with probability vector for cooperative spectrum sensing in cognitive radio networks,"We study cooperative spectrum sensing in cognitive radio networks (CRN) using machine learning techniques in this paper. A low-dimensional probability vector is proposed as the feature vector for machine learning based classification, instead of the N-dimensional energy vector in a CRN with a single primary user (PU) and N secondary users (SUs). This proposed method down-converts a high-dimensional feature vector to a constant two-dimensional feature vector for machine learning techniques while keeping the same spectrum sensing performance if not better. Due to its lower dimension, the probability vector based classification is capable of having a smaller training duration and a shorter classification time for testing vectors.",2016,10,55,4,False,Computer Science,2198997,Yingqi Lu,3458576.0,P. Zhu,,2111224376.0,Donglin Wang,1888358.0,M. Fattouche,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c2b278761f88326ac13148e4764bd406d42ad3ba,https://www.semanticscholar.org/paper/c2b278761f88326ac13148e4764bd406d42ad3ba,q-means: A quantum algorithm for unsupervised machine learning,"Quantum machine learning is one of the most promising applications of a full-scale quantum computer. Over the past few years, many quantum machine learning algorithms have been proposed that can potentially offer considerable speedups over the corresponding classical algorithms. In this paper, we introduce q-means, a new quantum algorithm for clustering which is a canonical problem in unsupervised machine learning. The $q$-means algorithm has convergence and precision guarantees similar to $k$-means, and it outputs with high probability a good approximation of the $k$ cluster centroids like the classical algorithm. Given a dataset of $N$ $d$-dimensional vectors $v_i$ (seen as a matrix $V \in \mathbb{R}^{N \times d})$ stored in QRAM, the running time of q-means is $\widetilde{O}\left( k d \frac{\eta}{\delta^2}\kappa(V)(\mu(V) + k \frac{\eta}{\delta}) + k^2 \frac{\eta^{1.5}}{\delta^2} \kappa(V)\mu(V) \right)$ per iteration, where $\kappa(V)$ is the condition number, $\mu(V)$ is a parameter that appears in quantum linear algebra procedures and $\eta = \max_{i} ||v_{i}||^{2}$. For a natural notion of well-clusterable datasets, the running time becomes $\widetilde{O}\left( k^2 d \frac{\eta^{2.5}}{\delta^3} + k^{2.5} \frac{\eta^2}{\delta^3} \right)$ per iteration, which is linear in the number of features $d$, and polynomial in the rank $k$, the maximum square norm $\eta$ and the error parameter $\delta$. Both running times are only polylogarithmic in the number of datapoints $N$. Our algorithm provides substantial savings compared to the classical $k$-means algorithm that runs in time $O(kdN)$ per iteration, particularly for the case of large datasets.",2018,36,103,6,False,Computer Science,2658063,Iordanis Kerenidis,52189727.0,Jonas Landman,Physics,47637299.0,Alessandro Luongo,49428348.0,A. Prakash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0de4cea40644bf1c75b4c92bcc493d85b15c81c2,https://www.semanticscholar.org/paper/0de4cea40644bf1c75b4c92bcc493d85b15c81c2,A Machine Learning-Based Approximation of Strong Branching,"We present in this paper a new generic approach to variable branching in branch and bound for mixed-integer linear problems. Our approach consists in imitating the decisions taken by a good branching strategy, namely strong branching, with a fast approximation. This approximated function is created by a machine learning technique from a set of observed branching decisions taken by strong branching. The philosophy of the approach is similar to reliability branching. However, our approach can catch more complex aspects of observed previous branchings to take a branching decision. The experiments performed on randomly generated and MIPLIB problems show promising results.",2017,21,106,17,True,Mathematics,3032280,A. Alvarez,2291059.0,Q. Louveaux,Computer Science,1695713.0,L. Wehenkel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
26feca29b05ac2abb7841f3a4172d0c28583ca33,https://www.semanticscholar.org/paper/26feca29b05ac2abb7841f3a4172d0c28583ca33,An Interpretable Machine Learning Model for Accurate Prediction of Sepsis in the ICU,"Objectives: Sepsis is among the leading causes of morbidity, mortality, and cost overruns in critically ill patients. Early intervention with antibiotics improves survival in septic patients. However, no clinically validated system exists for real-time prediction of sepsis onset. We aimed to develop and validate an Artificial Intelligence Sepsis Expert algorithm for early prediction of sepsis. Design: Observational cohort study. Setting: Academic medical center from January 2013 to December 2015. Patients: Over 31,000 admissions to the ICUs at two Emory University hospitals (development cohort), in addition to over 52,000 ICU patients from the publicly available Medical Information Mart for Intensive Care-III ICU database (validation cohort). Patients who met the Third International Consensus Definitions for Sepsis (Sepsis-3) prior to or within 4 hours of their ICU admission were excluded, resulting in roughly 27,000 and 42,000 patients within our development and validation cohorts, respectively. Interventions: None. Measurements and Main Results: High-resolution vital signs time series and electronic medical record data were extracted. A set of 65 features (variables) were calculated on hourly basis and passed to the Artificial Intelligence Sepsis Expert algorithm to predict onset of sepsis in the proceeding T hours (where T = 12, 8, 6, or 4). Artificial Intelligence Sepsis Expert was used to predict onset of sepsis in the proceeding T hours and to produce a list of the most significant contributing factors. For the 12-, 8-, 6-, and 4-hour ahead prediction of sepsis, Artificial Intelligence Sepsis Expert achieved area under the receiver operating characteristic in the range of 0.83–0.85. Performance of the Artificial Intelligence Sepsis Expert on the development and validation cohorts was indistinguishable. Conclusions: Using data available in the ICU in real-time, Artificial Intelligence Sepsis Expert can accurately predict the onset of sepsis in an ICU patient 4–12 hours prior to clinical recognition. A prospective study is necessary to determine the clinical utility of the proposed sepsis prediction model.",2017,31,379,8,True,Medicine,144422139,S. Nemati,34231949.0,A. Holder,,24816793.0,F. Razmi,49555023.0,Matthew D. Stanley,2384471.0,G. Clifford,3102078.0,T. Buchman,,,,,,,,,,,,,,,,,,,,,,,,,,,
b6bc0e56cfd57b3571adc469e2d830474b82c66e,https://www.semanticscholar.org/paper/b6bc0e56cfd57b3571adc469e2d830474b82c66e,Experimental Phase Estimation Enhanced By Machine Learning,"Phase estimation protocols provide a fundamental benchmark for the field of quantum metrology. The latter represents one of the most relevant applications of quantum theory, potentially enabling the capability of measuring unknown physical parameters with improved precision over classical strategies. Within this context, most theoretical and experimental studies have focused on determining the fundamental bounds and how to achieve them in the asymptotic regime where a large number of resources is employed. However, in most applications it is necessary to achieve optimal precisions by performing only a limited number of measurements. To this end, machine learning techniques can be applied as a powerful optimization tool. Here, we implement experimentally single-photon adaptive phase estimation protocols enhanced by machine learning, showing the capability of reaching optimal precision after a small number of trials. In particular, we introduce a new approach for Bayesian estimation that exhibit best performances for very low number of photons N. Furthermore, we study the resilience to noise of the tested methods, showing that the optimized Bayesian approach is very robust in the presence of imperfections. Application of this methodology can be envisaged in the more general multiparameter case, that represents a paradigmatic scenario for several tasks including imaging or Hamiltonian learning.",2017,112,64,3,True,Physics,32687701,Alessandro Lumino,27067685.0,E. Polino,Computer Science,26947571.0,A. S. Rab,152872062.0,G. Milani,35933774.0,N. Spagnolo,3253856.0,N. Wiebe,Mathematics,102982226.0,F. Sciarrino,,,,,,,,,,,,,,,,,,,,,,,,
acaddd7f05f6558708861bee3635218c78f21ea4,https://www.semanticscholar.org/paper/acaddd7f05f6558708861bee3635218c78f21ea4,Clonal operator and antibody clone algorithms,"Based on clonal selection theory, the main mechanisms of a clone in the immune system, which are explored in the field of artificial intelligence, are analyzed. An artificial immune system algorithm, antibody clone algorithm, is put forward. Compared with an improved gene algorithm, the new algorithm is shown to be an evolutionary strategy capable of solving complex machine learning tasks, like multi-modal optimization. Using Markov chain theories, it is proved that the antibody clone algorithm is convergent.",2002,8,85,3,False,Computer Science,49345807,Haifeng Du,144125122.0,L. Jiao,,2389051.0,Sun'an Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
47f1eb0dc42189ba7cf21b76598c8217eb1b6e05,https://www.semanticscholar.org/paper/47f1eb0dc42189ba7cf21b76598c8217eb1b6e05,Learning the Difference that Makes a Difference with Counterfactually-Augmented Data,"Despite alarm over the reliance of machine learning systems on so-called spurious patterns, the term lacks coherent meaning in standard statistical frameworks. However, the language of causality offers clarity: spurious associations are due to confounding (e.g., a common cause), but not direct or indirect causal effects. In this paper, we focus on natural language processing, introducing methods and resources for training models less sensitive to spurious patterns. Given documents and their initial labels, we task humans with revising each document so that it (i) accords with a counterfactual target label; (ii) retains internal coherence; and (iii) avoids unnecessary changes. Interestingly, on sentiment analysis and natural language inference tasks, classifiers trained on original data fail on their counterfactually-revised counterparts and vice versa. Classifiers trained on combined datasets perform remarkably well, just shy of those specialized to either domain. While classifiers trained on either original or manipulated data alone are sensitive to spurious features (e.g., mentions of genre), models trained on the combined data are less sensitive to this signal. Both datasets are publicly available.",2019,37,316,49,False,Computer Science,9264826,Divyansh Kaushik,144547315.0,E. Hovy,Mathematics,32219137.0,Zachary Chase Lipton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cb7ee710f96dfe999b2b047155fbba079e806b9d,https://www.semanticscholar.org/paper/cb7ee710f96dfe999b2b047155fbba079e806b9d,DeepBurning: Automatic generation of FPGA-based learning accelerators for the Neural Network family,"Recent advances in Neural Networks (NN) are enabling more and more innovative applications. As an energy-efficient hardware solution, machine learning accelerators for CNNs or traditional ANNs are also gaining popularity in the area of embedded vision, robotics and cyberphysics. However, the design parameters of NN models vary significantly from application to application. Hence, it's hard to provide one general and highly-efficient hardware solution to accommodate all of them, and it is also impractical for the domain-specific developers to customize their own hardware targeting on a specific NN model. To deal with this dilemma, this study proposes a design automation tool, DeepBurning, allowing the application developers to build from scratch learning accelerators that targets their specific NN models with custom configurations and optimized performance. DeepBurning includes a RTL-level accelerator generator and a coordinated compiler that generates the control flow and data layout under the user-specified constraints. The results can be used to implement FPGA-based NN accelerator or help generate chip design for early design stage. In general, DeepBurning supports a large family of NN models, and greatly simplifies the design flow of NN accelerators for the machine learning or AI application developers. The evaluation shows that the generated learning accelerators burnt to our FPGA board exhibit great power efficiency compared to state-of-the-art FPGA-based solutions.",2016,10,185,8,False,Computer Science,144810556,Ying Wang,2111063139.0,Jie Xu,,2276854.0,Yinhe Han,47892692.0,Huawei Li,40613624.0,Xiaowei Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a0bfd47e368fbd9caebd30734b9b08ff27f17a88,https://www.semanticscholar.org/paper/a0bfd47e368fbd9caebd30734b9b08ff27f17a88,Solar Flare Prediction Model with Three Machine-learning Algorithms using Ultraviolet Brightening and Vector Magnetograms,"We developed a flare prediction model using machine learning, which is optimized to predict the maximum class of flares occurring in the following 24 hr. Machine learning is used to devise algorithms that can learn from and make decisions on a huge amount of data. We used solar observation data during the period 2010–2015, such as vector magnetograms, ultraviolet (UV) emission, and soft X-ray emission taken by the Solar Dynamics Observatory and the Geostationary Operational Environmental Satellite. We detected active regions (ARs) from the full-disk magnetogram, from which ∼60 features were extracted with their time differentials, including magnetic neutral lines, the current helicity, the UV brightening, and the flare history. After standardizing the feature database, we fully shuffled and randomly separated it into two for training and testing. To investigate which algorithm is best for flare prediction, we compared three machine-learning algorithms: the support vector machine, k-nearest neighbors (k-NN), and extremely randomized trees. The prediction score, the true skill statistic, was higher than 0.9 with a fully shuffled data set, which is higher than that for human forecasts. It was found that k-NN has the highest performance among the three algorithms. The ranking of the feature importance showed that previous flare activity is most effective, followed by the length of magnetic neutral lines, the unsigned magnetic flux, the area of UV brightening, and the time differentials of features over 24 hr, all of which are strongly correlated with the flux emergence dynamics in an AR.",2016,77,95,7,True,Physics,40314738,N. Nishizuka,2332462.0,K. Sugiura,,50269104.0,Y. Kubo,144569801.0,M. Den,50658858.0,S. Watari,73406176.0,M. Ishii,,,,,,,,,,,,,,,,,,,,,,,,,,,
331709aae01b41ae664a94a70f08cd2a0c6b5d5d,https://www.semanticscholar.org/paper/331709aae01b41ae664a94a70f08cd2a0c6b5d5d,Application of Machine Learning to Classification of Volcanic Deformation in Routinely Generated InSAR Data,"Recent improvements in the frequency, type, and availability of satellite images mean it is now feasible to routinely study volcanoes in remote and inaccessible regions, including those with no ground‐based monitoring. In particular, Interferometric Synthetic Aperture Radar data can detect surface deformation, which has a strong statistical link to eruption. However, the data set produced by the recently launched Sentinel‐1 satellite is too large to be manually analyzed on a global basis. In this study, we systematically process >30,000 short‐term interferograms at over 900 volcanoes and apply machine learning algorithms to automatically detect volcanic ground deformation. We use a convolutional neutral network to classify interferometric fringes in wrapped interferograms with no atmospheric corrections. We employ a transfer learning strategy and test a range of pretrained networks, finding that AlexNet is best suited to this task. The positive results are checked by an expert and fed back for model updating. Following training with a combination of both positive and negative examples, this method reduced the number of interferograms to ∼100 which required further inspection, of which at least 39 are considered true positives. We demonstrate that machine learning can efficiently detect large, rapid deformation signals in wrapped interferograms, but further development is required to detect slow or small deformation patterns which do not generate multiple fringes in short duration interferograms. This study is the first to use machine learning approaches for detecting volcanic deformation in large data sets and demonstrates the potential of such techniques for developing alert systems based on satellite imagery.",2018,72,96,2,True,Computer Science,1756108,N. Anantrasirichai,2058850421.0,J. Biggs,,6429359.0,F. Albino,1710144.0,P. Hill,98897104.0,D. Bull,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d14ac2acf1b18e815385c631216eb4ee3a4fc842,https://www.semanticscholar.org/paper/d14ac2acf1b18e815385c631216eb4ee3a4fc842,"Artificial intelligence, machine learning and the evolution of healthcare","vol. 7, No. 3, MaRch 2018 223 First proposed by Professor John Mccarthy at Dartmouth college in the summer of 1956,1 artificial Intelligence (aI) – human intelligence exhibited by machines – has occupied the lexicon of successive generations of computer scientists, science fiction fans, and medical researchers. The aim of countless careers has been to build intelligent machines that can interpret the world as humans do, understand language, and learn from realworld examples. In the early part of this century, two events coincided that transformed the field of aI. The advent of widely available Graphic Processing Units (GPUs) meant that parallel processing was faster, cheaper, and more powerful. at the same time, the era of ‘Big Data’ – images, text, bioinformatics, medical records, and financial transactions, among others – was moving firmly into the mainstream, along with almost limitless data storage. These factors led to a dramatic resurgence in interest in aI in both academic circles and industries outside traditional computer science. once again, aI occupies the zeitgeist, and is poised to transform medicine at a basic science, clinical, healthcare management, and financial level. Terminology surrounding these technologies continues to evolve and can be a source of confusion for non-computer scientists. aI is broadly classified as: general aI, machines that replicate human thought, emotion, and reason (and remain, for now, in the realm of science fiction); and narrow aI, technologies that can perform specific tasks as well as, or better than, humans. Machine learning (Ml) is the study of computer algorithms that can learn complex relationships or patterns from empirical data and make accurate decisions.2 Rather than coding specific sets of instructions to accomplish a task, the machine is ‘trained’ using large amounts of data and algorithms that confer it the ability to learn how to perform the task. Unlike normal algorithms, it is the data that ‘tells’ the machine what the ‘good answer’ is, and learning occurs without explicit programming. Ml problems can be classified as supervised learning or unsupervised learning.3 In a supervised machine learning algorithm, such as face recognition, the machine is shown several examples of ‘face’ or ‘non-face’ and the algorithm learns to predict whether an unseen image is a face or not. In unsupervised learning, the images shown to the machine are not labelled as ‘face’ or ‘non-face’. artificial Neural Networks (aNN)4 are one group of algorithms used for machine learning. While aNNs have existed for over 60 years, they fell out of favour during the 1990s and 2000s. In the last half-decade, aNNs have had a resurgence under a new name: deep artificial networks (or ‘Deep learning’). aNNs are uniquely poised to take full advantage of the computational boost offered by GPUs, allowing them to crunch through data sets of enormous sizes. These range from computer vision tasks, such as image classification, object detection, face recognition, and optical character recognition (ocR), to natural language processing and even gameplaying problems (from mastering simple atari games to the recent alphaGo victory against human grandmasters).5 aNNs work by constructing layers upon layers of simple processing units (often referred to as ‘neurons’), interconnected via many differentially weighted connections. aNNs are ‘trained’ by using backpropagation algorithms, essentially telling the machine how to alter the internal parameters that are used to compute the representation in each layer from the representation in the previous Artificial intelligence, machine learning and the evolution of healthcare",2018,23,64,3,False,Computer Science,2117137636,L. D. Jones,2065189356.0,D. Golan,Medicine,2056986837.0,S. Hanna,2065560479.0,M. Ramachandran,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fc2eb1de019dbc74bfdb96d6685a29cca1bc8446,https://www.semanticscholar.org/paper/fc2eb1de019dbc74bfdb96d6685a29cca1bc8446,Machine Learning in Health Care: A Critical Appraisal of Challenges and Opportunities,"Examples of fully integrated machine learning models that drive clinical care are rare. Despite major advances in the development of methodologies that outperform clinical experts and growing prominence of machine learning in mainstream medical literature, major challenges remain. At Duke Health, we are in our fourth year developing, piloting, and implementing machine learning technologies in clinical care. To advance the translation of machine learning into clinical care, health system leaders must address barriers to progress and make strategic investments necessary to bring health care into a new digital age. Machine learning can improve clinical workflows in subtle ways that are distinct from how statistics has shaped medicine. However, most machine learning research occurs in siloes, and there are important, unresolved questions about how to retrain and validate models post-deployment. Academic medical centers that cultivate and value transdisciplinary collaboration are ideally suited to integrate machine learning in clinical care. Along with fostering collaborative environments, health system leaders must invest in developing new capabilities within the workforce and technology infrastructure beyond standard electronic health records. Now is the opportunity to break down barriers and achieve scalable growth in the number of high-impact collaborations between clinical researchers and machine learning experts to transform clinical care.",2019,20,41,1,False,Medicine,47057856,M. Sendak,145866390.0,M. Gao,Computer Science,48448064.0,M. Nichols,2026969787.0,Anthony L Lin,12308037.0,S. Balu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
796aa29837824d307ef43270f8f7f91bb20812ec,https://www.semanticscholar.org/paper/796aa29837824d307ef43270f8f7f91bb20812ec,mlr3: A modern object-oriented machine learning framework in R,"The R (R Core Team, 2019) package mlr3 and its associated ecosystem of extension packages implements a powerful, object-oriented and extensible framework for machine learning (ML) in R. It provides a unified interface to many learning algorithms available on CRAN, augmenting them with model-agnostic general-purpose functionality that is needed in every ML project, for example train-test-evaluation, resampling, preprocessing, hyperparameter tuning, nested resampling, and visualization of results from ML experiments. The package is a complete reimplementation of the mlr (Bischl et al., 2016) package that leverages many years of experience and learned best practices to provide a state-of-the-art system that is powerful, flexible, extensible, and maintainable. We target both practitioners who want to quickly apply ML algorithms to their problems and researchers who want to implement, benchmark, and compare their new methods in a structured environment. mlr3 is suitable for short scripts that test an idea, for complex multi-stage experiments with advanced functionality that use a broad range of ML functionality, as a foundation to implement new ML (meta-)algorithms (for example AutoML systems), and everything in between. Functional correctness is ensured through extensive unit and integration tests.",2019,8,109,8,True,Computer Science,143962282,Michel Lang,143780936.0,Martin Binder,,145444740.0,Jakob Richter,40896579.0,P. Schratz,3468393.0,F. Pfisterer,51123419.0,Stefan Coors,,51306278.0,Quay Au,8662947.0,Giuseppe Casalicchio,1722782.0,Lars Kotthoff,1686924.0,B. Bischl,,,,,,,,,,,,,,,,,,
801eee61f6ffaa1f2449a0e3f10e34caad1a4277,https://www.semanticscholar.org/paper/801eee61f6ffaa1f2449a0e3f10e34caad1a4277,Machine learning in cybersecurity: A review,"Machine learning technology has become mainstream in a large number of domains, and cybersecurity applications of machine learning techniques are plenty. Examples include malware analysis, especially for zero‐day malware detection, threat analysis, anomaly based intrusion detection of prevalent attacks on critical infrastructures, and many others. Due to the ineffectiveness of signature‐based methods in detecting zero day attacks or even slight variants of known attacks, machine learning‐based detection is being used by researchers in many cybersecurity products. In this review, we discuss several areas of cybersecurity where machine learning is used as a tool. We also provide a few glimpses of adversarial attacks on machine learning algorithms to manipulate training and test data of classifiers, to render such tools ineffective.",2019,67,42,1,False,Computer Science,39119171,Anand Handa,2109624069.0,Ashu Sharma,,2150905311.0,S. Shukla,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
eab0b1e5fc7d859e53509ec69ecb44b8da3f29f9,https://www.semanticscholar.org/paper/eab0b1e5fc7d859e53509ec69ecb44b8da3f29f9,Introduction to Causal Inference,"The goal of many sciences is to understand the mechanisms by which variables came to take on the values they have (that is, to find a generative model), and to predict what the values of those variables would be if the naturally occurring mechanisms were subject to outside manipulations. The past 30 years has seen a number of conceptual developments that are partial solutions to the problem of causal inference from observational sample data or a mixture of observational sample and experimental data, particularly in the area of graphical causal modeling. However, in many domains, problems such as the large numbers of variables, small samples sizes, and possible presence of unmeasured causes, remain serious impediments to practical applications of these developments. The articles in the Special Topic on Causality address these and other problems in applying graphical causal modeling algorithms. This introduction to the Special Topic on Causality provides a brief introduction to graphical causal modeling, places the articles in a broader context, and describes the differences between causal inference and ordinary machine learning classification and prediction problems.",2010,66,544,42,False,Computer Science,143648560,P. Spirtes,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5728b151bb5ab14d71ea05299b3f630fc64de31f,https://www.semanticscholar.org/paper/5728b151bb5ab14d71ea05299b3f630fc64de31f,Deep hedging,"We present a framework for hedging a portfolio of derivatives in the presence of market frictions such as transaction costs, liquidity constraints or risk limits using modern deep reinforcement machine learning methods. We discuss how standard reinforcement learning methods can be applied to non-linear reward structures, i.e. in our case convex risk measures. As a general contribution to the use of deep learning for stochastic processes, we also show in Section 4 that the set of constrained trading strategies used by our algorithm is large enough to ε-approximate any optimal solution. Our algorithm can be implemented efficiently even in high-dimensional situations using modern machine learning tools. Its structure does not depend on specific market dynamics, and generalizes across hedging instruments including the use of liquid derivatives. Its computational performance is largely invariant in the size of the portfolio as it depends mainly on the number of hedging instruments available. We illustrate our approach by an experiment on the S&P500 index and by showing the effect on hedging under transaction costs in a synthetic market driven by the Heston model, where we outperform the standard ‘complete-market’ solution.",2018,43,184,19,False,Economics,116509944,H. Buehler,49110267.0,L. Gonon,Mathematics,34274314.0,J. Teichmann,2064495228.0,B. Wood,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
eaf21b284c7fef5eca56db42edfc36bba0188471,https://www.semanticscholar.org/paper/eaf21b284c7fef5eca56db42edfc36bba0188471,Machine learning approaches to personalize early prediction of asthma exacerbations,"Patient telemonitoring results in an aggregation of significant amounts of information about patient disease trajectory. However, the potential use of this information for early prediction of exacerbations in adult asthma patients has not been systematically evaluated. The aim of this study was to explore the utility of telemonitoring data for building machine learning algorithms that predict asthma exacerbations before they occur. The study dataset comprised daily self‐monitoring reports consisting of 7001 records submitted by adult asthma patients during home telemonitoring. Predictive modeling included preparation of stratified training datasets, predictive feature selection, and evaluation of resulting classifiers. Using a 7‐day window, a naive Bayesian classifier, adaptive Bayesian network, and support vector machines were able to predict asthma exacerbation occurring on day 8, with sensitivity of 0.80, 1.00, and 0.84; specificity of 0.77, 1.00, and 0.80; and accuracy of 0.77, 1.00, and 0.80, respectively. Our study demonstrated that machine learning techniques have significant potential in developing personalized decision support for chronic disease telemonitoring systems. Future studies may benefit from a comprehensive predictive framework that combines telemonitoring data with other factors affecting the likelihood of developing acute exacerbation. Approaches implemented for advanced asthma exacerbation prediction may be extended to prediction of exacerbations in patients with other chronic health conditions.",2017,86,106,5,True,Medicine,145954113,J. Finkelstein,1757077.0,I. Jeong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7225c2a42990f850f692f8d82e7f3bfaf312145c,https://www.semanticscholar.org/paper/7225c2a42990f850f692f8d82e7f3bfaf312145c,Competence-based Curriculum Learning for Neural Machine Translation,"Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",2019,29,157,35,True,Computer Science,144888672,Emmanouil Antonios Platanios,3397269.0,Otilia Stretcu,Mathematics,1700325.0,Graham Neubig,1719347.0,B. Póczos,40975594.0,Tom Michael Mitchell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4f99a213d5c16a94b0eecd3f20cdabac3683df90,https://www.semanticscholar.org/paper/4f99a213d5c16a94b0eecd3f20cdabac3683df90,Ternary Compression for Communication-Efficient Federated Learning,"Learning over massive data stored in different locations is essential in many real-world applications. However, sharing data is full of challenges due to the increasing demands of privacy and security with the growing use of smart mobile devices and Internet of thing (IoT) devices. Federated learning provides a potential solution to privacy-preserving and secure machine learning, by means of jointly training a global model without uploading data distributed on multiple devices to a central server. However, most existing work on federated learning adopts machine learning models with full-precision weights, and almost all these models contain a large number of redundant parameters that do not need to be transmitted to the server, consuming an excessive amount of communication costs. To address this issue, we propose a federated trained ternary quantization (FTTQ) algorithm, which optimizes the quantized networks on the clients through a self-learning quantization factor. Theoretical proofs of the convergence of quantization factors, unbiasedness of FTTQ, as well as a reduced weight divergence are given. On the basis of FTTQ, we propose a ternary federated averaging protocol (T-FedAvg) to reduce the upstream and downstream communication of federated learning systems. Empirical experiments are conducted to train widely used deep learning models on publicly available data sets, and our results demonstrate that the proposed T-FedAvg is effective in reducing communication costs and can even achieve slightly better performance on non-IID data in contrast to the canonical federated learning algorithms.",2020,59,48,2,True,Computer Science,2136350969,Jinjin Xu,2903496.0,W. Du,Mathematics,2157835870.0,Yaochu Jin,1765250.0,Wangli He,39336044.0,Ran Cheng,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,
5b6aa32c69559c78403d977e34cb80ad2a472a64,https://www.semanticscholar.org/paper/5b6aa32c69559c78403d977e34cb80ad2a472a64,Benchmarking Machine Learning Technologies for Software Defect Detection,"Machine Learning approaches are good in solving problems that have less information. In most cases, the software domain problems characterize as a process of learning that depend on the various circumstances and changes accordingly. A predictive model is constructed by using machine learning approaches and classified them into defective and non-defective modules. Machine learning techniques help developers to retrieve useful information after the classification and enable them to analyse data from different perspectives. Machine learning techniques are proven to be useful in terms of software bug prediction. This study used public available data sets of software modules and provides comparative performance analysis of different machine learning techniques for software bug prediction. Results showed most of the machine learning methods performed well on software bug datasets.",2015,50,37,1,False,Computer Science,2146784,Saiqa Aleem,1739277.0,Luiz Fernando Capretz,,3085539.0,F. Ahmed,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dbfc5a911e50b275d9b8adb5d311286d50b3f9dc,https://www.semanticscholar.org/paper/dbfc5a911e50b275d9b8adb5d311286d50b3f9dc,Using machine learning on cardiorespiratory fitness data for predicting hypertension: The Henry Ford ExercIse Testing (FIT) Project,"This study evaluates and compares the performance of different machine learning techniques on predicting the individuals at risk of developing hypertension, and who are likely to benefit most from interventions, using the cardiorespiratory fitness data. The dataset of this study contains information of 23,095 patients who underwent clinician- referred exercise treadmill stress testing at Henry Ford Health Systems between 1991 and 2009 and had a complete 10-year follow-up. The variables of the dataset include information on vital signs, diagnosis and clinical laboratory measurements. Six machine learning techniques were investigated: LogitBoost (LB), Bayesian Network classifier (BN), Locally Weighted Naive Bayes (LWB), Artificial Neural Network (ANN), Support Vector Machine (SVM) and Random Tree Forest (RTF). Using different validation methods, the RTF model has shown the best performance (AUC = 0.93) and outperformed all other machine learning techniques examined in this study. The results have also shown that it is critical to carefully explore and evaluate the performance of the machine learning models using various model evaluation methods as the prediction accuracy can significantly differ.",2018,75,69,2,True,Computer Science,144250946,S. Sakr,5742338.0,Radwa Elshawi,Medicine,144454000.0,Amjad M Ahmed,32704135.0,W. Qureshi,4701643.0,C. Brawner,5834795.0,S. Keteyian,,3455659.0,M. Blaha,1397581130.0,M. Al-Mallah,,,,,,,,,,,,,,,,,,,,,,
21b51aeb5626f0fea9d4dec742616c62b4bd2113,https://www.semanticscholar.org/paper/21b51aeb5626f0fea9d4dec742616c62b4bd2113,Challenges and design choices for global weather and climate models based on machine learning,"Abstract. Can models that are based on deep learning and trained on atmospheric data compete with weather and climate models that are based on physical principles and the basic equations of motion? This question has been asked often recently due to the boom in deep-learning techniques. The question is valid given the huge amount of data that are available, the computational efficiency of deep-learning techniques and the limitations of today's weather and climate models in particular with respect to resolution and complexity.In this paper, the question will be discussed in the context of global weather forecasts. A toy model for global weather predictions will be presented and used to identify challenges and fundamental design choices for a forecast system based on neural networks.",2018,24,181,16,True,Computer Science,1389726126,P. Dueben,50734944.0,P. Bauer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
03f6ee322583d360acabf33b43d3a7db1d5d8d56,https://www.semanticscholar.org/paper/03f6ee322583d360acabf33b43d3a7db1d5d8d56,Opposition-Based Reinforcement Learning,"Reinforcement learning is a machine intelligence scheme for learning in highly dynamic, probabilistic environments. By interaction with the environment, reinforcement agents learn optimal control policies, especially in the absence of a priori knowledge and/or a sufficiently large amount of training data. Despite its advantages, however, reinforcement learning suffers from a major drawback – high calculation cost because convergence to an optimal solution usually requires that all states be visited frequently to ensure that policy is reliable. This is not always possible, however, due to the complex, high-dimensional state space in many applications. This paper introduces opposition-based reinforcement learning, inspired by opposition-based learning, to speed up convergence. Considering opposite actions simultaneously enables individual states to be updated more than once shortening exploration and expediting convergence. Three versions of Q-learning algorithm will be given as examples. Experimental results for the grid world problem of different sizes demonstrate the superior performance of the proposed approach.",2006,17,188,5,False,Computer Science,9315255,H. Tizhoosh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea414818831d2098e2876f465814564f9a5acabb,https://www.semanticscholar.org/paper/ea414818831d2098e2876f465814564f9a5acabb,A Simple and Generic Belief Tracking Mechanism for the Dialog State Tracking Challenge: On the believability of observed information,"This paper presents a generic dialogue state tracker that maintains beliefs over user goals based on a few simple domainindependent rules, using basic probability operations. The rules apply to observed system actions and partially observable user acts, without using any knowledge obtained from external resources (i.e. without requiring training data). The core insight is to maximise the amount of information directly gainable from an errorprone dialogue itself, so as to better lowerbound one’s expectations on the performance of more advanced statistical techniques for the task. The proposed method is evaluated in the Dialog State Tracking Challenge, where it achieves comparable performance in hypothesis accuracy to machine learning based systems. Consequently, with respect to different scenarios for the belief tracking problem, the potential superiority and weakness of machine learning approaches in general are investigated.",2013,15,165,7,False,Computer Science,47196992,Zhuoran Wang,1782798.0,Oliver Lemon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6297042abe4b19cb26c45bca67189dbcae300909,https://www.semanticscholar.org/paper/6297042abe4b19cb26c45bca67189dbcae300909,Identifying Stable Patterns over Time for Emotion Recognition from EEG,"In this paper, we investigate stable patterns of electroencephalogram (EEG) over time for emotion recognition using a machine learning approach. Up to now, various findings of activated patterns associated with different emotions have been reported. However, their stability over time has not been fully investigated yet. In this paper, we focus on identifying EEG stability in emotion recognition. We systematically evaluate the performance of various popular feature extraction, feature selection, feature smoothing and pattern classification methods with the DEAP dataset and a newly developed dataset called SEED for this study. Discriminative Graph regularized Extreme Learning Machine with differential entropy features achieves the best average accuracies of 69.67 and 91.07 percent on the DEAP and SEED datasets, respectively. The experimental results indicate that stable patterns exhibit consistency across sessions; the lateral temporal areas activate more for positive emotions than negative emotions in beta and gamma bands; the neural patterns of neutral emotions have higher alpha responses at parietal and occipital sites; and for negative emotions, the neural patterns have significant higher delta responses at parietal and occipital sites and higher gamma responses at prefrontal sites. The performance of our emotion recognition models shows that the neural patterns are relatively stable within and between sessions.",2016,74,380,37,True,Computer Science,3108302,Wei-Long Zheng,2682019.0,Jia-Yi Zhu,,1715839.0,Bao-Liang Lu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
027c7c1898a9bae0a20f1a06c603ebac5c059978,https://www.semanticscholar.org/paper/027c7c1898a9bae0a20f1a06c603ebac5c059978,Large margin transductive transfer learning,"Recently there has been increasing interest in the problem of transfer learning, in which the typical assumption that training and testing data are drawn from identical distributions is relaxed. We specifically address the problem of transductive transfer learning in which we have access to labeled training data and unlabeled testing data potentially drawn from different, yet related distributions, and the goal is to leverage the labeled training data to learn a classifier to correctly predict data from the testing distribution. We have derived efficient algorithms for transductive transfer learning based on a novel viewpoint and the Support Vector Machine (SVM) paradigm, of a large margin hyperplane classifier in a feature space. We show that our method can out-perform some recent state-of-the-art approaches for transfer learning on several data sets, with the added benefits of model and data separation and the potential to leverage existing work on support vector machines.",2009,49,107,19,False,Computer Science,2770631,Brian Quanz,144733837.0,Jun Huan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
882f42973556389220dc8c474af91a141f0f43c1,https://www.semanticscholar.org/paper/882f42973556389220dc8c474af91a141f0f43c1,Deep Learning for Infrared Thermal Image Based Machine Health Monitoring,"The condition of a machine can automatically be identified by creating and classifying features that summarize characteristics of measured signals. Currently, experts, in their respective fields, devise these features based on their knowledge. Hence, the performance and usefulness depends on the expert's knowledge of the underlying physics or statistics. Furthermore, if new and additional conditions should be detectable, experts have to implement new feature extraction methods. To mitigate the drawbacks of feature engineering, a method from the subfield of feature learning, i.e., deep learning (DL), more specifically convolutional neural networks (NNs), is researched in this paper. The objective of this paper is to investigate if and how DL can be applied to infrared thermal (IRT) video to automatically determine the condition of the machine. By applying this method on IRT data in two use cases, i.e., machine-fault detection and oil-level prediction, we show that the proposed system is able to detect many conditions in rotating machinery very accurately (i.e., 95 and 91.67% accuracy for the respective use cases), without requiring any detailed knowledge about the underlying physics, and thus having the potential to significantly simplify condition monitoring using complex sensor data. Furthermore, we show that by using the trained NNs, important regions in the IRT images can be identified related to specific conditions, which can potentially lead to new physical insights.",2017,24,130,4,True,Engineering,3001762,Olivier Janssens,115624058.0,R. Van de Walle,,2755281.0,M. Loccufier,134767416.0,S. Van Hoecke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
226aa30a0666b18fe1e90d4ed9e74311481f20dd,https://www.semanticscholar.org/paper/226aa30a0666b18fe1e90d4ed9e74311481f20dd,Machine learning paradigm for structural health monitoring,"Structural health diagnosis and prognosis is the goal of structural health monitoring. Vibration-based structural health monitoring methodology has been extensively investigated. However, the conventional vibration–based methods find it difficult to detect damages of actual structures because of a high incompleteness in the monitoring information (the number of sensors is much fewer with respect to the number of degrees of freedom of a structure), intense uncertainties in the structural conditions and monitoring systems, and coupled effects of damage and environmental actions on modal parameters. It is a truth that the performance and conditions of a structure must be embedded in the monitoring data (vehicles, wind, etc.; acceleration, displacement, cable force, strain, images, videos, etc.). Therefore, there is a need to develop completely novel structural health diagnosis and prognosis methodology based on the various monitoring data. Machine learning provides the advanced mathematical frameworks and algorithms that can help discover and model the performance and conditions of a structure through deep mining of monitoring data. Thus, machine learning takes an opportunity to establish novel machine learning paradigm for structural health diagnosis and prognosis theory termed the machine learning paradigm for structural health monitoring. This article sheds light on principles for machine learning paradigm for structural health monitoring with some examples and reviews the existing challenges and open questions in this field.",2020,137,57,2,False,Computer Science,49955399,Y. Bao,145824388.0,Hui Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
220ac48a22547a455d05f416e1fd22bbd0b0788d,https://www.semanticscholar.org/paper/220ac48a22547a455d05f416e1fd22bbd0b0788d,Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks,"Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that have tried to either map representations between the two domains, or learn to extract features that are domain-invariant. In this work, we approach the problem in a new light by learning in an unsupervised manner a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training.",2016,54,1164,98,True,Computer Science,2732737,Konstantinos Bousmalis,2286640.0,N. Silberman,,35363891.0,David Dohan,1761978.0,D. Erhan,1707347.0,Dilip Krishnan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5d150cec2775f9bc863760448f14104cc8f42368,https://www.semanticscholar.org/paper/5d150cec2775f9bc863760448f14104cc8f42368,Discovering governing equations from data by sparse identification of nonlinear dynamical systems,"Significance Understanding dynamic constraints and balances in nature has facilitated rapid development of knowledge and enabled technology, including aircraft, combustion engines, satellites, and electrical power. This work develops a novel framework to discover governing equations underlying a dynamical system simply from data measurements, leveraging advances in sparsity techniques and machine learning. The resulting models are parsimonious, balancing model complexity with descriptive ability while avoiding overfitting. There are many critical data-driven problems, such as understanding cognition from neural recordings, inferring climate patterns, determining stability of financial markets, predicting and suppressing the spread of disease, and controlling turbulence for greener transportation and energy. With abundant data and elusive laws, data-driven discovery of dynamics will continue to play an important role in these efforts. Extracting governing equations from data is a central challenge in many diverse areas of science and engineering. Data are abundant whereas models often remain elusive, as in climate science, neuroscience, ecology, finance, and epidemiology, to name only a few examples. In this work, we combine sparsity-promoting techniques and machine learning with nonlinear dynamical systems to discover governing equations from noisy measurement data. The only assumption about the structure of the model is that there are only a few important terms that govern the dynamics, so that the equations are sparse in the space of possible functions; this assumption holds for many physical systems in an appropriate basis. In particular, we use sparse regression to determine the fewest terms in the dynamic governing equations required to accurately represent the data. This results in parsimonious models that balance accuracy with model complexity to avoid overfitting. We demonstrate the algorithm on a wide range of problems, from simple canonical systems, including linear and nonlinear oscillators and the chaotic Lorenz system, to the fluid vortex shedding behind an obstacle. The fluid example illustrates the ability of this method to discover the underlying dynamics of a system that took experts in the community nearly 30 years to resolve. We also show that this method generalizes to parameterized systems and systems that are time-varying or have external forcing.",2015,61,1917,194,True,Mathematics,3083169,S. Brunton,2424683.0,J. Proctor,Medicine,144484983.0,J. Kutz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
db3ea129add716192f11d41e605c0020ee2488d7,https://www.semanticscholar.org/paper/db3ea129add716192f11d41e605c0020ee2488d7,REDD : A Public Data Set for Energy Disaggregation Research,"Energy and sustainability issues raise a large number of problems that can be tackled using approaches from data mining and machine learning, but traction of such problems has been slow due to the lack of publicly available data. In this paper we present the Reference Energy Disaggregation Data Set (REDD), a freely available data set containing detailed power usage information from several homes, which is aimed at furthering research on energy disaggregation (the task of determining the component appliance contributions from an aggregated electricity signal). We discuss past approaches to disaggregation and how they have influenced our design choices in collecting data, we describe the hardware and software setups for the data collection, and we present initial benchmark disaggregation results using a well-known Factorial Hidden Markov Model (FHMM) technique.",2011,19,1078,248,False,Computer Science,145116464,J. Z. Kolter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e2a240cc67c2909128f2154db2facba5810a94b0,https://www.semanticscholar.org/paper/e2a240cc67c2909128f2154db2facba5810a94b0,"Artificial Intelligence, Economics, and Industrial Organization","Machine learning (ML) and artificial intelligence (AI) have been around for many years. However, in the last 5 years, remarkable progress has been made using multilayered neural networks in diverse areas such as image recognition, speech recognition, and machine translation. AI is a general purpose technology that is likely to impact many industries. In this chapter I consider how machine learning availability might affect the industrial organization of both firms that provide AI services and industries that adopt AI technology. My intent is not to provide an extensive overview of this rapidly-evolving area, but instead to provide a short summary of some of the forces at work and to describe some possible areas for future research.",2018,46,116,3,True,Computer Science,2070970,H. Varian,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0858fb6efb0e7ef549db94813b9d6f896073d60a,https://www.semanticscholar.org/paper/0858fb6efb0e7ef549db94813b9d6f896073d60a,Bayesian Reinforcement Learning: A Survey,"Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.",2015,184,311,26,True,Computer Science,1678622,M. Ghavamzadeh,1712535.0,Shie Mannor,Mathematics,145134886.0,Joelle Pineau,3025260.0,Aviv Tamar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3a51659b4df7d43b450df6134af107ec4d2c203f,https://www.semanticscholar.org/paper/3a51659b4df7d43b450df6134af107ec4d2c203f,Extreme‐learning‐machine‐based land cover classification,"The extreme learning machine (ELM), a single hidden layer neural network based supervised classifier is used for remote sensing classifications. In comparison to the backpropagation neural network, which requires the setting of several user‐defined parameters and may produce local minima, the ELM requires setting of one parameter, and produces a unique solution for a set of randomly assigned weights. Two datasets, one multispectral and another hyperspectral, were used for classification. Accuracies of 89.0% and 91.1% are achieved with this classifier using multispectral and hyperspectral data, respectively. Results suggest that the ELM provides a classification accuracy comparable to a backpropagation neural network with both datasets. The computational cost using the ELM classifier (1.25 s with Enhanced Thematic Mapper (ETM+) and 0.675 s with Digital Airborne Imaging Spectrometer (DAIS) data) is very small in comparison to the backpropagation neural network.",2008,26,78,6,True,Computer Science,3639504,M. Pal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3ff76dab4aeeb9b17759329c290755b665a6d640,https://www.semanticscholar.org/paper/3ff76dab4aeeb9b17759329c290755b665a6d640,Machine Learning Based Botnet Identification Traffic,"The continued growth of the Internet has resulted in the increasing sophistication of toolkit and methods to conduct computer attacks and intrusions that are easy to use and publicly available to download, such as Zeus botnet toolkit. Botnets are responsible for many cyber-attacks, such as spam, distributed denial-of-service (DDoS), identity theft, and phishing. Most of existence botnet toolkits release updates for new features, development and support. This presents challenges in the detection and prevention of bots. Current botnet detection approaches mostly ineffective as botnets change their Command and Control (C&C) server structures, centralized (e.g., IRC, HTTP), distributed (e.g., P2P), and encryption deterrent. In this paper, based on real world data sets we present our preliminary research on predicting the new bots before they launch their attack. We propose a rich set of features of network traffic using Classification of Network Information Flow Analysis (CONIFA) framework to capture regularities in C&C communication channels and malicious traffic. We present a case study of applying the approach to a popular botnet toolkit, Zeus. The experimental evaluation suggest that it is possible to detect effectively botnets during the botnet C&C communication generated from new updated Zeus botnet toolkit by building the classifier using machine learning from an earlier version and before they launch their attacks using traffic behaviors. Also, show that there is similarity in C&C structures various Botnet toolkit versions and that the network characteristics of botnet C&C traffic is different from legitimate network traffic. Such methods could reduce many different resources needed to identify C&C communication channels and malicious traffic.",2016,34,68,1,True,Computer Science,144759750,A. Azab,2474250.0,M. Alazab,,2569386.0,Mahdi Aiash,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9a870af1c356f14c51c0af75bd9fec5f2e20f8f7,https://www.semanticscholar.org/paper/9a870af1c356f14c51c0af75bd9fec5f2e20f8f7,Debugging Machine Learning Tasks,"Unlike traditional programs (such as operating systems or word processors) which have large amounts of code, machine learning tasks use programs with relatively small amounts of code (written in machine learning libraries), but voluminous amounts of data. Just like developers of traditional programs debug errors in their code, developers of machine learning tasks debug and fix errors in their data. However, algorithms and tools for debugging and fixing errors in data are less common, when compared to their counterparts for detecting and fixing errors in code. In this paper, we consider classification tasks where errors in training data lead to misclassifications in test points, and propose an automated method to find the root causes of such misclassifications. Our root cause analysis is based on Pearl's theory of causation, and uses Pearl's PS (Probability of Sufficiency) as a scoring metric. Our implementation, Psi, encodes the computation of PS as a probabilistic program, and uses recent work on probabilistic programs and transformations on probabilistic programs (along with gray-box models of machine learning algorithms) to efficiently compute PS. Psi is able to identify root causes of data errors in interesting data sets.",2016,26,36,1,False,Computer Science,3237101,Aleksandar Chakarov,34894873.0,A. Nori,Mathematics,1685546.0,S. Rajamani,1774073.0,S. Sen,2541122.0,Deepak Vijaykeerthy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5de430fffd090658af35538fd2ceffa5dda8d96e,https://www.semanticscholar.org/paper/5de430fffd090658af35538fd2ceffa5dda8d96e,Linear Manifold Regularization for Large Scale Semi-supervised Learning,"The enormous wealth of unlabeled data in many applications of machine learning is beginning to pose challenges to the designers of semi-supervised learning methods. We are interested in developing linear classification algorithms to efficiently learn from massive partially labeled datasets. In this paper, we propose Linear Laplacian Support Vector Machines and Linear Laplacian Regularized Least Squares as promising solutions to this problem.",2005,12,72,12,False,,1808676,V. Sindhwani,1770745.0,P. Niyogi,,145520115.0,Mikhail Belkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d0aa1ae9d8df097524fda2b7ff0d4ee81ee8be73,https://www.semanticscholar.org/paper/d0aa1ae9d8df097524fda2b7ff0d4ee81ee8be73,Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a Memory-Based Approach,"We investigate the performance of two machine learning algorithms in the context of antispam filtering. The increasing volume of unsolicited bulk e-mail (spam) has generated a need for reliable anti-spam filters. Filters of this type have so far been based mostly on keyword patterns that are constructed by hand and perform poorly. The Naive Bayesian classifier has recently been suggested as an effective method to construct automatically anti-spam filters with superior performance. We investigate thoroughly the performance of the Naive Bayesian filter on a publicly available corpus, contributing towards standard benchmarks. At the same time, we compare the performance of the Naive Bayesian filter to an alternative memorybased learning approach, after introducing suitable cost-sensitive evaluation measures. Both methods achieve very accurate spam filtering, outperforming clearly the keyword-based filter of a widely used e-mail reader.",2000,21,403,45,False,Computer Science,1752430,Ion Androutsopoulos,4738873.0,G. Paliouras,,1745631.0,V. Karkaletsis,2589004.0,Georgios Sakkis,2124138.0,C. Spyropoulos,1800781.0,Panagiotis Stamatopoulos,,,,,,,,,,,,,,,,,,,,,,,,,,,
4532a6dc17973a44d068ef64d8a40b8070209fbd,https://www.semanticscholar.org/paper/4532a6dc17973a44d068ef64d8a40b8070209fbd,Two-Layer Multiple Kernel Learning,"Multiple Kernel Learning (MKL) aims to learn kernel machines for solving a real machine learning problem (e.g. classification) by exploring the combinations of multiple kernels. The traditional MKL approach is in general “shallow” in the sense that the target kernel is simply a linear (or convex) combination of some base kernels. In this paper, we investigate a framework of Multi-Layer Multiple Kernel Learning (MLMKL) that aims to learn “deep” kernel machines by exploring the combinations of multiple kernels in a multi-layer structure, which goes beyond the conventional MKL approach. Through a multiple layer mapping, the proposed MLMKL framework offers higher flexibility than the regular MKL for finding the optimal kernel for applications. As the first attempt to this new MKL framework, we present a Two-Layer Multiple Kernel Learning (2LMKL) method together with two efficient algorithms for classification tasks. We analyze their generalization performances and have conducted an extensive set of experiments over 16 benchmark datasets, in which encouraging results showed that our method performed better than the conventional MKL methods.",2011,32,99,21,False,Mathematics,2146277,Jinfeng Zhuang,1807998.0,I. Tsang,Computer Science,1741126.0,S. Hoi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7ef0075e5a42b807bb8ee1f5b2a64fe78ccbdfcd,https://www.semanticscholar.org/paper/7ef0075e5a42b807bb8ee1f5b2a64fe78ccbdfcd,Klasifikasi Citra Menggunakan Convolutional Neural Network (CNN) pada Caltech 101,"Deep Learning adalah sebuah bidang keilmuan baru dalam bidang Machine Learning yang akhir-akhir ini berkembang karena perkembangan teknologi GPU accelaration. Deep Learning memiliki kemampuan yang sangat baik dalam visi komputer. Salah satunya adalah pada kasus klasifikasi objek pada citra. Dengan mengimplementasikan salah satu metode machine learning yang dapat digunakan untuk klasifikasi citra objek yaitu CNN. Metode CNN terdiri dari dua tahap. Tahap pertama adalah klasifikasi citra menggunakan feedforward. Tahap kedua merupakan tahap pembelajaran dengan metode backpropagation. Sebelum dilakukan klasifikasi, terlebih dahulu dilakukan praproses dengan metode wrapping dan cropping untuk memfokuskan objek yang akan diklasifikasi. Selanjutnya dilakukan training menggunakan metode feedforward dan backpropagation. Terakhir adalah tahap klasifikasi menggunakan metode feedforward dengan bobot dan bias yang diperbarui. Hasil uji coba dari klasifikasi citra objek dengan tingkat confusion yang berbeda pada basis data Caltech 101 menghasilkan rata-rata nilai akurasi mencapai. Sehingga dapat disimpulkan bahwa metode CNN yang digunakan pada Tugas Akhir ini mampu melakukan klasifikasi dengan baik.",2016,14,104,7,True,Computer Science,143851555,I. E. Putra,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a9b901fcd68a3715d5ef186a398476bc1e762b0e,https://www.semanticscholar.org/paper/a9b901fcd68a3715d5ef186a398476bc1e762b0e,Deep Reinforcement Learning for Cyber Security,"The scale of Internet-connected systems has increased considerably, and these systems are being exposed to cyberattacks more than ever. The complexity and dynamics of cyberattacks require protecting mechanisms to be responsive, adaptive, and scalable. Machine learning, or more specifically deep reinforcement learning (DRL), methods have been proposed widely to address these issues. By incorporating deep learning into traditional RL, DRL is highly capable of solving complex, dynamic, and especially high-dimensional cyber defense problems. This article presents a survey of DRL approaches developed for cyber security. We touch on different vital aspects, including DRL-based security methods for cyber-physical systems, autonomous intrusion detection techniques, and multiagent DRL-based game theory simulations for defense strategies against cyberattacks. Extensive discussions and future research directions on DRL-based cyber security are also given. We expect that this comprehensive review provides the foundations for and facilitates future studies on exploring the potential of emerging DRL to cope with increasingly complex cyber security problems.",2019,212,92,4,True,Computer Science,32579488,T. Nguyen,1805668.0,V. Reddi,Mathematics,,,,,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,
d0c18f9af4b394c265c09ef792a4360cc4efa6a0,https://www.semanticscholar.org/paper/d0c18f9af4b394c265c09ef792a4360cc4efa6a0,Multimodal Machine Learning for Automated ICD Coding,"This study presents a multimodal machine learning model to predict ICD-10 diagnostic codes. We developed separate machine learning models that can handle data from different modalities, including unstructured text, semi-structured text and structured tabular data. We further employed an ensemble method to integrate all modality-specific models to generate ICD-10 codes. Key evidence was also extracted to make our prediction more convincing and explainable. We used the Medical Information Mart for Intensive Care III (MIMIC -III) dataset to validate our approach. For ICD code prediction, our best-performing model (micro-F1 = 0.7633, micro-AUC = 0.9541) significantly outperforms other baseline models including TF-IDF (micro-F1 = 0.6721, micro-AUC = 0.7879) and Text-CNN model (micro-F1 = 0.6569, micro-AUC = 0.9235). For interpretability, our approach achieves a Jaccard Similarity Coefficient (JSC) of 0.1806 on text data and 0.3105 on tabular data, where well-trained physicians achieve 0.2780 and 0.5002 respectively.",2018,44,62,5,False,Computer Science,2117101011,Keyang Xu,2055768861.0,Mike Lam,Mathematics,2054728417.0,Jingzhi Pang,2109103156.0,Xin Gao,82543609.0,Charlotte Band,2731090.0,P. Mathur,,32719808.0,F. Papay,1717205287.0,A. Khanna,6508641.0,J. Cywinski,2685055.0,K. Maheshwari,40526720.0,P. Xie,143977260.0,E. Xing,,,,,,,,,,,,,,
d09c60cd3e493923b63ae170ad1daaf9f6cb1a69,https://www.semanticscholar.org/paper/d09c60cd3e493923b63ae170ad1daaf9f6cb1a69,A review of KDD99 dataset usage in intrusion detection and machine learning between 2010 and 2015,"Although KDD99 dataset is more than 15 years old, it is still widely used in academic research. To investigate wide usage of this dataset in Machine Learning Research (MLR) and Intrusion Detection Systems (IDS); this study reviews 149 research articles from 65 journals indexed in Science Citation In- dex Expanded and Emerging Sources Citation Index during the last six years (2010–2015). If we include papers presented in other indexes and conferences, number of studies would be tripled. The number of published studies shows that KDD99 is the most used dataset in IDS and machine learning areas, and it is the de facto dataset for these research areas. To show recent usage of KDD99 and the related sub-dataset (NSL-KDD) in IDS and MLR, the following de- scriptive statistics about the reviewed studies are given: main contribution of articles, the applied algorithms,",2016,25,126,7,False,Computer Science,3397678,Atilla Özgür,40344877.0,H. Erdem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e3d5ee10d0489c768e943546038e3f53e7697349,https://www.semanticscholar.org/paper/e3d5ee10d0489c768e943546038e3f53e7697349,State of the Art on Neural Rendering,"Efficient rendering of photo‐realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo‐realistic images from hand‐crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo‐realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state‐of‐the‐art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photorealistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs. implicit control, generalization, and stochastic vs. deterministic synthesis. The second half of this state‐of‐the‐art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free‐viewpoint video, and the creation of photo‐realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.",2020,229,231,6,True,Computer Science,9102722,Ayush Tewari,2416503.0,Ohad Fried,,34105638.0,Justus Thies,5021307.0,V. Sitzmann,153360678.0,Stephen Lombardi,2454127.0,Kalyan Sunkavalli,,1401885873.0,Ricardo Martin-Brualla,145386542.0,Tomas Simon,2398245.0,Jason M. Saragih,2209612.0,M. Nießner,39192292.0,Rohit Pandey,34772804.0,S. Fanello,1731170.0,Gordon Wetzstein,2436356.0,Jun-Yan Zhu,1680185.0,C. Theobalt,1820412.0,Maneesh Agrawala,2177801.0,Eli Shechtman,1976171.0,Dan B. Goldman,51283751.0,Michael Zollhofer
eeaedfddff1e377e4eabaeb0e45cab4e9e5d6f65,https://www.semanticscholar.org/paper/eeaedfddff1e377e4eabaeb0e45cab4e9e5d6f65,Machine Learning at the Wireless Edge: Distributed Stochastic Gradient Descent Over-the-Air,"We study collaborative machine learning at the wireless edge, where power and bandwidth-limited devices (workers), with limited local datasets, implement distributed stochastic gradient descent (DSGD) over-the-air with the help of a remote parameter server (PS). We consider a wireless multiple access channel (MAC) from the workers to the PS for communicating the local gradient estimates. We first introduce a digital DSGD (D-DSGD) scheme, assuming that the workers operate on the boundary of the MAC capacity region at each iteration of the DSGD algorithm, and digitize their estimates within the bit budget allowed by the employed power allocation. We then introduce an analog scheme, called A-DSGD, motivated by the additive nature of the wireless MAC, where the workers send their gradient estimates over the MAC through the available channel bandwidth without employing any digital code. Numerical results show that A-DSGD converges much faster than D-DSGD. The improvement is particularly compelling at low power and low bandwidth regimes. We also observe that the performance of A-DSGD improves with the number of workers, while D-DSGD deteriorates, limiting the ability of the latter in harnessing the computation power of many edge devices.",2019,61,299,35,True,Computer Science,2240170,M. Amiri,1727814.0,Deniz Gündüz,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
24e5346ca36abd2eeb47b21e223f493f3d4b722b,https://www.semanticscholar.org/paper/24e5346ca36abd2eeb47b21e223f493f3d4b722b,Protecting against evaluation overfitting in empirical reinforcement learning,"Empirical evaluations play an important role in machine learning. However, the usefulness of any evaluation depends on the empirical methodology employed. Designing good empirical methodologies is difficult in part because agents can overfit test evaluations and thereby obtain misleadingly high scores. We argue that reinforcement learning is particularly vulnerable to environment overfitting and propose as a remedy generalized methodologies, in which evaluations are based on multiple environments sampled from a distribution. In addition, we consider how to summarize performance when scores from different environments may not have commensurate values. Finally, we present proof-of-concept results demonstrating how these methodologies can validate an intuitively useful range-adaptive tile coding method.",2011,35,85,2,False,Medicine,1766767,Shimon Whiteson,145177632.0,B. Tanner,Computer Science,39286677.0,Matthew E. Taylor,144848112.0,P. Stone,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c49eaab729b938ddf3542e74b3f9d4c68f75e585,https://www.semanticscholar.org/paper/c49eaab729b938ddf3542e74b3f9d4c68f75e585,Beamforming using the relevance vector machine,"Beamformers are spatial filters that pass source signals in particular focused locations while suppressing interference from elsewhere. The widely-used minimum variance adaptive beamformer (MVAB) creates such filters using a sample covariance estimate; however, the quality of this estimate deteriorates when the sources are correlated or the number of samples n is small. Herein, a modified beamformer is derived that replaces this problematic sample covariance with a robust maximum likelihood estimate obtained using the relevance vector machine (RVM), a Bayesian method for learning sparse models from possibly overcomplete feature sets. We prove that this substitution has the natural ability to remove the undesirable effects of correlations or limited data. When n becomes large and assuming uncorrelated sources, this method reduces to the exact MVAB. Simulations using direction-of-arrival data support these conclusions. Additionally, RVMs can potentially enhance a variety of traditional signal processing methods that rely on robust sample covariance estimates.",2007,15,59,9,False,Computer Science,2242717,D. Wipf,1758601.0,S. Nagarajan,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2ac34a6f11c8aec576fa8951ab8b1681fa22abc6,https://www.semanticscholar.org/paper/2ac34a6f11c8aec576fa8951ab8b1681fa22abc6,Learning Image Similarity from Flickr Groups Using Fast Kernel Machines,"Measuring image similarity is a central topic in computer vision. In this paper, we propose to measure image similarity by learning from the online Flickr image groups. We do so by: Choosing 103 Flickr groups, building a one-versus-all multiclass classifier to classify test images into a group, taking the set of responses of the classifiers as features, calculating the distance between feature vectors to measure image similarity. Experimental results on the Corel dataset and the PASCAL VOC 2007 dataset show that our approach performs better on image matching, retrieval, and classification than using conventional visual features. To build our similarity measure, we need one-versus-all classifiers that are accurate and can be trained quickly on very large quantities of data. We adopt an SVM classifier with a histogram intersection kernel. We describe a novel fast training algorithm for this classifier: the Stochastic Intersection Kernel MAchine (SIKMA) training algorithm. This method can produce a kernel classifier that is more accurate than a linear classifier on tens of thousands of examples in minutes.",2012,57,55,3,False,Mathematics,2096527,G. Wang,2433269.0,Derek Hoiem,Computer Science,144016256.0,D. Forsyth,,,,,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,
2ed7baf3dc26de1f934c938e2a806c28eea445a7,https://www.semanticscholar.org/paper/2ed7baf3dc26de1f934c938e2a806c28eea445a7,Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE),"A growing body of work underlines striking similarities between biological neural networks and recurrent, binary neural networks. A relatively smaller body of work, however, addresses the similarities between learning dynamics employed in deep artificial neural networks and synaptic plasticity in spiking neural networks. The challenge preventing this is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements for gradient backpropagation. Learning algorithms that approximate gradient backpropagation using local error functions can overcome this challenge. Here, we introduce Deep Continuous Local Learning (DECOLLE), a spiking neural network equipped with local error functions for online learning with no memory overhead for computing gradients. DECOLLE is capable of learning deep spatio temporal representations from spikes relying solely on local information, making it compatible with neurobiology and neuromorphic hardware. Synaptic plasticity rules are derived systematically from user-defined cost functions and neural dynamics by leveraging existing autodifferentiation methods of machine learning frameworks. We benchmark our approach on the event-based neuromorphic dataset N-MNIST and DvsGesture, on which DECOLLE performs comparably to the state-of-the-art. DECOLLE networks provide continuously learning machines that are relevant to biology and supportive of event-based, low-power computer vision architectures matching the accuracies of conventional computers on tasks where temporal precision and speed are essential.",2018,76,132,18,True,Computer Science,40647726,Jacques Kaiser,34516897.0,H. Mostafa,Medicine,1734355.0,E. Neftci,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23e95aa03b5ddf4380af437581372e4d8414c121,https://www.semanticscholar.org/paper/23e95aa03b5ddf4380af437581372e4d8414c121,"Backpropagation: theory, architectures, and applications","Contents: D.E. Rumelhart, R. Durbin, R. Golden, Y. Chauvin, Backpropagation: The Basic Theory. A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, K.J. Lang, Phoneme Recognition Using Time-Delay Neural Networks. C. Schley, Y. Chauvin, V. Henkle, Automated Aircraft Flare and Touchdown Control Using Neural Networks. F.J. Pineda, Recurrent Backpropagation Networks. M.C. Mozer, A Focused Backpropagation Algorithm for Temporal Pattern Recognition. D.H. Nguyen, B. Widrow, Nonlinear Control with Neural Networks. M.I. Jordan, D.E. Rumelhart, Forward Models: Supervised Learning with a Distal Teacher. S.J. Hanson, Backpropagation: Some Comments and Variations. A. Cleeremans, D. Servan-Schreiber, J.L. McClelland, Graded State Machines: The Representation of Temporal Contingencies in Feedback Networks. S. Becker, G.E. Hinton, Spatial Coherence as an Internal Teacher for a Neural Network. J.R. Bachrach, M.C. Mozer, Connectionist Modeling and Control of Finite State Systems Given Partial State Information. P. Baldi, Y. Chauvin, K. Hornik, Backpropagation and Unsupervised Learning in Linear Networks. R.J. Williams, D. Zipser, Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity. P. Baldi, Y. Chauvin, When Neural Networks Play Sherlock Homes. P. Baldi, Gradient Descent Learning Algorithms: A Unified Perspective.",1995,0,534,17,False,Computer Science,2952703,Y. Chauvin,2681887.0,D. Rumelhart,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
202b548c1bcb26b24d4c446e1434a94d46fcf461,https://www.semanticscholar.org/paper/202b548c1bcb26b24d4c446e1434a94d46fcf461,Error Moderation in Low-Cost Machine-Learning-Based Analog/RF Testing,"Machine-learning-based test methods for analog/RF devices have been the subject of intense investigation over the last decade. However, despite the significant cost benefits that these methods promise, they have seen a limited success in replacing the traditional specification testing, mainly due to the incurred test error which, albeit small, cannot meet industrial standards. To address this problem, we introduce a neural system that is trained not only to predict the pass/fail labels of devices based on a set of low-cost measurements, as aimed by the previous machine-learning-based test methods, but also to assess the confidence in this prediction. Devices for which this confidence is insufficient are then retested through the more expensive specification testing in order to reach an accurate test decision. Thus, this two-tier test approach sustains the high accuracy of specification testing while leveraging the low cost of machine-learning-based testing. In addition, by varying the desired level of confidence, it enables the exploration of the tradeoff between test cost and test accuracy and facilitates the development of cost-effective test plans. We discuss the structure and the training algorithm of an ontogenic neural network which is embodied in the neural system in the first tier, as well as the extraction of appropriate measurements such that only a small fraction of devices are funneled to the second tier. The proposed test-error-moderation method is demonstrated on a switched-capacitor filter and an ultrahigh-frequency receiver front end.",2008,44,113,3,False,Computer Science,1689094,H. Stratigopoulos,1743038.0,Y. Makris,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77332906ee0dabba93ddf6a088d80d8b13329574,https://www.semanticscholar.org/paper/77332906ee0dabba93ddf6a088d80d8b13329574,Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems: Perspectives and Challenges PART 1,"Machine learning and data mining algorithms are becoming increasingly important in analyzing large volume, multi-relational and ∗Copyright A.Cichocki et al. Please make reference to: A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao and D.P. Mandic (2016), “Tensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 1 Low-Rank Tensor Decompositions”, Foundations and Trends in Machine Learning: Vol. 9: No. 4-5, pp 249-429.",2016,327,81,6,True,Computer Science,145683892,A. Cichocki,1777061.0,Namgil Lee,,1738205.0,I. Oseledets,9377326.0,A. Phan,144251712.0,Qibin Zhao,9368519.0,D. Mandic,,,,,,,,,,,,,,,,,,,,,,,,,,,
84fed2a181f282fde8251882ea2a3f0b1e65bbe5,https://www.semanticscholar.org/paper/84fed2a181f282fde8251882ea2a3f0b1e65bbe5,"Combining Search, Social Media, and Traditional Data Sources to Improve Influenza Surveillance","We present a machine learning-based methodology capable of providing real-time (“nowcast”) and forecast estimates of influenza activity in the US by leveraging data from multiple data sources including: Google searches, Twitter microblogs, nearly real-time hospital visit records, and data from a participatory surveillance system. Our main contribution consists of combining multiple influenza-like illnesses (ILI) activity estimates, generated independently with each data source, into a single prediction of ILI utilizing machine learning ensemble approaches. Our methodology exploits the information in each data source and produces accurate weekly ILI predictions for up to four weeks ahead of the release of CDC’s ILI reports. We evaluate the predictive ability of our ensemble approach during the 2013–2014 (retrospective) and 2014–2015 (live) flu seasons for each of the four weekly time horizons. Our ensemble approach demonstrates several advantages: (1) our ensemble method’s predictions outperform every prediction using each data source independently, (2) our methodology can produce predictions one week ahead of GFT’s real-time estimates with comparable accuracy, and (3) our two and three week forecast estimates have comparable accuracy to real-time predictions using an autoregressive model. Moreover, our results show that considerable insight is gained from incorporating disparate data streams, in the form of social media and crowd sourced data, into influenza predictions in all time horizons.",2015,57,359,10,True,Mathematics,144982204,M. Santillana,145164518.0,A. Nguyen,Computer Science,1782853.0,Mark Dredze,143946641.0,Michael J. Paul,2332684.0,E. Nsoesie,144252495.0,J. Brownstein,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,
44148499c23789781873f1ca56449c33422045f4,https://www.semanticscholar.org/paper/44148499c23789781873f1ca56449c33422045f4,A Web Survey on the Use of Active Learning to Support Annotation of Text Data,"As supervised machine learning methods for addressing tasks in natural language processing (NLP) prove increasingly viable, the focus of attention is naturally shifted towards the creation of training data. The manual annotation of corpora is a tedious and time consuming process. To obtain high-quality annotated data constitutes a bottleneck in machine learning for NLP today. Active learning is one way of easing the burden of annotation. This paper presents a first probe into the NLP research community concerning the nature of the annotation projects undertaken in general, and the use of active learning as annotation support in particular.",2009,8,75,3,True,Computer Science,3357473,Katrin Tomanek,2653947.0,Fredrik Olsson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
598658595e887f677967769ed11ba28158f6ea8a,https://www.semanticscholar.org/paper/598658595e887f677967769ed11ba28158f6ea8a,Neural Additive Models: Interpretable Machine Learning with Neural Nets,"Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.",2020,48,134,35,False,Computer Science,29767024,Rishabh Agarwal,27737461.0,Nicholas Frosst,Mathematics,9117593.0,Xuezhou Zhang,145727186.0,R. Caruana,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
28f1f6cbe07b117387e2b07c11e7ac9c4ef8cf95,https://www.semanticscholar.org/paper/28f1f6cbe07b117387e2b07c11e7ac9c4ef8cf95,A Machine Learning Approach to Pedestrian Detection for Autonomous Vehicles Using High-Definition 3D Range Data,"This article describes an automated sensor-based system to detect pedestrians in an autonomous vehicle application. Although the vehicle is equipped with a broad set of sensors, the article focuses on the processing of the information generated by a Velodyne HDL-64E LIDAR sensor. The cloud of points generated by the sensor (more than 1 million points per revolution) is processed to detect pedestrians, by selecting cubic shapes and applying machine vision and machine learning algorithms to the XY, XZ, and YZ projections of the points contained in the cube. The work relates an exhaustive analysis of the performance of three different machine learning algorithms: k-Nearest Neighbours (kNN), Naïve Bayes classifier (NBC), and Support Vector Machine (SVM). These algorithms have been trained with 1931 samples. The final performance of the method, measured a real traffic scenery, which contained 16 pedestrians and 469 samples of non-pedestrians, shows sensitivity (81.2%), accuracy (96.2%) and specificity (96.8%).",2016,43,77,1,True,Computer Science,145754294,P. Navarro,1422427951.0,Carlos Fernández-Isla,Medicine,8840267.0,R. Borraz,2064132228.0,Diego Alonso,,,,,Engineering,,,,,,,,,,,,,,,,,,,,,,,,,,
31b74437e2e46790f02b91933195b412684370be,https://www.semanticscholar.org/paper/31b74437e2e46790f02b91933195b412684370be,Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation,"The discovery of novel materials and functional molecules can help to solve some of society’s most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering–generally denoted as inverse design–was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce SELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every SELFIES string corresponds to a valid molecule, and SELFIES can represent every molecule. SELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model’s internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.",2019,67,208,16,False,Computer Science,5906965,Mario Krenn,122433803.0,Florian Hase,Physics,133638577.0,AkshatKumar Nigam,35323511.0,Pascal Friederich,1380248954.0,Alán Aspuru-Guzik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5ce9f2b57c0a9952a58ae3b51ddefe8e7512ee9f,https://www.semanticscholar.org/paper/5ce9f2b57c0a9952a58ae3b51ddefe8e7512ee9f,Opportunities and challenges for quantum-assisted machine learning in near-term quantum computers,"With quantum computing technologies nearing the era of commercialization and quantum supremacy, machine learning (ML) appears as one of the promising ‘killer’ applications. Despite significant effort, there has been a disconnect between most quantum ML proposals, the needs of ML practitioners, and the capabilities of near-term quantum devices to demonstrate quantum enhancement in the near future. In this contribution to the focus collection ‘What would you do with 1000 qubits?’, we provide concrete examples of intractable ML tasks that could be enhanced with near-term devices. We argue that to reach this target, the focus should be on areas where ML researchers are struggling, such as generative models in unsupervised and semi-supervised learning, instead of the popular and more tractable supervised learning techniques. We also highlight the case of classical datasets with potential quantum-like statistical correlations where quantum models could be more suitable. We focus on hybrid quantum–classical approaches and illustrate some of the key challenges we foresee for near-term implementations. Finally, we introduce the quantum-assisted Helmholtz machine (QAHM), an attempt to use near-term quantum devices to tackle high-dimensional datasets of continuous variables. Instead of using quantum computers to assist deep learning, as previous approaches do, the QAHM uses deep learning to extract a low-dimensional binary representation of data, suitable for relatively small quantum processors which can assist the training of an unsupervised generative model. Although we illustrate this concept on a quantum annealer, other quantum platforms could benefit as well from this hybrid quantum–classical framework.",2017,208,123,3,True,Physics,1402938096,A. Perdomo-Ortiz,3468759.0,M. Benedetti,Mathematics,1422181059.0,John Realpe-Gómez,144637418.0,R. Biswas,,,,,Computer Science,,,,,,,,,,,,,,,,,,,,,,,,,,
e6386ba3a0c69000c16ba238467b75e4b2ef8682,https://www.semanticscholar.org/paper/e6386ba3a0c69000c16ba238467b75e4b2ef8682,Coordinated management of multiple interacting resources in chip multiprocessors: A machine learning approach,"Efficient sharing of system resources is critical to obtaining high utilization and enforcing system-level performance objectives on chip multiprocessors (CMPs). Although several proposals that address the management of a single microarchitectural resource have been published in the literature, coordinated management of multiple interacting resources on CMPs remains an open problem. We propose a framework that manages multiple shared CMP resources in a coordinated fashion to enforce higher-level performance objectives. We formulate global resource allocation as a machine learning problem. At runtime, our resource management scheme monitors the execution of each application, and learns a predictive model of system performance as a function of allocation decisions. By learning each application's performance response to different resource distributions, our approach makes it possible to anticipate the system-level performance impact of allocation decisions at runtime with little runtime overhead. As a result, it becomes possible to make reliable comparisons among different points in a vast and dynamically changing allocation space, allowing us to adapt our allocation decisions as applications undergo phase changes. Our evaluation concludes that a coordinated approach to managing multiple interacting resources is key to delivering high performance in multiprogrammed workloads, but this is possible only if accompanied by efficient search mechanisms. We also show that it is possible to build a single mechanism that consistently delivers high performance under various important performance metrics.",2008,28,276,16,False,Computer Science,2226590,Ramazan Bitirgen,1787439.0,Engin Ipek,,2109798451.0,José F. Martínez,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
db2b5d12152025f65d4e313c321a95be3aecbb29,https://www.semanticscholar.org/paper/db2b5d12152025f65d4e313c321a95be3aecbb29,Shop Class as Soulcraft: An Inquiry into the Value of Work,"Author Crawford argues for the revival of teaching the skills of tool, machine, and material use in the schools. He points to the decline in our knowledge of the artifacts of our culture and the resulting loss of self-determination. The teaching of manual competence has also provided a method of learning that is well suited to many students who are otherwise disenchanted with school.",2009,1,323,32,False,Computer Science,153856987,Matthew B. Crawford,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3ab11a96f6c475afa7e5977886da301be4ed2f7f,https://www.semanticscholar.org/paper/3ab11a96f6c475afa7e5977886da301be4ed2f7f,Tux2: Distributed Graph Computation for Machine Learning,"TUX2 is a new distributed graph engine that bridges graph computation and distributed machine learning. TUX2 inherits the benefits of an elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs; we extend and optimize it for distributed machine learning to support heterogeneity, a Stale Synchronous Parallel model, and a new MEGA (Mini-batch, Exchange, GlobalSync, and Apply) model. We have developed a set of representative distributed machine learning algorithms in TUX2, covering both supervised and unsupervised learning. Compared to implementations on distributedmachine learning platforms, writing these algorithms in TUX2 takes only about 25% of the code: Our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. Our extensive evaluation of TUX2, using large data sets with up to 64 billion edges, shows that TUX2 outperforms state-of-the-art distributed graph engines PowerGraph and PowerLyra by an order of magnitude, while beating two state-of-the-art distributed machine learning systems by at least 48%.",2017,43,53,2,False,Computer Science,9754946,Wencong Xiao,2870618.0,Jilong Xue,,11009920.0,Youshan Miao,1700892.0,Z. Li,2145774559.0,Cheng Chen,145217421.0,Ming Wu,,,Wei Li,93317102.0,Lidong Zhou,,,,,,,,,,,,,,,,,,,,,,
54660311a46976720bf05fc113776a14cbe5a0b5,https://www.semanticscholar.org/paper/54660311a46976720bf05fc113776a14cbe5a0b5,Load Forecasting Using Support Vector Machines: A Study on EUNITE Competition 2001,"Load forecasting is usually made by constructing models on relative information, such as climate and previous load demand data. In 2001, EUNITE network organized a competition aiming at mid-term load forecasting (predicting daily maximum load of the next 31 days). During the competition we proposed a support vector machine (SVM) model, which was the winning entry, to solve the problem. In this paper, we discuss in detail how SVM, a new learning technique, is successfully applied to load forecasting. In addition, motivated by the competition results and the approaches by other participants, more experiments and deeper analyses are conducted and presented here. Some important conclusions from the results are that temperature (or other types of climate information) might not be useful in such a mid-term load forecasting problem and that the introduction of time-series concept may improve the forecasting.",2004,25,590,31,True,Computer Science,2152690827,Bo-Juen Chen,1744179.0,Ming-Wei Chang,,1711460.0,Chih-Jen Lin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ed0f6ad0ab7b15dd0c639acbfa996beebe47ae10,https://www.semanticscholar.org/paper/ed0f6ad0ab7b15dd0c639acbfa996beebe47ae10,Survey on encoding schemes for genomic data representation and feature learning - from signal processing to machine learning,"Data-driven machine learning, especially deep learning technology, is becoming an important tool for handling big data issues in bioinformatics. In machine learning, DNA sequences are often converted to numerical values for data representation and feature learning in various applications. Similar conversion occurs in Genomic Signal Processing (GSP), where genome sequences are transformed into numerical sequences for signal extraction and recognition. This kind of conversion is also called encoding scheme. The diverse encoding schemes can greatly affect the performance of GSP applications and machine learning models. This paper aims to collect, analyze, discuss, and summarize the existing encoding schemes of genome sequence particularly in GSP as well as other genome analysis applications to provide a comprehensive reference for the genomic data representation and feature learning in machine learning.",2018,0,44,1,True,Computer Science,2052212942,Ning Yu,49970147.0,Zhihua Li,,50077927.0,Zeng Yu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
11db1789a34a1019cad47d04534b68a0b5dc0543,https://www.semanticscholar.org/paper/11db1789a34a1019cad47d04534b68a0b5dc0543,MACHINE LEARNING OF MORPHOSYNTACTIC STRUCTURE: LEMMATIZING UNKNOWN SLOVENE WORDS,"Automatic lemmatization is a core application for many language processing tasks. In inflectionally rich languages, such as Slovene, assigning the correct lemma (base form) to each word in a running text is not trivial, since for instance, nouns inflect for number and case, with a complex configuration of endings and stem modifications. The problem is especially difficult for unknown words, since word-forms cannot be matched against a morphological lexicon. This paper discusses a machine learning approach to the automatic lemmatization of unknown words in Slovene texts. We decompose the problem of learning to perform lemmatization into two subproblems: learning to perform morphosyntactic tagging of words in a text, and learning to perform morphological analysis, which produces the lemma from the word-form given the correct morphosyntactic tag. A statistics-based trigram tagger is used to learn morphosyntactic tagging and a first-order decision list learning system is used to learn rules for morphological analysis. We train the tagger on a manually annotated corpus consisting of 100,000 running words. We train the analyzer on open-class inflecting Slovene words, namely nouns, adjectives, and main verbs, together being characterized by more than 400 different morphosyntactic tags. The training set for the analyzer consists of a morphological lexicon containing 15,000 lemmas. We evaluate the learned model on word lists extracted from a corpus of Slovene texts containing 500,000 words, and show that our morphological analysis module achieves 98.6% accuracy, while the combination of the tagger and analyzer is 92.0% accurate on unknown inflecting Slovene words.",2004,30,83,1,False,Computer Science,1696434,T. Erjavec,1693549.0,S. Džeroski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5fcb0304312b77b7938cbf7bf7a68028966e7608,https://www.semanticscholar.org/paper/5fcb0304312b77b7938cbf7bf7a68028966e7608,Machine learning for improved image-based wavefront sensing.,"For large amounts of wavefront error, gradient-based optimization methods for image-based wavefront sensing are unlikely to converge when the starting guess for the wavefront differs greatly from the true wavefront. We use machine learning operating on a point-spread function to determine a good initial estimate of the wavefront. We show that our trained convolutional neural network provides good initial estimates in the presence of simulated detector noise and is more effective than using many random starting guesses for large amounts of wavefront error.",2018,2,104,1,False,Computer Science,40548429,S. Paine,2027310.0,J. Fienup,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
05a1df55ae6e98cea8d55ec018ec516bc6bc4306,https://www.semanticscholar.org/paper/05a1df55ae6e98cea8d55ec018ec516bc6bc4306,Machine learning approaches for improving condition-based maintenance of naval propulsion plants,"Availability, reliability and economic sustainability of naval propulsion plants are key elements to cope with because maintenance costs represent a large slice of total operational expenses. Depending on the adopted strategy, impact of maintenance on overall expenses can remarkably vary; for example, letting an asset running up until breakdown can lead to unaffordable costs. As a matter of fact, a desideratum is to progress maintenance technology of ship propulsion systems from breakdown or preventive maintenance up to more effective condition-based maintenance approaches. The central idea in condition-based maintenance is to monitor the propulsion equipment by exploiting heterogeneous sensors, enabling diagnosis and, most of all, prognosis of the propulsion system’s components and of their potential future failures. The success of condition-based maintenance clearly hinges on the capability of developing effective predictive models; for this purpose, effective use of machine learning methods is proposed in this article. In particular, authors take into consideration an application of condition-based maintenance to gas turbines used for vessel propulsion, where the performance and advantages of exploiting machine learning methods in modeling the degradation of the propulsion plant over time are tested. Experiments, conducted on data generated from a sophisticated simulator of a gas turbine, mounted on a Frigate characterized by a COmbined Diesel eLectric And Gas propulsion plant type, will allow to show the effectiveness of the proposed machine learning approaches and to benchmark them in a realistic maritime application.",2016,71,116,10,False,Engineering,8720641,Andrea Coraddu,1682762.0,L. Oneto,,2068386862.0,A. Ghio,47385577.0,S. Savio,1686031.0,D. Anguita,31850218.0,M. Figari,,,,,,,,,,,,,,,,,,,,,,,,,,,
6b343dd1814738da2d01edf5d81332701f005bcb,https://www.semanticscholar.org/paper/6b343dd1814738da2d01edf5d81332701f005bcb,Scaling Datalog for Machine Learning on Big Data,"In this paper, we present the case for a declarative foundation for data-intensive machine learning systems. Instead of creating a new system for each specific flavor of machine learning task, or hardcoding new optimizations, we argue for the use of recursive queries to program a variety of machine learning systems. By taking this approach, database query optimization techniques can be utilized to identify effective execution plans, and the resulting runtime plans can be executed on a single unified data-parallel query processing engine. As a proof of concept, we consider two programming models--Pregel and Iterative Map-Reduce-Update---from the machine learning domain, and show how they can be captured in Datalog, tuned for a specific task, and then compiled into an optimized physical plan. Experiments performed on a large computing cluster with real data demonstrate that this declarative approach can provide very good performance while offering both increased generality and programming ease.",2012,33,51,5,False,Computer Science,1925686,Yingyi Bu,1860208.0,Vinayak R. Borkar,,1703347.0,M. Carey,47888697.0,Joshua Rosen,1763100.0,Neoklis Polyzotis,3269316.0,Tyson Condie,,2965406.0,Markus Weimer,1709145.0,R. Ramakrishnan,,,,,,,,,,,,,,,,,,,,,,
627c14fe9097d459b8fd47e8a901694198be9d5d,https://www.semanticscholar.org/paper/627c14fe9097d459b8fd47e8a901694198be9d5d,Compressed Learning : Universal Sparse Dimensionality Reduction and Learning in the Measurement Domain,"In this paper, we provide theoretical results to show that compressed learning , learning directly in the compressed domain, is possible. In Particular, we provide tight bounds demonstrating that the linear kernel SVM’s classifier in the measurement domain, with high probability, has true accuracy close to the accuracy of the best linear threshold classifier in the data domain. We show that this is beneficial both from the compressed sensing and the machine learning points of view. Furthermore, we indicate that for a family of well-known compressed sensing matrices, compressed learning is universal, in the sense that learning and classification in the measurement domain works provided that the data are sparse in some, even unknown, basis. Moreover, we show that our results are also applicable to a family of smooth manifold-learning tasks. Finally, we support our claims with experimental results.",2009,36,168,16,False,,120874496,R. Calderbank,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7fddb827b562eb3fa3f07b4fc174871654be3423,https://www.semanticscholar.org/paper/7fddb827b562eb3fa3f07b4fc174871654be3423,High-Risk Breast Lesions: A Machine Learning Model to Predict Pathologic Upgrade and Reduce Unnecessary Surgical Excision.,"Purpose To develop a machine learning model that allows high-risk breast lesions (HRLs) diagnosed with image-guided needle biopsy that require surgical excision to be distinguished from HRLs that are at low risk for upgrade to cancer at surgery and thus could be surveilled. Materials and Methods Consecutive patients with biopsy-proven HRLs who underwent surgery or at least 2 years of imaging follow-up from June 2006 to April 2015 were identified. A random forest machine learning model was developed to identify HRLs at low risk for upgrade to cancer. Traditional features such as age and HRL histologic results were used in the model, as were text features from the biopsy pathologic report. Results One thousand six HRLs were identified, with a cancer upgrade rate of 11.4% (115 of 1006). A machine learning random forest model was developed with 671 HRLs and tested with an independent set of 335 HRLs. Among the most important traditional features were age and HRL histologic results (eg, atypical ductal hyperplasia). An important text feature from the pathologic reports was ""severely atypical."" Instead of surgical excision of all HRLs, if those categorized with the model to be at low risk for upgrade were surveilled and the remainder were excised, then 97.4% (37 of 38) of malignancies would have been diagnosed at surgery, and 30.6% (91 of 297) of surgeries of benign lesions could have been avoided. Conclusion This study provides proof of concept that a machine learning model can be applied to predict the risk of upgrade of HRLs to cancer. Use of this model could decrease unnecessary surgery by nearly one-third and could help guide clinical decision making with regard to surveillance versus surgical excision of HRLs. © RSNA, 2017.",2017,36,106,0,True,Medicine,8306823,M. Bahl,1741283.0,R. Barzilay,,2064168078.0,Adam B Yedidia,36278644.0,N. Locascio,49297123.0,L. Yu,4828241.0,C. Lehman,,,,,,,,,,,,,,,,,,,,,,,,,,,
7d056b511378a056ee4321f5f14e1f73616e7f65,https://www.semanticscholar.org/paper/7d056b511378a056ee4321f5f14e1f73616e7f65,Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling,"The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. It arises in various application domains, including distributed tracking and localization, multi-agent coordination, estimation in sensor networks, and large-scale machine learning. We develop and analyze distributed algorithms based on dual subgradient averaging, and we provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis allows us to clearly separate the convergence of the optimization algorithm itself and the effects of communication dependent on the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network, and confirm this prediction's sharpness both by theoretical lower bounds and simulations for various networks. Our approach includes the cases of deterministic optimization and communication, as well as problems with stochastic optimization and/or communication.",2010,52,1056,114,True,Mathematics,1734693,John C. Duchi,40333747.0,Alekh Agarwal,Computer Science,1721860.0,M. Wainwright,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e7c36f2caeb0bd6d691c4500d223abd482c61180,https://www.semanticscholar.org/paper/e7c36f2caeb0bd6d691c4500d223abd482c61180,Intrusion detection in SCADA systems using machine learning techniques,"In this paper we present a intrusion detection module capable of detecting malicious network traffic in a SCADA (Supervisory Control and Data Acquisition) system. Malicious data in a SCADA system disrupt its correct functioning and tamper with its normal operation. OCSVM (One-Class Support Vector Machine) is an intrusion detection mechanism that does not need any labeled data for training or any information about the kind of anomaly is expecting for the detection process. This feature makes it ideal for processing SCADA environment data and automate SCADA performance monitoring. The OCSVM module developed is trained by network traces off line and detect anomalies in the system real time. The module is part of an IDS (Intrusion Detection System) system developed under CockpitCI project and communicates with the other parts of the system by the exchange of IDMEF (Intrusion Detection Message Exchange Format) messages that carry information about the source of the incident, the time and a classification of the alarm.",2014,97,131,14,True,Computer Science,3001662,L. Maglaras,2149531404.0,Jianmin Jiang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
198c08b8d7fba77458cb6484268528e975727d8b,https://www.semanticscholar.org/paper/198c08b8d7fba77458cb6484268528e975727d8b,Machine learning application in MOOCs: Dropout prediction,"Massive Open Online Course(MOOC) is undergoing explosive growth recently, both the number of MOOC platforms and courses are increasing dramatically during these years. One of the major concerns in MOOC is high dropout rate, we study dropout prediction in MOOCs, using student's learning activities data in a period of time to measure how likely students would drop out in next couple of days. We collect 39 courses data from XuetangX platform, which is based on the open source Edx platform. Using supervised classification approach in the machine learning field, we achieve 89% accuracy in dropout prediction task with gradient boosting decision tree model. We describe details in drop out prediction framework, including data extraction from Edx platform, data preprocessing, feature engineering and performance test on several supervised classification models.",2016,10,59,5,False,Computer Science,2118675149,Jiajun Liang,2151086786.0,Chao Li,,2149971996.0,Li Zheng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
316be673c8f35274ddc8090c69581b5be0a3332d,https://www.semanticscholar.org/paper/316be673c8f35274ddc8090c69581b5be0a3332d,Privacy as Protection of the Incomputable Self: From Agnostic to Agonistic Machine Learning,"Abstract This Article takes the perspective of law and philosophy, integrating insights from computer science. First, I will argue that in the era of big data analytics we need an understanding of privacy that is capable of protecting what is uncountable, incalculable or incomputable about individual persons. To instigate this new dimension of the right to privacy, I expand previous work on the relational nature of privacy, and the productive indeterminacy of human identity it implies, into an ecological understanding of privacy, taking into account the technological environment that mediates the constitution of human identity. Second, I will investigate how machine learning actually works, detecting a series of design choices that inform the accuracy of the outcome, each entailing trade-offs that determine the relevance, validity and reliability of the algorithm’s accuracy for real life problems. I argue that incomputability does not call for a rejection of machine learning per se but calls for a research design that enables those who will be affected by the algorithms to become involved and to learn how machines learn — resulting in a better understanding of their potential and limitations. A better understanding of the limitations that are inherent in machine learning will deflate some of the eschatological expectations, and provide for better decision-making about whether and if so how to implement machine learning in specific domains or contexts. I will highlight how a reliable research design aligns with purpose limitation as core to its methodological integrity. This Article, then, advocates a practice of “agonistic machine learning” that will contribute to responsible decisions about the integration of data-driven applications into our environments while simultaneously bringing them under the Rule of Law. This should also provide the best means to achieve effective protection against overdetermination of individuals by machine inferences.",2019,0,63,9,True,Computer Science,40652172,M. Hildebrandt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
59bf142f94ee5193481fde751107a54842cbdaef,https://www.semanticscholar.org/paper/59bf142f94ee5193481fde751107a54842cbdaef,PRODIGY4.0: The Manual and Tutorial,"PRODIGY is a general-purpose problem-solving architecture that serves as a basis for research in planning, machine learning, apprentice-type knowledge-refinement interfaces, and expert systems. This document is a manual for the latest version of the PRODIGY system, PRODIGY4.0, and includes descriptions of the PRODIGY representation language, control structure, user interface, abstraction module, and other features. The tutorial style is meant to provide the reader with the ability to run PRODIGY and make use of all the basic features, as well as gradually learning the more esoteric aspects of PRODIGY4.0.",1992,37,218,14,False,Computer Science,143731071,J. Blythe,143712374.0,J. Carbonell,,1741101.0,Oren Etzioni,145526918.0,Y. Gil,143676491.0,R. Joseph,2055496930.0,Dan Kahn,,1745117.0,Craig A. Knoblock,26602711.0,Steven N. Minton,2110390815.0,Alicia Pérez,2069834710.0,Scott Reilly,1956361.0,M. Veloso,2108239090.0,Xuemei Wang,,,,,,,,,,,,,,
484b3c21c75c6676c26703fa33c0d38747bd90be,https://www.semanticscholar.org/paper/484b3c21c75c6676c26703fa33c0d38747bd90be,Presto: distributed machine learning and graph processing with sparse matrices,"It is cumbersome to write machine learning and graph algorithms in data-parallel models such as MapReduce and Dryad. We observe that these algorithms are based on matrix computations and, hence, are inefficient to implement with the restrictive programming and communication interface of such frameworks.In this paper we show that array-based languages such as R [3] are suitable for implementing complex algorithms and can outperform current data parallel solutions. Since R is single-threaded and does not scale to large datasets, we have built Presto, a distributed system that extends R and addresses many of its limitations. Presto efficiently shares sparse structured data, can leverage multi-cores, and dynamically partitions data to mitigate load imbalance. Our results show the promise of this approach: many important machine learning and graph algorithms can be expressed in a single framework and are substantially faster than those in Hadoop and Spark.",2013,41,98,7,False,Computer Science,2697906,S. Venkataraman,152711509.0,Erik Bodzsar,,48315986.0,Indrajit Roy,3274692.0,A. AuYoung,143986781.0,R. Schreiber,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6976e5762d5cb285cbea8644a2406600248ea19a,https://www.semanticscholar.org/paper/6976e5762d5cb285cbea8644a2406600248ea19a,Predictive Power of Machine Learning for Optimizing Solar Water Heater Performance: The Potential Application of High-Throughput Screening,"Predicting the performance of solar water heater (SWH) is challenging due to the complexity of the system. Fortunately, knowledge-based machine learning can provide a fast and precise prediction method for SWH performance. With the predictive power of machine learning models, we can further solve a more challenging question: how to cost-effectively design a high-performance SWH? Here, we summarize our recent studies and propose a general framework of SWH design using a machine learning-based high-throughput screening (HTS) method. Design of water-in-glass evacuated tube solar water heater (WGET-SWH) is selected as a case study to show the potential application of machine learning-based HTS to the design and optimization of solar energy systems.",2017,53,54,1,True,Chemistry,2152377478,Hao Li,46271321.0,Zhijian Liu,,2152352792.0,Kejun Liu,10389367.0,Zhien Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
