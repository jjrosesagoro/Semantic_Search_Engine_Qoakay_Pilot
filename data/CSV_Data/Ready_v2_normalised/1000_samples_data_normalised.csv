paperId,url,title,abstract,year,referenceCount,citationCount,influentialCitationCount,isOpenAccess,fieldsOfStudy/0,fieldsOfStudy/1,authors/0/authorId,authors/0/name,authors/1/authorId,authors/1/name,authors/2/authorId,authors/2/name,authors/3/authorId,authors/3/name,authors/4/authorId,authors/4/name,authors/5/authorId,authors/5/name,fieldsOfStudy/2,authors/6/authorId,authors/6/name,authors/7/authorId,authors/7/name,authors/8/authorId,authors/8/name,authors/9/authorId,authors/9/name,authors/10/authorId,authors/10/name,authors/11/authorId,authors/11/name,authors/12/authorId,authors/12/name,authors/13/authorId,authors/13/name,authors/14/authorId,authors/14/name,authors/15/authorId,authors/15/name,authors/16/authorId,authors/16/name,authors/17/authorId,authors/17/name,authors/18/authorId,authors/18/name,authors/19/authorId,authors/19/name,authors/20/authorId,authors/20/name,authors/21/authorId,authors/21/name,authors/22/authorId,authors/22/name,authors/23/authorId,authors/23/name,authors/24/authorId,authors/24/name,authors/25/authorId,authors/25/name,authors/26/authorId,authors/26/name,authors/27/authorId,authors/27/name,authors/28/authorId,authors/28/name,authors/29/authorId,authors/29/name,authors/30/authorId,authors/30/name,authors/31/authorId,authors/31/name
ec7c68427a26f812532b1c913c68fcf84b7de58e,https://www.semanticscholar.org/paper/ec7c68427a26f812532b1c913c68fcf84b7de58e,Beyond the point cloud: from transductive to semi-supervised learning,"Due to its occurrence in engineering domains and implications for natural learning, the problem of utilizing unlabeled data is attracting increasing attention in machine learning. A large body of recent literature has focussed on the transductive setting where labels of unlabeled examples are estimated by learning a function defined only over the point cloud data. In a truly semi-supervised setting however, a learning machine has access to labeled and unlabeled examples and must make predictions on data points never encountered before. In this paper, we show how to turn transductive and standard supervised learning algorithms into semi-supervised learners. We construct a family of data-dependent norms on Reproducing Kernel Hilbert Spaces (RKHS). These norms allow us to warp the structure of the RKHS to reflect the underlying geometry of the data. We derive explicit formulas for the corresponding new kernels. Our approach demonstrates state of the art performance on a variety of classification tasks.",2005,32,491,70,False,Mathematics,Computer Science,1808676,V. Sindhwani,1770745.0,P. Niyogi,145520115.0,Mikhail Belkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1cef66e9f54dd3a855b8bcc53dbbfa6b7d642b93,https://www.semanticscholar.org/paper/1cef66e9f54dd3a855b8bcc53dbbfa6b7d642b93,구글 TensorFlow 소개,"TensorFlow 2.0 in ActionTensorFlow 1.x Deep Learning CookbookMachine Learning with TensorFlow 1.xMachine Learning with TensorFlow, Second EditionTensorFlow 2 Pocket PrimerProgramming with TensorFlowTensorFlow Machine Learning ProjectsHands-On Neural Networks with TensorFlow 2.0TensorFlow for Deep LearningTensor Flow Pocket PrimerNatural Language Processing with TensorFlowTensorFlow: Powerful Predictive Analytics with TensorFlowHands-On Convolutional Neural Networks with TensorFlowTensorFlow 2.0 Computer Vision CookbookIntelligent Mobile Projects with TensorFlowLearning TensorFlow.jsDeep Learning with TensorFlow 2 and KerasLearning TensorFlowTensorFlow 2 Pocket ReferenceMachine Learning Using TensorFlow CookbookTensorFlow 2.0 Quick Start GuideTensorFlow Machine Learning CookbookLearn TensorFlow 2.0Learn TensorFlow in 24 HoursHands-On Computer Vision with TensorFlow 2Mastering Computer Vision with TensorFlow 2.xPro Deep Learning with TensorFlowHands-On Machine Learning with TensorFlow.jsTensorFlow for Deep LearningTinyMLLearning TensorFlow.jsDeep Learning with TensorFlow 2 and Keras Second EditionDeep Learning with TensorFlowMastering TensorFlow 1.xAdopting TensorFlow for Real-World AITensorFlow For DummiesArtificial Intelligence with PythonHands-On Machine Learning with Scikit-Learn, Keras, and TensorFlowLearn TensorFlow EnterpriseThe TensorFlow Workshop",2015,1,369,13,True,Computer Science,,65794932,김종영,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f1d73520d51a0f19c686d9f4bf7af1db33763a2a,https://www.semanticscholar.org/paper/f1d73520d51a0f19c686d9f4bf7af1db33763a2a,A support vector machine approach for detection of microcalcifications,"We investigate an approach based on support vector machines (SVMs) for detection of microcalcification (MC) clusters in digital mammograms, and propose a successive enhancement learning scheme for improved performance. SVM is a machine-learning method, based on the principle of structural risk minimization, which performs well when applied to data outside the training set. We formulate MC detection as a supervised-learning problem and apply SVM to develop the detection algorithm. We use the SVM to detect at each location in the image whether an MC is present or not. We tested the proposed method using a database of 76 clinical mammograms containing 1120 MCs. We use free-response receiver operating characteristic curves to evaluate detection performance, and compare the proposed algorithm with several existing methods. In our experiments, the proposed SVM framework outperformed all the other methods tested. In particular, a sensitivity as high as 94% was achieved by the SVM method at an error rate of one false-positive cluster per image. The ability of SVM to outperform several well-known methods developed for the widely studied problem of MC detection suggests that SVM is a promising technique for object detection in a medical imaging application.",2002,37,559,19,False,Computer Science,Medicine,11895669,I. El-Naqa,46286398.0,Yongyi Yang,3067670.0,M. Wernick,143949487.0,N. Galatsanos,1781483.0,R. Nishikawa,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
63be611dc8cf09ed081b4ad2b1756420b782b0be,https://www.semanticscholar.org/paper/63be611dc8cf09ed081b4ad2b1756420b782b0be,A Perspective on Deep Imaging,"The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.",2016,59,324,6,False,Computer Science,Biology,2108295460,Ge Wang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
08745f22d0abbe66e486f0985c985ecf1eab4e9e,https://www.semanticscholar.org/paper/08745f22d0abbe66e486f0985c985ecf1eab4e9e,Question classification using support vector machines,"Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors (NN), Naive Bayes (NB), Decision Tree (DT), Sparse Network of Winnows (SNoW), and Support Vector Machines (SVM) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the SVM outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the SVM to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the TREC QA track.",2003,25,675,79,True,Computer Science,,37510526,Dell Zhang,1740222.0,Wee Sun Lee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7b5440285956e5d2ff848583775de35c089cc12f,https://www.semanticscholar.org/paper/7b5440285956e5d2ff848583775de35c089cc12f,Incremental Learning with Support Vector Machines,"Support vector machines (SVMs) have become a popular tool for machine learning with large amounts of high dimensional data. In this paper an approach for incremental learning with support vector machines is presented, that improves the existing approach of Syed et al. (1999). An insight into the interpretability of support vectors is also given.",2001,13,585,34,True,Computer Science,,1804144,S. Rüping,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
831edc3d67457db83da40d260e93bfd7559347ae,https://www.semanticscholar.org/paper/831edc3d67457db83da40d260e93bfd7559347ae,"Dyna, an integrated architecture for learning, planning, and reacting","Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.",1990,29,721,64,True,Computer Science,,1699645,R. Sutton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
356125478f5d06b564b420755a4944254045bbbe,https://www.semanticscholar.org/paper/356125478f5d06b564b420755a4944254045bbbe,Support vector learning,"Foreword The Support Vector Machine has recently been introduced as a new technique for solving various function estimation problems, including the pattern recognition problem. To develop such a technique, it was necessary to rst extract factors responsible for future generalization, to obtain bounds on generalization that depend on these factors, and lastly to develop a technique that constructively minimizes these bounds. The subject of this book are methods based on combining advanced branches of statistics and functional analysis, developing these theories into practical algorithms that perform better than existing heuristic approaches. The book provides a comprehensive analysis of what can be done using Support Vector Machines, achieving record results in real-life pattern recognition problems. In addition, it proposes a new form of nonlinear Principal Component Analysis using Support Vector kernel techniques, which I consider as the most natural and elegant way for generalization of classical Principal Component Analysis. In many ways the Support Vector machine became so popular thanks to works of Bernhard Schh olkopf. The work, submitted for the title of Doktor der Naturwis-senschaften, appears as excellent. It is a substantial contribution to Machine Learning technology.",1997,113,632,42,False,Computer Science,,1707625,B. Schölkopf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e72fdc7471e78adec50a6b41792254ce966ce70f,https://www.semanticscholar.org/paper/e72fdc7471e78adec50a6b41792254ce966ce70f,Learning to detect malicious executables in the wild,"In this paper, we describe the development of a fielded application for detecting malicious executables in the wild. We gathered 1971 benign and 1651 malicious executables and encoded each as a training example using n-grams of byte codes as features. Such processing resulted in more than 255 million distinct n-grams. After selecting the most relevant n-grams for prediction, we evaluated a variety of inductive methods, including naive Bayes, decision trees, support vector machines, and boosting. Ultimately, boosted decision trees outperformed other methods with an area under the roc curve of 0.996. Results also suggest that our methodology will scale to larger collections of executables. To the best of our knowledge, ours is the only fielded application for this task developed using techniques from machine learning and data mining.",2004,51,494,39,False,Computer Science,,145116464,J. Z. Kolter,1715421.0,M. Maloof,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
23d2d8b687d31b11573473a7c7792b7ec08d0745,https://www.semanticscholar.org/paper/23d2d8b687d31b11573473a7c7792b7ec08d0745,Learning Kernel Classifiers - Theory and Algorithms,"From the Publisher: 
Linear classifiers in kernel spaces have emerged as a major topic within the field of machine learning. The kernel technique takes the linear classifier--a limited, but well-established and comprehensively studied model--and extends its applicability to a wide range of nonlinear pattern-recognition tasks such as natural language processing, machine vision, and biological sequence analysis. This book provides the first comprehensive overview of both the theory and algorithms of kernel classifiers, including the most recent developments. It begins by describing the major algorithmic advances: kernel perceptron learning, kernel Fisher discriminants, support vector machines, relevance vector machines, Gaussian processes, and Bayes point machines. Then follows a detailed introduction to learning theory, including VC and PAC-Bayesian theory, data-dependent structural risk minimization, and compression bounds. Throughout, the book emphasizes the interaction between theory and algorithms: how learning algorithms work and why. The book includes many examples, complete pseudo code of the algorithms presented, and an extensive source code library.",2001,0,485,37,False,Computer Science,,3234984,R. Herbrich,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1889b9c3e8bc1118448b95fca38d6eff0bfca64d,https://www.semanticscholar.org/paper/1889b9c3e8bc1118448b95fca38d6eff0bfca64d,Learning the Kernel with Hyperkernels,"This paper addresses the problem of choosing a kernel suitable for estimation with a support vector machine, hence further automating machine learning. This goal is achieved by defining a reproducing kernel Hilbert space on the space of kernels itself. Such a formulation leads to a statistical estimation problem similar to the problem of minimizing a regularized risk functional.We state the equivalent representer theorem for the choice of kernels and present a semidefinite programming formulation of the resulting optimization problem. Several recipes for constructing hyperkernels are provided, as well as the details of common machine learning problems. Experimental results for classification, regression and novelty detection on UCI data show the feasibility of our approach.",2005,58,364,24,False,Mathematics,Computer Science,1706780,Cheng Soon Ong,46234526.0,Alex Smola,143957317.0,R. C. Williamson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b,https://www.semanticscholar.org/paper/b8ff8c7ab23eb70d4179c15a8a6b0efa1a493b8b,Steps toward Artificial Intelligence,"The problems of heuristic programming-of making computers solve really difficult problems-are divided into five main areas: Search, Pattern-Recognition, Learning, Planning, and Induction. A computer can do, in a sense, only what it is told to do. But even when we do not know how to solve a certain problem, we may program a machine (computer) to Search through some large space of solution attempts. Unfortunately, this usually leads to an enormously inefficient process. With Pattern-Recognition techniques, efficiency can often be improved, by restricting the application of the machine's methods to appropriate problems. Pattern-Recognition, together with Learning, can be used to exploit generalizations based on accumulated experience, further reducing search. By analyzing the situation, using Planning methods, we may obtain a fundamental improvement by replacing the given search with a much smaller, more appropriate exploration. To manage broad classes of problems, machines will need to construct models of their environments, using some scheme for Induction. Wherever appropriate, the discussion is supported by extensive citation of the literature and by descriptions of a few of the most successful heuristic (problem-solving) programs constructed to date.",1995,113,1356,53,True,Computer Science,,1847175,M. Minsky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
84f6eeeac62804de0502a54c444bacfad0f64730,https://www.semanticscholar.org/paper/84f6eeeac62804de0502a54c444bacfad0f64730,Ontology Learning and Its Application to Automated Terminology Translation,"Our OntoLearn system is an infrastructure for automated ontology learning from domain text. It is the only system, as far as we know, that uses natural language processing and machine learning techniques, and is part of a more general ontology engineering architecture. We describe the system and an experiment in which we used a machine-learned tourism ontology to automatically translate multiword terms from English to Italian. The method can apply to other domains without manual adaptation.",2003,25,399,20,False,Computer Science,,1733928,R. Navigli,1782522.0,P. Velardi,2420171.0,Aldo Gangemi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a9565f70db3a6b5e4ff1df272238b8d7a84a1337,https://www.semanticscholar.org/paper/a9565f70db3a6b5e4ff1df272238b8d7a84a1337,Experience with a learning personal assistant,"Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user. For example, an assistant that helps schedule a particular user’s calendar will have to know that user’s scheduling preferences. This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants. We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience. Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users. Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants.",1994,25,513,28,True,Computer Science,,40975594,Tom Michael Mitchell,145727186.0,R. Caruana,1758106.0,D. Freitag,14828509.0,J. McDermott,39787642.0,David Zabowski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6b0966c51d66e3097fc9f9d704bc43fdd963e90e,https://www.semanticscholar.org/paper/6b0966c51d66e3097fc9f9d704bc43fdd963e90e,Learning in Humans and Machines: Towards an Interdisciplinary Learning Science,"Chapter headings: Towards an Interdisciplinary Learning Science (P. Reimann, H. Spada). A Cognitive Psychological Approach to Learning (S. Vosniadou). Learning to Do and Learning to Understand: A Lesson and a Challenge for Cognitive Modeling (S. Ohlsson). Machine Learning: Case Studies of an Interdisciplinary Approach (W. Emde). Mental and Physical Artifacts in Cognitive Practices (R. Saljo). Learning Theory and Instructional Science (E. De Corte). Knowledge Representation Changes in Humans and Machines (L. Saitta and Task Force 1). Multi-Objective Learning with Multiple Representations (M. Van Someren, P. Reimann). Order Effects in Incremental Learning (P. Langley). Situated Learning and Transfer (H. Gruber et al.). The Evolution of Research on Collaborative Learning (P. Dillenbourg et al.). A Developmental Case Study on Sequential Learning: The Day-Night Cycle (K. Morik, S. Vosniadou). Subject index. Author index.",1995,0,479,3,False,Computer Science,,2926339,H. Spada,50405957.0,Reimann,20779579.0,P. Reimann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d18f64aa830075ed3e10206907f32c8fb2aa189d,https://www.semanticscholar.org/paper/d18f64aa830075ed3e10206907f32c8fb2aa189d,INTRODUCTION TO STATISTICAL LEARNING THEORY AND SUPPORT VECTOR MACHINES,"Data based machine learning covers a wide range of topics from pattern recognition to function regression and density estimation. Most of the existing methods are based on traditional statistics, which provides conclusion only for the situation where sample size is tending to infinity. So they may not work in practical cases of limited samples. Statistical Learning Theory or SLT is a small sample statistics by Vapnik et al., which concerns mainly the statistic principles when samples are limited, especially the properties of learning procedure in such cases. SLT provides us a new framework for the general learning problem, and a novel powerful learning method called Support Vector Machine or SVM, which can solve small sample learning problems better. It is believed that the study of SLT and SVM is becoming a new hot area in the field of machine learning. This review introduces the basic ideas of SLT and SVM, their major characteristics and some current research trends.",2000,0,419,16,False,Computer Science,,48728564,Zhang Xuegong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
01a61d9b9183ce11c89e36d9e1f24614c98f3ee8,https://www.semanticscholar.org/paper/01a61d9b9183ce11c89e36d9e1f24614c98f3ee8,Query Learning with Large Margin Classifiers,The active selection of instances can significantly improve the generalisation performance of a learning machine. Large margin classifiers such as support vector machines classify data using the most informative instances (the support vectors). This makes them natural candidates for instance selection strategies. In this paper we propose an algorithm for the training of support vector machines using instance selection. We give a theoretical justification for the strategy and experimental results on real and artificial data demonstrating its effectiveness. The technique is most efficient when the data set can be learnt using few support vectors.,2000,24,423,33,False,Computer Science,,145990261,C. Campbell,1685083.0,N. Cristianini,46234526.0,Alex Smola,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398,https://www.semanticscholar.org/paper/5fe5ed2a3b50becdbbcd17e7733653d5ef6ac398,Hidden Markov Support Vector Machines,"This paper presents a novel discriminative learning technique for label sequences based on a combination of the two most successful learning algorithms, Support Vector Machines and Hidden Markov Models which we call Hidden Markov Support Vector Machine. The proposed architecture handles dependencies between neighboring labels using Viterbi decoding. In contrast to standard HMM training, the learning procedure is discriminative and is based on a maximum/soft margin criterion. Compared to previous methods like Conditional Random Fields, Maximum Entropy Markov Models and label sequence boosting, HM-SVMs have a number of advantages. Most notably, it is possible to learn non-linear discriminant functions via kernel functions. At the same time, HM-SVMs share the key advantages with other discriminative methods, in particular the capability to deal with overlapping features. We report experimental evaluations on two tasks, named entity recognition and part-of-speech tagging, that demonstrate the competitiveness of the proposed approach.",2003,13,564,100,False,Mathematics,Computer Science,1783941,Y. Altun,1765700.0,Ioannis Tsochantaridis,143936663.0,Thomas Hofmann,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
008abebf4a9404db9050c9d2fbca769f4faf3ca6,https://www.semanticscholar.org/paper/008abebf4a9404db9050c9d2fbca769f4faf3ca6,Learning by Transduction,"We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.",1998,10,421,18,False,Computer Science,Mathematics,1793317,A. Gammerman,145675281.0,V. Vovk,50560492.0,V. Vapnik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62f40a5d8f099caaecdbb91fefd7bdfde3f32cf2,https://www.semanticscholar.org/paper/62f40a5d8f099caaecdbb91fefd7bdfde3f32cf2,Toward Practical Smile Detection,"Machine learning approaches have produced some of the highest reported performances for facial expression recognition. However, to date, nearly all automatic facial expression recognition research has focused on optimizing performance on a few databases that were collected under controlled lighting conditions on a relatively small number of subjects. This paper explores whether current machine learning methods can be used to develop an expression recognition system that operates reliably in more realistic conditions. We explore the necessary characteristics of the training data set, image registration, feature representation, and machine learning algorithms. A new database, GENKI, is presented which contains pictures, photographed by the subjects themselves, from thousands of different people in many different real-world imaging conditions. Results suggest that human-level expression recognition accuracy in real-life illumination conditions is achievable with machine learning technology. However, the data sets currently used in the automatic expression recognition literature to evaluate progress may be overly constrained and could potentially lead research into locally optimal algorithmic solutions.",2009,47,316,32,False,Computer Science,Medicine,143973061,J. Whitehill,2724380.0,G. Littlewort,2039025.0,I. Fasel,2218905.0,M. Bartlett,1741200.0,J. Movellan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb,https://www.semanticscholar.org/paper/9308cfdabf5303534b97d9ce5bfbb2c919a3f9cb,WEKA: The Waikato Environment for Knowledge Analysis,"WEKA is a workbench designed to aid in the application of machine learning technology to real world data sets, in particular, data sets from New Zealand’s agricultural sector. In order to do this a range of machine learning techniques are presented to the user in such a way as to hide the idiosyncrasies of input and output formats, as well as allow an exploratory approach in applying the technology. The system presented is a component based one that also has application in machine learning research and education.",1996,7,546,53,False,Engineering,,2072119972,Stephen R. Garner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b7d89441fcf28ca1a365af4d739709a7075a5db2,https://www.semanticscholar.org/paper/b7d89441fcf28ca1a365af4d739709a7075a5db2,Knowledge Discovery with Support Vector Machines,"An easy-to-follow introduction to support vector machines This book provides an in-depth, easy-to-follow introduction to support vector machines drawing only from minimal, carefully motivated technical and mathematical background material. It begins with a cohesive discussion of machine learning and goes on to cover: Knowledge discovery environments Describing data mathematically Linear decision surfaces and functions Perceptron learning Maximum margin classifiers Support vector machines Elements of statistical learning theory Multi-class classification Regression with support vector machines Novelty detection Complemented with hands-on exercises, algorithm descriptions, and data sets, Knowledge Discovery with Support Vector Machines is an invaluable textbook for advanced undergraduate and graduate courses. It is also an excellent tutorial on support vector machines for professionals who are pursuing research in machine learning and related areas.",2009,0,287,31,True,Computer Science,,1775719,L. Hamel,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2e75ad22a6865c3402925f12317dd0897c67ebe6,https://www.semanticscholar.org/paper/2e75ad22a6865c3402925f12317dd0897c67ebe6,Predicting Structured Data,"Machine learning develops intelligent computer systems that are able to generalize from previously seen examples. A new domain of machine learning, in which the prediction must satisfy the additional constraints found in structured data, poses one of machine learning's greatest challenges: learning functional dependencies between arbitrary input and output domains. This volume presents and analyzes the state of the art in machine learning algorithms and theory in this novel field. The contributors discuss applications as diverse as machine translation, document markup, computational biology, and information extraction, among others, providing a timely overview of an exciting field. Contributors Yasemin Altun, Gokhan Bakir [no dot over i], Olivier Bousquet, Sumit Chopra, Corinna Cortes, Hal Daume III, Ofer Dekel, Zoubin Ghahramani, Raia Hadsell, Thomas Hofmann, Fu Jie Huang, Yann LeCun, Tobias Mann, Daniel Marcu, David McAllester, Mehryar Mohri, William Stafford Noble, Fernando Perez-Cruz, Massimiliano Pontil, Marc'Aurelio Ranzato, Juho Rousu, Craig Saunders, Bernhard Scholkopf, Matthias W. Seeger, Shai Shalev-Shwartz, John Shawe-Taylor, Yoram Singer, Alexander J. Smola, Sandor Szedmak, Ben Taskar, Ioannis Tsochantaridis, S.V.N Vishwanathan, Jason Weston Gokhan Bakir [no dot over i] is Research Scientist at the Max Planck Institute for Biological Cybernetics in Tubingen, Germany. Thomas Hofmann is a Director of Engineering at Google's Engineering Center in Zurich and Adjunct Associate Professor of Computer Science at Brown University. Bernhard Scholkopf is Director of the Max Planck Institute for Biological Cybernetics and Professor at the Technical University Berlin. Alexander J. Smola is Senior Principal Researcher and Machine Learning Program Leader at National ICT Australia/Australian National University, Canberra. Ben Taskar is Assistant Professor in the Computer and Information Science Department at the University of Pennsylvania. S. V. N. Vishwanathan is Senior Researcher in the Statistical Machine Learning Program, National ICT Australia with an adjunct appointment at the Research School for Information Sciences and Engineering, Australian National University.",2007,21,433,25,False,Computer Science,,1815154,G. Bakir,153379696.0,T. Hofmann,1707625.0,B. Schölkopf,118460398.0,Taskar B Smola Aj,145713874.0,S. Vishwanathan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e22d10b9e55bb6ed3adfd2f5136a5972fc683565,https://www.semanticscholar.org/paper/e22d10b9e55bb6ed3adfd2f5136a5972fc683565,Connectionist expert systems,"Connectionist networks can be used as expert system knowledge bases. Furthermore, such networks can be constructed from training examples by machine learning techniques. This gives a way to automate the generation of expert systems for classification problems.",1988,41,708,24,False,Computer Science,,2889692,S. I. Gallant,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
572d3c5fe5a2908d308701fc88dc4484d44fba14,https://www.semanticscholar.org/paper/572d3c5fe5a2908d308701fc88dc4484d44fba14,Applications of Support Vector Machines in Chemistry,"Kernel-based techniques (such as support vector machines, Bayes point machines, kernel principal component analysis, and Gaussian processes) represent a major development in machine learning algorithms. Support vector machines (SVM) are a group of supervised learning methods that can be applied to classification or regression. In a short period of time, SVM found numerous applications in chemistry, such as in drug design (discriminating between ligands and nonligands, inhibitors and noninhibitors, etc.), quantitative structure-activity relationships (QSAR, where SVM regression is used to predict various physical, chemical, or biological properties), chemometrics (optimization of chromatographic separation or compound concentration prediction from spectral data as examples), sensors (for qualitative and quantitative prediction from sensor data), chemical engineering (fault detection and modeling of industrial processes), and text mining (automatic recognition of scientific information). Support vector machines represent an extension to nonlinear models of the generalized portrait algorithm developed by Vapnik and Lerner. The SVM algorithm is based on the statistical learning theory and the Vapnik–Chervonenkis",2007,211,382,17,False,Chemistry,,1717939,O. Ivanciuc,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3b0c8c67d17634f6f1f7a74d43b46e1782c1271e,https://www.semanticscholar.org/paper/3b0c8c67d17634f6f1f7a74d43b46e1782c1271e,The class imbalance problem: A systematic study,"In machine learning problems, differences in prior class probabilities -- or class imbalances -- have been reported to hinder the performance of some standard classifiers, such as decision trees. T...",2002,0,504,19,False,Computer Science,,1644084993,JapkowiczNathalie,1644085000.0,StephenShaju,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a1f94bd5954fbbdbf8aebfabe2760fc73c05eab8,https://www.semanticscholar.org/paper/a1f94bd5954fbbdbf8aebfabe2760fc73c05eab8,Learning to Predict Rare Events in Event Sequences,"Learning to predict rare events from sequences of events with categorical features is an important, real-world, problem that existing statistical and machine learning methods are not well suited to solve. This paper describes timeweaver, a genetic algorithm based machine learning system that predicts rare events by identifying predictive temporal and sequential patterns. Timeweaver is applied to the task of predicting telecommunication equipment failures from 110,000 alarm messages and is shown to outperform existing learning methods.",1998,17,285,10,False,Computer Science,,2596290,Gary M. Weiss,3025731.0,H. Hirsh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c26bdb2c2b62212c26b4f2e91ea5f9011656a56c,https://www.semanticscholar.org/paper/c26bdb2c2b62212c26b4f2e91ea5f9011656a56c,Handling concept drifts in incremental learning with support vector machines,"With the increase in the size of real-world databases, there is an ever-increasing need to scale up inductive learning algorithms. Incremental learning techniques are one possible solution to the scalability problem. In this paper, we propose three ctiteria to evaluate the robustness and reliability of incremental learning methods, and use them to study the robustness of an incremental training method for Support Vector Machines. We provide empirical results using benchmark machine learning datasets to show that support vectors form a svccdnct and suficient set for block-by-block incremental learning.",1999,10,266,16,False,Computer Science,,118811391,N. Syed,2146397025.0,Huan Liu,38817267.0,K. Sung,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d08b933458716a85888956165d07243505d4e4b2,https://www.semanticscholar.org/paper/d08b933458716a85888956165d07243505d4e4b2,Model Induction with Support Vector Machines: Introduction and Applications,"The rapid advance in information processing systems in recent decades had directed engineering research towards the development of intelligent systems that can evolve models of natural phenomena automatically—“by themselves,” so to speak. In this respect, a wide range of machine learning techniques like decision trees, artificial neural networks (ANNs), Bayesian methods, fuzzy-rule based systems, and evolutionary algorithms have been successfully applied to model different civil engineering systems. In this study, the possibility of using yet another machine learning paradigm that is firmly based on the theory of statistical learning, namely that of the support vector machine (SVM), is investigated. An interesting property of this approach is that it is an approximate implementation of a structural risk minimization (SRM) induction principle that aims at minimizing a bound on the generalization error of a model, rather than minimizing only the mean square error over the data set. In this paper, the basic ...",2001,50,490,23,False,Computer Science,,2314824,Y. Dibike,3237410.0,S. Velickov,1895474.0,D. Solomatine,5440697.0,M. Abbott,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62ec643e5415b6ab89f7186d3631f5549fd8a0cc,https://www.semanticscholar.org/paper/62ec643e5415b6ab89f7186d3631f5549fd8a0cc,Support Vector Machines and Kernel Methods: The New Generation of Learning Machines,"Kernel methods, a new generation of learning algorithms, utilize techniques from optimization, statistics, and functional analysis to achieve maximal generality, flexibility, and performance. These algorithms are different from earlier techniques used in machine learning in many respects: For example, they are explicitly based on a theoretical model of learning rather than on loose analogies with natural learning systems or other heuristics. They come with theoretical guarantees about their performance and have a modular design that makes it possible to separately implement and analyze their components. They are not affected by the problem of local minima because their training amounts to convex optimization. In the last decade, a sizable community of theoreticians and practitioners has formed around these methods, and a number of practical applications have been realized. Although the research is not concluded, already now kernel methods are considered the state of the art in several machine learning tasks. Their ease of use, theoretical appeal, and remarkable performance have made them the system of choice for many learning problems. Successful applications range from text categorization to handwriting recognition to classification of gene-expression data.",2002,11,210,23,False,Computer Science,,1685083,N. Cristianini,1707625.0,B. Schölkopf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7dddc25995ea77c1ec4de48e1291e6f9b21c877b,https://www.semanticscholar.org/paper/7dddc25995ea77c1ec4de48e1291e6f9b21c877b,An Overview on Theory and Algorithm of Support Vector Machines,"Statistical learning theory is the statistical theory of smallsample,and it focuses on the statistical law and the nature of learning of small samples.Support vector machine is a new machine learning method based on statistical learning theory,and it has become the research field of machine learning because of its excellent performance.This paper describes the theoretical basis of support vector machines(SVM) systematically,sums up the mainstream machine training algorithms of traditional SVM and some new learning models and algorithms detailedly,and finally points out the research and development prospects of support vector machine.",2011,49,138,6,False,Computer Science,,1413670056,Qi Bing-juan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d215c7a1f7176c8a536e1ca79e56fcd405cfa770,https://www.semanticscholar.org/paper/d215c7a1f7176c8a536e1ca79e56fcd405cfa770,Probabilistic finite-state machines - part I,"Probabilistic finite-state machines are used today in a variety of areas in pattern recognition, or in fields to which pattern recognition is linked: computational linguistics, machine learning, time series analysis, circuit testing, computational biology, speech recognition, and machine translation are some of them. In Part I of this paper, we survey these generative objects and study their definitions and properties. In Part II, we study the relation of probabilistic finite-state automata with other well-known devices that generate strings as hidden Markov models and n-grams and provide theorems, algorithms, and properties that represent a current state of the art of these objects.",2005,145,271,31,True,Computer Science,Medicine,143783065,E. Vidal,1981024.0,F. Thollard,8715530.0,C. D. L. Higuera,1696761.0,F. Casacuberta,1798578.0,Rafael C. Carrasco,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
f50e54684086a91bb481f76f7180c1ef3c4cb312,https://www.semanticscholar.org/paper/f50e54684086a91bb481f76f7180c1ef3c4cb312,Deep Convolutional Transfer Learning Network: A New Method for Intelligent Fault Diagnosis of Machines With Unlabeled Data,"The success of intelligent fault diagnosis of machines relies on the following two conditions: 1) labeled data with fault information are available; and 2) the training and testing data are drawn from the same probability distribution. However, for some machines, it is difficult to obtain massive labeled data. Moreover, even though labeled data can be obtained from some machines, the intelligent fault diagnosis method trained with such labeled data possibly fails in classifying unlabeled data acquired from the other machines due to data distribution discrepancy. These problems limit the successful applications of intelligent fault diagnosis of machines with unlabeled data. As a potential tool, transfer learning adapts a model trained in a source domain to its application in a target domain. Based on the transfer learning, we propose a new intelligent method named deep convolutional transfer learning network (DCTLN). A DCTLN consists of two modules: condition recognition and domain adaptation. The condition recognition module is constructed by a one-dimensional (1-D) convolutional neural network (CNN) to automatically learn features and recognize health conditions of machines. The domain adaptation module facilitates the 1-D CNN to learn domain-invariant features by maximizing domain recognition errors and minimizing the probability distribution distance. The effectiveness of the proposed method is verified using six transfer fault diagnosis experiments.",2019,29,482,24,False,Computer Science,,2110660923,Liang Guo,1829456.0,Y. Lei,3372340.0,Saibo Xing,2059615925.0,Tao Yan,2157947735.0,Naipeng Li,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
b5e5a7eee59dd740897c0c3d1ada96c2e2a7e0a7,https://www.semanticscholar.org/paper/b5e5a7eee59dd740897c0c3d1ada96c2e2a7e0a7,An Introduction to Statistical Learning: with Applications in R,"An Introduction to Statistical Learning provides an accessible overview of the field of statistical learning, an essential toolset for making sense of the vast and complex data sets that have emerged in fields ranging from biology to finance to marketing to astrophysics in the past twenty years. This book presents some of the most important modeling and prediction techniques, along with relevant applications. Topics include linear regression, classification, resampling methods, shrinkage approaches, tree-based methods, support vector machines, clustering, and more. Color graphics and real-world examples are used to illustrate the methods presented. Since the goal of this textbook is to facilitate the use of these statistical learning techniques by practitioners in science, industry, and other fields, each chapter contains a tutorial on implementing the analyses and methods presented in R, an extremely popular open source statistical software platform. Two of the authors co-wrote The Elements of Statistical Learning (Hastie, Tibshirani and Friedman, 2nd edition 2009), a popular reference book for statistics and machine learning researchers. An Introduction to Statistical Learning covers many of the same topics, but at a level accessible to a much broader audience. This book is targeted at statisticians and non-statisticians alike who wish to use cutting-edge statistical learning techniques to analyze their data. The text assumes only a previous course in linear regression and no knowledge of matrix algebra.",2013,0,3474,351,False,Computer Science,,2115376,D. Witten,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
51886def908b16d11685ea23eb2124dfe961754f,https://www.semanticscholar.org/paper/51886def908b16d11685ea23eb2124dfe961754f,Semi-Supervised and Unsupervised Extreme Learning Machines,"Extreme learning machines (ELMs) have proven to be efficient and effective learning mechanisms for pattern classification and regression. However, ELMs are primarily applied to supervised learning problems. Only a few existing research papers have used ELMs to explore unlabeled data. In this paper, we extend ELMs for both semi-supervised and unsupervised tasks based on the manifold regularization, thus greatly expanding the applicability of ELMs. The key advantages of the proposed algorithms are as follows: 1) both the semi-supervised ELM (SS-ELM) and the unsupervised ELM (US-ELM) exhibit learning capability and computational efficiency of ELMs; 2) both algorithms naturally handle multiclass classification or multicluster clustering; and 3) both algorithms are inductive and can handle unseen data at test time directly. Moreover, it is shown in this paper that all the supervised, semi-supervised, and unsupervised ELMs can actually be put into a unified framework. This provides new perspectives for understanding the mechanism of random feature mapping, which is the key concept in ELM theory. Empirical study on a wide range of data sets demonstrates that the proposed algorithms are competitive with the state-of-the-art semi-supervised or unsupervised learning algorithms in terms of accuracy and efficiency.",2014,67,621,60,False,Medicine,Computer Science,143983679,Gao Huang,1760750.0,Shiji Song,2694479.0,J. Gupta,145253556.0,Cheng Wu,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
d1208ac421cf8ff67b27d93cd19ae42b8d596f95,https://www.semanticscholar.org/paper/d1208ac421cf8ff67b27d93cd19ae42b8d596f95,Deep learning with COTS HPC systems,"Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks.",2013,32,703,54,False,Computer Science,,144638694,Adam Coates,2570381.0,Brody Huval,2156632012.0,Tao Wang,25629078.0,David J. Wu,2301680.0,Bryan Catanzaro,34699434.0,A. Ng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
5d90f06bb70a0a3dced62413346235c02b1aa086,https://www.semanticscholar.org/paper/5d90f06bb70a0a3dced62413346235c02b1aa086,Learning Multiple Layers of Features from Tiny Images,"Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images.",2009,15,20087,6576,False,Computer Science,,2064160,A. Krizhevsky,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ad51e96bc42096840f674d8790a3397755967712,https://www.semanticscholar.org/paper/ad51e96bc42096840f674d8790a3397755967712,The Machine That Changed the World,"This book is based on the Massachusetts Institute of Technology (MIT) 5-million-dollar 5-year study on the future of the automobile. Designated the International Motor Vehicle Program (IMVP), the MIT study explored the differences between mass production and lean production in the automobile industry. Lean production, pioneered by Eiji Toyoda and Taiichi Ohno at the Toyota Motor Company in Japan, combines the advantages of craft and mass production, while avoiding the high cost of the former and the rigidity of the latter. Toward this end, lean producers employ teams of multiskilled workers at all levels of the organization and use highly flexible, increasingly automated machines to produce volumes of products in enormous variety. Lean production (a term coined by IMVP researcher John Krafcik) is ""lean"" because it uses less of everything compared with mass production. Also, it requires keeping far less than half the needed inventory on site, results in many fewer defects, and produces a greater and ever growing variety of products. Lean production changes how people work. Most will find their jobs more challenging and will become more productive, but, at the same time, they may find their work more stressful. Lean production calls for learning far more professional skills (than in mass production) and applying these creatively in a team setting (rather than a rigid hierarchy). This book is organized in three sections. The first, ""The Origins of Lean Production,"" traces the evolution of lean production. The second, ""The Elements of Lean Production,"" looks at how lean production works in factory operations, product development, supply-system coordination, customer relations and as a total lean enterprise. Finally, in the third section, ""Diffusing Lean Production,"" the authors examine how lean production is spreading across the world and to other industries and, in the process, is revolutionizing how people live and work. Also examined are the barriers that are preventing companies and countries from becoming lean. Creative ways leanness can be achieved are suggested.",1991,0,4072,32,False,Business,,3429542,J. Womack,38709990.0,Daniel T. Jones,50012339.0,D. Roos,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
8ff61b8e097ccdb784a35b466ba9e130c2502513,https://www.semanticscholar.org/paper/8ff61b8e097ccdb784a35b466ba9e130c2502513,"Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond","Chapters 2–7 make up Part II of the book: artificial neural networks. After introducing the basic concepts of neurons and artificial neuron learning rules in Chapter 2, Chapter 3 describes a particular formalism, based on signal-plus-noise, for the learning problem in general. After presenting the basic neural network types this chapter reviews the principal algorithms for error function minimization/optimization and shows how these learning issues are addressed in various supervised models. Chapter 4 deals with issues in unsupervised learning networks, such as the Hebbian learning rule, principal component learning, and learning vector quantization. Various techniques and learning paradigms are covered in Chapters 3–6, and especially the properties and relative merits of the multilayer perceptron networks, radial basis function networks, self-organizing feature maps and reinforcement learning are discussed in the respective four chapters. Chapter 7 presents an in-depth examination of performance issues in supervised learning, such as accuracy, complexity, convergence, weight initialization, architecture selection, and active learning. Par III (Chapters 8–15) offers an extensive presentation of techniques and issues in evolutionary computing. Besides the introduction to the basic concepts in evolutionary computing, it elaborates on the more important and most frequently used techniques on evolutionary computing paradigm, such as genetic algorithms, genetic programming, evolutionary programming, evolutionary strategies, differential evolution, cultural evolution, and co-evolution, including design aspects, representation, operators and performance issues of each paradigm. The differences between evolutionary computing and classical optimization are also explained. Part IV (Chapters 16 and 17) introduces swarm intelligence. It provides a representative selection of recent literature on swarm intelligence in a coherent and readable form. It illustrates the similarities and differences between swarm optimization and evolutionary computing. Both particle swarm optimization and ant colonies optimization are discussed in the two chapters, which serve as a guide to bringing together existing work to enlighten the readers, and to lay a foundation for any further studies. Part V (Chapters 18–21) presents fuzzy systems, with topics ranging from fuzzy sets, fuzzy inference systems, fuzzy controllers, to rough sets. The basic terminology, underlying motivation and key mathematical models used in the field are covered to illustrate how these mathematical tools can be used to handle vagueness and uncertainty. This book is clearly written and it brings together the latest concepts in computational intelligence in a friendly and complete format for undergraduate/postgraduate students as well as professionals new to the field. With about 250 pages covering such a wide variety of topics, it would be impossible to handle everything at a great length. Nonetheless, this book is an excellent choice for readers who wish to familiarize themselves with computational intelligence techniques or for an overview/introductory course in the field of computational intelligence. Learning with Kernels: Support Vector Machines, Regularization, Optimization, and Beyond—Bernhard Schölkopf and Alexander Smola, (MIT Press, Cambridge, MA, 2002, ISBN 0-262-19475-9). Reviewed by Amir F. Atiya.",2005,10,8334,1154,False,Computer Science,,1686506,A. Atiya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e60ff004dde5c13ec53087872cfcdd12e85beb57,https://www.semanticscholar.org/paper/e60ff004dde5c13ec53087872cfcdd12e85beb57,Learning Deep Architectures for AI,"Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.",2007,242,7878,501,True,Computer Science,,1751762,Yoshua Bengio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
78947497cbbffc691aac3f590d972130259af9ce,https://www.semanticscholar.org/paper/78947497cbbffc691aac3f590d972130259af9ce,Distance Metric Learning for Large Margin Nearest Neighbor Classification,"The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.",2005,34,5238,741,False,Mathematics,Computer Science,7446832,Kilian Q. Weinberger,1796044.0,L. Saul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a675fe5a7d99ac6f7ff91fa084462faefe616148,https://www.semanticscholar.org/paper/a675fe5a7d99ac6f7ff91fa084462faefe616148,What video games have to teach us about learning and literacy,"Good computer and video games like System Shock 2, Deus Ex, Pikmin, Rise of Nations, Neverwinter Nights, and Xenosaga: Episode 1 are learning machines. They get themselves learned and learned well, so that they get played long and hard by a great many people. This is how they and their designers survive and perpetuate themselves. If a game cannot be learned and even mastered at a certain level, it won't get played by enough people, and the company that makes it will go broke. Good learning in games is a capitalist-driven Darwinian process of selection of the fittest. Of course, game designers could have solved their learning problems by making games shorter and easier, by dumbing them down, so to speak. But most gamers don't want short and easy games. Thus, designers face and largely solve an intriguing educational dilemma, one also faced by schools and workplaces: how to get people, often young people, to learn and master something that is long and challenging--and enjoy it, to boot.",2007,86,6595,571,False,Computer Science,Psychology,34622402,J. Gee,,,,,,,,,,,Sociology,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
22feb6532228392457664becc48b3096d9858505,https://www.semanticscholar.org/paper/22feb6532228392457664becc48b3096d9858505,Asymptotic Equivalence of Bayes Cross Validation and Widely Applicable Information Criterion in Singular Learning Theory,"In regular statistical models, the leave-one-out cross-validation is asymptotically equivalent to the Akaike information criterion. However, since many learning machines are singular statistical models, the asymptotic behavior of the cross-validation remains unknown. In previous studies, we established the singular learning theory and proposed a widely applicable information criterion, the expectation value of which is asymptotically equal to the average Bayes generalization loss. In the present paper, we theoretically compare the Bayes cross-validation loss and the widely applicable information criterion and prove two theorems. First, the Bayes cross-validation loss is asymptotically equivalent to the widely applicable information criterion as a random variable. Therefore, model selection and hyperparameter optimization using these two values are asymptotically equivalent. Second, the sum of the Bayes generalization error and the Bayes cross-validation error is asymptotically equal to 2λ/n, where λ is the real log canonical threshold and n is the number of training samples. Therefore the relation between the cross-validation error and the generalization error is determined by the algebraic geometrical structure of a learning machine. We also clarify that the deviance information criteria are different from the Bayes cross-validation and the widely applicable information criterion.",2010,62,1763,350,False,Computer Science,Mathematics,2041910832,Sumio Watanabe,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
033ca7d9f4302a9349424f56c297657dd5d98616,https://www.semanticscholar.org/paper/033ca7d9f4302a9349424f56c297657dd5d98616,Extreme Learning Machines [Trends & Controversies],"This special issue includes eight original works that detail the further developments of ELMs in theories, applications, and hardware implementation. In ""Representational Learning with ELMs for Big Data,"" Liyanaarachchi Lekamalage Chamara Kasun, Hongming Zhou, Guang-Bin Huang, and Chi Man Vong propose using the ELM as an auto-encoder for learning feature representations using singular values. In ""A Secure and Practical Mechanism for Outsourcing ELMs in Cloud Computing,"" Jiarun Lin, Jianping Yin, Zhiping Cai, Qiang Liu, Kuan Li, and Victor C.M. Leung propose a method for handling large data applications by outsourcing to the cloud that would dramatically reduce ELM training time. In ""ELM-Guided Memetic Computation for Vehicle Routing,"" Liang Feng, Yew-Soon Ong, and Meng-Hiot Lim consider the ELM as an engine for automating the encapsulation of knowledge memes from past problem-solving experiences. In ""ELMVIS: A Nonlinear Visualization Technique Using Random Permutations and ELMs,"" Anton Akusok, Amaury Lendasse, Rui Nian, and Yoan Miche propose an ELM method for data visualization based on random permutations to map original data and their corresponding visualization points. In ""Combining ELMs with Random Projections,"" Paolo Gastaldo, Rodolfo Zunino, Erik Cambria, and Sergio Decherchi analyze the relationships between ELM feature-mapping schemas and the paradigm of random projections. In ""Reduced ELMs for Causal Relation Extraction from Unstructured Text,"" Xuefeng Yang and Kezhi Mao propose combining ELMs with neuron selection to optimize the neural network architecture and improve the ELM ensemble's computational efficiency. In ""A System for Signature Verification Based on Horizontal and Vertical Components in Hand Gestures,"" Beom-Seok Oh, Jehyoung Jeon, Kar-Ann Toh, Andrew Beng Jin Teoh, and Jaihie Kim propose a novel paradigm for hand signature biometry for touchless applications without the need for handheld devices. Finally, in ""An Adaptive and Iterative Online Sequential ELM-Based Multi-Degree-of-Freedom Gesture Recognition System,"" Hanchao Yu, Yiqiang Chen, Junfa Liu, and Guang-Bin Huang propose an online sequential ELM-based efficient gesture recognition algorithm for touchless human-machine interaction.",2013,43,298,10,False,Computer Science,,49943757,E. Cambria,145678691.0,G. Huang,2996971.0,L. L. C. Kasun,2986982.0,Hongming Zhou,1807914.0,C. Vong,3162738.0,Jiarun Lin,,145311707.0,Jianping Yin,143942560.0,Zhiping Cai,48873711.0,Qiang Liu,144648291.0,Kuan Li,143698682.0,Victor C. M. Leung,50032417.0,Liang Feng,8748397.0,Y. Ong,145236613.0,M. Lim,2798381.0,Anton Akusok,1731049.0,A. Lendasse,2478137.0,F. Corona,2249000.0,Rui Nian,3194493.0,Y. Miché,1906359.0,P. Gastaldo,145140641.0,R. Zunino,2674856.0,S. Decherchi,48520418.0,Xuefeng Yang,144067957.0,K. Mao,2926212.0,B. Oh,1858747.0,Je-Hyoung Jeon,143629972.0,K. Toh,9162326.0,A. Teoh,47964426.0,Jaihie Kim,3273556.0,Hanchao Yu,2109360525.0,Yiqiang Chen,2108414452.0,Junfa Liu
d4599b177559dd5ede4dda9d6d96aa149fc71942,https://www.semanticscholar.org/paper/d4599b177559dd5ede4dda9d6d96aa149fc71942,An Efficient Learning Procedure for Deep Boltzmann Machines,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent statistics are estimated using a variational approximation that tends to focus on a single mode, and data-independent statistics are estimated using persistent Markov chains. The use of two quite different techniques for estimating the two types of statistic that enter into the gradient of the log likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer pretraining phase that initializes the weights sensibly. The pretraining also allows the variational inference to be initialized sensibly with a single bottom-up pass. We present results on the MNIST and NORB data sets showing that deep Boltzmann machines learn very good generative models of handwritten digits and 3D objects. We also show that the features discovered by deep Boltzmann machines are a very effective way to initialize the hidden layers of feedforward neural nets, which are then discriminatively fine-tuned.",2012,72,416,46,True,Medicine,Computer Science,145124475,R. Salakhutdinov,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1fcbefeb0beae4470cf40df74cd116b1d4bdcae4,https://www.semanticscholar.org/paper/1fcbefeb0beae4470cf40df74cd116b1d4bdcae4,An introduction to kernel-based learning algorithms,"This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.",2001,161,3591,179,False,Computer Science,Mathematics,145034054,K. Müller,2459012.0,S. Mika,152597562.0,Gunnar Rätsch,34628173.0,K. Tsuda,1707625.0,B. Schölkopf,,,Medicine,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fd8ce955dc0c570b66305dfbc65e4ed5f37658d0,https://www.semanticscholar.org/paper/fd8ce955dc0c570b66305dfbc65e4ed5f37658d0,"Induction: Processes of Inference, Learning, and Discovery","Two psychologists, a computer scientist, and a philosopher have collaborated to present a framework for understanding processes of inductive reasoning and learning in organisms and machines. Theirs is the first major effort to bring the ideas of several disciplines to bear on a subject that has been a topic of investigation since the time of Socrates. The result is an integrated account that treats problem solving and induction in terms of rule-based mental models. Induction is included in the Computational Models of Cognition and Perception Series. A Bradford Book.",1987,0,2272,88,False,Computer Science,,94653581,J. H. Holland,2009767.0,K. Holyoak,2518186.0,R. Nisbett,1756123.0,P. Thagard,1743808.0,S. Smoliar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e219a61354d972a28954e655a7c53373508a08b6,https://www.semanticscholar.org/paper/e219a61354d972a28954e655a7c53373508a08b6,Regularized multi--task learning,"Past empirical work has shown that learning multiple related tasks from data simultaneously can be advantageous in terms of predictive performance relative to learning these tasks independently. In this paper we present an approach to multi--task learning based on the minimization of regularization functionals similar to existing ones, such as the one for Support Vector Machines (SVMs), that have been successfully used in the past for single--task learning. Our approach allows to model the relation between tasks in terms of a novel kernel function that uses a task--coupling parameter. We implement an instance of the proposed approach similar to SVMs and test it empirically using simulated as well as real data. The experimental results show that the proposed method performs better than existing multi--task learning methods and largely outperforms single--task learning using SVMs.",2004,28,1512,197,True,Mathematics,Computer Science,1801089,T. Evgeniou,1704699.0,M. Pontil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
02adea3455cd7b09e1dac9ddf2637a1e7ae84005,https://www.semanticscholar.org/paper/02adea3455cd7b09e1dac9ddf2637a1e7ae84005,Inductive learning algorithms and representations for text categorization,"1. ABSTRACT Text categorization – the assignment of natural language texts to one or more predefined categories based on their content – is an important component in many information organization and management tasks. We compare the effectiveness of five different automatic learning algorithms for text categorization in terms of learning speed, realtime classification speed, and classification accuracy. We also examine training set size, and alternative document representations. Very accurate text classifiers can be learned automatically from training examples. Linear Support Vector Machines (SVMs) are particularly promising because they are very accurate, quick to train, and quick to evaluate. 1.1",1998,31,1400,66,True,Computer Science,,1728602,S. Dumais,144189092.0,John C. Platt,1934343.0,David Hecherman,1764547.0,M. Sahami,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4,https://www.semanticscholar.org/paper/12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4,Online learning with kernels,"Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection.",2001,62,1074,148,False,Computer Science,Mathematics,1700597,Jyrki Kivinen,46234526.0,Alex Smola,143957317.0,R. C. Williamson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
77e379fd57ea44638fc628623e383eccada82689,https://www.semanticscholar.org/paper/77e379fd57ea44638fc628623e383eccada82689,Kernel Methods for Deep Learning,"We introduce a new family of positive-definite kernel functions that mimic the computation in large, multilayer neural nets. These kernel functions can be used in shallow architectures, such as support vector machines (SVMs), or in deep kernel-based architectures that we call multilayer kernel machines (MKMs). We evaluate SVMs and MKMs with these kernel functions on problems designed to illustrate the advantages of deep architectures. On several problems, we obtain better results than previous, leading benchmarks from both SVMs with Gaussian kernels as well as deep belief nets.",2009,72,637,105,False,Computer Science,,6572037,Youngmin Cho,1796044.0,L. Saul,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0af75728bec67f698a8c619645165de13780c2fa,https://www.semanticscholar.org/paper/0af75728bec67f698a8c619645165de13780c2fa,Learning Multiple Tasks with Kernel Methods,"We study the problem of learning many related tasks simultaneously using kernel methods and regularization. The standard single-task kernel methods, such as support vector machines and regularization networks, are extended to the case of multi-task learning. Our analysis shows that the problem of estimating many task functions with regularization can be cast as a single task learning problem if a family of multi-task kernel functions we define is used. These kernels model relations among the tasks and are derived from a novel form of regularizers. Specific kernels that can be used for multi-task learning are provided and experimentally tested on two real data sets. In agreement with past empirical work on multi-task learning, the experiments show that learning multiple related tasks simultaneously using the proposed approach can significantly outperform standard single-task learning particularly when there are many related tasks but few data per task.",2005,35,931,100,False,Mathematics,Computer Science,1801089,T. Evgeniou,1708279.0,C. Micchelli,1704699.0,M. Pontil,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
00cd1dab559a9671b692f39f14c1573ab2d1416b,https://www.semanticscholar.org/paper/00cd1dab559a9671b692f39f14c1573ab2d1416b,Efficient Learning of Deep Boltzmann Machines,"We present a new approximate inference algorithm for Deep Boltzmann Machines (DBM’s), a generative model with many layers of hidden variables. The algorithm learns a separate “recognition” model that is used to quickly initialize, in a single bottom-up pass, the values of the latent variables in all hidden layers. We show that using such a recognition model, followed by a combined top-down and bottom-up pass, it is possible to efficiently learn a good generative model of high-dimensional highly-structured sensory input. We show that the additional computations required by incorporating a top-down feedback plays a critical role in the performance of a DBM, both as a generative and discriminative model. Moreover, inference is only at most three times slower compared to the approximate inference in a Deep Belief Network (DBN), making large-scale learning of DBM’s practical. Finally, we demonstrate that the DBM’s trained using the proposed approximate inference algorithm perform well compared to DBN’s and SVM’s on the MNIST handwritten digit, OCR English letters, and NORB visual object recognition tasks.",2010,19,368,26,False,Mathematics,Computer Science,145124475,R. Salakhutdinov,1777528.0,H. Larochelle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
609e5cc1da126d7f760d1444b43b4fae41602841,https://www.semanticscholar.org/paper/609e5cc1da126d7f760d1444b43b4fae41602841,Less is More: Active Learning with Support Vector Machines,"We describe a simple active learning heuristic which greatly enhances the generalization behavior of support vector machines (SVMs) on several practical document classification tasks. We observe a number of benefits, the most surprising of which is that a SVM trained on a wellchosen subset of the available corpus frequently performs better than one trained on all available data. The heuristic for choosing this subset is simple to compute, and makes no use of information about the test set. Given that the training time of SVMs depends heavily on the training set size, our heuristic not only offers better performance with fewer data, it frequently does so in less time than the naive approach of training on all available data.",2000,22,929,76,False,Computer Science,,1761866,Greg Schohn,50742419.0,David A. Cohn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
4673a47a4f6719e350196f4086a65d08f946df25,https://www.semanticscholar.org/paper/4673a47a4f6719e350196f4086a65d08f946df25,Learning by Design: Good Video Games as Learning Machines,"This article asks how good video and computer game designers manage to get new players to learn long, complex and difficult games. The short answer is that designers of good games have hit on excellent methods for getting people to learn and to enjoy learning. The longer answer is more complex. Integral to this answer are the good principles of learning built into successful games. The author discusses 13 such principles under the headings of ‘Empowered Learners’, ‘Problem Solving’ and ‘Understanding’ and concludes that the main impediment to implementing these principles in formal education is cost. This, however, is not only (or even so much) monetary cost. It is, importantly, the cost of changing minds about how and where learning is done and of changing one of our most profoundly change-resistant institutions: the school.",2005,22,694,64,False,Computer Science,,34622402,J. Gee,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
506f516d8b60ba74e5ce811a458ec4fd72d714b2,https://www.semanticscholar.org/paper/506f516d8b60ba74e5ce811a458ec4fd72d714b2,An Introduction to Support Vector Machines and Other Kernel-Based Learning Methods,"This book is an introduction to support vector machines and related kernel methods in supervised learning, whose task is to estimate an input-output functional relationship from a training set of examples. A learning problem is referred to as classification if its output take discrete values in a set of possible categories and regression if it has continuous real-valued output.",2001,0,730,83,False,Computer Science,,2146325920,Tong Zhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
424a6e62084d919bfc2e39a507c263e5991ebdad,https://www.semanticscholar.org/paper/424a6e62084d919bfc2e39a507c263e5991ebdad,Self-Normalizing Neural Networks,"Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ""scaled exponential linear units"" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: this http URL.",2017,60,1437,178,False,Computer Science,Mathematics,1994964,G. Klambauer,2465270.0,Thomas Unterthiner,144831680.0,Andreas Mayr,3308557.0,S. Hochreiter,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
3127190433230b3dc1abd0680bb58dced4bcd90e,https://www.semanticscholar.org/paper/3127190433230b3dc1abd0680bb58dced4bcd90e,Large Scale Distributed Deep Networks,"Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm.",2012,31,3190,321,False,Computer Science,,49959210,J. Dean,32131713.0,G. Corrado,3089272.0,R. Monga,2118440152.0,Kai Chen,145139947.0,Matthieu Devin,2827616.0,Quoc V. Le,,1715548.0,Mark Z. Mao,1706809.0,Marc'Aurelio Ranzato,33666044.0,A. Senior,2080690.0,P. Tucker,143781496.0,Ke Yang,34699434.0,A. Ng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
6bdb186ec4726e00a8051119636d4df3b94043b5,https://www.semanticscholar.org/paper/6bdb186ec4726e00a8051119636d4df3b94043b5,Caffe: Convolutional Architecture for Fast Feature Embedding,"Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",2014,11,14131,1802,False,Computer Science,,39978391,Yangqing Jia,1782282.0,Evan Shelhamer,7408951.0,Jeff Donahue,3049736.0,Sergey Karayev,2117314646.0,Jonathan Long,2983898.0,Ross B. Girshick,,1687120.0,S. Guadarrama,1753210.0,Trevor Darrell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
7260c0692f8d265e11c4e9c4c8ef4c185bd587ad,https://www.semanticscholar.org/paper/7260c0692f8d265e11c4e9c4c8ef4c185bd587ad,Building machines that learn and think like people,"Abstract Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.",2016,429,1723,81,True,Computer Science,Medicine,2373318,B. Lake,37774552.0,T. Ullman,1763295.0,J. Tenenbaum,1831199.0,S. Gershman,,,,,Mathematics,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
e50f4d3316d13841c287dcdf5479d7820d593571,https://www.semanticscholar.org/paper/e50f4d3316d13841c287dcdf5479d7820d593571,Factorization Machines with libFM,"Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.
 Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM.",2012,49,1104,164,False,Computer Science,,2843982,Steffen Rendle,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
52e2ac397f0c8d5f533959905df899bc328d9f85,https://www.semanticscholar.org/paper/52e2ac397f0c8d5f533959905df899bc328d9f85,Reinforcement Learning with Hierarchies of Machines,"We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and ""behavior-based"" or ""teleo-reactive"" approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states.",1997,19,813,54,False,Computer Science,,145726861,Ronald E. Parr,145107462.0,Stuart J. Russell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
a538b05ebb01a40323997629e171c91aa28b8e2f,https://www.semanticscholar.org/paper/a538b05ebb01a40323997629e171c91aa28b8e2f,Rectified Linear Units Improve Restricted Boltzmann Machines,"Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ""Stepped Sigmoid Units"" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.",2010,21,14138,1118,False,Mathematics,Computer Science,2073603971,Vinod Nair,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
62ed272e0e8b7be356c7f7595f5b7a22797a1c3e,https://www.semanticscholar.org/paper/62ed272e0e8b7be356c7f7595f5b7a22797a1c3e,Support vector machines for classification and regression.,"The increasing interest in Support Vector Machines (SVMs) over the past 15 years is described. Methods are illustrated using simulated case studies, and 4 experimental case studies, namely mass spectrometry for studying pollution, near infrared analysis of food, thermal analysis of polymers and UV/visible spectroscopy of polyaromatic hydrocarbons. The basis of SVMs as two-class classifiers is shown with extensive visualisation, including learning machines, kernels and penalty functions. The influence of the penalty error and radial basis function radius on the model is illustrated. Multiclass implementations including one vs. all, one vs. one, fuzzy rules and Directed Acyclic Graph (DAG) trees are described. One-class Support Vector Domain Description (SVDD) is described and contrasted to conventional two- or multi-class classifiers. The use of Support Vector Regression (SVR) is illustrated including its application to multivariate calibration, and why it is useful when there are outliers and non-linearities.",2010,41,2065,192,False,Chemistry,Medicine,2793728,R. Brereton,31847168.0,G. Lloyd,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
85021c84383d18a7a4434d76dc8135fc6bdc0aa6,https://www.semanticscholar.org/paper/85021c84383d18a7a4434d76dc8135fc6bdc0aa6,Deep Boltzmann Machines,"We present a new learning algorithm for Boltzmann machines that contain many layers of hidden variables. Data-dependent expectations are estimated using a variational approximation that tends to focus on a single mode, and dataindependent expectations are approximated using persistent Markov chains. The use of two quite different techniques for estimating the two types of expectation that enter into the gradient of the log-likelihood makes it practical to learn Boltzmann machines with multiple hidden layers and millions of parameters. The learning can be made more efficient by using a layer-by-layer “pre-training” phase that allows variational inference to be initialized with a single bottomup pass. We present results on the MNIST and NORB datasets showing that deep Boltzmann machines learn good generative models and perform well on handwritten digit and visual object recognition tasks.",2009,22,2126,258,False,Computer Science,,145124475,R. Salakhutdinov,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8,https://www.semanticscholar.org/paper/74b1a9e50f18af8a7b9f8dd38f40e0466ad7a8e8,Transductive Inference for Text Classification using Support Vector Machines,"This paper introduces Transductive Support Vector Machines (TSVMs) for text classi cation. While regular Support Vector Machines (SVMs) try to induce a general decision function for a learning task, Transductive Support Vector Machines take into account a particular test set and try to minimize misclassi cations of just those particular examples. The paper presents an analysis of why TSVMs are well suited for text classi cation. These theoretical ndings are supported by experiments on three test collections. The experiments show substantial improvements over inductive methods, especially for small training sets, cutting the number of labeled training examples down to a twentieth on some tasks. This work also proposes an algorithm for training TSVMs e ciently, handling 10,000 examples and more.",1999,24,3130,455,False,Computer Science,,1680188,T. Joachims,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
c834bddd5e75a64ca9bb80c195cf84345c38bb9b,https://www.semanticscholar.org/paper/c834bddd5e75a64ca9bb80c195cf84345c38bb9b,A Short Introduction to Boosting,"Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the underlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting’s relationship to support-vector machines. Some examples of recent applications of boosting are also described.",1999,51,3215,236,False,Computer Science,,1703537,Y. Freund,1716301.0,R. Schapire,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36,https://www.semanticscholar.org/paper/ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36,Introduction to Support Vector Machines,"Support Vector Machines (SVM’s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM’s introduce the notion of a “kernel induced feature space” which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM’s is that the higher-dimensional space doesn’t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system’s likelihood to perform well on unseen data) of SVM’s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM’s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM’s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than “yes/no” classification).",2002,20,2403,250,False,Computer Science,,50263663,D. Boswell,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1626c940a64ad96a7ed53d7d6c0df63c6696956b,https://www.semanticscholar.org/paper/1626c940a64ad96a7ed53d7d6c0df63c6696956b,Restricted Boltzmann machines for collaborative filtering,"Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system.",2007,17,1908,159,False,Computer Science,,145124475,R. Salakhutdinov,1714004.0,A. Mnih,1695689.0,Geoffrey E. Hinton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ff2c2e3e83d1e8828695484728393c76ee07a101,https://www.semanticscholar.org/paper/ff2c2e3e83d1e8828695484728393c76ee07a101,"Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations","The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system.",1986,0,15850,146,False,Computer Science,,2681887,D. Rumelhart,1701656.0,James L. McClelland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
de39028ea1122d09649a42b12e04c8f349ebe8a6,https://www.semanticscholar.org/paper/de39028ea1122d09649a42b12e04c8f349ebe8a6,Autopoiesis and Cognition : The Realization of the Living (Boston Studies in the Philosophy of Scie,"Editorial Preface General Table Of Contents Foreword Introduction (by Professor Maturana) Biology Of Cognition Dedication Table of Contents I. Introduction II. The Problem III. Cognitive Function in General A. The Observer B. The Living System C. Evolution D. The Cognitive Process IV. Cognitive Function in Particular A. Nerve Cells B. Architecture C. Function D. Representation E. Description F. Thinking G. Natural Language H. Memory and Learning I. The Observer V. Problems in the Neurophysiology of Cognition VI. Conclusions VII. Post Scriptum Autopoiesis: The Organization Of The Living Preface (by Sir Stafford Beer) Introduction I. On Machines, living and Otherwise 1. Machines 2. Living Machines II. Dispensability of Teleonomy 1. Purposelessness 2. Individuality III. Embodiments of Autopoiesis 1. Descriptive and Causal Notions 2. Molecular Embodiments 3. Origin IV. Diversity of Autopoiesis 1. Subordination to the Condition of Unity 2. Plasticity of Ontogeny 3. Reproduction, a Complication of the Unity 4. Evolution, a Historical Network 5. Second and Third Order Autopoietic Systems V. Presence of Autopoiesis 1. Biological Implications 2. Epistemological Implications 3. Cognitive Implications Appendix: The Nervous System Glossary Bibliography Index Of Names",1980,0,4711,223,False,Psychology,,2701850,H. Maturana,144089631.0,F. Varela,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
9008cdacbdcff8a218a6928e94fe7c6dfc237b24,https://www.semanticscholar.org/paper/9008cdacbdcff8a218a6928e94fe7c6dfc237b24,Training support vector machines: an application to face detection,"We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.",1997,19,2891,110,False,Computer Science,Mathematics,1781874,E. Osuna,1771659.0,R. Freund,1804489.0,F. Girosi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
0ca966cb390b442b10cb76aa3fddee6b613f4f0f,https://www.semanticscholar.org/paper/0ca966cb390b442b10cb76aa3fddee6b613f4f0f,Incorporating Diversity in Active Learning with Support Vector Machines,"In many real world applications, active selection of training examples can significantly reduce the number of labelled training examples to learn a classification function. Different strategies in the field of support vector machines have been proposed that iteratively select a single new example from a set of unlabelled examples, query the corresponding class label and then perform retraining of the current classifier. However, to reduce computational time for training, it might be necessary to select batches of new training examples instead of single examples. Strategies for single examples can be extended straightforwardly to select batches by choosing the h > 1 examples that get the highest values for the individual selection criterion. We present a new approach that is especially designed to construct batches and incorporates a diversity measure. It has low computational requirements making it feasible for large scale problems with several thousands of examples. Experimental results indicate that this approach provides a faster method to attain a level of generalization accuracy in terms of the number of labelled examples.",2003,18,499,60,False,Computer Science,,2047100972,K. Brinker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
