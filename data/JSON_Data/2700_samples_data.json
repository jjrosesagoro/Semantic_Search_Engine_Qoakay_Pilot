{"total": 5120627, "offset": 2600, "next": 2700, "data": [{"paperId": "2d9107e6dead98af2f3099618eb58767e7d4961a", "url": "https://www.semanticscholar.org/paper/2d9107e6dead98af2f3099618eb58767e7d4961a", "title": "Credit rating by hybrid machine learning techniques", "abstract": null, "year": 2010, "referenceCount": 27, "citationCount": 144, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1707262", "name": "Chih-Fong Tsai"}, {"authorId": "2107982221", "name": "Ming-Lun Chen"}]}, {"paperId": "169bb50e92c1a5feb756295e4a9e5492e79cf382", "url": "https://www.semanticscholar.org/paper/169bb50e92c1a5feb756295e4a9e5492e79cf382", "title": "Automatic bug triage using text categorization", "abstract": "Bug triage, deciding what to do with an incoming bug report, is taking up increasing amount of developer resources in large open-source projects. In this paper, we propose to apply machine learning techniques to assist in bug triage by using text categorization to predict the developer that should work on the bug based on the bug\u2019s description. We demonstrate our approach on a collection of 15,859 bug reports from a large open-source project. Our evaluation shows that our prototype, using supervised Bayesian learning, can correctly predict 30% of the report assignments to", "year": 2004, "referenceCount": 10, "citationCount": 430, "influentialCitationCount": 49, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2271666", "name": "Davor Cubranic"}, {"authorId": "1739674", "name": "G. Murphy"}]}, {"paperId": "0f1b469eaeac1d86fd4a14e71322771e2d85ac76", "url": "https://www.semanticscholar.org/paper/0f1b469eaeac1d86fd4a14e71322771e2d85ac76", "title": "Fast Asymmetric Learning for Cascade Face Detection", "abstract": "A cascade face detector uses a sequence of node classifiers to distinguish faces from nonfaces. This paper presents a new approach to design node classifiers in the cascade detector. Previous methods used machine learning algorithms that simultaneously select features and form ensemble classifiers. We argue that if these two parts are decoupled, we have the freedom to design a classifier that explicitly addresses the difficulties caused by the asymmetric learning goal. There are three contributions in this paper: The first is a categorization of asymmetries in the learning goal and why they make face detection hard. The second is the forward feature selection (FFS) algorithm and a fast precomputing strategy for AdaBoost. FFS and the fast AdaBoost can reduce the training time by approximately 100 and 50 times, in comparison to a naive implementation of the AdaBoost feature selection method. The last contribution is a linear asymmetric classifier (LAC), a classifier that explicitly handles the asymmetric learning goal as a well-defined constrained optimization problem. We demonstrated experimentally that LAC results in an improved ensemble classifier performance.", "year": 2008, "referenceCount": 55, "citationCount": 265, "influentialCitationCount": 25, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1808816", "name": "Jianxin Wu"}, {"authorId": "145736680", "name": "S. Brubaker"}, {"authorId": "31562428", "name": "M. D. Mullin"}, {"authorId": "144177248", "name": "James M. Rehg"}]}, {"paperId": "2bb4965aeb6a6cf2a27df0456d02bc616c89a572", "url": "https://www.semanticscholar.org/paper/2bb4965aeb6a6cf2a27df0456d02bc616c89a572", "title": "Automatic Construction of Decision Trees from Data: A Multi-Disciplinary Survey", "abstract": null, "year": 1998, "referenceCount": 610, "citationCount": 1008, "influentialCitationCount": 47, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32980796", "name": "Sreerama K. Murthy"}]}, {"paperId": "3c8151fd25b9c6897bd8fa5c4a88aebba49f1cb8", "url": "https://www.semanticscholar.org/paper/3c8151fd25b9c6897bd8fa5c4a88aebba49f1cb8", "title": "C4.5, Class Imbalance, and Cost Sensitivity: Why Under-Sampling beats Over-Sampling", "abstract": "This paper takes a new look at two sampling schemes commonly used to adapt machine learning algorithms to imbalanced classes and misclassification costs. It uses a performance analysis technique called cost curves to explore the interaction of over and undersampling with the decision tree learner C4.5. C4.5 was chosen as, when combined with one of the sampling schemes, it is quickly becoming the community standard when evaluating new cost sensitive learning algorithms. This paper shows that using C4.5 with undersampling establishes a reasonable standard for algorithmic comparison. But it is recommended that the cheapest class classifier be part of that standard as it can be better than under-sampling for relatively modest costs. Over-sampling, however, shows little sensitivity, there is often little dierence in performance when misclassification costs are changed.", "year": 2003, "referenceCount": 13, "citationCount": 827, "influentialCitationCount": 37, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2506249", "name": "C. Drummond"}, {"authorId": "1796214", "name": "R. Holte"}]}, {"paperId": "44a97f4eaaefaf5338f8aed2913d5debb2459f7e", "url": "https://www.semanticscholar.org/paper/44a97f4eaaefaf5338f8aed2913d5debb2459f7e", "title": "Deep Models Under the GAN: Information Leakage from Collaborative Deep Learning", "abstract": "Deep Learning has recently become hugely popular in machine learning for its ability to solve end-to-end learning systems, in which the features and the classifiers are learned simultaneously, providing significant improvements in classification accuracy in the presence of highly-structured and large databases. Its success is due to a combination of recent algorithmic breakthroughs, increasingly powerful computers, and access to significant amounts of data. Researchers have also considered privacy implications of deep learning. Models are typically trained in a centralized manner with all the data being processed by the same training algorithm. If the data is a collection of users' private data, including habits, personal pictures, geographical positions, interests, and more, the centralized server will have access to sensitive information that could potentially be mishandled. To tackle this problem, collaborative deep learning models have recently been proposed where parties locally train their deep learning structures and only share a subset of the parameters in the attempt to keep their respective training sets private. Parameters can also be obfuscated via differential privacy (DP) to make information extraction even more challenging, as proposed by Shokri and Shmatikov at CCS'15. Unfortunately, we show that any privacy-preserving collaborative deep learning is susceptible to a powerful attack that we devise in this paper. In particular, we show that a distributed, federated, or decentralized deep learning approach is fundamentally broken and does not protect the training sets of honest participants. The attack we developed exploits the real-time nature of the learning process that allows the adversary to train a Generative Adversarial Network (GAN) that generates prototypical samples of the targeted training set that was meant to be private (the samples generated by the GAN are intended to come from the same distribution as the training data). Interestingly, we show that record-level differential privacy applied to the shared parameters of the model, as suggested in previous work, is ineffective (i.e., record-level DP is not designed to address our attack).", "year": 2017, "referenceCount": 94, "citationCount": 782, "influentialCitationCount": 79, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1997436", "name": "B. Hitaj"}, {"authorId": "1700850", "name": "G. Ateniese"}, {"authorId": "1388508441", "name": "F. P\u00e9rez-Cruz"}]}, {"paperId": "0e2b3faed39561f712c3b14a08c7c36272d9857a", "url": "https://www.semanticscholar.org/paper/0e2b3faed39561f712c3b14a08c7c36272d9857a", "title": "Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units", "abstract": "Recently, convolutional neural networks (CNNs) have been used as a powerful tool to solve many problems of machine learning and computer vision. In this paper, we aim to provide insight on the property of convolutional neural networks, as well as a generic method to improve the performance of many CNN architectures. Specifically, we first examine existing CNN models and observe an intriguing property that the filters in the lower layers form pairs (i.e., filters with opposite phase). Inspired by our observation, we propose a novel, simple yet effective activation scheme called concatenated ReLU (CRelu) and theoretically analyze its reconstruction property in CNNs. We integrate CRelu into several state-of-the-art CNN architectures and demonstrate improvement in their recognition performance on CIFAR-10/100 and ImageNet datasets with fewer trainable parameters. Our results suggest that better understanding of the properties of CNNs can lead to significant performance improvement with a simple modification.", "year": 2016, "referenceCount": 35, "citationCount": 411, "influentialCitationCount": 43, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3163480", "name": "Wenling Shang"}, {"authorId": "1729571", "name": "Kihyuk Sohn"}, {"authorId": "2061137049", "name": "Diogo Almeida"}, {"authorId": "1697141", "name": "Honglak Lee"}]}, {"paperId": "bb2e442b2acb4530aa28d24e45578f84447d0425", "url": "https://www.semanticscholar.org/paper/bb2e442b2acb4530aa28d24e45578f84447d0425", "title": "A Review of Class Imbalance Problem", "abstract": "Class imbalance is one of the challenges of machine learning and data mining fields. Imbalance data sets degrades the performance of data mining and machine learning techniques as the overall accuracy and decision making be biased to the majority class, which lead to misclassifying the minority class samples or furthermore treated them as noise. This paper proposes a general survey for class imbalance problem solutions and the most significant investigations recently introduced by researchers.", "year": 2014, "referenceCount": 65, "citationCount": 144, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3364303", "name": "Shaza M. Abd Elrahman"}, {"authorId": "145731499", "name": "A. Abraham"}]}, {"paperId": "f0af0029293dc8f242894f113baf15d68228ec4d", "url": "https://www.semanticscholar.org/paper/f0af0029293dc8f242894f113baf15d68228ec4d", "title": "Attentional Factorization Machines: Learning the Weight of Feature Interactions via Attention Networks", "abstract": "Factorization Machines (FMs) are a supervised learning approach that enhances the linear regression model by incorporating the second-order feature interactions. Despite effectiveness, FM can be hindered by its modelling of all feature interactions with the same weight, as not all feature interactions are equally useful and predictive. For example, the interactions with useless features may even introduce noises and adversely degrade the performance. In this work, we improve FM by discriminating the importance of different feature interactions. We propose a novel model named Attentional Factorization Machine (AFM), which learns the importance of each feature interaction from data via a neural attention network. Extensive experiments on two real-world datasets demonstrate the effectiveness of AFM. Empirically, it is shown on regression task AFM betters FM with a $8.6\\%$ relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep and DeepCross with a much simpler structure and fewer model parameters. Our implementation of AFM is publicly available at: this https URL", "year": 2017, "referenceCount": 37, "citationCount": 582, "influentialCitationCount": 87, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145974111", "name": "Jun Xiao"}, {"authorId": "2111972888", "name": "Hao Ye"}, {"authorId": "7792071", "name": "Xiangnan He"}, {"authorId": "5462268", "name": "Hanwang Zhang"}, {"authorId": "144894849", "name": "Fei Wu"}, {"authorId": "144078686", "name": "Tat-Seng Chua"}]}, {"paperId": "494e63c4a333bf3be8a28dc0213b0e0d39c2ef04", "url": "https://www.semanticscholar.org/paper/494e63c4a333bf3be8a28dc0213b0e0d39c2ef04", "title": "A Kernel Theory of Modern Data Augmentation", "abstract": "Data augmentation, a technique in which a training set is expanded with class-preserving transformations, is ubiquitous in modern machine learning pipelines. In this paper, we seek to establish a theoretical framework for understanding data augmentation. We approach this from two directions: First, we provide a general model of augmentation as a Markov process, and show that kernels appear naturally with respect to this model, even when we do not employ kernel classification. Next, we analyze more directly the effect of augmentation on kernel classifiers, showing that data augmentation can be approximated by first-order feature averaging and second-order variance regularization components. These frameworks both serve to illustrate the ways in which data augmentation affects the downstream learning model, and the resulting analyses provide novel connections between prior work in invariant kernels, tangent propagation, and robust optimization. Finally, we provide several proof-of-concept applications showing that our theory can be useful for accelerating machine learning workflows, such as reducing the amount of computation needed to train using augmented data, and predicting the utility of a transformation prior to training.", "year": 2018, "referenceCount": 56, "citationCount": 122, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "24593911", "name": "Tri Dao"}, {"authorId": "39499001", "name": "Albert Gu"}, {"authorId": "143711421", "name": "Alexander J. Ratner"}, {"authorId": "145260024", "name": "Virginia Smith"}, {"authorId": "1801197", "name": "Christopher De Sa"}, {"authorId": "2114485554", "name": "C. R\u00e9"}]}, {"paperId": "c14ea670799f2a5d5c1b1d951bf1131c9f672b3b", "url": "https://www.semanticscholar.org/paper/c14ea670799f2a5d5c1b1d951bf1131c9f672b3b", "title": "Experiments on multistrategy learning by meta-learning", "abstract": "In this paper, we propose meta-leaming as a general technique to combine the results of multiple learning algorithms, each applied to a set of training data. We detail several metalearning strategies for combining independently learned classifiers, each computed by different algorithms, to improve overall prediction accuracy. The overall resulting classifier is composed of the classifiers generated by the different learning algorithms and a meta-classifier generated by a meta-learning strategy. The strategies described here are independent of the learning algorithms used. Preliminm-y experiments using different strategies and learning algorithms on two molecular biology sequence analysis data sets demonstrate encouraging results. Machine learning techniques are central to automated knowledge discovery systems and hence our approach can enhance the effectiveness of such systems.", "year": 1993, "referenceCount": 22, "citationCount": 213, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144316325", "name": "P. Chan"}, {"authorId": "1807433", "name": "S. Stolfo"}]}, {"paperId": "c1b9cb72f74e3091a00de438423314b902aff023", "url": "https://www.semanticscholar.org/paper/c1b9cb72f74e3091a00de438423314b902aff023", "title": "A Short Introduction to Learning with Kernels", "abstract": null, "year": 2002, "referenceCount": 38, "citationCount": 80, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}, {"authorId": "46234526", "name": "Alex Smola"}]}, {"paperId": "9f744e48091a24b569435d070920e60db45f4fdc", "url": "https://www.semanticscholar.org/paper/9f744e48091a24b569435d070920e60db45f4fdc", "title": "Protein secondary structure prediction using logic-based machine learning.", "abstract": "Many attempts have been made to solve the problem of predicting protein secondary structure from the primary sequence but the best performance results are still disappointing. In this paper, the use of a machine learning algorithm which allows relational descriptions is shown to lead to improved performance. The Inductive Logic Programming computer program, Golem, was applied to learning secondary structure prediction rules for alpha/alpha domain type proteins. The input to the program consisted of 12 non-homologous proteins (1612 residues) of known structure, together with a background knowledge describing the chemical and physical properties of the residues. Golem learned a small set of rules that predict which residues are part of the alpha-helices--based on their positional relationships and chemical and physical properties. The rules were tested on four independent non-homologous proteins (416 residues) giving an accuracy of 81% (+/- 2%). This is an improvement, on identical data, over the previously reported result of 73% by King and Sternberg (1990, J. Mol. Biol., 216, 441-457) using the machine learning program PROMIS, and of 72% using the standard Garnier-Osguthorpe-Robson method. The best previously reported result in the literature for the alpha/alpha domain type is 76%, achieved using a neural net approach. Machine learning also has the advantage over neural network and statistical methods in producing more understandable results.", "year": 1992, "referenceCount": 27, "citationCount": 140, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "145147566", "name": "S. Muggleton"}, {"authorId": "65901274", "name": "R. King"}, {"authorId": "145410834", "name": "M. Sternberg"}]}, {"paperId": "2de0a40e9a5d4f1feb07d61af5a5d87a069653f0", "url": "https://www.semanticscholar.org/paper/2de0a40e9a5d4f1feb07d61af5a5d87a069653f0", "title": "Data Mining: An Overview from a Database Perspective", "abstract": "Mining information and knowledge from large databases has been recognized by many researchers as a key research topic in database systems and machine learning, and by many industrial companies as an important area with an opportunity of major revenues. Researchers in many different fields have shown great interest in data mining. Several emerging applications in information-providing services, such as data warehousing and online services over the Internet, also call for various data mining techniques to better understand user behavior, to improve the service provided and to increase business opportunities. In response to such a demand, this article provides a survey, from a database researcher's point of view, on the data mining techniques developed recently. A classification of the available data mining techniques is provided and a comparative study of such techniques is presented.", "year": 1996, "referenceCount": 142, "citationCount": 2514, "influentialCitationCount": 100, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "153314745", "name": "Ming-Syan Chen"}, {"authorId": "145325584", "name": "Jiawei Han"}, {"authorId": "144019071", "name": "Philip S. Yu"}]}, {"paperId": "40927c5d81988a1151639fad150cbc74f64e0d68", "url": "https://www.semanticscholar.org/paper/40927c5d81988a1151639fad150cbc74f64e0d68", "title": "Time Series Prediction Using Support Vector Machines: A Survey", "abstract": "Time series prediction techniques have been used in many real-world applications such as financial market prediction, electric utility load forecasting , weather and environmental state prediction, and reliability forecasting. The underlying system models and time series data generating processes are generally complex for these applications and the models for these systems are usually not known a priori. Accurate and unbiased estimation of the time series data produced by these systems cannot always be achieved using well known linear techniques, and thus the estimation process requires more advanced time series prediction algorithms. This paper provides a survey of time series prediction applications using a novel machine learning approach: support vector machines (SVM). The underlying motivation for using SVMs is the ability of this methodology to accurately forecast time series data when the underlying system processes are typically nonlinear, non-stationary and not defined a-priori. SVMs have also been proven to outperform other non-linear techniques including neural-network based non-linear prediction techniques such as multi-layer perceptrons.The ultimate goal is to provide the reader with insight into the applications using SVM for time series prediction, to give a brief tutorial on SVMs for time series prediction, to outline some of the advantages and challenges in using SVMs for time series prediction, and to provide a source for the reader to locate books, technical journals, and other online SVM research resources.", "year": 2009, "referenceCount": 101, "citationCount": 811, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2693432", "name": "N. Sapankevych"}, {"authorId": "145014123", "name": "R. Sankar"}]}, {"paperId": "e0222a1ae6874f7fff128c3da8769ab95963da04", "url": "https://www.semanticscholar.org/paper/e0222a1ae6874f7fff128c3da8769ab95963da04", "title": "Reinforced Mnemonic Reader for Machine Reading Comprehension", "abstract": "In this paper, we introduce the Reinforced Mnemonic Reader for machine reading comprehension tasks, which enhances previous attentive readers in two aspects. First, a reattention mechanism is proposed to refine current attentions by directly accessing to past attentions that are temporally memorized in a multi-round alignment architecture, so as to avoid the problems of attention redundancy and attention deficiency. Second, a new optimization approach, called dynamic-critical reinforcement learning, is introduced to extend the standard supervised method. It always encourages to predict a more acceptable answer so as to address the convergence suppression problem occurred in traditional reinforcement learning algorithms. Extensive experiments on the Stanford Question Answering Dataset (SQuAD) show that our model achieves state-of-the-art results. Meanwhile, our model outperforms previous systems by over 6% in terms of both Exact Match and F1 metrics on two adversarial SQuAD datasets.", "year": 2017, "referenceCount": 49, "citationCount": 195, "influentialCitationCount": 25, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "8367832", "name": "Minghao Hu"}, {"authorId": "49236691", "name": "Yuxing Peng"}, {"authorId": "1557265719", "name": "Zhen Huang"}, {"authorId": "1767521", "name": "Xipeng Qiu"}, {"authorId": "49807919", "name": "Furu Wei"}, {"authorId": "143849609", "name": "M. Zhou"}]}, {"paperId": "32e97eef94beacace020e79322cef0e1e5a76ee0", "url": "https://www.semanticscholar.org/paper/32e97eef94beacace020e79322cef0e1e5a76ee0", "title": "Gradient calculations for dynamic recurrent neural networks: a survey", "abstract": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed.", "year": 1995, "referenceCount": 299, "citationCount": 618, "influentialCitationCount": 38, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1700974", "name": "Barak A. Pearlmutter"}]}, {"paperId": "9899c9d7f3a5b5d9fc918675c0843754d160c986", "url": "https://www.semanticscholar.org/paper/9899c9d7f3a5b5d9fc918675c0843754d160c986", "title": "Computational learning theory: survey and selected bibliography", "abstract": "Permission to copy without fee all or part of this material ie granted provided that the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication and its date appear, and notica is given that copying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, requires a fee and/or specific permission. 24th ANNUAL ACM STOC 5/92/VICTORiA, B. C., CANADA G 1992 ACM ()-89791.51 2-7/92/0004/03~j -..$1 .~() the only textbook in the field is Natarajan\u2019s [101]. Surveys by Laird [83] and Valiant [129] are valuable. Somewhat more peripheral are the European meetings on Analogical and Inductive Inference, AH, and the AI machine learning communit y\u2019s annual International Conference on Machine Learning. In addition, the general AI meetings, AAAI and IJCAI, currently have a large number of papers devoted to learning, as do the neural net meetings.", "year": 1992, "referenceCount": 128, "citationCount": 213, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2911738", "name": "D. Angluin"}]}, {"paperId": "3ccf5c97d89f0f5d6eb40c8feb2bcd387223e9c0", "url": "https://www.semanticscholar.org/paper/3ccf5c97d89f0f5d6eb40c8feb2bcd387223e9c0", "title": "Machine Learning Methods for Classifying Human Physical Activity from On-Body Accelerometers", "abstract": "The use of on-body wearable sensors is widespread in several academic and industrial domains. Of great interest are their applications in ambulatory monitoring and pervasive computing systems; here, some quantitative analysis of human motion and its automatic classification are the main computational tasks to be pursued. In this paper, we discuss how human physical activity can be classified using on-body accelerometers, with a major emphasis devoted to the computational algorithms employed for this purpose. In particular, we motivate our current interest for classifiers based on Hidden Markov Models (HMMs). An example is illustrated and discussed by analysing a dataset of accelerometer time series.", "year": 2010, "referenceCount": 60, "citationCount": 695, "influentialCitationCount": 40, "isOpenAccess": true, "fieldsOfStudy": ["Engineering", "Computer Science", "Medicine"], "authors": [{"authorId": "1758858", "name": "A. Mannini"}, {"authorId": "1688761", "name": "A. Sabatini"}]}, {"paperId": "26ee8223907bf0fe00ab3760f1d7bc1ae1a89a73", "url": "https://www.semanticscholar.org/paper/26ee8223907bf0fe00ab3760f1d7bc1ae1a89a73", "title": "The seven tools of causal inference, with reflections on machine learning", "abstract": "The kind of causal inference seen in natural human thought can be \"algorithmitized\" to help produce human-level machine intelligence.", "year": 2019, "referenceCount": 62, "citationCount": 327, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145430701", "name": "J. Pearl"}]}, {"paperId": "d37d2fdc32d6483d95898d92f5deb971cfac354a", "url": "https://www.semanticscholar.org/paper/d37d2fdc32d6483d95898d92f5deb971cfac354a", "title": "Online portfolio selection: A survey", "abstract": "Online portfolio selection is a fundamental problem in computational finance, which has been extensively studied across several research communities, including finance, statistics, artificial intelligence, machine learning, and data mining. This article aims to provide a comprehensive survey and a structural understanding of online portfolio selection techniques published in the literature. From an online machine learning perspective, we first formulate online portfolio selection as a sequential decision problem, and then we survey a variety of state-of-the-art approaches, which are grouped into several major categories, including benchmarks, Follow-the-Winner approaches, Follow-the-Loser approaches, Pattern-Matching--based approaches, and Meta-Learning Algorithms. In addition to the problem formulation and related algorithms, we also discuss the relationship of these algorithms with the capital growth theory so as to better understand the similarities and differences of their underlying trading ideas. This article aims to provide a timely and comprehensive survey for both machine learning and data mining researchers in academia and quantitative portfolio managers in the financial industry to help them understand the state of the art and facilitate their research and practical applications. We also discuss some open issues and evaluate some emerging new trends for future research.", "year": 2012, "referenceCount": 207, "citationCount": 219, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Economics"], "authors": [{"authorId": "2185909392", "name": "Bin Li"}, {"authorId": "1741126", "name": "S. Hoi"}]}, {"paperId": "0f5b9b46b43c729d554d4120c4e13828c01f40db", "url": "https://www.semanticscholar.org/paper/0f5b9b46b43c729d554d4120c4e13828c01f40db", "title": "A support vector machine approach to decision trees", "abstract": "Key ideas from statistical learning theory and support vector machines are generalized to decision trees. A support vector machine is used for each decision in the tree. The \"optimal\" decision tree is characterized, and both a primal and dual space formulation for constructing the tree are proposed. The result is a method for generating logically simple decision trees with multivariate linear, nonlinear or linear decisions. By varying the kernel function used, the decisions may consist of linear threshold units, polynomials, sigmoidal neural networks, or radial basis function networks. The preliminary results indicate that the method produces simple trees that generalize well with respect to other decision tree algorithms and single support vector machines.", "year": 1998, "referenceCount": 18, "citationCount": 250, "influentialCitationCount": 29, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "2065549778", "name": "Kristin P. Bennett"}, {"authorId": "48889923", "name": "J. A. Blue"}]}, {"paperId": "e89cb97bc83badf8c6cc0e2439ee4a035cba72d9", "url": "https://www.semanticscholar.org/paper/e89cb97bc83badf8c6cc0e2439ee4a035cba72d9", "title": "Designing Neural Networks using Genetic Algorithms", "abstract": "RoboCup has come a long way since it\u2019s creation in \u201997 [1] and is a respected place for machine learning researchers to try out new algorithms in a competitive fashion. RoboCup is now an international competition that draws many teams and respected researchers looking for a chance to create the best team. Originally we set out to create a team to compete in RoboCup. This was an ambitious project, and we had hopes to finish within the next year. For this semester, we chose to scale down the RoboCup team towards a smaller research area to try our learning algorithm on. The scaled down version of the RoboCup soccer environment is known as the \u201dKeepaway Testbed\u201d and was started by Peter Stone, University of Texas [2]. Here the task is simple, you have two teams on the field each with the same number of players. Instead of trying to score a goal on the opponent the teams are given tasks, and one team is labeled the keepers and the other is labeled the takers. It is the task of the keepers to maintain possesion of the ball and it is the task of the takers to take the ball. The longer the keepers are able to maintain possesion of the ball the better the team. There are several advantages to this environment. First, it provides some of the essential characteristics of a real soccer game. Typically it is believed that if a team is able to maintain possesion of the ball for long periods of time they will win the match. Secondly, it provides realistic behavior much the same as the original RoboCup server. This is accomplished by introducing noise into the system similar to the original RoboCup, and similar to what would be received by real robots. Finally, when you want to go through the learning process this environment is capable of stopping play once the takers have touched the ball, and the environment is capable of starting a new trial based on that occurrence. Although the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train our agents, we still needed to scale down the problem in order to do a feasibility study. Based on the Keepaway testbed, we created a simulation world with one simple task. One agent is placed into the world and has to locate the position of the goal. This can be thought of as an agent in a soccer environment needing to locate either the ball or another teammate. It was in this environment where we tested our methods for learning autonomous agents.", "year": 1989, "referenceCount": 0, "citationCount": 951, "influentialCitationCount": 40, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48890329", "name": "G. Miller"}, {"authorId": "14024873", "name": "P. Todd"}, {"authorId": "1983880", "name": "Shailesh U. Hegde"}]}, {"paperId": "e0c0043c43c03de6e2c26a605dc8b8e08872a8a0", "url": "https://www.semanticscholar.org/paper/e0c0043c43c03de6e2c26a605dc8b8e08872a8a0", "title": "SparCML: high-performance sparse communication for machine learning", "abstract": "Applying machine learning techniques to the quickly growing data in science and industry requires highly-scalable algorithms. Large datasets are most commonly processed \"data parallel\" distributed across many nodes. Each node's contribution to the overall gradient is summed using a global allreduce. This allreduce is the single communication and thus scalability bottleneck for most machine learning workloads. We observe that frequently, many gradient values are (close to) zero, leading to sparse of sparsifyable communications. To exploit this insight, we analyze, design, and implement a set of communication-efficient protocols for sparse input data, in conjunction with efficient machine learning algorithms which can leverage these primitives. Our communication protocols generalize standard collective operations, by allowing processes to contribute arbitrary sparse input data vectors. Our generic communication library, SparCML1, extends MPI to support additional features, such as non-blocking (asynchronous) operations and low-precision data representations. As such, SparCML and its techniques will form the basis of future highly-scalable machine learning frameworks.", "year": 2018, "referenceCount": 72, "citationCount": 84, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "37833666", "name": "C\u00e9dric Renggli"}, {"authorId": "3311387", "name": "Dan Alistarh"}, {"authorId": "1713648", "name": "T. Hoefler"}]}, {"paperId": "20edd347dfdefc89b8969aaa9885bec932f60647", "url": "https://www.semanticscholar.org/paper/20edd347dfdefc89b8969aaa9885bec932f60647", "title": "Selecting Directors Using Machine Learning", "abstract": "\n Can algorithms assist firms in their decisions on nominating corporate directors? Directors predicted by algorithms to perform poorly indeed do perform poorly compared to a realistic pool of candidates in out-of-sample tests. Predictably bad directors are more likely to be male, accumulate more directorships, and have larger networks than the directors the algorithm would recommend in their place. Companies with weaker governance structures are more likely to nominate them. Our results suggest that machine learning holds promise for understanding the process by which governance structures are chosen and has potential to help real-world firms improve their governance.", "year": 2018, "referenceCount": 90, "citationCount": 76, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "119529265", "name": "Isil Erel"}, {"authorId": "118889103", "name": "L. Stern"}, {"authorId": "40348583", "name": "Chenhao Tan"}, {"authorId": "14154154", "name": "M. Weisbach"}]}, {"paperId": "86eb2582a934b47a1fcaee8c84511e70d7897c1d", "url": "https://www.semanticscholar.org/paper/86eb2582a934b47a1fcaee8c84511e70d7897c1d", "title": "Genetic programming: a paradigm for genetically breeding populations of computer programs to solve problems", "abstract": "Many seemingly different problems in artificial intelligence, symbolic processing, and machine learning can be viewed as requiring discovery of a computer program that produces some desired output for particular inputs. When viewed in this way, the process of solving these problems becomes equivalent to searching a space of possible computer programs for a most fit individual computer program. The new \"genetic programming\" paradigm described herein provides a way to search for this most fit individual computer program. In this new \"genetic programming\" paradigm, populations of computer programs are genetically bred using the Darwinian principle of survival of the fittest and using a genetic crossover (recombination) operator appropriate for genetically mating computer programs. In this paper, the process of formulating and solving problems using this new paradigm is illustrated using examples from various areas. Examples come from the areas of machine learning of a function; planning; sequence induction; function function identification (including symbolic regression, empirical discovery, \"data to function\" symbolic integration, \"data to function\" symbolic differentiation); solving equations, including differential equations, integral equations, and functional equations); concept formation; automatic programming; pattern recognition, time-optimal control; playing differential pursuer-evader games; neural network design; and finding a game-playing strategyfor a discrete game in extensive form.", "year": 1990, "referenceCount": 95, "citationCount": 728, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1732302", "name": "J. Koza"}]}, {"paperId": "0cc52fd81656fad2d6d85e29b1360e1b6a919d42", "url": "https://www.semanticscholar.org/paper/0cc52fd81656fad2d6d85e29b1360e1b6a919d42", "title": "Theory-Guided Machine Learning in Materials Science", "abstract": "Materials scientists are increasingly adopting the use of machine learning tools to discover hidden trends in data and make predictions. Applying concepts from data science without foreknowledge of their limitations and the unique qualities of materials data, however, could lead to errant conclusions. The differences that exist between various kinds of experimental and calculated data require careful choices of data processing and machine learning methods. Here, we outline potential pitfalls involved in using machine learning without robust protocols. We address some problems of overfitting to training data using decision trees as an example, rational descriptor selection in the field of perovskites, and preserving physical interpretability in the application of dimensionality reducing techniques such as principal component analysis. We show how proceeding without the guidance of domain knowledge can lead to both quantitatively and qualitatively incorrect predictive models.", "year": 2016, "referenceCount": 44, "citationCount": 101, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "2062674058", "name": "Nicholas Wagner"}, {"authorId": "5137392", "name": "J. Rondinelli"}]}, {"paperId": "c2a69bdc003fac58d6254cb49155734bcd6eb118", "url": "https://www.semanticscholar.org/paper/c2a69bdc003fac58d6254cb49155734bcd6eb118", "title": "Probabilistic logic learning", "abstract": "The past few years have witnessed an significant interest in probabilistic logic learning, i.e. in research lying at the intersection of probabilistic reasoning, logical representations, and machine learning. A rich variety of different formalisms and learning techniques have been developed. This paper provides an introductory survey and overview of the state-of-the-art in probabilistic logic learning through the identification of a number of important probabilistic, logical and learning concepts.", "year": 2003, "referenceCount": 104, "citationCount": 170, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1740042", "name": "L. D. Raedt"}, {"authorId": "1746871", "name": "K. Kersting"}]}, {"paperId": "2cff932a9fd1d1d4c7589a587015f79f1c291bc2", "url": "https://www.semanticscholar.org/paper/2cff932a9fd1d1d4c7589a587015f79f1c291bc2", "title": "A Machine Learning Approach to Musical Style Recognition", "abstract": "Much of the work on perception and understanding of music by computers has focused on low-level\nperceptual features such as pitch and tempo. Our work demonstrates that machine learning can be\nused to build effective style classifiers for interactive performance systems. We also present an analysis explaining why these techniques work so well when hand-coded approaches have consistently\nfailed. We also describe a reliable real-time performance style classifier.", "year": 1997, "referenceCount": 4, "citationCount": 183, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697732", "name": "R. Dannenberg"}, {"authorId": "2691933", "name": "B. Thom"}, {"authorId": "2140403023", "name": "David S. Watson"}]}, {"paperId": "a9ced913e39cccce2c61519acba8782d6a8d257b", "url": "https://www.semanticscholar.org/paper/a9ced913e39cccce2c61519acba8782d6a8d257b", "title": "Machine learning of inductive bias", "abstract": "A solution to get the problem off, have you found it? Really? What kind of solution do you resolve the problem? From what sources? Well, there are so many questions that we utter every day. No matter how you will get the solution, it will mean better. You can take the reference from some books. And the machine learning of inductive bias is one book that we really recommend you to read, to get more solutions in solving this problem.", "year": 1986, "referenceCount": 0, "citationCount": 74, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "65915339", "name": "P. Goff"}]}, {"paperId": "1fccba11583dc9e1030713d61bd65e9e9990e39f", "url": "https://www.semanticscholar.org/paper/1fccba11583dc9e1030713d61bd65e9e9990e39f", "title": "Human-Centered Artificial Intelligence and Machine Learning", "abstract": "Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.", "year": 2019, "referenceCount": 14, "citationCount": 108, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2757194", "name": "Mark O. Riedl"}]}, {"paperId": "00b7f44857676600805172e99be6f9f2987e98eb", "url": "https://www.semanticscholar.org/paper/00b7f44857676600805172e99be6f9f2987e98eb", "title": "Statistical Learning with Sparsity: The Lasso and Generalizations", "abstract": "Discover New Methods for Dealing with High-Dimensional Data A sparse statistical model has only a small number of nonzero parameters or weights; therefore, it is much easier to estimate and interpret than a dense model. Statistical Learning with Sparsity: The Lasso and Generalizations presents methods that exploit sparsity to help recover the underlying signal in a set of data. Top experts in this rapidly evolving field, the authors describe the lasso for linear regression and a simple coordinate descent algorithm for its computation. They discuss the application of 1 penalties to generalized linear models and support vector machines, cover generalized penalties such as the elastic net and group lasso, and review numerical methods for optimization. They also present statistical inference methods for fitted (lasso) models, including the bootstrap, Bayesian methods, and recently developed approaches. In addition, the book examines matrix decomposition, sparse multivariate analysis, graphical models, and compressed sensing. It concludes with a survey of theoretical results for the lasso. In this age of big data, the number of features measured on a person or object can be large and might be larger than the number of observations. This book shows how the sparsity assumption allows us to tackle these problems and extract useful and reproducible patterns from big datasets. Data analysts, computer scientists, and theorists will appreciate this thorough and up-to-date treatment of sparse statistical modeling.", "year": 2015, "referenceCount": 301, "citationCount": 1766, "influentialCitationCount": 196, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1784682", "name": "T. Hastie"}, {"authorId": "1761784", "name": "R. Tibshirani"}, {"authorId": "1721860", "name": "M. Wainwright"}]}, {"paperId": "fc8f9ff657cd7d257ce5285a40166a3b2ae26f57", "url": "https://www.semanticscholar.org/paper/fc8f9ff657cd7d257ce5285a40166a3b2ae26f57", "title": "Survey and Benchmarking of Machine Learning Accelerators", "abstract": "Advances in multicore processors and accelerators have opened the flood gates to greater exploration and application of machine learning techniques to a variety of applications. These advances, along with breakdowns of several trends including Moore\u2019s Law, have prompted an explosion of processors and accelerators that promise even greater computational and machine learning capabilities. These processors and accelerators are coming in many forms, from CPUs and GPUs to ASICs, FPGAs, and dataflow accelerators. This paper surveys the current state of these processors and accelerators that have been publicly announced with performance and power consumption numbers. The performance and power values are plotted on a scatter graph and a number of dimensions and observations from the trends on this plot are discussed and analyzed. For instance, there are interesting trends in the plot regarding power consumption, numerical precision, and inference versus training. We then select and benchmark two commercially available low size, weight, and power (SWaP) accelerators as these processors are the most interesting for embedded and mobile machine learning inference applications that are most applicable to the DoD and other SWaP constrained users. We determine how they actually perform with real-world images and neural network models, compare those results to the reported performance and power consumption values and evaluate them against an Intel CPU that is used in some embedded applications.", "year": 2019, "referenceCount": 97, "citationCount": 102, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2097629", "name": "A. Reuther"}, {"authorId": "1684116", "name": "P. Michaleas"}, {"authorId": "2111328084", "name": "Michael Jones"}, {"authorId": "74882299", "name": "V. Gadepally"}, {"authorId": "2331418", "name": "S. Samsi"}, {"authorId": "3257323", "name": "J. Kepner"}]}, {"paperId": "7a7c37d1e1456bc604f5dd9047a4c2cb26ec91f8", "url": "https://www.semanticscholar.org/paper/7a7c37d1e1456bc604f5dd9047a4c2cb26ec91f8", "title": "Monotonicity maintenance in information-theoretic machine learning algorithms", "abstract": null, "year": 2004, "referenceCount": 15, "citationCount": 69, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1399062322", "name": "A. Ben-David"}]}, {"paperId": "432e5d09f080c95286dc461f419b0685b41eb7e3", "url": "https://www.semanticscholar.org/paper/432e5d09f080c95286dc461f419b0685b41eb7e3", "title": "Application of machine learning to epileptic seizure onset detection and treatment", "abstract": "Epilepsy is a chronic disorder of the central nervous system that predisposes individuals to experiencing recurrent seizures. It affects 3 million Americans and 50 million people world-wide. A seizure is a transient aberration in the brain's electrical activity that produces disruptive physical symptoms such as a lapse in attention and memory, a sensory hallucination, or a whole-body convulsion. Approximately 1 out of every 3 individuals with epilepsy continues to experience frequent seizures despite treatment with multiple anti-epileptic drugs. These intractable seizures pose a serious risk of injury, limit the independence and mobility of an individual, and result in both social isolation and economic hardship. This thesis presents novel technology intended to ease the burden of intractable seizures. At its heart is a method for computerized detection of seizure onset. The method uses machine learning to construct patient-specific classifiers that are capable of rapid, sensitive, and specific detection of seizure onset. The algorithm detects the onset of a seizure through analysis of the brain's electrical activity alone or in concert with other physiologic signals. When trained on 2 or more seizures and tested on 844 hours of continuous scalp EEG from 23 pediatric epilepsy patients, our algorithm detected 96% of 163 test seizures with a median detection delay of 3 seconds and a median false detection rate of 2 false detections per 24 hour period. In this thesis we also discuss how our detector can be embedded within a lowpower, implantable medical device to enable the delivery of just-in-time therapy that has the potential to either eliminate or attenuate the clinical symptoms associated with seizures. Finally, we report on the in-hospital use of our detector to enable delay-sensitive therapeutic and diagnostic applications. We demonstrate the feasibility of using the algorithm to control the Vagus Nerve Stimulator (an implantable neurostimulator for the treatment of intractable seizures), and to initiate ictal SPECT (a functional neuroimaging modality useful for localizing the cerebral site of origin of a seizure). Thesis Supervisor: John V. Guttag Title: Professor of Electrical Engineering and Computer Science", "year": 2009, "referenceCount": 74, "citationCount": 586, "influentialCitationCount": 71, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "2614750", "name": "A. Shoeb"}]}, {"paperId": "4eabb5b4cf137c27917a8b91c471a6a2b9407469", "url": "https://www.semanticscholar.org/paper/4eabb5b4cf137c27917a8b91c471a6a2b9407469", "title": "A Review of Android Malware Detection Approaches Based on Machine Learning", "abstract": "Android applications are developing rapidly across the mobile ecosystem, but Android malware is also emerging in an endless stream. Many researchers have studied the problem of Android malware detection and have put forward theories and methods from different perspectives. Existing research suggests that machine learning is an effective and promising way to detect Android malware. Notwithstanding, there exist reviews that have surveyed different issues related to Android malware detection based on machine learning. We believe our work complements the previous reviews by surveying a wider range of aspects of the topic. This paper presents a comprehensive survey of Android malware detection approaches based on machine learning. We briefly introduce some background on Android applications, including the Android system architecture, security mechanisms, and classification of Android malware. Then, taking machine learning as the focus, we analyze and summarize the research status from key perspectives such as sample acquisition, data preprocessing, feature selection, machine learning models, algorithms, and the evaluation of detection effectiveness. Finally, we assess the future prospects for research into Android malware detection based on machine learning. This review will help academics gain a full picture of Android malware detection based on machine learning. It could then serve as a basis for subsequent researchers to start new work and help to guide research in the field more generally.", "year": 2020, "referenceCount": 274, "citationCount": 72, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1828725529", "name": "Kaijun Liu"}, {"authorId": "50433409", "name": "Shengwei Xu"}, {"authorId": "3027277", "name": "Guoai Xu"}, {"authorId": "2108160165", "name": "Miao Zhang"}, {"authorId": "2112672824", "name": "Dawei Sun"}, {"authorId": "2109497851", "name": "Haifeng Liu"}]}, {"paperId": "d3992c3d89e5ec05e0a6c96c4956f2ff9f7da023", "url": "https://www.semanticscholar.org/paper/d3992c3d89e5ec05e0a6c96c4956f2ff9f7da023", "title": "Machine learning and algorithmic fairness in public and population health", "abstract": null, "year": 2021, "referenceCount": 95, "citationCount": 30, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51429443", "name": "V. Mhasawade"}, {"authorId": "2110150937", "name": "Yuan Zhao"}, {"authorId": "3144230", "name": "R. Chunara"}]}, {"paperId": "6ca6097a09f07eb4d35e1730b793881f5f434b0a", "url": "https://www.semanticscholar.org/paper/6ca6097a09f07eb4d35e1730b793881f5f434b0a", "title": "Machine learning techniques and drug design.", "abstract": "The interest in the application of machine learning techniques (MLT) as drug design tools is growing in the last decades. The reason for this is related to the fact that the drug design is very complex and requires the use of hybrid techniques. A brief review of some MLT such as self-organizing maps, multilayer perceptron, bayesian neural networks, counter-propagation neural network and support vector machines is described in this paper. A comparison between the performance of the described methods and some classical statistical methods (such as partial least squares and multiple linear regression) shows that MLT have significant advantages. Nowadays, the number of studies in medicinal chemistry that employ these techniques has considerably increased, in particular the use of support vector machines. The state of the art and the future trends of MLT applications encompass the use of these techniques to construct more reliable QSAR models. The models obtained from MLT can be used in virtual screening studies as well as filters to develop/discovery new chemicals. An important challenge in the drug design field is the prediction of pharmacokinetic and toxicity properties, which can avoid failures in the clinical phases. Therefore, this review provides a critical point of view on the main MLT and shows their potential ability as a valuable tool in drug design.", "year": 2012, "referenceCount": 131, "citationCount": 110, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "14182494", "name": "J. C. Gertrudes"}, {"authorId": "6019396", "name": "V. Maltarollo"}, {"authorId": "2190685008", "name": "R. A. Silva"}, {"authorId": "34797219", "name": "P. R. Oliveira"}, {"authorId": "2082452", "name": "K. M. Honorio"}, {"authorId": "1398462782", "name": "A. B. F. D. DA SILVA"}]}, {"paperId": "fff7a7aa9483ab4f35019f422840a6eb8b756b33", "url": "https://www.semanticscholar.org/paper/fff7a7aa9483ab4f35019f422840a6eb8b756b33", "title": "Support Vector Machines for Classification", "abstract": null, "year": 2015, "referenceCount": 0, "citationCount": 143, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144707373", "name": "M. Awad"}, {"authorId": "49862773", "name": "R. Khanna"}]}, {"paperId": "72e93aa6767ee683de7f001fa72f1314e40a8f35", "url": "https://www.semanticscholar.org/paper/72e93aa6767ee683de7f001fa72f1314e40a8f35", "title": "Building high-level features using large scale unsupervised learning", "abstract": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art.", "year": 2011, "referenceCount": 56, "citationCount": 2169, "influentialCitationCount": 108, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2827616", "name": "Quoc V. Le"}, {"authorId": "1706809", "name": "Marc'Aurelio Ranzato"}, {"authorId": "3089272", "name": "R. Monga"}, {"authorId": "145139947", "name": "Matthieu Devin"}, {"authorId": "32131713", "name": "G. Corrado"}, {"authorId": "2118440152", "name": "Kai Chen"}, {"authorId": "49959210", "name": "J. Dean"}, {"authorId": "34699434", "name": "A. Ng"}]}, {"paperId": "0e5dfca7f4d996d0dd6ab469bb06bed5619b0a65", "url": "https://www.semanticscholar.org/paper/0e5dfca7f4d996d0dd6ab469bb06bed5619b0a65", "title": "Nonvolatile Memory Materials for Neuromorphic Intelligent Machines", "abstract": "Recent progress in deep learning extends the capability of artificial intelligence to various practical tasks, making the deep neural network (DNN) an extremely versatile hypothesis. While such DNN is virtually built on contemporary data centers of the von Neumann architecture, physical (in part) DNN of non\u2010von Neumann architecture, also known as neuromorphic computing, can remarkably improve learning and inference efficiency. Particularly, resistance\u2010based nonvolatile random access memory (NVRAM) highlights its handy and efficient application to the multiply\u2013accumulate (MAC) operation in an analog manner. Here, an overview is given of the available types of resistance\u2010based NVRAMs and their technological maturity from the material\u2010 and device\u2010points of view. Examples within the strategy are subsequently addressed in comparison with their benchmarks (virtual DNN in deep learning). A spiking neural network (SNN) is another type of neural network that is more biologically plausible than the DNN. The successful incorporation of resistance\u2010based NVRAM in SNN\u2010based neuromorphic computing offers an efficient solution to the MAC operation and spike timing\u2010based learning in nature. This strategy is exemplified from a material perspective. Intelligent machines are categorized according to their architecture and learning type. Also, the functionality and usefulness of NVRAM\u2010based neuromorphic computing are addressed.", "year": 2018, "referenceCount": 204, "citationCount": 127, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Materials Science"], "authors": [{"authorId": "6302138", "name": "D. Jeong"}, {"authorId": "2557762", "name": "C. Hwang"}]}, {"paperId": "12244deb997152492d96c6246ec21b2b9804800d", "url": "https://www.semanticscholar.org/paper/12244deb997152492d96c6246ec21b2b9804800d", "title": "Text Detection and Character Recognition in Scene Images with Unsupervised Feature Learning", "abstract": "Reading text from photographs is a challenging problem that has received a significant amount of attention. Two key components of most systems are (i) text detection from images and (ii) character recognition, and many recent methods have been proposed to design better feature representations and models for both. In this paper, we apply methods recently developed in machine learning -- specifically, large-scale algorithms for learning the features automatically from unlabeled data -- and show that they allow us to construct highly effective classifiers for both detection and recognition to be used in a high accuracy end-to-end system.", "year": 2011, "referenceCount": 28, "citationCount": 407, "influentialCitationCount": 24, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144638694", "name": "Adam Coates"}, {"authorId": "37742741", "name": "Blake Carpenter"}, {"authorId": "2065131508", "name": "Carl Case"}, {"authorId": "145031342", "name": "S. Satheesh"}, {"authorId": "39086009", "name": "B. Suresh"}, {"authorId": "41154933", "name": "Tao Wang"}, {"authorId": "25629078", "name": "David J. Wu"}, {"authorId": "34699434", "name": "A. Ng"}]}, {"paperId": "ae523e2f137fa2a4f5a6cbcc443ba63db2642a96", "url": "https://www.semanticscholar.org/paper/ae523e2f137fa2a4f5a6cbcc443ba63db2642a96", "title": "Supervised Speech Separation Based on Deep Learning: An Overview", "abstract": "Speech separation is the task of separating target speech from background interference. Traditionally, speech separation is studied as a signal processing problem. A more recent approach formulates speech separation as a supervised learning problem, where the discriminative patterns of speech, speakers, and background noise are learned from training data. Over the past decade, many supervised separation algorithms have been put forward. In particular, the recent introduction of deep learning to supervised speech separation has dramatically accelerated progress and boosted separation performance. This paper provides a comprehensive overview of the research on deep learning based supervised speech separation in the last several years. We first introduce the background of speech separation and the formulation of supervised separation. Then, we discuss three main components of supervised separation: learning machines, training targets, and acoustic features. Much of the overview is on separation algorithms where we review monaural methods, including speech enhancement (speech-nonspeech separation), speaker separation (multitalker separation), and speech dereverberation, as well as multimicrophone techniques. The important issue of generalization, unique to supervised learning, is discussed. This overview provides a historical perspective on how advances are made. In addition, we discuss a number of conceptual issues, including what constitutes the target source.", "year": 2017, "referenceCount": 216, "citationCount": 861, "influentialCitationCount": 45, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "38053687", "name": "Deliang Wang"}, {"authorId": "2855690", "name": "Jitong Chen"}]}, {"paperId": "ae4b0b63ff26e52792be7f60bda3ed5db83c1577", "url": "https://www.semanticscholar.org/paper/ae4b0b63ff26e52792be7f60bda3ed5db83c1577", "title": "A Bayesian Perspective on Generalization and Stochastic Gradient Descent", "abstract": "We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al. (2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the \"noise scale\" $g = \\epsilon (\\frac{N}{B} - 1) \\approx \\epsilon N/B$, where $\\epsilon$ is the learning rate, $N$ the training set size and $B$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, $B_{opt} \\propto \\epsilon N$. We verify these predictions empirically.", "year": 2017, "referenceCount": 37, "citationCount": 282, "influentialCitationCount": 24, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2157770601", "name": "Samuel L. Smith"}, {"authorId": "2827616", "name": "Quoc V. Le"}]}, {"paperId": "021dc78bcb747888ad69f143516b12ca328d632b", "url": "https://www.semanticscholar.org/paper/021dc78bcb747888ad69f143516b12ca328d632b", "title": "Prediction Modeling Using EHR Data: Challenges, Strategies, and a Comparison of Machine Learning Approaches", "abstract": "Background:Electronic health record (EHR) databases contain vast amounts of information about patients. Machine learning techniques such as Boosting and support vector machine (SVM) can potentially identify patients at high risk for serious conditions, such as heart disease, from EHR data. However, these techniques have not yet been widely tested. Objective:To model detection of heart failure more than 6 months before the actual date of clinical diagnosis using machine learning techniques applied to EHR data. To compare the performance of logistic regression, SVM, and Boosting, along with various variable selection methods in heart failure prediction. Research Design:Geisinger Clinic primary care patients with data in the EHR data from 2001 to 2006 diagnosed with heart failure between 2003 and 2006 were identified. Controls were randomly selected matched on sex, age, and clinic for this nested case-control study. Measures:Area under the curve (AUC) of receiver operator characteristic curve was computed for each method using 10-fold cross-validation. The number of variables selected by each method was compared. Results:Logistic regression with model selection based on Bayesian information criterion provided the most parsimonious model, with about 10 variables selected on average, while maintaining a high AUC (0.77 in 10-fold cross-validation). Boosting with strict variable importance threshold provided similar performance. Conclusions:Heart failure was predicted more than 6 months before clinical diagnosis, with AUC of about 0.76, using logistic regression and Boosting. These results were achieved even with strict model selection criteria. SVM had the poorest performance, possibly because of imbalanced data.", "year": 2010, "referenceCount": 14, "citationCount": 297, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "4307937", "name": "Jionglin Wu"}, {"authorId": "34263961", "name": "J. Roy"}, {"authorId": "49523072", "name": "W. Stewart"}]}, {"paperId": "cc2a5a9d07e46484422cc7628486f31f4b2fe074", "url": "https://www.semanticscholar.org/paper/cc2a5a9d07e46484422cc7628486f31f4b2fe074", "title": "Introduction To Gaussian Processes", "abstract": "Adaptive and Natural Computing AlgorithmsAdvances in Knowledge Discovery and Data MiningLectures on Gaussian ProcessesAn introduction to continuity and related topics for general Gaussian ProcessesLarge Deviations for Gaussian QueuesGaussian Markov Random FieldsAsymptotic Methods in the Theory of Gaussian Processes and FieldsModelling and Control of Dynamic Systems Using Gaussian Process ModelsAn Introduction to Continuity, Extrema, and Related Topics for General Gaussian ProcessesThe Gaussian Approximation PotentialAn Introduction to Continuity, Extrema, and Related Topics for General Gaussian ProcessesStochastic Analysis for Gaussian Random Processes and FieldsMachine LearningNeural Networks and Machine LearningBayesian Time Series ModelsGraphical Models for Machine Learning and Digital CommunicationMarkov Processes, Gaussian Processes, and Local TimesThe Generic ChainingTime Series AnalysisProbability in Banach SpacesDark DataMachine LearningSurrogatesIntroduction and Implementations of the Kalman FilterMachine Learning and Knowledge Discovery in DatabasesLearning Kernel ClassifiersThe Concentration of Measure PhenomenonBayesian Data Analysis, Third EditionKernels for Vector-Valued FunctionsAdvanced Lectures on Machine LearningGaussian Processes for Machine LearningEfficient Reinforcement Learning Using Gaussian ProcessesGaussian Process Regression Analysis for Functional DataGaussian Processes on TreesBayesian Learning for Neural NetworksReinforcement Learning and Optimal ControlInterpolation of Spatial DataQuantum Processes Systems, and InformationHigh-Dimensional ProbabilityIntroduction to Empirical Processes and Semiparametric Inference The three volume proceedings LNAI 11906 \u2013 11908 constitutes the refereed proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases, ECML PKDD 2019, held in W\u00fcrzburg, Germany, in September 2019. The total of 130 regular papers presented in these volumes was carefully reviewed and selected from 733 submissions; there are 10 papers in the demo track. The contributions were organized in topical sections named as follows: Part I: pattern mining; clustering, anomaly and outlier detection, and autoencoders; dimensionality reduction and feature selection; social networks and graphs; decision trees, interpretability, and causality; strings and streams; privacy and security; optimization. Part II: supervised learning; multi-label learning; large-scale learning; deep learning; probabilistic models; natural language processing. Part III: reinforcement learning and bandits; ranking; applied data science: computer vision and explanation; applied data science: healthcare; applied data science: e-commerce, finance, and advertising; applied data science: rich data; applied data science: applications; demo track. Chapter \"Incorporating Dependencies in Spectral Kernels for Gaussian Processes\" is available open access under a Creative Commons Attribution 4.0 International License via link.springer.com.This book is devoted to a systematic analysis of asymptotic behavior of distributions of various typical functionals of Gaussian random variables and fields. The text begins with an extended introduction, which explains fundamental ideas and sketches the basic methods fully presented later in the book. Good approximate formulas and sharp estimates of the remainders are obtained for a large class of Gaussian and similar processes. The author devotes special attention to the development of asymptotic analysis methods, emphasizing the method of comparison, the double-sum method and the method of moments. The author has added an extended introduction and has significantly revised the text for this translation, particularly the material on the double-sum method.This monograph opens up new horizons for engineers and researchers in academia and in industry dealing with or interested in new developments in the field of system identification and control. It emphasizes guidelines for working solutions and practical advice for their implementation rather than the theoretical background of Gaussian process (GP) models. The book demonstrates the potential of this recent development in probabilistic machine-learning methods and gives the reader an intuitive understanding of the topic. The current state of the art is treated along with possible future directions for research. Systems control design relies on mathematical models and these may be developed from measurement data. This process of system identification, when based on GP models, can play an integral part of control design in data-based control and its description as such is an essential aspect of the text. The background of GP regression is introduced first with system identification and incorporation of prior knowledge then leading into full-blown control. The book is illustrated by extensive use of examples, line drawings, and graphical presentation of computer-simulation results and plant measurements. The research results presented are applied in real-life case studies drawn from successful applications including: a gas\u2013liquid separator control; urban-traffic signal modelling and reconstruction; and prediction of atmospheric ozone concentration. A MATLAB\u00ae toolbox, for identification and simulation of dynamic GP models is provided for download.Sensor data fusion is the process of combining error-prone, heterogeneous, incomplete, and ambiguous data to gather a higher level of situational awareness. In principle, all living creatures are fusing information from their complementary senses to coordinate their actions and to detect and localize danger. In sensor data fusion, this process is transferred to electronic systems, which rely on some \"awareness\" of what is happening in certain areas of interest. By means of probability theory and statistics, it is possible to model the relationship between the state space and the sensor data. The number of ingredients of the resulting Kalman filter is limited, but its applications are not.Kosorok\u2019s brilliant text provides a self-contained introduction to empirical processes and semiparametric inference. These powerful research techniques are surprisingly useful for developing methods of statistical inference for complex models and in understanding the properties of such methods. This is an authoritative text that covers all the bases, and also a friendly and gradual introduction to the area. The book can be used as research reference and textbook.The two-volume set LNAI 12084 and 12085 constitutes the thoroughly refereed proceedings of the 24th Pacific-Asia Conference on Knowledge Discovery and Data Mining, PAKDD 2020, which was due to be held in Singapore, in May 2020. The conference was held virtually due to the COVID-19 pandemic. The 135 full papers presented were carefully reviewed and selected from 628 submissions. The papers present new ideas, original research results, and practical development experiences from all KDD related areas, including data mining, data warehousing, machine learning, artificial intelligence, databases, statistics, knowledge engineering, visualization, decision-making systems, and the emerging applications. They are organized in the following topical sections: recommender systems; classification; clustering; mining social networks; representation learning and embedding; mining behavioral data; deep learning; feature extraction and selection; human, domain, organizational and social factors in data mining; mining sequential data; mining imbalanced data; association; privacy and security; supervised learning; novel algorithms; mining multimedia/multi-dimensional data; application; mining graph and network data; anomaly detection and analytics; mining spatial, temporal, unstructured and semi-structured data; sentiment analysis; statistical/graphical model; multi-source/distributed/parallel/cloud computing.Isoperimetric, measure concentration and random", "year": 2008, "referenceCount": 8, "citationCount": 535, "influentialCitationCount": 44, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "145797336", "name": "Iain Murray"}]}, {"paperId": "fcba51774867c77f491581d3625d375a0a8f473b", "url": "https://www.semanticscholar.org/paper/fcba51774867c77f491581d3625d375a0a8f473b", "title": "Estimating a Kernel Fisher Discriminant in the Presence of Label Noise", "abstract": "Data noise is present in many machine learning problems domains, some of these are well studied but others have received less attention. In this paper we propose an algorithm for constructing a kernel Fisher discriminant (KFD) from training examples with noisy labels. The approach allows to associate with each example a probability of the label being flipped. We utilise an expectation maximization (EM) algorithm for updating the probabilities. The E-step uses class conditional probabilities estimated as a by-product of the KFD algorithm. The M-step updates the flip probabilities and determines the parameters of the discriminant. We demonstrate the feasibility of the approach on two real-world data-sets.", "year": 2001, "referenceCount": 27, "citationCount": 211, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145306271", "name": "Neil D. Lawrence"}, {"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}]}, {"paperId": "591cf181a0b4ab385322053e0320ef69461d1a3f", "url": "https://www.semanticscholar.org/paper/591cf181a0b4ab385322053e0320ef69461d1a3f", "title": "Learning to detect phishing emails", "abstract": "Each month, more attacks are launched with the aim of making web users believe that they are communicating with a trusted entity for the purpose of stealing account information, logon credentials, and identity information in general. This attack method, commonly known as \"phishing,\" is most commonly initiated by sending out emails with links to spoofed websites that harvest information. We present a method for detecting these attacks, which in its most general form is an application of machine learning on a feature set designed to highlight user-targeted deception in electronic communication. This method is applicable, with slight modification, to detection of phishing websites, or the emails used to direct victims to these sites. We evaluate this method on a set of approximately 860 such phishing emails, and 6950 non-phishing emails, and correctly identify over 96% of the phishing emails while only mis-classifying on the order of 0.1% of the legitimate emails. We conclude with thoughts on the future for such techniques to specifically identify deception, specifically with respect to the evolutionary nature of the attacks and information available.", "year": 2007, "referenceCount": 30, "citationCount": 643, "influentialCitationCount": 53, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3165536", "name": "Ian Fette"}, {"authorId": "2464164", "name": "N. Sadeh"}, {"authorId": "1693125", "name": "A. Tomasic"}]}, {"paperId": "f156dafea123de94c9e197c5f123bececcfa515c", "url": "https://www.semanticscholar.org/paper/f156dafea123de94c9e197c5f123bececcfa515c", "title": "Supervised Machine-learning Predictive Analytics for Prediction of Postinduction Hypotension", "abstract": "What We Already Know about This Topic The ability to predict postinduction hypotension remains limited and challenging due to the multitude of data elements that may be considered Novel machine-learning algorithms may offer a systematic approach to predict postinduction hypotension, but are understudied What This Article Tells Us That Is New Among 13,323 patients undergoing a variety of surgical procedures, 8.9% experienced a mean arterial pressure less than 55 mmHg within 10 min of induction start While some machine-learning algorithms perform worse than logistic regression, several techniques may be superior Gradient boosting machine, with tuning, demonstrates a receiver operating characteristic area under the curve of 0.76, a negative predictive value of 19%, and positive predictive value of 96% Background: Hypotension is a risk factor for adverse perioperative outcomes. Machine-learning methods allow large amounts of data for development of robust predictive analytics. The authors hypothesized that machine-learning methods can provide prediction for the risk of postinduction hypotension. Methods: Data was extracted from the electronic health record of a single quaternary care center from November 2015 to May 2016 for patients over age 12 that underwent general anesthesia, without procedure exclusions. Multiple supervised machine-learning classification techniques were attempted, with postinduction hypotension (mean arterial pressure less than 55 mmHg within 10 min of induction by any measurement) as primary outcome, and preoperative medications, medical comorbidities, induction medications, and intraoperative vital signs as features. Discrimination was assessed using cross-validated area under the receiver operating characteristic curve. The best performing model was tuned and final performance assessed using split-set validation. Results: Out of 13,323 cases, 1,185 (8.9%) experienced postinduction hypotension. Area under the receiver operating characteristic curve using logistic regression was 0.71 (95% CI, 0.70 to 0.72), support vector machines was 0.63 (95% CI, 0.58 to 0.60), naive Bayes was 0.69 (95% CI, 0.67 to 0.69), k-nearest neighbor was 0.64 (95% CI, 0.63 to 0.65), linear discriminant analysis was 0.72 (95% CI, 0.71 to 0.73), random forest was 0.74 (95% CI, 0.73 to 0.75), neural nets 0.71 (95% CI, 0.69 to 0.71), and gradient boosting machine 0.76 (95% CI, 0.75 to 0.77). Test set area for the gradient boosting machine was 0.74 (95% CI, 0.72 to 0.77). Conclusions: The success of this technique in predicting postinduction hypotension demonstrates feasibility of machine-learning models for predictive analytics in the field of anesthesiology, with performance dependent on model selection and appropriate tuning.", "year": 2018, "referenceCount": 49, "citationCount": 98, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "7991579", "name": "S. Kendale"}, {"authorId": "50348602", "name": "Prathamesh Kulkarni"}, {"authorId": "30488552", "name": "A. Rosenberg"}, {"authorId": "2152449237", "name": "Jing Wang"}]}, {"paperId": "23b13dc758bc0bf70e783a617085283d824216b1", "url": "https://www.semanticscholar.org/paper/23b13dc758bc0bf70e783a617085283d824216b1", "title": "Efficient Learning of Selective Bayesian Network Classifiers", "abstract": "In this paper, we empirically evaluate algorithms for learning four types of Bayesian network (BN) classifiers \u2013 Na\u00efve-Bayes, tree augmented Na\u00efve-Bayes (TANs), BN augmented Na\u00efve-Bayes (BANs) and general BNs (GBNs), where the GBNs and BANs are learned using two variants of a conditionalindependence based BN-learning algorithm. Based on their performance, we then define a new type of classifier. Experimental results show the resulting classifiers, learned using the proposed learning algorithms, are competitive with (or superior to) the best classifiers, based on both Bayesian networks and other formalisms, and that the computational time for learning and using these classifiers is relatively small. These results argue that BN classifiers deserve more attention in machine learning and data mining communities.", "year": 1996, "referenceCount": 18, "citationCount": 101, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2110447955", "name": "Moninder Singh"}, {"authorId": "37838196", "name": "G. Provan"}]}, {"paperId": "2dc18f661b400033abd1086b917c451d3358aef2", "url": "https://www.semanticscholar.org/paper/2dc18f661b400033abd1086b917c451d3358aef2", "title": "Visible Machine Learning for Biomedicine", "abstract": null, "year": 2018, "referenceCount": 17, "citationCount": 98, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Biology"], "authors": [{"authorId": "21310951", "name": "M. Yu"}, {"authorId": "5214991", "name": "Jianzhu Ma"}, {"authorId": "38531806", "name": "J. Fisher"}, {"authorId": "5768509", "name": "J. Kreisberg"}, {"authorId": "35016672", "name": "Benjamin J. Raphael"}, {"authorId": "144261466", "name": "T. Ideker"}]}, {"paperId": "003da8eefbe5af381a122a329d2254bb6f9bab1d", "url": "https://www.semanticscholar.org/paper/003da8eefbe5af381a122a329d2254bb6f9bab1d", "title": "Machine Learning: Frontmatter", "abstract": null, "year": 2012, "referenceCount": 0, "citationCount": 76, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144474086", "name": "P. Flach"}]}, {"paperId": "ccecdf227475db9be6f0fa8f431c1db51e9dcd83", "url": "https://www.semanticscholar.org/paper/ccecdf227475db9be6f0fa8f431c1db51e9dcd83", "title": "Reinforcement Learning and Its Relationship to Supervised Learning", "abstract": "The modern study of approximate dynamic programming (DP) combines ideas from several research traditions. Among these is the field of Artificial Intelligence, whose earliest period focussed on creating artificial learning systems. Today, Machine Learning is an active branch of Artificial Intelligence (although it includes researchers from many other disciplines as well) devoted to continuing the development of artificial learning systems. Some of the problems studied in Machine Learning concern stochastic sequential decision processes, and some approaches to solving them are based on DP. These problems and algorithms fall under the general heading of reinforcement learning. In this chapter, we discuss stochastic sequential decision processes from the perspective of Machine Learning, focussing on reinforcement learning and its relationship to the more commmonly studied supervised learning problems. Machine Learning is the study ofmethods for constructing and improving software systems by analyzing examples of their behavior rather than by directly programming them. Machine Learning methods are appropriate in application settings where people are unable to provide precise specifications for desired program behavior, but where examples of desired behavior are available, or where it is possible to assign a measure of goodness to examples of behavior. Such situations include optical character recognition, handwriting recognition, speech recognition, automated steering of automobiles, and robot control and navigation. A key property of tasks in which examples of desired behavior are available is that people can perform them quite easily, but people cannot articulate exactly how they perform them. Hence, people", "year": 2004, "referenceCount": 53, "citationCount": 85, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46333892", "name": "J. Si"}, {"authorId": "1730590", "name": "A. Barto"}, {"authorId": "1852241", "name": "Warrren B Powell"}, {"authorId": "145033828", "name": "D. Wunsch"}]}, {"paperId": "2b50026b7b1054ef8e3643fcd7ef89d7b278a068", "url": "https://www.semanticscholar.org/paper/2b50026b7b1054ef8e3643fcd7ef89d7b278a068", "title": "Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning", "abstract": "Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.", "year": 2018, "referenceCount": 121, "citationCount": 360, "influentialCitationCount": 40, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1967156", "name": "Nicolas Papernot"}, {"authorId": "144061974", "name": "P. Mcdaniel"}]}, {"paperId": "aa4b666c04016063c8039ad01914b394a9b2f4a6", "url": "https://www.semanticscholar.org/paper/aa4b666c04016063c8039ad01914b394a9b2f4a6", "title": "Large Margin Multi-Task Metric Learning", "abstract": "Multi-task learning (MTL) improves the prediction performance on multiple, different but related, learning problems through shared parameters or representations. One of the most prominent multi-task learning algorithms is an extension to support vector machines (svm) by Evgeniou et al. [15]. Although very elegant, multi-task svm is inherently restricted by the fact that support vector machines require each class to be addressed explicitly with its own weight vector which, in a multi-task setting, requires the different learning tasks to share the same set of classes. This paper proposes an alternative formulation for multi-task learning by extending the recently published large margin nearest neighbor (1mnn) algorithm to the MTL paradigm. Instead of relying on separating hyperplanes, its decision function is based on the nearest neighbor rule which inherently extends to many classes and becomes a natural fit for multi-task learning. We evaluate the resulting multi-task 1mnn on real-world insurance data and speech classification problems and show that it consistently outperforms single-task kNN under several metrics and state-of-the-art MTL classifiers.", "year": 2010, "referenceCount": 22, "citationCount": 287, "influentialCitationCount": 30, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3003084", "name": "S. Parameswaran"}, {"authorId": "7446832", "name": "Kilian Q. Weinberger"}]}, {"paperId": "2678e213cec548d278879ceaf01582ee8913cc3f", "url": "https://www.semanticscholar.org/paper/2678e213cec548d278879ceaf01582ee8913cc3f", "title": "Classification Techniques in Machine Learning: Applications and Issues", "abstract": "Classification is a data mining (machine learning) technique used to predict group membership for data instances. There are several classification techniques that can be used for classification purpose. In this paper, we present the basic classification techniques. Later we discuss some major types of classification method including Bayesian networks, decision tree induction, k-nearest neighbor classifier and Support Vector Machines (SVM) with their strengths, weaknesses, potential applications and issues with their available solution. The goal of this study is to provide a comprehensive review of different classification techniques in machine learning. This work will be helpful for both academia and new comers in the field of machine learning to further strengthen the basis of classification methods.", "year": 2017, "referenceCount": 76, "citationCount": 96, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46225914", "name": "Aized Amin Soofi"}, {"authorId": "144674832", "name": "A. Awan"}]}, {"paperId": "c74cb2dc88baec89af714577ddafa454ab785135", "url": "https://www.semanticscholar.org/paper/c74cb2dc88baec89af714577ddafa454ab785135", "title": "Bolasso: model consistent Lasso estimation through the bootstrap", "abstract": "We consider the least-square linear regression problem with regularization by the l1-norm, a problem usually referred to as the Lasso. In this paper, we present a detailed asymptotic analysis of model consistency of the Lasso. For various decays of the regularization parameter, we compute asymptotic equivalents of the probability of correct model selection (i.e., variable selection). For a specific rate decay, we show that the Lasso selects all the variables that should enter the model with probability tending to one exponentially fast, while it selects all other variables with strictly positive probability. We show that this property implies that if we run the Lasso for several bootstrapped replications of a given sample, then intersecting the supports of the Lasso bootstrap estimates leads to consistent model selection. This novel variable selection algorithm, referred to as the Bolasso, is compared favorably to other linear regression methods on synthetic data and datasets from the UCI machine learning repository.", "year": 2008, "referenceCount": 19, "citationCount": 406, "influentialCitationCount": 76, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "144570279", "name": "F. Bach"}]}, {"paperId": "267a2a7cced527b2c279c60ab123c821ef1fd409", "url": "https://www.semanticscholar.org/paper/267a2a7cced527b2c279c60ab123c821ef1fd409", "title": "MaSh: Machine Learning for Sledgehammer", "abstract": null, "year": 2013, "referenceCount": 58, "citationCount": 76, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2944406", "name": "D. K\u00fchlwein"}, {"authorId": "1837185", "name": "J. Blanchette"}, {"authorId": "1784106", "name": "C. Kaliszyk"}, {"authorId": "2087993", "name": "J. Urban"}]}, {"paperId": "a1c922be467d1c0c64b963e65dae41778b81b2a0", "url": "https://www.semanticscholar.org/paper/a1c922be467d1c0c64b963e65dae41778b81b2a0", "title": "Deep Learning Scaling is Predictable, Empirically", "abstract": "Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. \nThis paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the \"steepness\" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.", "year": 2017, "referenceCount": 40, "citationCount": 320, "influentialCitationCount": 26, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3130228", "name": "J. Hestness"}, {"authorId": "46617804", "name": "Sharan Narang"}, {"authorId": "2774880", "name": "Newsha Ardalani"}, {"authorId": "2040049", "name": "G. Diamos"}, {"authorId": "35450887", "name": "Heewoo Jun"}, {"authorId": "7880519", "name": "Hassan Kianinejad"}, {"authorId": "8176660", "name": "Md. Mostofa Ali Patwary"}, {"authorId": "2152916796", "name": "Yang Yang"}, {"authorId": "2389316", "name": "Yanqi Zhou"}]}, {"paperId": "d67019e2c610e5b1b0e3661bc1ccc5e0e93d2476", "url": "https://www.semanticscholar.org/paper/d67019e2c610e5b1b0e3661bc1ccc5e0e93d2476", "title": "Learning in Humans and Machines", "abstract": "Discusses the analysis, comparison and integration of computational approaches to learning and research on human learning. This book aims to provide the reader with an overview of the prolific research on learning throughout the disciplines. It also highlights the important research issues and methodologies.", "year": 1995, "referenceCount": 0, "citationCount": 95, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "20779579", "name": "P. Reimann"}, {"authorId": "2926339", "name": "H. Spada"}]}, {"paperId": "af10f3c1c0859aa620623f760c8a29e78f177f7f", "url": "https://www.semanticscholar.org/paper/af10f3c1c0859aa620623f760c8a29e78f177f7f", "title": "Population Based Training of Neural Networks", "abstract": "Neural networks dominate the modern machine learning landscape, but their training and success still suffer from sensitivity to empirical choices of hyperparameters such as model architecture, loss function, and optimisation algorithm. In this work we present \\emph{Population Based Training (PBT)}, a simple asynchronous optimisation algorithm which effectively utilises a fixed computational budget to jointly optimise a population of models and their hyperparameters to maximise performance. Importantly, PBT discovers a schedule of hyperparameter settings rather than following the generally sub-optimal strategy of trying to find a single fixed set to use for the whole course of training. With just a small modification to a typical distributed hyperparameter training framework, our method allows robust and reliable training of models. We demonstrate the effectiveness of PBT on deep reinforcement learning problems, showing faster wall-clock convergence and higher final performance of agents by optimising over a suite of hyperparameters. In addition, we show the same method can be applied to supervised learning for machine translation, where PBT is used to maximise the BLEU score directly, and also to training of Generative Adversarial Networks to maximise the Inception score of generated images. In all cases PBT results in the automatic discovery of hyperparameter schedules and model selection which results in stable training and better final performance.", "year": 2017, "referenceCount": 55, "citationCount": 471, "influentialCitationCount": 82, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "3093886", "name": "Max Jaderberg"}, {"authorId": "2795508", "name": "Valentin Dalibard"}, {"authorId": "2217144", "name": "Simon Osindero"}, {"authorId": "144792148", "name": "Wojciech M. Czarnecki"}, {"authorId": "7408951", "name": "Jeff Donahue"}, {"authorId": "143653164", "name": "Ali Razavi"}, {"authorId": "1689108", "name": "Oriol Vinyals"}, {"authorId": "1484039896", "name": "Tim Green"}, {"authorId": "2768462", "name": "Iain Dunning"}, {"authorId": "34838386", "name": "K. Simonyan"}, {"authorId": "143939165", "name": "Chrisantha Fernando"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"}]}, {"paperId": "1776b5ba5d2b17f6e6a043d57d36126e2af90315", "url": "https://www.semanticscholar.org/paper/1776b5ba5d2b17f6e6a043d57d36126e2af90315", "title": "Algorithmic Learning in a Random World", "abstract": "Algorithmic Learning in a Random World describes recent theoretical and experimental developments in building computable approximations to Kolmogorov's algorithmic notion of randomness. Based on these approximations, a new set of machine learning algorithms have been developed that can be used to make predictions and to estimate their confidence and credibility in high-dimensional spaces under the usual assumption that the data are independent and identically distributed (assumption of randomness). Another aim of this unique monograph is to outline some limits of predictions: The approach based on algorithmic theory of randomness allows for the proof of impossibility of prediction in certain situations. The book describes how several important machine learning problems, such as density estimation in high-dimensional spaces, cannot be solved if the only assumption is randomness.", "year": 2005, "referenceCount": 1, "citationCount": 192, "influentialCitationCount": 30, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "145675281", "name": "V. Vovk"}, {"authorId": "1793317", "name": "A. Gammerman"}, {"authorId": "145500409", "name": "G. Shafer"}]}, {"paperId": "ad34a4b66a24f4607577da0994ecb7250957a599", "url": "https://www.semanticscholar.org/paper/ad34a4b66a24f4607577da0994ecb7250957a599", "title": "Artificial intelligence as the next step towards precision pathology", "abstract": "Pathology is the cornerstone of cancer care. The need for accuracy in histopathologic diagnosis of cancer is increasing as personalized cancer therapy requires accurate biomarker assessment. The appearance of digital image analysis holds promise to improve both the volume and precision of histomorphological evaluation. Recently, machine learning, and particularly deep learning, has enabled rapid advances in computational pathology. The integration of machine learning into routine care will be a milestone for the healthcare sector in the next decade, and histopathology is right at the centre of this revolution. Examples of potential high\u2010value machine learning applications include both model\u2010based assessment of routine diagnostic features in pathology, and the ability to extract and identify novel features that provide insights into a disease. Recent groundbreaking results have demonstrated that applications of machine learning methods in pathology significantly improves metastases detection in lymph nodes, Ki67 scoring in breast cancer, Gleason grading in prostate cancer and tumour\u2010infiltrating lymphocyte (TIL) scoring in melanoma. Furthermore, deep learning models have also been demonstrated to be able to predict status of some molecular markers in lung, prostate, gastric and colorectal cancer based on standard HE slides. Moreover, prognostic (survival outcomes) deep neural network models based on digitized HE slides have been demonstrated in several diseases, including lung cancer, melanoma and glioma. In this review, we aim to present and summarize the latest developments in digital image analysis and in the application of artificial intelligence in diagnostic pathology.", "year": 2020, "referenceCount": 120, "citationCount": 130, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "7225832", "name": "B. \u00c1cs"}, {"authorId": "3154395", "name": "M. Rantalainen"}, {"authorId": "2055412", "name": "J. Hartman"}]}, {"paperId": "96434a024a6dec6e238d357d2ba50b652c69eb22", "url": "https://www.semanticscholar.org/paper/96434a024a6dec6e238d357d2ba50b652c69eb22", "title": "Learning to Recognize Volcanoes on Venus", "abstract": null, "year": 1998, "referenceCount": 63, "citationCount": 90, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2511655", "name": "M. Burl"}, {"authorId": "1769909", "name": "L. Asker"}, {"authorId": "50860274", "name": "Padhraic Smyth"}, {"authorId": "1695784", "name": "U. Fayyad"}, {"authorId": "1690922", "name": "P. Perona"}, {"authorId": "103752485", "name": "L. Crumpler"}, {"authorId": "102243890", "name": "J. Aubele"}]}, {"paperId": "82f15be44f82665d9bd7c6654727e629798d0356", "url": "https://www.semanticscholar.org/paper/82f15be44f82665d9bd7c6654727e629798d0356", "title": "Machine learning-based quantitative texture analysis of CT images of small renal masses: Differentiation of angiomyolipoma without visible fat from renal cell carcinoma", "abstract": null, "year": 2018, "referenceCount": 41, "citationCount": 138, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "47536992", "name": "Zhichao Feng"}, {"authorId": "39871796", "name": "Pengfei Rong"}, {"authorId": "1612992235", "name": "Peng Cao"}, {"authorId": "2118411883", "name": "Qingyu Zhou"}, {"authorId": "2111465547", "name": "Wenwei Zhu"}, {"authorId": "2110155148", "name": "Zhimin Yan"}, {"authorId": "2145484365", "name": "Qianyun Liu"}, {"authorId": "2158625493", "name": "Wei Wang"}]}, {"paperId": "064cd41d323441209ce1484a9bba02a22b625088", "url": "https://www.semanticscholar.org/paper/064cd41d323441209ce1484a9bba02a22b625088", "title": "Selective Transfer Machine for Personalized Facial Action Unit Detection", "abstract": "Automatic facial action unit (AFA) detection from video is a long-standing problem in facial expression analysis. Most approaches emphasize choices of features and classifiers. They neglect individual differences in target persons. People vary markedly in facial morphology (e.g., heavy versus delicate brows, smooth versus deeply etched wrinkles) and behavior. Individual differences can dramatically influence how well generic classifiers generalize to previously unseen persons. While a possible solution would be to train person-specific classifiers, that often is neither feasible nor theoretically compelling. The alternative that we propose is to personalize a generic classifier in an unsupervised manner (no additional labels for the test subjects are required). We introduce a transductive learning method, which we refer to Selective Transfer Machine (STM), to personalize a generic classifier by attenuating person-specific biases. STM achieves this effect by simultaneously learning a classifier and re-weighting the training samples that are most relevant to the test subject. To evaluate the effectiveness of STM, we compared STM to generic classifiers and to cross-domain learning methods in three major databases: CK+, GEMEP-FERA and RU-FACS. STM outperformed generic classifiers in all.", "year": 2013, "referenceCount": 40, "citationCount": 274, "influentialCitationCount": 28, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "39336289", "name": "Wen-Sheng Chu"}, {"authorId": "143867160", "name": "F. D. L. Torre"}, {"authorId": "1737918", "name": "J. Cohn"}]}, {"paperId": "d53cd4dc81a93f496c92864509de9c9c8ca4513e", "url": "https://www.semanticscholar.org/paper/d53cd4dc81a93f496c92864509de9c9c8ca4513e", "title": "Machine Learning Methods in the Environmental Sciences: Contents", "abstract": "Machine learning methods, having originated from computational intelligence (i.e. artificial intelligence), are now ubiquitous in the environmental sciences. This is the first single-authored textbook to give a unified treatment of machine learning methods and their applications in the environmental sciences. Machine learning methods began to infiltrate the environmental sciences in the 1990s. Today, thanks to their powerful nonlinear modelling capability, they are no longer an exotic fringe species, as they are heavily used in satellite data processing, in general circulation models (GCM), in weather and climate prediction, air quality forecasting, analysis and modelling of environmental data, oceanographic and hydrological forecasting, ecological modelling, and in the monitoring of snow, ice and forests, etc. End-of-chapter review questions are included, allowing readers to develop their problem-solving skills and monitor their understanding of the material presented. An appendix lists websites available for downloading computer code and data sources. A resources website is available containing datasets for exercises, and additional material to keep the book completely up-to-date. This book presents machine learning methods and their applications in the environmental sciences (including satellite remote sensing, atmospheric science, climate science, oceanography, hydrology and ecology), written at a level suitable for beginning graduate students and advanced undergraduates. It is also valuable for researchers and practitioners in environmental sciences interested in applying these new methods to their own work.", "year": 2009, "referenceCount": 1, "citationCount": 121, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1781249", "name": "W. Hsieh"}]}, {"paperId": "0652e6120d72dc0162a17b5b0e49e9f5e12bc668", "url": "https://www.semanticscholar.org/paper/0652e6120d72dc0162a17b5b0e49e9f5e12bc668", "title": "A Meta-Instrument for Interactive, On-the-Fly Machine Learning", "abstract": "Supervised learning methods have long been used to allow musical interface designers to generate new mappings by example. We propose a method for harnessing machine learning algorithms within a radically interactive paradigm, in which the designer may repeatedly generate examples, train a learner, evaluate outcomes, and modify parameters in real-time within a single software environment. We describe our meta-instrument, the Wekinator, which allows a user to engage in on-the-fly learning using arbitrary control modalities and sound synthesis environments. We provide details regarding the system implementation and discuss our experiences using the Wekinator for experimentation and performance.", "year": 2009, "referenceCount": 22, "citationCount": 126, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1745615", "name": "R. Fiebrink"}, {"authorId": "52163445", "name": "D. Trueman"}, {"authorId": "1716507", "name": "P. Cook"}]}, {"paperId": "bb56bc6676fb88d67a071fd482a349286e20b16d", "url": "https://www.semanticscholar.org/paper/bb56bc6676fb88d67a071fd482a349286e20b16d", "title": "A Survey of ReRAM-Based Architectures for Processing-In-Memory and Neural Networks", "abstract": "As data movement operations and power-budget become key bottlenecks in the design of computing systems, the interest in unconventional approaches such as processing-in-memory (PIM), machine learning (ML), and especially neural network (NN)-based accelerators has grown significantly. Resistive random access memory (ReRAM) is a promising technology for efficiently architecting PIM- and NN-based accelerators due to its capabilities to work as both: High-density/low-energy storage and in-memory computation/search engine. In this paper, we present a survey of techniques for designing ReRAM-based PIM and NN architectures. By classifying the techniques based on key parameters, we underscore their similarities and differences. This paper will be valuable for computer architects, chip designers and researchers in the area of machine learning.", "year": 2018, "referenceCount": 40, "citationCount": 90, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "38918006", "name": "Sparsh Mittal"}]}, {"paperId": "c608337d64ea3240d4f1978d2a82710867e2d42a", "url": "https://www.semanticscholar.org/paper/c608337d64ea3240d4f1978d2a82710867e2d42a", "title": "Support vector learning for ordinal regression", "abstract": "We investigate the problem of predicting variables of ordinal scale. This task is referred to as ordinal regression and is complementary to the standard machine learning tasks of classification and metric regression. In contrast to statistical models we present a distribution independent formulation of the problem together with uniform bounds of the risk functional. The approach presented is based on a mapping from objects to scalar utility values. Similar to support vector methods we derive a new learning algorithm for the task of ordinal regression based on large margin rank boundaries. We give experimental results for an information retrieval task: learning the order of documents with respect to an initial query. Experimental results indicate that the presented algorithm outperforms more naive approaches to ordinal regression such as support vector classification and support vector regression in the case of more than two ranks.", "year": 1999, "referenceCount": 22, "citationCount": 488, "influentialCitationCount": 65, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "3234984", "name": "R. Herbrich"}, {"authorId": "1686971", "name": "T. Graepel"}, {"authorId": "1743272", "name": "K. Obermayer"}]}, {"paperId": "bf7dcbee272428a2aa3c534200743ff7ab2047f8", "url": "https://www.semanticscholar.org/paper/bf7dcbee272428a2aa3c534200743ff7ab2047f8", "title": "MASON: A Multiagent Simulation Environment", "abstract": "MASON is a fast, easily extensible, discrete-event multi-agent simulation toolkit in Java, designed to serve as the basis for a wide range of multi-agent simulation tasks ranging from swarm robotics to machine learning to social complexity environments. MASON carefully delineates between model and visualization, allowing models to be dynamically detached from or attached to visualizers, and to change platforms mid-run. This paper describes the MASON system, its motivation, and its basic architectural design. It then compares MASON to related multi-agent libraries in the public domain, and discusses six applications of the system built over the past year which suggest its breadth of utility.", "year": 2005, "referenceCount": 22, "citationCount": 937, "influentialCitationCount": 78, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1706276", "name": "S. Luke"}, {"authorId": "1403820685", "name": "C. Cioffi-Revilla"}, {"authorId": "1703826", "name": "Liviu Panait"}, {"authorId": "153850291", "name": "Keith Sullivan"}, {"authorId": "35149981", "name": "G. Balan"}]}, {"paperId": "808073f5802d627439ec981c1e7f361e67e624b9", "url": "https://www.semanticscholar.org/paper/808073f5802d627439ec981c1e7f361e67e624b9", "title": "The Indian Buffet Process: An Introduction and Review", "abstract": "The Indian buffet process is a stochastic process defining a probability distribution over equivalence classes of sparse binary matrices with a finite number of rows and an unbounded number of columns. This distribution is suitable for use as a prior in probabilistic models that represent objects using a potentially infinite array of features, or that involve bipartite graphs in which the size of at least one class of nodes is unknown. We give a detailed derivation of this distribution, and illustrate its use as a prior in an infinite latent feature model. We then review recent applications of the Indian buffet process in machine learning, discuss its extensions, and summarize its connections to other stochastic processes.", "year": 2011, "referenceCount": 78, "citationCount": 405, "influentialCitationCount": 85, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1799860", "name": "T. Griffiths"}, {"authorId": "1744700", "name": "Zoubin Ghahramani"}]}, {"paperId": "aaee5d92d738f9fd175f0eac5d09ca150ab0fe34", "url": "https://www.semanticscholar.org/paper/aaee5d92d738f9fd175f0eac5d09ca150ab0fe34", "title": "Ouroboros: early identification of at-risk students without models based on legacy data", "abstract": "This paper focuses on the problem of identifying students, who are at risk of failing their course. The presented method proposes a solution in the absence of data from previous courses, which are usually used for training machine learning models. This situation typically occurs in new courses. We present the concept of a \"self-learner\" that builds the machine learning models from the data generated during the current course. The approach utilises information about already submitted assessments, which introduces the problem of imbalanced data for training and testing the classification models. There are three main contributions of this paper: (1) the concept of training the models for identifying at-risk students using data from the current course, (2) specifying the problem as a classification task, and (3) tackling the challenge of imbalanced data, which appears both in training and testing data. The results show the comparison with the traditional approach of learning the models from the legacy course data, validating the proposed concept.", "year": 2017, "referenceCount": 36, "citationCount": 86, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2358235", "name": "Martin Hlosta"}, {"authorId": "1964006", "name": "Z. Zdr\u00e1hal"}, {"authorId": "2392127", "name": "J. Zendulka"}]}, {"paperId": "1c990e329b7b36be9bc061a9d68dbcf37181f0c5", "url": "https://www.semanticscholar.org/paper/1c990e329b7b36be9bc061a9d68dbcf37181f0c5", "title": "Quantum Machine Learning over Infinite Dimensions.", "abstract": "Machine learning is a fascinating and exciting field within computer science. Recently, this excitement has been transferred to the quantum information realm. Currently, all proposals for the quantum version of machine learning utilize the finite-dimensional substrate of discrete variables. Here we generalize quantum machine learning to the more complex, but still remarkably practical, infinite-dimensional systems. We present the critical subroutines of quantum machine learning algorithms for an all-photonic continuous-variable quantum computer that can lead to exponential speedups in situations where classical algorithms scale polynomially. Finally, we also map out an experimental implementation which can be used as a blueprint for future photonic demonstrations.", "year": 2016, "referenceCount": 35, "citationCount": 65, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Physics"], "authors": [{"authorId": "121325140", "name": "Hoi-Kwan Lau"}, {"authorId": "34296116", "name": "R. Pooser"}, {"authorId": "3433061", "name": "G. Siopsis"}, {"authorId": "2260031", "name": "C. Weedbrook"}]}, {"paperId": "46d1888ae8647f59b75c486dc9b16115f0337b04", "url": "https://www.semanticscholar.org/paper/46d1888ae8647f59b75c486dc9b16115f0337b04", "title": "A Critical Review of Machine Learning of Energy Materials", "abstract": "Machine learning (ML) is rapidly revolutionizing many fields and is starting to change landscapes for physics and chemistry. With its ability to solve complex tasks autonomously, ML is being exploited as a radically new way to help find material correlations, understand materials chemistry, and accelerate the discovery of materials. Here, an in\u2010depth review of the application of ML to energy materials, including rechargeable alkali\u2010ion batteries, photovoltaics, catalysts, thermoelectrics, piezoelectrics, and superconductors, is presented. A conceptual framework is first provided for ML in materials science, with a broad overview of different ML techniques as well as best practices. This is followed by a critical discussion of how ML is applied in energy materials. This review is concluded with the perspectives on major challenges and opportunities in this exciting field.", "year": 2020, "referenceCount": 326, "citationCount": 190, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Materials Science"], "authors": [{"authorId": "143915066", "name": "Chi Chen"}, {"authorId": "11844524", "name": "Yunxing Zuo"}, {"authorId": "15698847", "name": "Weike Ye"}, {"authorId": "2108568792", "name": "Xiang-Guo Li"}, {"authorId": "98796862", "name": "Z. Deng"}, {"authorId": "2381325", "name": "S. Ong"}]}, {"paperId": "74063993638ae987572d675ce140b72241b8b944", "url": "https://www.semanticscholar.org/paper/74063993638ae987572d675ce140b72241b8b944", "title": "Advances of Four Machine Learning Methods for Spatial Data Handling: a Review", "abstract": null, "year": 2020, "referenceCount": 149, "citationCount": 60, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1882431", "name": "Peijun Du"}, {"authorId": "29906525", "name": "X. Bai"}, {"authorId": "2013432115", "name": "Kun Tan"}, {"authorId": "12874668", "name": "Zhaohui Xue"}, {"authorId": "40087450", "name": "A. Samat"}, {"authorId": "2061313", "name": "J. Xia"}, {"authorId": "30873409", "name": "Erzhu Li"}, {"authorId": "2243292", "name": "H. Su"}, {"authorId": "2026504639", "name": "Wei Liu"}]}, {"paperId": "995277edd6cdb097a163144e0d94462babfd777b", "url": "https://www.semanticscholar.org/paper/995277edd6cdb097a163144e0d94462babfd777b", "title": "Fast machine-learning online optimization of ultra-cold-atom experiments", "abstract": null, "year": 2015, "referenceCount": 91, "citationCount": 149, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Medicine", "Computer Science"], "authors": [{"authorId": "46474863", "name": "P. Wigley"}, {"authorId": "103080067", "name": "P. Everitt"}, {"authorId": "2064059868", "name": "A. van den Hengel"}, {"authorId": "39699652", "name": "J. Bastian"}, {"authorId": "47664178", "name": "M. A. Sooriyabandara"}, {"authorId": "145383991", "name": "G. McDonald"}, {"authorId": "37157384", "name": "K. Hardman"}, {"authorId": "6108268", "name": "C. Quinlivan"}, {"authorId": "47752937", "name": "P. Manju"}, {"authorId": "46456359", "name": "C. Kuhn"}, {"authorId": "2059067361", "name": "I. Petersen"}, {"authorId": "2197173", "name": "A. Luiten"}, {"authorId": "27756039", "name": "J. Hope"}, {"authorId": "3244532", "name": "N. Robins"}, {"authorId": "2703011", "name": "M. Hush"}]}, {"paperId": "19334a3ba2c1b4e9a37bac9f498280da28be7f62", "url": "https://www.semanticscholar.org/paper/19334a3ba2c1b4e9a37bac9f498280da28be7f62", "title": "Machine learning-based imaging system for surface defect inspection", "abstract": null, "year": 2016, "referenceCount": 19, "citationCount": 164, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "46213951", "name": "Je-Kang Park"}, {"authorId": "3330512", "name": "Bae-keun Kwon"}, {"authorId": "2109138892", "name": "Jun-Hyub Park"}, {"authorId": "39732859", "name": "D. Kang"}]}, {"paperId": "4baee71da9b4e8ea1feec5bbed8249eed670272e", "url": "https://www.semanticscholar.org/paper/4baee71da9b4e8ea1feec5bbed8249eed670272e", "title": "Machine Learning Methods for Property Prediction in Chemoinformatics: Quo Vadis?", "abstract": "This paper is focused on modern approaches to machine learning, most of which are as yet used infrequently or not at all in chemoinformatics. Machine learning methods are characterized in terms of the \"modes of statistical inference\" and \"modeling levels\" nomenclature and by considering different facets of the modeling with respect to input/ouput matching, data types, models duality, and models inference. Particular attention is paid to new approaches and concepts that may provide efficient solutions of common problems in chemoinformatics: improvement of predictive performance of structure-property (activity) models, generation of structures possessing desirable properties, model applicability domain, modeling of properties with functional endpoints (e.g., phase diagrams and dose-response curves), and accounting for multiple molecular species (e.g., conformers or tautomers).", "year": 2012, "referenceCount": 305, "citationCount": 173, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1685042", "name": "A. Varnek"}, {"authorId": "2539452", "name": "I. Baskin"}]}, {"paperId": "1b1d097926a976114afbe2026cc13f1bcbec72e1", "url": "https://www.semanticscholar.org/paper/1b1d097926a976114afbe2026cc13f1bcbec72e1", "title": "Classification and Learning Methods for Character Recognition: Advances and Remaining Problems", "abstract": null, "year": 2008, "referenceCount": 81, "citationCount": 56, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1689269", "name": "Cheng-Lin Liu"}, {"authorId": "34878566", "name": "H. Fujisawa"}]}, {"paperId": "59a916cdc943f0282908e6f3fa0360f4c5fb78d0", "url": "https://www.semanticscholar.org/paper/59a916cdc943f0282908e6f3fa0360f4c5fb78d0", "title": "Stabilizing Transformers for Reinforcement Learning", "abstract": "Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.", "year": 2019, "referenceCount": 57, "citationCount": 164, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3166516", "name": "Emilio Parisotto"}, {"authorId": "2107148568", "name": "H. F. Song"}, {"authorId": "34269227", "name": "Jack W. Rae"}, {"authorId": "1996134", "name": "Razvan Pascanu"}, {"authorId": "1854385", "name": "\u00c7aglar G\u00fcl\u00e7ehre"}, {"authorId": "35880964", "name": "Siddhant M. Jayakumar"}, {"authorId": "3093886", "name": "Max Jaderberg"}, {"authorId": "31713635", "name": "Raphael Lopez Kaufman"}, {"authorId": "31993415", "name": "Aidan Clark"}, {"authorId": "30155667", "name": "Seb Noury"}, {"authorId": "46378362", "name": "M. Botvinick"}, {"authorId": "2801204", "name": "N. Heess"}, {"authorId": "2315504", "name": "R. Hadsell"}]}, {"paperId": "18bc1d4271abe8dd6e16179cdb06524a4f396e16", "url": "https://www.semanticscholar.org/paper/18bc1d4271abe8dd6e16179cdb06524a4f396e16", "title": "Snorkel: Rapid Training Data Creation with Weak Supervision", "abstract": "Labeling training data is increasingly the largest bottleneck in deploying machine learning systems. We present Snorkel, a first-of-its-kind system that enables users to train state-of- the-art models without hand labeling any training data. Instead, users write labeling functions that express arbitrary heuristics, which can have unknown accuracies and correlations. Snorkel denoises their outputs without access to ground truth by incorporating the first end-to-end implementation of our recently proposed machine learning paradigm, data programming. We present a flexible interface layer for writing labeling functions based on our experience over the past year collaborating with companies, agencies, and research labs. In a user study, subject matter experts build models 2.8\u00d7 faster and increase predictive performance an average 45.5% versus seven hours of hand labeling. We study the modeling tradeoffs in this new setting and propose an optimizer for automating tradeoff decisions that gives up to 1.8\u00d7 speedup per pipeline execution. In two collaborations, with the U.S. Department of Veterans Affairs and the U.S. Food and Drug Administration, and on four open-source text and image data sets representative of other deployments, Snorkel provides 132% average improvements to predictive performance over prior heuristic approaches and comes within an average 3.60% of the predictive performance of large hand-curated training sets.", "year": 2017, "referenceCount": 68, "citationCount": 715, "influentialCitationCount": 120, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"], "authors": [{"authorId": "143711421", "name": "Alexander J. Ratner"}, {"authorId": "2870504", "name": "Stephen H. Bach"}, {"authorId": "33918804", "name": "Henry R. Ehrenberg"}, {"authorId": "31592365", "name": "Jason Alan Fries"}, {"authorId": "144766615", "name": "Sen Wu"}, {"authorId": "2114485554", "name": "C. R\u00e9"}]}, {"paperId": "2ac11a0b84580e88bb8c6e5edb38c92fe1456864", "url": "https://www.semanticscholar.org/paper/2ac11a0b84580e88bb8c6e5edb38c92fe1456864", "title": "How to steal a machine learning classifier with deep learning", "abstract": "This paper presents an exploratory machine learning attack based on deep learning to infer the functionality of an arbitrary classifier by polling it as a black box, and using returned labels to build a functionally equivalent machine. Typically, it is costly and time consuming to build a classifier, because this requires collecting training data (e.g., through crowdsourcing), selecting a suitable machine learning algorithm (through extensive tests and using domain-specific knowledge), and optimizing the underlying hyperparameters (applying a good understanding of the classifier's structure). In addition, all this information is typically proprietary and should be protected. With the proposed black-box attack approach, an adversary can use deep learning to reliably infer the necessary information by using labels previously obtained from the classifier under attack, and build a functionally equivalent machine learning classifier without knowing the type, structure or underlying parameters of the original classifier. Results for a text classification application demonstrate that deep learning can infer Naive Bayes and SVM classifiers with high accuracy and steal their functionalities. This new attack paradigm with deep learning introduces additional security challenges for online machine learning algorithms and raises the need for novel mitigation strategies to counteract the high fidelity inference capability of deep learning.", "year": 2017, "referenceCount": 15, "citationCount": 68, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2118897349", "name": "Yi Shi"}, {"authorId": "3099456", "name": "Y. Sagduyu"}, {"authorId": "143960345", "name": "Alexander Grushin"}]}, {"paperId": "8f7a7aea33c942e626832f014a831361b7fa39c7", "url": "https://www.semanticscholar.org/paper/8f7a7aea33c942e626832f014a831361b7fa39c7", "title": "PotentialNet for Molecular Property Prediction", "abstract": "The arc of drug discovery entails a multiparameter optimization problem spanning vast length scales. The key parameters range from solubility (angstroms) to protein\u2013ligand binding (nanometers) to in vivo toxicity (meters). Through feature learning\u2014instead of feature engineering\u2014deep neural networks promise to outperform both traditional physics-based and knowledge-based machine learning models for predicting molecular properties pertinent to drug discovery. To this end, we present the PotentialNet family of graph convolutions. These models are specifically designed for and achieve state-of-the-art performance for protein\u2013ligand binding affinity. We further validate these deep neural networks by setting new standards of performance in several ligand-based tasks. In parallel, we introduce a new metric, the Regression Enrichment Factor EF\u03c7(R), to measure the early enrichment of computational models for chemical data. Finally, we introduce a cross-validation strategy based on structural homology clustering that can more accurately measure model generalizability, which crucially distinguishes the aims of machine learning for drug discovery from standard machine learning tasks.", "year": 2018, "referenceCount": 60, "citationCount": 210, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "5932099", "name": "Evan N. Feinberg"}, {"authorId": "39460558", "name": "Debnil Sur"}, {"authorId": "9957625", "name": "Zhenqin Wu"}, {"authorId": "8751433", "name": "B. Husic"}, {"authorId": "80646261", "name": "Huanghao Mai"}, {"authorId": "2154902070", "name": "Yang Li"}, {"authorId": "51248807", "name": "Saisai Sun"}, {"authorId": "49500220", "name": "Jianyi Yang"}, {"authorId": "2378027", "name": "Bharath Ramsundar"}, {"authorId": "1806271", "name": "V. Pande"}]}, {"paperId": "c4706f9fd62527c8576613fbcdb266559c08b405", "url": "https://www.semanticscholar.org/paper/c4706f9fd62527c8576613fbcdb266559c08b405", "title": "A Review of Machine Learning and IoT in Smart Transportation", "abstract": "With the rise of the Internet of Things (IoT), applications have become smarter and connected devices give rise to their exploitation in all aspects of a modern city. As the volume of the collected data increases, Machine Learning (ML) techniques are applied to further enhance the intelligence and the capabilities of an application. The field of smart transportation has attracted many researchers and it has been approached with both ML and IoT techniques. In this review, smart transportation is considered to be an umbrella term that covers route optimization, parking, street lights, accident prevention/detection, road anomalies, and infrastructure applications. The purpose of this paper is to make a self-contained review of ML techniques and IoT applications in Intelligent Transportation Systems (ITS) and obtain a clear view of the trends in the aforementioned fields and spot possible coverage needs. From the reviewed articles it becomes profound that there is a possible lack of ML coverage for the Smart Lighting Systems and Smart Parking applications. Additionally, route optimization, parking, and accident/detection tend to be the most popular ITS applications among researchers.", "year": 2019, "referenceCount": 80, "citationCount": 199, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "112959015", "name": "Fotios Zantalis"}, {"authorId": "2884543", "name": "G. Koulouras"}, {"authorId": "1696951", "name": "S. Karabetsos"}, {"authorId": "2075010", "name": "Dionisis Kandris"}]}, {"paperId": "83e89d56d0d0e1dfd8b52213e6cc2e191aaaf34b", "url": "https://www.semanticscholar.org/paper/83e89d56d0d0e1dfd8b52213e6cc2e191aaaf34b", "title": "Machine Learning on Graphs: A Model and Comprehensive Taxonomy", "abstract": "There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.", "year": 2020, "referenceCount": 171, "citationCount": 141, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3442125", "name": "Ines Chami"}, {"authorId": "1389570466", "name": "Sami Abu-El-Haija"}, {"authorId": "2271808", "name": "Bryan Perozzi"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}, {"authorId": "1702318", "name": "K. Murphy"}]}, {"paperId": "a9fee459ed211f53bfadef22e3ab774d0e927358", "url": "https://www.semanticscholar.org/paper/a9fee459ed211f53bfadef22e3ab774d0e927358", "title": "Computing Semantic Relatedness Using Wikipedia-based Explicit Semantic Analysis", "abstract": "Computing semantic relatedness of natural language texts requires access to vast amounts of common-sense and domain-specific world knowledge. We propose Explicit Semantic Analysis (ESA), a novel method that represents the meaning of texts in a high-dimensional space of concepts derived from Wikipedia. We use machine learning techniques to explicitly represent the meaning of any text as a weighted vector of Wikipedia-based concepts. Assessing the relatedness of texts in this space amounts to comparing the corresponding vectors using conventional metrics (e.g., cosine). Compared with the previous state of the art, using ESA results in substantial improvements in correlation of computed relatedness scores with human judgments: from r = 0.56 to 0.75 for individual words and from r = 0.60 to 0.72 for texts. Importantly, due to the use of natural concepts, the ESA model is easy to explain to human users.", "year": 2007, "referenceCount": 48, "citationCount": 2353, "influentialCitationCount": 377, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1718798", "name": "E. Gabrilovich"}, {"authorId": "2309269", "name": "Shaul Markovitch"}]}, {"paperId": "1a270a3e6a014943845660dc0630763deb6d4d09", "url": "https://www.semanticscholar.org/paper/1a270a3e6a014943845660dc0630763deb6d4d09", "title": "Prediction Models of Functional Outcomes for Individuals in the Clinical High-Risk State for Psychosis or With Recent-Onset Depression: A Multimodal, Multisite Machine Learning Analysis", "abstract": "Importance Social and occupational impairments contribute to the burden of psychosis and depression. There is a need for risk stratification tools to inform personalized functional-disability preventive strategies for individuals in at-risk and early phases of these illnesses. Objective To determine whether predictors associated with social and role functioning can be identified in patients in clinical high-risk (CHR) states for psychosis or with recent-onset depression (ROD) using clinical, imaging-based, and combined machine learning; assess the geographic, transdiagnostic, and prognostic generalizability of machine learning and compare it with human prognostication; and explore sequential prognosis encompassing clinical and combined machine learning. Design, Setting, and Participants This multisite naturalistic study followed up patients in CHR states, with ROD, and with recent-onset psychosis, and healthy control participants for 18 months in 7 academic early-recognition services in 5 European countries. Participants were recruited between February 2014 and May 2016, and data were analyzed from April 2017 to January 2018. ain Outcomes and Measures Performance and generalizability of prognostic models. Results A total of 116 individuals in CHR states (mean [SD] age, 24.0 [5.1] years; 58 [50.0%] female) and 120 patients with ROD (mean [SD] age, 26.1 [6.1] years; 65 [54.2%] female) were followed up for a mean (SD) of 329 (142) days. Machine learning predicted the 1-year social-functioning outcomes with a balanced accuracy of 76.9% of patients in CHR states and 66.2% of patients with ROD using clinical baseline data. Balanced accuracy in models using structural neuroimaging was 76.2% in patients in CHR states and 65.0% in patients with ROD, and in combined models, it was 82.7% for CHR states and 70.3% for ROD. Lower functioning before study entry was a transdiagnostic predictor. Medial prefrontal and temporo-parieto-occipital gray matter volume (GMV) reductions and cerebellar and dorsolateral prefrontal GMV increments had predictive value in the CHR group; reduced mediotemporal and increased prefrontal-perisylvian GMV had predictive value in patients with ROD. Poor prognoses were associated with increased risk of psychotic, depressive, and anxiety disorders at follow-up in patients in the CHR state but not ones with ROD. Machine learning outperformed expert prognostication. Adding neuroimaging machine learning to clinical machine learning provided a 1.9-fold increase of prognostic certainty in uncertain cases of patients in CHR states, and a 10.5-fold increase of prognostic certainty for patients with ROD. Conclusions and Relevance Precision medicine tools could augment effective therapeutic strategies aiming at the prevention of social functioning impairments in patients with CHR states or with ROD.", "year": 2018, "referenceCount": 130, "citationCount": 122, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3214043", "name": "N. Koutsouleris"}, {"authorId": "1414119219", "name": "L. Kambeitz-Ilankovic"}, {"authorId": "1982479", "name": "S. Ruhrmann"}, {"authorId": "40892922", "name": "M. Rosen"}, {"authorId": "32098768", "name": "A. Ruef"}, {"authorId": "5672681", "name": "D. Dwyer"}, {"authorId": "5250359", "name": "M. Paolini"}, {"authorId": "15568397", "name": "Katharine Chisholm"}, {"authorId": "2876416", "name": "J. Kambeitz"}, {"authorId": "40900560", "name": "T. Haidl"}, {"authorId": "12462273", "name": "A. Schmidt"}, {"authorId": "40210909", "name": "J. Gillam"}, {"authorId": "1398034341", "name": "F. Schultze-Lutter"}, {"authorId": "2660554", "name": "P. Falkai"}, {"authorId": "145371637", "name": "M. Reiser"}, {"authorId": "146173036", "name": "A. Riecher-R\u00f6ssler"}, {"authorId": "4361455", "name": "R. Upthegrove"}, {"authorId": "34718106", "name": "J. Hietala"}, {"authorId": "6257052", "name": "R. Salokangas"}, {"authorId": "8770059", "name": "C. Pantelis"}, {"authorId": "2875471", "name": "E. Meisenzahl"}, {"authorId": "2086924", "name": "S. Wood"}, {"authorId": "2423864", "name": "D. Beque"}, {"authorId": "3110755", "name": "P. Brambilla"}, {"authorId": "48682353", "name": "S. Borgwardt"}]}, {"paperId": "230579a14d54ae00073d6c3522ffcef313320be9", "url": "https://www.semanticscholar.org/paper/230579a14d54ae00073d6c3522ffcef313320be9", "title": "Compression of Neural Machine Translation Models via Pruning", "abstract": "Neural Machine Translation (NMT), like many other deep learning domains, typically suffers from over-parameterization, resulting in large storage sizes. This paper examines three simple magnitude-based pruning schemes to compress NMT models, namely class-blind, class-uniform, and class-distribution, which differ in terms of how pruning thresholds are computed for the different classes of weights in the NMT architecture. We demonstrate the efficacy of weight pruning as a compression technique for a state-of-the-art NMT system. We show that an NMT model with over 200 million parameters can be pruned by 40% with very little performance loss as measured on the WMT'14 English-German translation task. This sheds light on the distribution of redundancy in the NMT architecture. Our main result is that with retraining, we can recover and even surpass the original performance with an 80%-pruned model.", "year": 2016, "referenceCount": 36, "citationCount": 190, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "13070498", "name": "A. See"}, {"authorId": "1707242", "name": "Minh-Thang Luong"}, {"authorId": "144783904", "name": "Christopher D. Manning"}]}, {"paperId": "9f6c28b20458f7f7de0989c2f8296d67162610f2", "url": "https://www.semanticscholar.org/paper/9f6c28b20458f7f7de0989c2f8296d67162610f2", "title": "Chinese Word Segmentation as Character Tagging", "abstract": "In this paper we report results of a supervised machine-learning approach to Chinese word segmentation. A maximum entropy tagger is trained on manually annotated data to automatically assign to Chinese characters, or hanzi, tags that indicate the position of a hanzi within a word. The tagged output is then converted into segmented text for evaluation. Preliminary results show that this approach is competitive against other supervised machine-learning segmenters reported in previous studies, achieving precision and recall rates of 95.01% and 94.94% respectively, trained on a 237K-word training set.", "year": 2003, "referenceCount": 20, "citationCount": 185, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1702849", "name": "Nianwen Xue"}]}, {"paperId": "338a891907dce447da9a0fa2f27221bd35164163", "url": "https://www.semanticscholar.org/paper/338a891907dce447da9a0fa2f27221bd35164163", "title": "Mining the peanut gallery: opinion extraction and semantic classification of product reviews", "abstract": "The web contains a wealth of product reviews, but sifting through them is a daunting task. Ideally, an opinion mining tool would process a set of search results for a given item, generating a list of product attributes (quality, features, etc.) and aggregating opinions about each of them (poor, mixed, good). We begin by identifying the unique properties of this problem and develop a method for automatically distinguishing between positive and negative reviews. Our classifier draws on information retrieval techniques for feature extraction and scoring, and the results for various metrics and heuristics vary depending on the testing situation. The best methods work as well as or better than traditional machine learning. When operating on individual sentences collected from web searches, performance is limited due to noise and ambiguity. But in the context of a complete web-based tool and aided by a simple method for grouping sentences into attributes, the results are qualitatively quite useful.", "year": 2003, "referenceCount": 33, "citationCount": 2380, "influentialCitationCount": 112, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "31744438", "name": "Kushal Dave"}, {"authorId": "145840115", "name": "S. Lawrence"}, {"authorId": "1766638", "name": "D. Pennock"}]}, {"paperId": "e7d6c69fe45659e3698bd990d6a369e39bfb6b2f", "url": "https://www.semanticscholar.org/paper/e7d6c69fe45659e3698bd990d6a369e39bfb6b2f", "title": "Voltage Stability Prediction Using Active Machine Learning", "abstract": "An active machine learning technique for monitoring the voltage stability in transmission systems is presented. It has been shown that machine learning algorithms may be used to supplement the traditional simulation approach, but they suffer from the difficulties of online machine learning model update and offline training data preparation. We propose an active learning solution to enhance existing machine learning applications by actively interacting with the online prediction and offline training process. The technique identifies operating points where machine learning predictions based on power system measurements contradict with actual system conditions. By creating the training set around the identified operating points, it is possible to improve the capability of machine learning tools to predict future power system states. The technique also accelerates the offline training process by reducing the amount of simulations on a detailed power system model around operating points where correct predictions are made. Experiments show a significant advantage in relation to the training time, prediction time, and number of measurements that need to be queried to achieve high prediction accuracy.", "year": 2017, "referenceCount": 33, "citationCount": 71, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2279271", "name": "V. Malbasa"}, {"authorId": "46882254", "name": "Ce Zheng"}, {"authorId": "2527605", "name": "Po-Chen Chen"}, {"authorId": "47398218", "name": "T. Popovic"}, {"authorId": "2998246", "name": "M. Kezunovic"}]}, {"paperId": "30b24f45a0b5ddc4303a30ad2e3979a16b3f2b16", "url": "https://www.semanticscholar.org/paper/30b24f45a0b5ddc4303a30ad2e3979a16b3f2b16", "title": "Introduction to statistical relational learning", "abstract": "Handling inherent uncertainty and exploiting compositional structure are fundamental to understanding and designing large-scale systems. Statistical relational learning builds on ideas from probability theory and statistics to address uncertainty while incorporating tools from logic, databases and programming languages to represent structure. In Introduction to Statistical Relational Learning, leading researchers in this emerging area of machine learning describe current formalisms, models, and algorithms that enable effective and robust reasoning about richly structured systems and data. The early chapters provide tutorials for material used in later chapters, offering introductions to representation, inference and learning in graphical models, and logic. The book then describes object-oriented approaches, including probabilistic relational models, relational Markov networks, and probabilistic entity-relationship models as well as logic-based formalisms including Bayesian logic programs, Markov logic, and stochastic logic programs. Later chapters discuss such topics as probabilistic models with unknown objects, relational dependency networks, reinforcement learning in relational domains, and information extraction. By presenting a variety of approaches, the book highlights commonalities and clarifies important differences among proposed approaches and, along the way, identifies important representational and algorithmic issues. Numerous applications are provided throughout.Lise Getoor is Assistant Professor in the Department of Computer Science at the University of Maryland. Ben Taskar is Assistant Professor in the Computer and Information Science Department at the University of Pennsylvania.", "year": 2007, "referenceCount": 11, "citationCount": 593, "influentialCitationCount": 42, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1746034", "name": "L. Getoor"}, {"authorId": "1685978", "name": "B. Taskar"}]}, {"paperId": "feaeda74182d7f9890a6ff5a1733ee21a288cbd3", "url": "https://www.semanticscholar.org/paper/feaeda74182d7f9890a6ff5a1733ee21a288cbd3", "title": "Electronic skins and machine learning for intelligent soft robots", "abstract": "Developments in e-skins and machine learning may achieve tactile sensing and proprioception for autonomous, deployable soft robots. Soft robots have garnered interest for real-world applications because of their intrinsic safety embedded at the material level. These robots use deformable materials capable of shape and behavioral changes and allow conformable physical contact for manipulation. Yet, with the introduction of soft and stretchable materials to robotic systems comes a myriad of challenges for sensor integration, including multimodal sensing capable of stretching, embedment of high-resolution but large-area sensor arrays, and sensor fusion with an increasing volume of data. This Review explores the emerging confluence of e-skins and machine learning, with a focus on how roboticists can combine recent developments from the two fields to build autonomous, deployable soft robots, integrated with capabilities for informative touch and proprioception to stand up to the challenges of real-world environments.", "year": 2020, "referenceCount": 145, "citationCount": 170, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "40030879", "name": "Benjamin Shih"}, {"authorId": "51051515", "name": "Dylan S. Shah"}, {"authorId": "1644316962", "name": "Jinxing Li"}, {"authorId": "3455927", "name": "T. G. Thuruthel"}, {"authorId": "1783778", "name": "Yong\u2010Lae Park"}, {"authorId": "34567297", "name": "F. Iida"}, {"authorId": "71995610", "name": "Z. Bao"}, {"authorId": "1400632409", "name": "Rebecca Kramer\u2010Bottiglio"}, {"authorId": "48724645", "name": "M. Tolley"}]}, {"paperId": "70e8e1457aadbee439d47a2fe071007b1cf1dece", "url": "https://www.semanticscholar.org/paper/70e8e1457aadbee439d47a2fe071007b1cf1dece", "title": "Compression and machine learning: a new perspective on feature space vectors", "abstract": "The use of compression algorithms in machine learning tasks such as clustering and classification has appeared in a variety of fields, sometimes with the promise of reducing problems of explicit feature selection. The theoretical justification for such methods has been founded on an upper bound on Kolmogorov complexity and an idealized information space. An alternate view shows compression algorithms implicitly map strings into implicit feature space vectors, and compression-based similarity measures compute similarity within these feature spaces. Thus, compression-based methods are not a \"parameter free\" magic bullet for feature selection and data representation, but are instead concrete similarity measures within defined feature spaces, and are therefore akin to explicit feature vector models used in standard machine learning algorithms. To underscore this point, we find theoretical and empirical connections between traditional machine learning vector models and compression, encouraging cross-fertilization in future work", "year": 2006, "referenceCount": 32, "citationCount": 117, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1733143", "name": "D. Sculley"}, {"authorId": "1729374", "name": "C. Brodley"}]}, {"paperId": "9aee764d4c7d9d1361a3629be6a317c0e8c73d99", "url": "https://www.semanticscholar.org/paper/9aee764d4c7d9d1361a3629be6a317c0e8c73d99", "title": "Managing Diversity in Regression Ensembles", "abstract": "Ensembles are a widely used and effective technique in machine learning---their success is commonly attributed to the degree of disagreement, or 'diversity', within the ensemble. For ensembles where the individual estimators output crisp class labels, this 'diversity' is not well understood and remains an open research issue. For ensembles of regression estimators, the diversity can be exactly formulated in terms of the covariance between individual estimator outputs, and the optimum level is expressed in terms of a bias-variance-covariance trade-off. Despite this, most approaches to learning ensembles use heuristics to encourage the right degree of diversity. In this work we show how to explicitly control diversity through the error function. The first contribution of this paper is to show that by taking the combination mechanism for the ensemble into account we can derive an error function for each individual that balances ensemble diversity with individual accuracy. We show the relationship between this error function and an existing algorithm called negative correlation learning, which uses a heuristic penalty term added to the mean squared error function. It is demonstrated that these methods control the bias-variance-covariance trade-off systematically, and can be utilised with any estimator capable of minimising a quadratic error function, for example MLPs, or RBF networks. As a second contribution, we derive a strict upper bound on the coefficient of the penalty term, which holds for any estimator that can be cast in a generalised linear regression framework, with mild assumptions on the basis functions. Finally we present the results of an empirical study, showing significant improvements over simple ensemble learning, and finding that this technique is competitive with a variety of methods, including boosting, bagging, mixtures of experts, and Gaussian processes, on a number of tasks.", "year": 2005, "referenceCount": 28, "citationCount": 341, "influentialCitationCount": 32, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "145485821", "name": "Gavin Brown"}, {"authorId": "1688492", "name": "J. Wyatt"}, {"authorId": "4023505", "name": "P. Ti\u0148o"}]}, {"paperId": "31240cf20c338535aeb490c6b0eb55fc1fbab2af", "url": "https://www.semanticscholar.org/paper/31240cf20c338535aeb490c6b0eb55fc1fbab2af", "title": "Machine Learning in Mental Health", "abstract": "High prevalence of mental illness and the need for effective mental health care, combined with recent advances in AI, has led to an increase in explorations of how the field of machine learning (ML) can assist in the detection, diagnosis and treatment of mental health problems. ML techniques can potentially offer new routes for learning patterns of human behavior; identifying mental health symptoms and risk factors; developing predictions about disease progression; and personalizing and optimizing therapies. Despite the potential opportunities for using ML within mental health, this is an emerging research area, and the development of effective ML-enabled applications that are implementable in practice is bound up with an array of complex, interwoven challenges. Aiming to guide future research and identify new directions for advancing development in this important domain, this article presents an introduction to, and a systematic review of, current ML work regarding psycho-socially based mental health conditions from the computing and HCI literature. A quantitative synthesis and qualitative narrative review of 54 papers that were included in the analysis surfaced common trends, gaps, and challenges in this space. Discussing our findings, we (i) reflect on the current state-of-the-art of ML work for mental health, (ii) provide concrete suggestions for a stronger integration of human-centered and multi-disciplinary approaches in research and development, and (iii) invite more consideration of the potentially far-reaching personal, social, and ethical implications that ML models and interventions can have, if they are to find widespread, successful adoption in real-world mental health contexts.", "year": 2020, "referenceCount": 280, "citationCount": 77, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Psychology"], "authors": [{"authorId": "1944467", "name": "Anja Thieme"}, {"authorId": "145763736", "name": "D. Belgrave"}, {"authorId": "2659843", "name": "Gavin Doherty"}]}, {"paperId": "1e26eee1ffe5d7ed9280f4e5af602e3a4585cbaf", "url": "https://www.semanticscholar.org/paper/1e26eee1ffe5d7ed9280f4e5af602e3a4585cbaf", "title": "Syskill & Webert: Identifying Interesting Web Sites", "abstract": "We describe Syskill & Webert, a software agent that learns to rate pages on the World Wide Web (WWW), deciding what pages might interest a user. The user rates explored pages on a three point scale, and Syskill & Webert learns a user profile by analyzing the information on each page. The user profile can be used in two ways. First, it can be used to suggest which links a user would be interested in exploring. Second, it can be used to construct a LYCOS query to find pages that would interest a user. We compare six different algorithms from machine learning and information retrieval on this task. We find that the naive Bayesian classifier offers several advantages over other learning algorithms on this task. Furthermore, we find that an initial portion of a web page is sufficient for making predictions on its interestingness substantially reducing the amount of network transmission required to make predictions.", "year": 1996, "referenceCount": 18, "citationCount": 806, "influentialCitationCount": 44, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1694780", "name": "M. Pazzani"}, {"authorId": "1694959", "name": "Jack Muramatsu"}, {"authorId": "1691741", "name": "Daniel Billsus"}]}, {"paperId": "c1a46d9a0972fb0ec0977d24a191f612e7401369", "url": "https://www.semanticscholar.org/paper/c1a46d9a0972fb0ec0977d24a191f612e7401369", "title": "Distributed Learning with Regularized Least Squares", "abstract": "We study distributed learning with the least squares regularization scheme in a reproducing kernel Hilbert space (RKHS). By a divide-and-conquer approach, the algorithm partitions a data set into disjoint data subsets, applies the least squares regularization scheme to each data subset to produce an output function, and then takes an average of the individual output functions as a final global estimator or predictor. We show with error bounds in expectation in both the $L^2$-metric and RKHS-metric that the global output function of this distributed learning is a good approximation to the algorithm processing the whole data in one single machine. Our error bounds are sharp and stated in a general setting without any eigenfunction assumption. The analysis is achieved by a novel second order decomposition of operator differences in our integral operator approach. Even for the classical least squares regularization scheme in the RKHS associated with a general kernel, we give the best learning rate in the literature.", "year": 2016, "referenceCount": 46, "citationCount": 140, "influentialCitationCount": 31, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2432506", "name": "Shaobo Lin"}, {"authorId": "2116513495", "name": "Xin Guo"}, {"authorId": "1758237", "name": "Ding-Xuan Zhou"}]}, {"paperId": "595c811c70d2e6ea29ea8aabe201d7290e6ef23c", "url": "https://www.semanticscholar.org/paper/595c811c70d2e6ea29ea8aabe201d7290e6ef23c", "title": "Weakly Supervised Learning for Hedge Classification in Scientific Literature", "abstract": "We investigate automatic classification of speculative language (\u2018hedging\u2019), in biomedical text using weakly supervised machine learning. Our contributions include a precise description of the task with annotation guidelines, analysis and discussion, a probabilistic weakly supervised learning model, and experimental evaluation of the methods presented. We show that hedge classification is feasible using weakly supervised ML, and point toward avenues for future research.", "year": 2007, "referenceCount": 21, "citationCount": 207, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2310842", "name": "Ben Medlock"}, {"authorId": "145693410", "name": "Ted Briscoe"}]}]}