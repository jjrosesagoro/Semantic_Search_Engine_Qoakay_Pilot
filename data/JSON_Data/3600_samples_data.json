{"total": 5120627, "offset": 3500, "next": 3600, "data": [{"paperId": "0a8ccbf9f337c08984a769b64a50b1984e13fbd6", "url": "https://www.semanticscholar.org/paper/0a8ccbf9f337c08984a769b64a50b1984e13fbd6", "title": "Toolkits and Libraries for Deep Learning", "abstract": null, "year": 2017, "referenceCount": 12, "citationCount": 111, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "144917634", "name": "B. Erickson"}, {"authorId": "1986491", "name": "P. Korfiatis"}, {"authorId": "2015410", "name": "Z. Akkus"}, {"authorId": "4729578", "name": "T. Kline"}, {"authorId": "4655969", "name": "K. Philbrick"}]}, {"paperId": "a134313c09f23f8c7c8c5ec4e6b578784ab496b5", "url": "https://www.semanticscholar.org/paper/a134313c09f23f8c7c8c5ec4e6b578784ab496b5", "title": "Accelerated Monte Carlo simulations with restricted Boltzmann machines", "abstract": "Despite their exceptional flexibility and popularity, the Monte Carlo methods often suffer from slow mixing times for challenging statistical physics problems. We present a general strategy to overcome this difficulty by adopting ideas and techniques from the machine learning community. We fit the unnormalized probability of the physical model to a feedforward neural network and reinterpret the architecture as a restricted Boltzmann machine. Then, exploiting its feature detection ability, we utilize the restricted Boltzmann machine for efficient Monte Carlo updates and to speed up the simulation of the original physical system. We implement these ideas for the Falicov-Kimball model and demonstrate improved acceptance ratio and autocorrelation time near the phase transition point.", "year": 2016, "referenceCount": 47, "citationCount": 168, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Mathematics"], "authors": [{"authorId": "2143701665", "name": "Li Huang"}, {"authorId": "2152514294", "name": "Lei Wang"}]}, {"paperId": "79ac1ac6fcbb0e75e67eecefdfa2978fc4dd09f9", "url": "https://www.semanticscholar.org/paper/79ac1ac6fcbb0e75e67eecefdfa2978fc4dd09f9", "title": "'First Aid for You': getting to know your learning style using machine learning", "abstract": "Research on learning has shown that students learn differently and that they prefer to use different type of resources. Adaptive educational systems can support different learning characteristics by building a model of the student's learning behaviour and subsequently adapting the learning environment to match different needs. However major challenges exist, as it is not clear how a student model of learning style can be accurately built. One solution may be in the use of machine learning techniques. This paper presents 'First Aid For You', a novel adaptive educational system that dynamically determines learning style using machine learning techniques. The paper describes how it uses the Feider & Solomon Index of Learning Style to design an environment for different learning styles. It also describes how, as the student interacts with the learning environment, it uses the Naive Bayes algorithm to predict the student's preferred learning style and adaptively customize the learning environment.", "year": 2005, "referenceCount": 11, "citationCount": 34, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145240878", "name": "Declan Kelly"}, {"authorId": "1704775", "name": "B. Tangney"}]}, {"paperId": "5abb0aacf11480b702ffa727de46c134348b1390", "url": "https://www.semanticscholar.org/paper/5abb0aacf11480b702ffa727de46c134348b1390", "title": "Machine Learning Methods for Planning", "abstract": null, "year": 1994, "referenceCount": 0, "citationCount": 75, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145293454", "name": "Steven Minton"}]}, {"paperId": "1f58e8d4c827037d4c2a1afc695a88704e088beb", "url": "https://www.semanticscholar.org/paper/1f58e8d4c827037d4c2a1afc695a88704e088beb", "title": "Inductive Representation Learning on Temporal Graphs", "abstract": "Inductive representation learning on temporal graphs is an important step toward salable machine learning on real-world dynamic networks. The evolving nature of temporal dynamic graphs requires handling new nodes as well as capturing temporal patterns. The node embeddings, which are now functions of time, should represent both the static node features and the evolving topological structures. Moreover, node and topological features can be temporal as well, whose patterns the node embeddings should also capture. We propose the temporal graph attention (TGAT) layer to efficiently aggregate temporal-topological neighborhood features as well as to learn the time-feature interactions. For TGAT, we use the self-attention mechanism as building block and develop a novel functional time encoding technique based on the classical Bochner's theorem from harmonic analysis. By stacking TGAT layers, the network recognizes the node embeddings as functions of time and is able to inductively infer embeddings for both new and observed nodes as the graph evolves. The proposed approach handles both node classification and link prediction task, and can be naturally extended to include the temporal edge features. We evaluate our method with transductive and inductive tasks under temporal settings with two benchmark and one industrial dataset. Our TGAT model compares favorably to state-of-the-art baselines as well as the previous temporal graph embedding approaches.", "year": 2020, "referenceCount": 38, "citationCount": 178, "influentialCitationCount": 58, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2118285164", "name": "Da Xu"}, {"authorId": "39772097", "name": "Chuanwei Ruan"}, {"authorId": "3064955", "name": "Evren K\u00f6rpeoglu"}, {"authorId": "152663193", "name": "Sushant Kumar"}, {"authorId": "1684085", "name": "Kannan Achan"}]}, {"paperId": "6a2efd7e59ec314e29850e744be05eb65ccdfbc4", "url": "https://www.semanticscholar.org/paper/6a2efd7e59ec314e29850e744be05eb65ccdfbc4", "title": "Making AI Forget You: Data Deletion in Machine Learning", "abstract": "Intense recent discussions have focused on how to provide individuals with control over when their data can and cannot be used --- the EU's Right To Be Forgotten regulation is an example of this effort. In this paper we initiate a framework studying what to do when it is no longer permissible to deploy models derivative from specific user data. In particular, we formulate the problem of efficiently deleting individual data points from trained machine learning models. For many standard ML models, the only way to completely remove an individual's data is to retrain the whole model from scratch on the remaining data, which is often not computationally practical. We investigate algorithmic principles that enable efficient data deletion in ML. For the specific setting of k-means clustering, we propose two provably efficient deletion algorithms which achieve an average of over 100X improvement in deletion efficiency across 6 datasets, while producing clusters of comparable statistical quality to a canonical k-means++ baseline.", "year": 2019, "referenceCount": 95, "citationCount": 142, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "35308773", "name": "Antonio A. Ginart"}, {"authorId": "152565355", "name": "M. Guan"}, {"authorId": "1806083", "name": "G. Valiant"}, {"authorId": "145085305", "name": "James Y. Zou"}]}, {"paperId": "271c040ea880abc2470f72690ed89bc3d8a11a2c", "url": "https://www.semanticscholar.org/paper/271c040ea880abc2470f72690ed89bc3d8a11a2c", "title": "Improving the accuracy and speed of support vector learning machines", "abstract": null, "year": 1997, "referenceCount": 0, "citationCount": 222, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "79783680", "name": "C. Burges"}, {"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}, {"authorId": "2099637865", "name": "Mozer"}, {"authorId": "2054400760", "name": "M. Jordan"}, {"authorId": "2214848", "name": "T. Petsche"}]}, {"paperId": "d4200ce8f26606b5f20511c879d3ff4d21709b6c", "url": "https://www.semanticscholar.org/paper/d4200ce8f26606b5f20511c879d3ff4d21709b6c", "title": "A review of machine learning for the optimization of production processes", "abstract": null, "year": 2019, "referenceCount": 128, "citationCount": 97, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "150978254", "name": "D. Weichert"}, {"authorId": "94564331", "name": "Patrick Link"}, {"authorId": "2020810780", "name": "Anke Stoll"}, {"authorId": "1804144", "name": "S. R\u00fcping"}, {"authorId": "2295665", "name": "S. Ihlenfeldt"}, {"authorId": "145057418", "name": "S. Wrobel"}]}, {"paperId": "dc62320d22a861d1d2e0eff7b150b06afc98e1f6", "url": "https://www.semanticscholar.org/paper/dc62320d22a861d1d2e0eff7b150b06afc98e1f6", "title": "Learning and Generalization: With Applications to Neural Networks", "abstract": "How does a machine learn a new concept on the basis of examples? This second edition takes account of important new developments in the field. It also deals extensively with the theory of learning control systems, now comparably mature to learning of neural networks.", "year": 2002, "referenceCount": 0, "citationCount": 180, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145271313", "name": "M. Vidyasagar"}]}, {"paperId": "5568c78e4dd7bbe5ff56ad4477370014221aee5d", "url": "https://www.semanticscholar.org/paper/5568c78e4dd7bbe5ff56ad4477370014221aee5d", "title": "MACHINE LEARNING METHODS FOR SPAM E-MAIL CLASSIFICATION", "abstract": "The increasing volume of unsolicited bulk e-mail (also known as spam) has generated a need for reliable anti-spam filters. Machine learning techniques now days used to automatically filter the spam e-mail in a very successful rate. In this paper we review some of the most popular machine learning methods (Bayesian classification, k-NN, ANNs, SVMs, Artificial immune system and Rough sets) and of their applicability to the problem of spam Email classification. Descriptions of the algorithms are presented, and the comparison of their performance on the SpamAssassin spam corpus is presented.", "year": 2011, "referenceCount": 17, "citationCount": 105, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "38185321", "name": "W. Awad"}, {"authorId": "70354616", "name": "S. Elseuofi"}]}, {"paperId": "38a93d6d335ba266979534d7f8ee279f439b7af3", "url": "https://www.semanticscholar.org/paper/38a93d6d335ba266979534d7f8ee279f439b7af3", "title": "Two view learning: SVM-2K, Theory and Practice", "abstract": "Kernel methods make it relatively easy to define complex high-dimensional feature spaces. This raises the question of how we can identify the relevant subspaces for a particular learning task. When two views of the same phenomenon are available kernel Canonical Correlation Analysis (KCCA) has been shown to be an effective preprocessing step that can improve the performance of classification algorithms such as the Support Vector Machine (SVM). This paper takes this observation to its logical conclusion and proposes a method that combines this two stage learning (KCCA followed by SVM) into a single optimisation termed SVM-2K. We present both experimental and theoretical analysis of the approach showing encouraging results and insights.", "year": 2005, "referenceCount": 10, "citationCount": 341, "influentialCitationCount": 59, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48173155", "name": "Jason D. R. Farquhar"}, {"authorId": "1763894", "name": "D. Hardoon"}, {"authorId": "37192632", "name": "H. Meng"}, {"authorId": "1404459229", "name": "J. Shawe-Taylor"}, {"authorId": "2540580", "name": "S. Szedm\u00e1k"}]}, {"paperId": "563f4a2274b7b33063f00a0c5a15f3d8e49ce0bf", "url": "https://www.semanticscholar.org/paper/563f4a2274b7b33063f00a0c5a15f3d8e49ce0bf", "title": "Characterizing EMG data using machine-learning tools", "abstract": null, "year": 2014, "referenceCount": 88, "citationCount": 108, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1966879", "name": "Jamileh Yousefi"}, {"authorId": "1397781384", "name": "A. Hamilton-Wright"}]}, {"paperId": "0ab77b31909b18a3c12d4ae619c76740c9d34464", "url": "https://www.semanticscholar.org/paper/0ab77b31909b18a3c12d4ae619c76740c9d34464", "title": "Optimizing transition states via kernel-based machine learning.", "abstract": "We present a method for optimizing transition state theory dividing surfaces with support vector machines. The resulting dividing surfaces require no a priori information or intuition about reaction mechanisms. To generate optimal dividing surfaces, we apply a cycle of machine-learning and refinement of the surface by molecular dynamics sampling. We demonstrate that the machine-learned surfaces contain the relevant low-energy saddle points. The mechanisms of reactions may be extracted from the machine-learned surfaces in order to identify unexpected chemically relevant processes. Furthermore, we show that the machine-learned surfaces significantly increase the transmission coefficient for an adatom exchange involving many coupled degrees of freedom on a (100) surface when compared to a distance-based dividing surface.", "year": 2012, "referenceCount": 47, "citationCount": 83, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "3583421", "name": "Zachary D. Pozun"}, {"authorId": "39960184", "name": "K. Hansen"}, {"authorId": "2204809", "name": "D. Sheppard"}, {"authorId": "48041657", "name": "M. Rupp"}, {"authorId": "145034054", "name": "K. M\u00fcller"}, {"authorId": "2368771", "name": "G. Henkelman"}]}, {"paperId": "1d06cd33aa016fa3af012607b4a483c6aa8db51f", "url": "https://www.semanticscholar.org/paper/1d06cd33aa016fa3af012607b4a483c6aa8db51f", "title": "Chemical shifts in molecular solids by machine learning", "abstract": null, "year": 2018, "referenceCount": 95, "citationCount": 132, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Materials Science", "Medicine", "Physics"], "authors": [{"authorId": "46190944", "name": "Federico M. Paruzzo"}, {"authorId": "7650149", "name": "Albert Hofstetter"}, {"authorId": "34778608", "name": "F. Musil"}, {"authorId": "153048885", "name": "Sandip De"}, {"authorId": "1917770", "name": "M. Ceriotti"}, {"authorId": "4750978", "name": "L. Emsley"}]}, {"paperId": "d15f9480b40956281ff330769fb179abf5a6303d", "url": "https://www.semanticscholar.org/paper/d15f9480b40956281ff330769fb179abf5a6303d", "title": "Suicide Note Classification Using Natural Language Processing: A Content Analysis", "abstract": "Suicide is the second leading cause of death among 25\u201334 year olds and the third leading cause of death among 15\u201325 year olds in the United States. In the Emergency Department, where suicidal patients often present, estimating the risk of repeated attempts is generally left to clinical judgment. This paper presents our second attempt to determine the role of computational algorithms in understanding a suicidal patient's thoughts, as represented by suicide notes. We focus on developing methods of natural language processing that distinguish between genuine and elicited suicide notes. We hypothesize that machine learning algorithms can categorize suicide notes as well as mental health professionals and psychiatric physician trainees do. The data used are comprised of suicide notes from 33 suicide completers and matched to 33 elicited notes from healthy control group members. Eleven mental health professionals and 31 psychiatric trainees were asked to decide if a note was genuine or elicited. Their decisions were compared to nine different machine-learning algorithms. The results indicate that trainees accurately classified notes 49% of the time, mental health professionals accurately classified notes 63% of the time, and the best machine learning algorithm accurately classified the notes 78% of the time. This is an important step in developing an evidence-based predictor of repeated suicide attempts because it shows that natural language processing can aid in distinguishing between classes of suicidal notes.", "year": 2010, "referenceCount": 38, "citationCount": 199, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "1787579", "name": "J. Pestian"}, {"authorId": "6132923", "name": "H. Nasrallah"}, {"authorId": "1784716", "name": "P. Matykiewicz"}, {"authorId": "31607871", "name": "Aurora J Bennett"}, {"authorId": "6672398", "name": "A. Leenaars"}]}, {"paperId": "552399b84cf3a23a1ba9fcf37e1e977ecbb89430", "url": "https://www.semanticscholar.org/paper/552399b84cf3a23a1ba9fcf37e1e977ecbb89430", "title": "Oracle inequalities in empirical risk minimization and sparse recovery problems", "abstract": null, "year": 2011, "referenceCount": 149, "citationCount": 484, "influentialCitationCount": 92, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "1784986", "name": "V. Koltchinskii"}, {"authorId": "1403570089", "name": "\u00c9cole d'\u00e9t\u00e9 de probabilit\u00e9s de Saint-Flour"}]}, {"paperId": "c2df572eb56d57124bbbb87d1e213bfa0699e721", "url": "https://www.semanticscholar.org/paper/c2df572eb56d57124bbbb87d1e213bfa0699e721", "title": "Star-galaxy Classification Using Deep Convolutional Neural Networks", "abstract": "Most existing star-galaxy classifiers use the reduced summary information from catalogs, requiring careful feature extraction and selection. The latest advances in machine learning that use deep convolutional neural networks allow a machine to automatically learn the features directly from data, minimizing the need for input from human experts. We present a star-galaxy classification framework that uses deep convolutional neural networks (ConvNets) directly on the reduced, calibrated pixel values. Using data from the Sloan Digital Sky Survey (SDSS) and the Canada-France-Hawaii Telescope Lensing Survey (CFHTLenS), we demonstrate that ConvNets are able to produce accurate and well-calibrated probabilistic classifications that are competitive with conventional machine learning techniques. Future advances in deep learning may bring more success with current and forthcoming photometric surveys, such as the Dark Energy Survey (DES) and the Large Synoptic Survey Telescope (LSST), because deep neural networks require very little, manual feature engineering.", "year": 2016, "referenceCount": 86, "citationCount": 116, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics"], "authors": [{"authorId": "31717443", "name": "Edward J. Kim"}, {"authorId": "2594694", "name": "R. Brunner"}]}, {"paperId": "58dba823fc1d5f5d45c6fb7414aba7fed7011fb8", "url": "https://www.semanticscholar.org/paper/58dba823fc1d5f5d45c6fb7414aba7fed7011fb8", "title": "A Winnow-Based Approach to Context-Sensitive Spelling Correction", "abstract": null, "year": 1998, "referenceCount": 46, "citationCount": 305, "influentialCitationCount": 21, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2650619", "name": "Andrew R. Golding"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "65e1ada2360b42368a5f9f5e40ff436051c6fa84", "url": "https://www.semanticscholar.org/paper/65e1ada2360b42368a5f9f5e40ff436051c6fa84", "title": "Pomegranate: fast and flexible probabilistic modeling in python", "abstract": "We present pomegranate, an open source machine learning package for probabilistic modeling in Python. Probabilistic modeling encompasses a wide range of methods that explicitly describe uncertainty using probability distributions. Three widely used probabilistic models implemented in pomegranate are general mixture models, hidden Markov models, and Bayesian networks. A primary focus of pomegranate is to abstract away the complexities of training models from their definition. This allows users to focus on specifying the correct model for their application instead of being limited by their understanding of the underlying algorithms. An aspect of this focus involves the collection of additive sufficient statistics from data sets as a strategy for training models. This approach trivially enables many useful learning strategies, such as out-of-core learning, minibatch learning, and semi-supervised learning, without requiring the user to consider how to partition data or modify the algorithms to handle these tasks themselves. pomegranate is written in Cython to speed up calculations and releases the global interpreter lock to allow for built-in multithreaded parallelism, making it competitive with---or outperform---other implementations of similar algorithms. This paper presents an overview of the design choices in pomegranate, and how they have enabled complex features to be supported by simple code.", "year": 2017, "referenceCount": 16, "citationCount": 126, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "30932674", "name": "Jacob Schreiber"}]}, {"paperId": "0d3b51f94a5934fdda8f1e728f1b56c9bfeb1cb6", "url": "https://www.semanticscholar.org/paper/0d3b51f94a5934fdda8f1e728f1b56c9bfeb1cb6", "title": "Learning Finite State Machines With Self-Clustering Recurrent Networks", "abstract": "Recent work has shown that recurrent neural networks have the ability to learn finite state automata from examples. In particular, networks using second-order units have been successful at this task. In studying the performance and learning behavior of such networks we have found that the second-order network model attempts to form clusters in activation space as its internal representation of states. However, these learned states become unstable as longer and longer test input strings are presented to the network. In essence, the network forgets where the individual states are in activation space. In this paper we propose a new method to force such a network to learn stable states by introducing discretization into the network and using a pseudo-gradient learning rule to perform training. The essence of the learning rule is that in doing gradient descent, it makes use of the gradient of a sigmoid function as a heuristic hint in place of that of the hard-limiting function, while still using the discretized value in the feedback update path. The new structure uses isolated points in activation space instead of vague clusters as its internal representation of states. It is shown to have similar capabilities in learning finite state automata as the original network, but without the instability problem. The proposed pseudo-gradient learning rule may also be used as a basis for training other types of networks that have hard-limiting threshold activation functions.", "year": 1993, "referenceCount": 25, "citationCount": 147, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2075413715", "name": "Zheng Zeng"}, {"authorId": "145135018", "name": "R. Goodman"}, {"authorId": "50860274", "name": "Padhraic Smyth"}]}, {"paperId": "bfbdc3374125aa4ae76951d0ee352c15d02f33cf", "url": "https://www.semanticscholar.org/paper/bfbdc3374125aa4ae76951d0ee352c15d02f33cf", "title": "Learning drifting concepts: Example selection vs. example weighting", "abstract": "For many learning tasks where data is collected over an extended period of time, its underlying distribution is likely to change. A typical example is information filtering, i.e. the adaptive classification of documents with respect to a particular user interest. Both the interest of the user and the document content change over time. A filtering system should be able to adapt to such concept changes. This paper proposes several methods to handle such concept drifts with support vector machines. The methods either maintain an adaptive time window on the training data [13], select representative training examples, or weight the training examples [15]. The key idea is to automatically adjust the window size, the example selection, and the example weighting, respectively, so that the estimated generalization error is minimized. The approaches are both theoretically well-founded as well as effective and efficient in practice. Since they do not require complicated parameterization, they are simpler to use and more robust than comparable heuristics. \n \nExperiments with simulated concept drift scenarios based on real-world text data compare the new methods with other window management approaches. We show that they can effectively select an appropriate window size, example selection, and example weighting, respectively, in a robust way. We also explain how the proposed example selection and weighting approaches can be turned into incremental approaches. Since most evaluation methods for machine learning, like e.g. cross-validation, assume that the examples are independent and identically distributed, which is clearly unrealistic in the case of concept drift, alternative evaluation schemes are used to estimate and optimize the performance of each learning step within the concept drift handling frameworks as well as to evaluate and compare the different frameworks.", "year": 2004, "referenceCount": 24, "citationCount": 453, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2274294", "name": "R. Klinkenberg"}]}, {"paperId": "59080987514d001fe8fe792d9b5ca7ef4f386674", "url": "https://www.semanticscholar.org/paper/59080987514d001fe8fe792d9b5ca7ef4f386674", "title": "Stacked Sparse Autoencoder-Based Deep Network for Fault Diagnosis of Rotating Machinery", "abstract": "As a breakthrough in the field of machine fault diagnosis, deep learning has great potential to extract more abstract and discriminative features automatically without much prior knowledge compared with other methods, such as the signal processing and analysis-based methods and machine learning methods with shallow architectures. One of the most important aspects in measuring the extracted features is whether they can explore more information of the inputs and avoid redundancy to be representative. Thus, a stacked sparse autoencoder (SAE)-based machine fault diagnosis method is proposed in this paper. The penalty term of the SAE can help mine essential information and avoid redundancy. To help the constructed diagnosis network further mine more abstract and representative high-level features, the collected non-stationary and transient signals are preprocessed with ensemble empirical mode decomposition and autoregressive (AR) models to obtain AR parameters, which are extracted based on the intrinsic mode functions (IMFs) and regarded as the low-level features for the inputs of the proposed diagnosis network. Only the first four IMFs are considered, because fault information is mainly reflected in high-frequency IMFs. Experiments and comparisons are complemented to validate the superiority of the presented diagnosis network. Results fully demonstrate that the stacked SAE-based diagnosis method can extract more discriminative high-level features and has a better performance in rotating machinery fault diagnosis compared with the traditional machine learning methods with shallow architectures.", "year": 2017, "referenceCount": 31, "citationCount": 156, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "36684421", "name": "Yumei Qi"}, {"authorId": "145842489", "name": "Changqing Shen"}, {"authorId": "2152689352", "name": "Dong Wang"}, {"authorId": "46865040", "name": "Juanjuan Shi"}, {"authorId": "46813633", "name": "Xingxing Jiang"}, {"authorId": "143915181", "name": "Zhongkui Zhu"}]}, {"paperId": "b3f8348133c1d2f76f1dc1272f748a0b28874d80", "url": "https://www.semanticscholar.org/paper/b3f8348133c1d2f76f1dc1272f748a0b28874d80", "title": "All of Statistics: A Concise Course in Statistical Inference", "abstract": "WINNER OF THE 2005 DEGROOT PRIZE! This book is for people who want to learn probability and statistics quickly. It brings together many of the main ideas in modern statistics in one place. The book is suitable for students and researchers in statistics, computer science, data mining and machine learning. This book covers a much wider range of topics than a typical introductory text on mathematical statistics. It includes modern topics like nonparametric curve estimation, bootstrapping and classification, topics that are usually relegated to follow-up courses. The reader is assumed to know calculus and a little linear algebra. No previous knowledge of probability and statistics is required. The text can be used at the advanced undergraduate and graduate level.", "year": 2004, "referenceCount": 114, "citationCount": 1572, "influentialCitationCount": 154, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1733999", "name": "L. Wasserman"}]}, {"paperId": "8d6adaa16ed0af9935a1130a305c85e8bdf8780d", "url": "https://www.semanticscholar.org/paper/8d6adaa16ed0af9935a1130a305c85e8bdf8780d", "title": "An Algorithmic Perspective on Imitation Learning", "abstract": "As robots and other intelligent agents move from simple environments and problems to more complex, unstructured settings, manually programming their behavior has become increasingly challenging and expensive. Often, it is easier for a teacher to demonstrate a desired behavior rather than attempt to manually engineer it. This process of learning from demonstrations, and the study of algorithms to do so, is called imitation learning. This work provides an introduction to imitation learning. It covers the underlying assumptions, approaches, and how they relate; the rich set of algorithms developed to tackle the problem; and advice on effective tools and implementation. We intend this paper to serve two audiences. First, we want to familiarize machine learning experts with the challenges of imitation learning, particularly those arising in robotics, and the interesting theoretical and practical distinctions between it and more familiar frameworks like statistical supervised learning theory and reinforcement learning. Second, we want to give roboticists and experts in applied artificial intelligence a broader appreciation for the frameworks and tools available for imitation learning. We pay particular attention to the intimate connection between imitation learning approaches and those of structured prediction Daume III et al. [2009]. To structure this discussion, we categorize imitation learning techniques based on the following key criteria which drive algorithmic decisions: \n \n1) The structure of the policy space. Is the learned policy a time-index trajectory (trajectory learning), a mapping from observations to actions (so called behavioral cloning [Bain and Sammut, 1996]), or the result of a complex optimization or planning problem at each execution as is common in inverse optimal control methods [Kalman, 1964, Moylan and Anderson, 1973]. \n \n2) The information available during training and testing. In particular, is the learning algorithm privy to the full state that the teacher possess? Is the learner able to interact with the teacher and gather corrections or more data? Does the learner have a (typically a priori) model of the system with which it interacts? Does the learner have access to the reward (cost) function that the teacher is attempting to optimize? \n \n3) The notion of success. Different algorithmic approaches provide varying guarantees on the resulting learned behavior. These guarantees range from weaker (e.g., measuring disagreement with the agent\u2019s decision) to stronger (e.g., providing guarantees on the performance of the learner with respect to a true cost function, either known or unknown). We organize our work by paying particular attention to distinction (1): dividing imitation learning into directly replicating desired behavior (sometimes called behavioral cloning) and learning the hidden objectives of the desired behavior from demonstrations (called inverse optimal control or inverse reinforcement learning [Russell, 1998]). In the latter case, behavior arises as the result of an optimization problem solved for each new instance that the learner faces. In addition to method analysis, we discuss the design decisions a practitioner must make when selecting an imitation learning approach. Moreover, application examples\u2014such as robots that play table tennis [Kober and Peters, 2009], programs that play the game of Go [Silver et al., 2016], and systems that understand natural language [Wen et al., 2015]\u2014 illustrate the properties and motivations behind different forms of imitation learning. We conclude by presenting a set of open questions and point towards possible future research directions for machine learning.", "year": 2018, "referenceCount": 240, "citationCount": 446, "influentialCitationCount": 26, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40229316", "name": "Takayuki Osa"}, {"authorId": "34906504", "name": "J. Pajarinen"}, {"authorId": "26599977", "name": "G. Neumann"}, {"authorId": "1756566", "name": "J. Bagnell"}, {"authorId": "1689992", "name": "P. Abbeel"}, {"authorId": "145197867", "name": "Jan Peters"}]}, {"paperId": "8007deb2bcde438025e874d871db8b8a08de8249", "url": "https://www.semanticscholar.org/paper/8007deb2bcde438025e874d871db8b8a08de8249", "title": "A Review of Deep Machine Learning", "abstract": "The rapid increase of information and accessibility in recent years has activated a paradigm shift in algorithm design for artificial intelligence. Recently, deep learning (a surrogate of Machine Learning) have won several contests in pattern recognition and machine learning. This review comprehensively summarises relevant studies, much of it from prior state-of-the-art techniques. This paper also discusses the motivations and principles regarding learning algorithms for deep architectures.", "year": 2016, "referenceCount": 76, "citationCount": 58, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51126682", "name": "Ben-Bright Benuwa"}, {"authorId": "144754529", "name": "Yongzhao Zhan"}, {"authorId": "2794292", "name": "B. Ghansah"}, {"authorId": "35337087", "name": "Dickson Keddy Wornyo"}, {"authorId": "1574149876", "name": "Frank Banaseka Kataka"}]}, {"paperId": "592cf47a700e57da53fdacf9c232c41679f4c4d9", "url": "https://www.semanticscholar.org/paper/592cf47a700e57da53fdacf9c232c41679f4c4d9", "title": "Introduction to Scikit-learn", "abstract": null, "year": 2019, "referenceCount": 0, "citationCount": 37, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "19182651", "name": "Ekaba Bisong"}]}, {"paperId": "86e534c62bd47399cf746e4f9420e46f97cc51ca", "url": "https://www.semanticscholar.org/paper/86e534c62bd47399cf746e4f9420e46f97cc51ca", "title": "Predicting the Thermodynamic Stability of Solids Combining Density Functional Theory and Machine Learning", "abstract": "We perform a large scale benchmark of machine learning methods for the prediction of the thermodynamic stability of solids. We start by constructing a data set that comprises density functional theory calculations of around 250000 cubic perovskite systems. This includes all possible perovskite and antiperovskite crystals that can be generated with elements from hydrogen to bismuth, excluding rare gases and lanthanides. Incidentally, these calculations already reveal a large number of systems (around 500) that are thermodynamically stable but that are not present in crystal structure databases. Moreover, some of these phases have unconventional compositions and define completely new families of perovskites. This data set is then used to train and test a series of machine learning algorithms to predict the energy distance to the convex hull of stability. In particular, we study the performance of ridge regression, random forests, extremely randomized trees (including adaptive boosting), and neural networks....", "year": 2017, "referenceCount": 58, "citationCount": 167, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Materials Science"], "authors": [{"authorId": "2149390974", "name": "Jonathan Schmidt"}, {"authorId": "143978374", "name": "Jingming Shi"}, {"authorId": "9615687", "name": "Pedro Borlido"}, {"authorId": "2142932570", "name": "Limin Chen"}, {"authorId": "5517714", "name": "S. Botti"}, {"authorId": "145096746", "name": "M. Marques"}]}, {"paperId": "e9d783c81b53ce967ae343a33cbbbcb4aaf3280f", "url": "https://www.semanticscholar.org/paper/e9d783c81b53ce967ae343a33cbbbcb4aaf3280f", "title": "Interpretation of Neural Networks is Fragile", "abstract": "In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.", "year": 2017, "referenceCount": 34, "citationCount": 504, "influentialCitationCount": 51, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "27316199", "name": "Amirata Ghorbani"}, {"authorId": "144948925", "name": "Abubakar Abid"}, {"authorId": "145085305", "name": "James Y. Zou"}]}, {"paperId": "0708709074ce4312cc0927557e149651e79108dc", "url": "https://www.semanticscholar.org/paper/0708709074ce4312cc0927557e149651e79108dc", "title": "Comparison of Machine Learning Techniques for Fetal Heart Rate Classification", "abstract": "Cardiotocography is a monitoring technique providing important and vital information on fetal status during antepartum and intrapartum periods. The advances in modern obstetric practice allowed many robust and reliable machine learning techniques to be utilized in classifying fetal heart rate signals. The role of machine learning approaches in diagnosing diseases is becoming increasingly essential and intertwined. The main aim of the present study is to determine the most efficient machine learning technique to classify fetal heart rate signals. Therefore, the research has been focused on the widely used and practical machine learning techniques, such as artificial neural network, support vector machine, extreme learning machine, radial basis function network, and random forest. In a comparative way, fetal heart rate signals were classified as normal or hypoxic using the aforementioned machine learning techniques. The performance metrics derived from confusion matrix were used to measure classifiers\u2019 success. According to experimental results, although all machine learning techniques produced satisfactory results, artificial neural network yielded the rather well results with the sensitivity of 99.73% and specificity of 97.94%. The study results show that the artificial neural network was superior to other algorithms.", "year": 2017, "referenceCount": 27, "citationCount": 52, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "27845795", "name": "Zafer C\u00f6mert"}, {"authorId": "3275025", "name": "A. F. Kocamaz"}]}, {"paperId": "6fe68e797ea932fe27459f8e7402667959355f9d", "url": "https://www.semanticscholar.org/paper/6fe68e797ea932fe27459f8e7402667959355f9d", "title": "Byzantine-Tolerant Machine Learning", "abstract": "The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space. \nWe first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \\cdot (d + \\log n))$.", "year": 2017, "referenceCount": 27, "citationCount": 45, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3094352", "name": "Peva Blanchard"}, {"authorId": "9623412", "name": "El Mahdi El Mhamdi"}, {"authorId": "1727558", "name": "R. Guerraoui"}, {"authorId": "1718150", "name": "J. Stainer"}]}, {"paperId": "4b2df153540a64b293818bbd9b1c00d28a01025c", "url": "https://www.semanticscholar.org/paper/4b2df153540a64b293818bbd9b1c00d28a01025c", "title": "DBA: Distributed Backdoor Attacks against Federated Learning", "abstract": "Backdoor attacks aim to manipulate a subset of training data by injecting adversarial triggers such that machine learning models trained on the tampered dataset will make arbitrarily (targeted) incorrect prediction on the testset with the same trigger embedded. While federated learning (FL) is capable of aggregating information provided by different parties for training a better model, its distributed learning methodology and inherently heterogeneous data distribution across parties may bring new vulnerabilities. In addition to recent centralized backdoor attacks on FL where each party embeds the same global trigger during training, we propose the distributed backdoor attack (DBA) --- a novel threat assessment framework developed by fully exploiting the distributed nature of FL. DBA decomposes a global trigger pattern into separate local patterns and embed them into the training set of different adversarial parties respectively. Compared to standard centralized backdoors, we show that DBA is substantially more persistent and stealthy against FL on diverse datasets such as finance and image data. We conduct extensive experiments to show that the attack success rate of DBA is significantly higher than centralized backdoors under different settings. Moreover, we find that distributed attacks are indeed more insidious, as DBA can evade two state-of-the-art robust FL algorithms against centralized backdoors. We also provide explanations for the effectiveness of DBA via feature visual interpretation and feature importance ranking. To further explore the properties of DBA, we test the attack performance by varying different trigger factors, including local trigger variations (size, gap, and location), scaling factor in FL, data distribution, and poison ratio and interval. Our proposed DBA and thorough evaluation results shed lights on characterizing the robustness of FL.", "year": 2020, "referenceCount": 21, "citationCount": 234, "influentialCitationCount": 33, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "150961077", "name": "Chulin Xie"}, {"authorId": "2112441287", "name": "Keli Huang"}, {"authorId": "153191489", "name": "Pin-Yu Chen"}, {"authorId": "2165246687", "name": "Bo Li"}]}, {"paperId": "1bcbe4649fe2805e90ba02e85121b1e083a617ec", "url": "https://www.semanticscholar.org/paper/1bcbe4649fe2805e90ba02e85121b1e083a617ec", "title": "Gaussian approximation potentials: A brief tutorial introduction", "abstract": "\u00a9 2015 Wiley Periodicals, Inc. We present a swift walk-through of our recent work that uses machine learning to fit interatomic potentials based on quantum mechanical data. We describe our Gaussian approximation potentials (GAP) framework, discuss a variety of descriptors, how to train the model on total energies and derivatives, and the simultaneous use of multiple models of different complexity. We also show a small example using QUIP, the software sandbox implementation of GAP that is available for noncommercial use.", "year": 2015, "referenceCount": 18, "citationCount": 380, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Chemistry", "Materials Science"], "authors": [{"authorId": "3938091", "name": "A. Bart\u00f3k"}, {"authorId": "2559761", "name": "G\u00e1bor Cs\u00e1nyi"}]}, {"paperId": "481dd25896ac531707870c9b8c179cce20013401", "url": "https://www.semanticscholar.org/paper/481dd25896ac531707870c9b8c179cce20013401", "title": "Towards Personalized Federated Learning", "abstract": "In parallel with the rapid adoption of artificial intelligence (AI) empowered by advances in AI research, there has been growing awareness and concerns of data privacy. Recent significant developments in the data regulation landscape have prompted a seismic shift in interest toward privacy-preserving AI. This has contributed to the popularity of Federated Learning (FL), the leading paradigm for the training of machine learning models on data silos in a privacy-preserving manner. In this survey, we explore the domain of personalized FL (PFL) to address the fundamental challenges of FL on heterogeneous data, a universal characteristic inherent in all real-world datasets. We analyze the key motivations for PFL and present a unique taxonomy of PFL techniques categorized according to the key challenges and personalization strategies in PFL. We highlight their key ideas, challenges, opportunities, and envision promising future trajectories of research toward a new PFL architectural design, realistic PFL benchmarking, and trustworthy PFL approaches.", "year": 2021, "referenceCount": 132, "citationCount": 90, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2051715391", "name": "A. Tan"}, {"authorId": "2110984588", "name": "Han Yu"}, {"authorId": "101457473", "name": "Li-zhen Cui"}, {"authorId": "153096457", "name": "Qiang Yang"}]}, {"paperId": "fa0fe8f97a20e2f82f466506ebb29d9c27eb6869", "url": "https://www.semanticscholar.org/paper/fa0fe8f97a20e2f82f466506ebb29d9c27eb6869", "title": "Nonparametric Divergence Estimation with Applications to Machine Learning on Distributions", "abstract": "Low-dimensional embedding, manifold learning, clustering, classification, and anomaly detection are among the most important problems in machine learning. The existing methods usually consider the case when each instance has a fixed, finite-dimensional feature representation. Here we consider a different setting. We assume that each instance corresponds to a continuous probability distribution. These distributions are unknown, but we are given some i.i.d. samples from each distribution. Our goal is to estimate the distances between these distributions and use these distances to perform low-dimensional embedding, clustering/classification, or anomaly detection for the distributions. We present estimation algorithms, describe how to apply them for machine learning tasks on distributions, and show empirical results on synthetic data, real word images, and astronomical data sets.", "year": 2011, "referenceCount": 41, "citationCount": 105, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1719347", "name": "B. P\u00f3czos"}, {"authorId": "2068236719", "name": "L. Xiong"}, {"authorId": "1753432", "name": "J. Schneider"}]}, {"paperId": "8c8215b7f8111839f0066010a530a3a9f57ba15e", "url": "https://www.semanticscholar.org/paper/8c8215b7f8111839f0066010a530a3a9f57ba15e", "title": "AntNet: Distributed Stigmergetic Control for Communications Networks", "abstract": "This paper introduces AntNet, a novel approach to the adaptive learning of routing tables in communications networks. AntNet is a distributed, mobile agents based Monte Carlo system that was inspired by recent work on the ant colony metaphor for solving optimization problems. AntNet's agents concurrently explore the network and exchange collected information. The communication among the agents is indirect and asynchronous, mediated by the network itself. This form of communication is typical of social insects and is called stigmergy. We compare our algorithm with six state-of-the-art routing algorithms coming from the telecommunications and machine learning fields. The algorithms' performance is evaluated over a set of realistic testbeds. We run many experiments over real and artificial IP datagram networks with increasing number of nodes and under several paradigmatic spatial and temporal traffic distributions. Results are very encouraging. AntNet showed superior performance under all the experimental conditions with respect to its competitors. We analyze the main characteristics of the algorithm and try to explain the reasons for its superiority.", "year": 1998, "referenceCount": 55, "citationCount": 1794, "influentialCitationCount": 166, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1744127", "name": "G. D. Caro"}, {"authorId": "153570946", "name": "M. Dorigo"}]}, {"paperId": "36628978f1e717e1769ecb2ff9e723301fe73919", "url": "https://www.semanticscholar.org/paper/36628978f1e717e1769ecb2ff9e723301fe73919", "title": "Semantic Clone Detection Using Machine Learning", "abstract": "If two fragments of source code are identical to each other, they are called code clones. Code clones introduce difficulties in software maintenance and cause bug propagation. In this paper, we present a machine learning framework to automatically detect clones in software, which is able to detect Types-3 and the most complicated kind of clones, Type-4 clones. Previously used traditional features are often weak in detecting the semantic clones The novel aspects of our approach are the extraction of features from abstract syntax trees (AST) and program dependency graphs (PDG), representation of a pair of code fragments as a vector and the use of classification algorithms. The key benefit of this approach is that our approach can find both syntactic and semantic clones extremely well. Our evaluation indicates that using our new AST and PDG features is a viable methodology, since they improve detecting clones on the IJaDataset 2.0.", "year": 2016, "referenceCount": 29, "citationCount": 33, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "8860667", "name": "Abdullah Sheneamer"}, {"authorId": "34694214", "name": "J. Kalita"}]}, {"paperId": "2a8193af464d6bf2599b6ceb9c8260e71fbd7797", "url": "https://www.semanticscholar.org/paper/2a8193af464d6bf2599b6ceb9c8260e71fbd7797", "title": "MACHINE LEARNING FOR DATA STREAMS", "abstract": null, "year": 2018, "referenceCount": 1, "citationCount": 75, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1762931", "name": "A. Bifet"}, {"authorId": "1745702", "name": "Ricard Gavald\u00e0"}, {"authorId": "144282963", "name": "G. Holmes"}, {"authorId": "1737420", "name": "B. Pfahringer"}, {"authorId": "2059992293", "name": "Francis R. Bach"}]}, {"paperId": "6e974bc4ae8f0907f5ee438c3231ac6c88a52c2f", "url": "https://www.semanticscholar.org/paper/6e974bc4ae8f0907f5ee438c3231ac6c88a52c2f", "title": "A comparative study on machine learning algorithms for indoor positioning", "abstract": "Fingerprinting based positioning is commonly used for indoor positioning. In this method, initially a radio map is created using Received Signal Strength (RSS) values that are measured from predefined reference points. During the positioning, the best match between the observed RSS values and existing RSS values in the radio map is established as the predicted position. In the positioning literature, machine learning algorithms have widespread usage in estimating positions. One of the main problems in indoor positioning systems is to find out appropriate machine learning algorithm. In this paper, selected machine learning algorithms are compared in terms of positioning accuracy and computation time. In the experiments, UJIIndoorLoc indoor positioning database is used. Experimental results reveal that k-Nearest Neighbor (k-NN) algorithm is the most suitable one during the positioning. Additionally, ensemble algorithms such as AdaBoost and Bagging are applied to improve the decision tree classifier performance nearly same as k-NN that is resulted as the best classifier for indoor positioning.", "year": 2015, "referenceCount": 32, "citationCount": 87, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2052269684", "name": "Sinem Bozkurt"}, {"authorId": "1403231390", "name": "Gulin Elibol"}, {"authorId": "2548455", "name": "Serkan G\u00fcnal"}, {"authorId": "2272032", "name": "U\u011fur Yayan"}]}, {"paperId": "37a052144b510b8827634c38146b190d8b2c8d0b", "url": "https://www.semanticscholar.org/paper/37a052144b510b8827634c38146b190d8b2c8d0b", "title": "Medical Image Synthesis for Data Augmentation and Anonymization using Generative Adversarial Networks", "abstract": null, "year": 2018, "referenceCount": 22, "citationCount": 370, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1797022", "name": "Hoo-Chang Shin"}, {"authorId": "3333746", "name": "Neil A. Tenenholtz"}, {"authorId": "4347478", "name": "Jameson K. Rogers"}, {"authorId": "1862952", "name": "C. Schwarz"}, {"authorId": "2296039", "name": "M. Senjem"}, {"authorId": "2204041", "name": "J. Gunter"}, {"authorId": "1798929", "name": "K. Andriole"}, {"authorId": "37305969", "name": "Mark H. Michalski"}]}, {"paperId": "00dd3e0bc0339e1630cc01604486c59db13b2ca8", "url": "https://www.semanticscholar.org/paper/00dd3e0bc0339e1630cc01604486c59db13b2ca8", "title": "Federated Adversarial Domain Adaptation", "abstract": "Federated learning improves data privacy and efficiency in machine learning performed over networks of distributed devices, such as mobile phones, IoT and wearable devices, etc. Yet models trained with federated learning can still fail to generalize to new devices due to the problem of domain shift. Domain shift occurs when the labeled data collected by source nodes statistically differs from the target node's unlabeled data. In this work, we present a principled approach to the problem of federated domain adaptation, which aims to align the representations learned among the different nodes with the data distribution of the target node. Our approach extends adversarial adaptation techniques to the constraints of the federated setting. In addition, we devise a dynamic attention mechanism and leverage feature disentanglement to enhance knowledge transfer. Empirically, we perform extensive experiments on several image and text classification tasks and show promising results under unsupervised federated domain adaptation setting.", "year": 2019, "referenceCount": 59, "citationCount": 121, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2960713", "name": "Xingchao Peng"}, {"authorId": "2109595998", "name": "Zijun Huang"}, {"authorId": "2117789022", "name": "Yizhe Zhu"}, {"authorId": "2903226", "name": "Kate Saenko"}]}, {"paperId": "12c7c05e9286178d93b3bb7995ba9bf38e189a67", "url": "https://www.semanticscholar.org/paper/12c7c05e9286178d93b3bb7995ba9bf38e189a67", "title": "Heart disease prediction using machine learning algorithms", "abstract": "According to recent survey by WHO (World health organization) 17.9 million people die each year because of heart related diseases and it is increasing rapidly. With the increasing population and disease, it is become a challenge to diagnosing disease and providing the appropriate treatment at the right time. But there is a light of hope that recent advances in technology have accelerated the public health sector by developing advanced functional biomedical solutions. This paper aims at analyzing the various data mining techniques namely Naive Bayes, Random Forest Classification, Decision tree and Support Vector Machine by using a qualified dataset for Heart disease prediction which is consist of various attributes like gender, age, chest pain type, blood pressure, blood sugar etc. The research includes finding the correlations between the various attributes of the dataset by utilizing the standard data mining techniques and hence using the attributes suitably to predict the chances of a heart disease. These machine learning techniques take less time for the prediction of the disease with more accuracy which will reduce the dispose of valuable lives all over the world.", "year": 2019, "referenceCount": 14, "citationCount": 85, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2157752694", "name": "M. Ali"}, {"authorId": "151487460", "name": "Md. Iftiyar Khan"}, {"authorId": "2113544519", "name": "M. A. Imran"}, {"authorId": "151431829", "name": "Musnath Siddiki"}]}, {"paperId": "079932bf6ff8b99c899172ba60071818f6b5dfcb", "url": "https://www.semanticscholar.org/paper/079932bf6ff8b99c899172ba60071818f6b5dfcb", "title": "Poseidon: An Efficient Communication Architecture for Distributed Deep Learning on GPU Clusters", "abstract": "Deep learning models can take weeks to train on a single GPU-equipped machine, necessitating scaling out DL training to a GPU-cluster. However, current distributed DL implementations can scale poorly due to substantial parameter synchronization over the network, because the high throughput of GPUs allows more data batches to be processed per unit time than CPUs, leading to more frequent network synchronization. We present Poseidon, an efficient communication architecture for distributed DL on GPUs. Poseidon exploits the layered model structures in DL programs to overlap communication and computation, reducing bursty network communication. Moreover, Poseidon uses a hybrid communication scheme that optimizes the number of bytes required to synchronize each layer, according to layer properties and the number of machines. We show that Poseidon is applicable to different DL frameworks by plugging Poseidon into Caffe and TensorFlow. We show that Poseidon enables Caffe and TensorFlow to achieve 15.5x speed-up on 16 single-GPU machines, even with limited bandwidth (10GbE) and the challenging VGG19-22K network for image classification. Moreover, Poseidon-enabled TensorFlow achieves 31.5x speed-up with 32 single-GPU machines on Inception-V3, a 50% improvement over the open-source TensorFlow (20x speed-up).", "year": 2017, "referenceCount": 37, "citationCount": 253, "influentialCitationCount": 29, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1682058", "name": "H. Zhang"}, {"authorId": "2109692193", "name": "Zeyu Zheng"}, {"authorId": "1704538", "name": "Shizhen Xu"}, {"authorId": "143716171", "name": "Wei Dai"}, {"authorId": "1707357", "name": "Qirong Ho"}, {"authorId": "40250403", "name": "Xiaodan Liang"}, {"authorId": "2749311", "name": "Zhiting Hu"}, {"authorId": "1766143", "name": "Jinliang Wei"}, {"authorId": "40526720", "name": "P. Xie"}, {"authorId": "143977260", "name": "E. Xing"}]}, {"paperId": "37be889f4654312109dc9c53395fe117adb0f72b", "url": "https://www.semanticscholar.org/paper/37be889f4654312109dc9c53395fe117adb0f72b", "title": "Stable architectures for deep neural networks", "abstract": "Deep neural networks have become invaluable tools for supervised machine learning, e.g. classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Critical issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper, we propose new forward propagation techniques inspired by systems of ordinary differential equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks. The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.", "year": 2017, "referenceCount": 56, "citationCount": 445, "influentialCitationCount": 61, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Physics"], "authors": [{"authorId": "145761835", "name": "E. Haber"}, {"authorId": "49418655", "name": "Lars Ruthotto"}]}, {"paperId": "e44da7d8c71edcc6e575fa7faadd5e75785a7901", "url": "https://www.semanticscholar.org/paper/e44da7d8c71edcc6e575fa7faadd5e75785a7901", "title": "Fine-grained Analysis of Sentence Embeddings Using Auxiliary Prediction Tasks", "abstract": "There is a lot of research interest in encoding variable length sentences into fixed length vectors, in a way that preserves the sentence meanings. Two common methods include representations based on averaging word vectors, and representations based on the hidden states of recurrent neural networks such as LSTMs. The sentence vectors are used as features for subsequent machine learning tasks or for pre-training in the context of deep learning. However, not much is known about the properties that are encoded in these sentence representations and about the language information they capture. We propose a framework that facilitates better understanding of the encoded representations. We define prediction tasks around isolated aspects of sentence structure (namely sentence length, word content, and word order), and score representations by the ability to train a classifier to solve each prediction task when using the representation as input. We demonstrate the potential contribution of the approach by analyzing different sentence representation mechanisms. The analysis sheds light on the relative strengths of different sentence embedding methods with respect to these low level prediction tasks, and on the effect of the encoded vector's dimensionality on the resulting representations.", "year": 2016, "referenceCount": 54, "citationCount": 417, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2727584", "name": "Yossi Adi"}, {"authorId": "2098460", "name": "Einat Kermany"}, {"authorId": "2083259", "name": "Yonatan Belinkov"}, {"authorId": "3120346", "name": "Ofer Lavi"}, {"authorId": "2089067", "name": "Yoav Goldberg"}]}, {"paperId": "29ef90762761693ff585452d8ed1f1b36933692e", "url": "https://www.semanticscholar.org/paper/29ef90762761693ff585452d8ed1f1b36933692e", "title": "Generic Black-Box End-to-End Attack Against State of the Art API Call Based Malware Classifiers", "abstract": null, "year": 2017, "referenceCount": 56, "citationCount": 141, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39100619", "name": "Ishai Rosenberg"}, {"authorId": "1720589", "name": "A. Shabtai"}, {"authorId": "1732091", "name": "L. Rokach"}, {"authorId": "1724372", "name": "Y. Elovici"}]}, {"paperId": "9258b404f89d6188711a12a1f7f622c0af557bf2", "url": "https://www.semanticscholar.org/paper/9258b404f89d6188711a12a1f7f622c0af557bf2", "title": "Handbook of Blind Source Separation: Independent Component Analysis and Applications", "abstract": "Edited by the people who were forerunners in creating the field, together with contributions from 34 leading international experts, this handbook provides the definitive reference on Blind Source Separation, giving a broad and comprehensive description of all the core principles and methods, numerical algorithms and major applications in the fields of telecommunications, biomedical engineering and audio, acoustic and speech processing. Going beyond a machine learning perspective, the book reflects recent results in signal processing and numerical analysis, and includes topics such as optimization criteria, mathematical tools, the design of numerical algorithms, convolutive mixtures, and time frequency approaches. This Handbook is an ideal reference for university researchers, RD algebraic identification of under-determined mixtures, time-frequency methods, Bayesian approaches, blind identification under non negativity approaches, semi-blind methods for communicationsShows the applications of the methods to key application areas such as telecommunications, biomedical engineering, speech, acoustic, audio and music processing, while also giving a general method for developing applications", "year": 2010, "referenceCount": 260, "citationCount": 1581, "influentialCitationCount": 198, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1783831", "name": "P. Comon"}, {"authorId": "1696508", "name": "C. Jutten"}]}, {"paperId": "2d37f36d8b91b26f75142718916c7d842c68e155", "url": "https://www.semanticscholar.org/paper/2d37f36d8b91b26f75142718916c7d842c68e155", "title": "Machine Learning Biochemical Networks from Temporal Logic Properties", "abstract": null, "year": 2006, "referenceCount": 49, "citationCount": 137, "influentialCitationCount": 14, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2352452", "name": "L. Calzone"}, {"authorId": "1403272679", "name": "Nathalie Chabrier-Rivier"}, {"authorId": "1822220", "name": "F. Fages"}, {"authorId": "145684209", "name": "S. Soliman"}]}, {"paperId": "b7b5bea7b4d40003a6887794652ea07196a97134", "url": "https://www.semanticscholar.org/paper/b7b5bea7b4d40003a6887794652ea07196a97134", "title": "A New Learning Algorithm for Mean Field Boltzmann Machines", "abstract": null, "year": 2002, "referenceCount": 11, "citationCount": 141, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1678311", "name": "M. Welling"}, {"authorId": "1695689", "name": "Geoffrey E. Hinton"}]}, {"paperId": "2d2fe4a73c98933ae9b8df73c8452b0d8be6475e", "url": "https://www.semanticscholar.org/paper/2d2fe4a73c98933ae9b8df73c8452b0d8be6475e", "title": "An Intelligent Fault Diagnosis Method Using Unsupervised Feature Learning Towards Mechanical Big Data", "abstract": "Intelligent fault diagnosis is a promising tool to deal with mechanical big data due to its ability in rapidly and efficiently processing collected signals and providing accurate diagnosis results. In traditional intelligent diagnosis methods, however, the features are manually extracted depending on prior knowledge and diagnostic expertise. Such processes take advantage of human ingenuity but are time-consuming and labor-intensive. Inspired by the idea of unsupervised feature learning that uses artificial intelligence techniques to learn features from raw data, a two-stage learning method is proposed for intelligent diagnosis of machines. In the first learning stage of the method, sparse filtering, an unsupervised two-layer neural network, is used to directly learn features from mechanical vibration signals. In the second stage, softmax regression is employed to classify the health conditions based on the learned features. The proposed method is validated by a motor bearing dataset and a locomotive bearing dataset, respectively. The results show that the proposed method obtains fairly high diagnosis accuracies and is superior to the existing methods for the motor bearing dataset. Because of learning features adaptively, the proposed method reduces the need of human labor and makes intelligent fault diagnosis handle big data more easily.", "year": 2016, "referenceCount": 55, "citationCount": 740, "influentialCitationCount": 24, "isOpenAccess": false, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "1829456", "name": "Y. Lei"}, {"authorId": "46691607", "name": "Feng Jia"}, {"authorId": "143922195", "name": "Jing Lin"}, {"authorId": "3372340", "name": "Saibo Xing"}, {"authorId": "144558270", "name": "S. Ding"}]}, {"paperId": "7ae0f1af230b92462b812b92e2ceb4436d0a2f40", "url": "https://www.semanticscholar.org/paper/7ae0f1af230b92462b812b92e2ceb4436d0a2f40", "title": "TMVA - Toolkit for Multivariate Data Analysis", "abstract": "n high-energy physics, with the search for ever smaller signals in ever larger data sets, it has become essential to extract a maximum of the available information from the data. Multivariate classification methods based on machine learning techniques have become a fundamental ingredient to most analyses. Also the multivariate classifiers themselves have significantly evolved in recent years. Statisticians have found new ways to tune and to combine classifiers to further gain in performance. Integrated into the analysis framework ROOT, TMVA is a toolkit which hosts a large variety of multivariate classification algorithms. They range from rectangular cut optimization using a genetic algorithm and from one- and multidimensional likelihood estimators, over linear and nonlinear discriminants and neural networks, to sophisticated more recent classifiers such as a support vector machine, boosted decision trees and rule ensemble fitting. TMVA manages the simultaneous training, testing, and performance evaluation of all these classifiers with a user-friendly interface, and expedites the application of the trained classifiers to data.", "year": 2007, "referenceCount": 19, "citationCount": 1388, "influentialCitationCount": 120, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "authors": [{"authorId": "66701971", "name": "A. Hoecker"}, {"authorId": "66950770", "name": "P. Speckmayer"}, {"authorId": "2312911", "name": "J. Stelzer"}, {"authorId": "70628594", "name": "J. Therhaag"}, {"authorId": "115707269", "name": "E. Toerne"}, {"authorId": "121590494", "name": "H. Voss"}, {"authorId": "115347577", "name": "M. Backes"}, {"authorId": "103772254", "name": "T. Carli"}, {"authorId": "2066069521", "name": "O. Cohen"}, {"authorId": "1726061", "name": "A. Christov"}, {"authorId": "1401991417", "name": "D. Dannheim"}, {"authorId": "115666522", "name": "K. Danielowski"}, {"authorId": "1382860530", "name": "S. Henrot-Versill\u00e9"}, {"authorId": "66613865", "name": "M. Jachowski"}, {"authorId": "50608314", "name": "K. Kraszewski"}, {"authorId": "93224147", "name": "A. Krasznahorkay"}, {"authorId": "1760456", "name": "M. Kruk"}, {"authorId": "66680597", "name": "Y. Mahalalel"}, {"authorId": "4606774", "name": "R. Ospanov"}, {"authorId": "6394461", "name": "X. Prudent"}, {"authorId": "2060261053", "name": "A. Robert"}, {"authorId": "52199815", "name": "D. Schouten"}, {"authorId": "2968014", "name": "F. Tegenfeldt"}, {"authorId": "31234212", "name": "A. Voigt"}, {"authorId": "102261051", "name": "K. Voss"}, {"authorId": "145285008", "name": "M. Wolter"}, {"authorId": "40936428", "name": "A. Zemla"}]}, {"paperId": "13d8d4b8515717fba3b7dbe6cbafa64511140230", "url": "https://www.semanticscholar.org/paper/13d8d4b8515717fba3b7dbe6cbafa64511140230", "title": "Multisubject Learning for Common Spatial Patterns in Motor-Imagery BCI", "abstract": "Motor-imagery-based brain-computer interfaces (BCIs) commonly use the common spatial pattern filter (CSP) as preprocessing step before feature extraction and classification. The CSP method is a supervised algorithm and therefore needs subject-specific training data for calibration, which is very time consuming to collect. In order to reduce the amount of calibration data that is needed for a new subject, one can apply multitask (from now on called multisubject) machine learning techniques to the preprocessing phase. Here, the goal of multisubject learning is to learn a spatial filter for a new subject based on its own data and that of other subjects. This paper outlines the details of the multitask CSP algorithm and shows results on two data sets. In certain subjects a clear improvement can be seen, especially when the number of training trials is relatively low.", "year": 2011, "referenceCount": 18, "citationCount": 507, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2585222", "name": "D. Devlaminck"}, {"authorId": "1775487", "name": "B. Wyns"}, {"authorId": "1399050539", "name": "M. Grosse-Wentrup"}, {"authorId": "33985443", "name": "G. Otte"}, {"authorId": "1920462", "name": "P. Santens"}]}, {"paperId": "46e3575ccea71b0ef6041f38c20e1efa6b86c1b4", "url": "https://www.semanticscholar.org/paper/46e3575ccea71b0ef6041f38c20e1efa6b86c1b4", "title": "Machine learning for continuous innovation in battery technologies", "abstract": null, "year": 2020, "referenceCount": 13, "citationCount": 59, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "7995028", "name": "Muratahan Aykol"}, {"authorId": "144509011", "name": "Patrick K. Herring"}, {"authorId": "91617076", "name": "Abraham Anapolsky"}]}, {"paperId": "fa3dab883d05d525ac1c54001b6178830c4bbc52", "url": "https://www.semanticscholar.org/paper/fa3dab883d05d525ac1c54001b6178830c4bbc52", "title": "Corpus linguistics and naive discriminative learning", "abstract": "Three classifiers from machine learning (the generalized linear mixed model, memory based learning, and support vector machines) are compared with a naive discriminative learning classifier, derived from basic principles of error-driven learning characterizing animal and human learning. Tested on the dative alternation in English, using the Switchboard data from (BRESNAN; CUENI; NIKITINA; BAAYEN, 2007), naive discriminative learning emerges with stateof-the-art predictive accuracy. Naive discriminative learning offers a united framework for understanding the learning of probabilistic distributional patterns, for classification, and for a cognitive grounding of distinctive collexeme analysis.", "year": 2011, "referenceCount": 59, "citationCount": 79, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2918948", "name": "R. Baayen"}]}, {"paperId": "4fc2c857a093aa833f8624bc7f21b4b2c78ef9c3", "url": "https://www.semanticscholar.org/paper/4fc2c857a093aa833f8624bc7f21b4b2c78ef9c3", "title": "Machine Learning with TensorFlow", "abstract": "Summary Machine Learning with Tensor Flow gives readers a solid foundation in machine-learning concepts plus hands-on experience coding Tensor Flow with Python. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology TensorFlow, Google's library for large-scale machine learning, simplifies often-complex computations by representing them as graphs and efficiently mapping parts of the graphs to machines in a cluster or to the processors of a single machine. About the Book Machine Learning with Tensor Flow gives readers a solid foundation in machine-learning concepts plus hands-on experience coding TensorFlow with Python. You'll learn the basics by working with classic prediction, classification, and clustering algorithms. Then, you'll move on to the money chapters: exploration of deep-learning concepts like autoencoders, recurrent neural networks, and reinforcement learning. Digest this book and you will be ready to use TensorFlow for machine-learning and deep-learning applications of your own. What's Inside Matching your tasks to the right machine-learning and deep-learning approachesVisualizing algorithms with Tensor Board Understanding and using neural networks About the Reader Written for developers experienced with Python and algebraic concepts like vectors and matrices. About the Author Author Nishant Shukla is a computer vision researcher focused on applying machine-learning techniques in robotics. Senior technical editor, Kenneth Fricklas, is a seasoned developer, author, and machine-learning practitioner.", "year": 2018, "referenceCount": 0, "citationCount": 38, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2070358507", "name": "N. Shukla"}]}, {"paperId": "dc1817567355cd08e9d0d1669211e0d964c0ad17", "url": "https://www.semanticscholar.org/paper/dc1817567355cd08e9d0d1669211e0d964c0ad17", "title": "Mining specifications", "abstract": "Program verification is a promising approach to improving program quality, because it can search all possible program executions for specific errors. However, the need to formally describe correct behavior or errors is a major barrier to the widespread adoption of program verification, since programmers historically have been reluctant to write formal specifications. Automating the process of formulating specifications would remove a barrier to program verification and enhance its practicality.This paper describes specification mining, a machine learning approach to discovering formal specifications of the protocols that code must obey when interacting with an application program interface or abstract data type. Starting from the assumption that a working program is well enough debugged to reveal strong hints of correct protocols, our tool infers a specification by observing program execution and concisely summarizing the frequent interaction patterns as state machines that capture both temporal and data dependences. These state machines can be examined by a programmer, to refine the specification and identify errors, and can be utilized by automatic verification tools, to find bugs.Our preliminary experience with the mining tool has been promising. We were able to learn specifications that not only captured the correct protocol, but also discovered serious bugs.", "year": 2002, "referenceCount": 39, "citationCount": 723, "influentialCitationCount": 52, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2654231", "name": "Glenn Ammons"}, {"authorId": "1991345", "name": "R. Bod\u00edk"}, {"authorId": "1752633", "name": "J. Larus"}]}, {"paperId": "c3af641a9a7fcb73924a211f97421d2b43c4599f", "url": "https://www.semanticscholar.org/paper/c3af641a9a7fcb73924a211f97421d2b43c4599f", "title": "Machine Learning", "abstract": null, "year": 2021, "referenceCount": 0, "citationCount": 44, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1439122425", "name": "Ethem Alpayd\u0131n"}]}, {"paperId": "b710f0c9ca89f997371ce06c142ad25be1b35cca", "url": "https://www.semanticscholar.org/paper/b710f0c9ca89f997371ce06c142ad25be1b35cca", "title": "Interpretable Decision Sets: A Joint Framework for Description and Prediction", "abstract": "One of the most important obstacles to deploying predictive models is the fact that humans do not understand and trust them. Knowing which variables are important in a model's prediction and how they are combined can be very powerful in helping people understand and trust automatic decision making systems. Here we propose interpretable decision sets, a framework for building predictive models that are highly accurate, yet also highly interpretable. Decision sets are sets of independent if-then rules. Because each rule can be applied independently, decision sets are simple, concise, and easily interpretable. We formalize decision set learning through an objective function that simultaneously optimizes accuracy and interpretability of the rules. In particular, our approach learns short, accurate, and non-overlapping rules that cover the whole feature space and pay attention to small but important classes. Moreover, we prove that our objective is a non-monotone submodular function, which we efficiently optimize to find a near-optimal set of rules. Experiments show that interpretable decision sets are as accurate at classification as state-of-the-art machine learning techniques. They are also three times smaller on average than rule-based models learned by other methods. Finally, results of a user study show that people are able to answer multiple-choice questions about the decision boundaries of interpretable decision sets and write descriptions of classes based on them faster and more accurately than with other rule-based models that were designed for interpretability. Overall, our framework provides a new approach to interpretable machine learning that balances accuracy, interpretability, and computational efficiency.", "year": 2016, "referenceCount": 62, "citationCount": 551, "influentialCitationCount": 52, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "1892673", "name": "Himabindu Lakkaraju"}, {"authorId": "2870504", "name": "Stephen H. Bach"}, {"authorId": "1702139", "name": "J. Leskovec"}]}, {"paperId": "c68d2aa30087cc0ba9d01532dba67d99850561f7", "url": "https://www.semanticscholar.org/paper/c68d2aa30087cc0ba9d01532dba67d99850561f7", "title": "Acoustic detection and classification of Microchiroptera using machine learning: lessons learned from automatic speech recognition.", "abstract": "Current automatic acoustic detection and classification of microchiroptera utilize global features of individual calls (i.e., duration, bandwidth, frequency extrema), an approach that stems from expert knowledge of call sonograms. This approach parallels the acoustic phonetic paradigm of human automatic speech recognition (ASR), which relied on expert knowledge to account for variations in canonical linguistic units. ASR research eventually shifted from acoustic phonetics to machine learning, primarily because of the superior ability of machine learning to account for signal variation. To compare machine learning with conventional methods of detection and classification, nearly 3000 search-phase calls were hand labeled from recordings of five species: Pipistrellus bodenheimeri, Molossus molossus, Lasiurus borealis, L. cinereus semotus, and Tadarida brasiliensis. The hand labels were used to train two machine learning models: a Gaussian mixture model (GMM) for detection and classification and a hidden Markov model (HMM) for classification. The GMM detector produced 4% error compared to 32% error for a baseline broadband energy detector, while the GMM and HMM classifiers produced errors of 0.6 +/- 0.2% compared to 16.9 +/- 1.1% error for a baseline discriminant function analysis classifier. The experiments showed that machine learning algorithms produced errors an order of magnitude smaller than those for conventional methods.", "year": 2006, "referenceCount": 22, "citationCount": 76, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1679256", "name": "Mark D. Skowronski"}, {"authorId": "1748400", "name": "J. Harris"}]}, {"paperId": "0d86fbca1fbdb7c8e5a10c2d2c5a0ff3e07eea15", "url": "https://www.semanticscholar.org/paper/0d86fbca1fbdb7c8e5a10c2d2c5a0ff3e07eea15", "title": "A Kernel Statistical Test of Independence", "abstract": "Although kernel measures of independence have been widely applied in machine learning (notably in kernel ICA), there is as yet no method to determine whether they have detected statistically significant dependence. We provide a novel test of the independence hypothesis for one particular kernel independence measure, the Hilbert-Schmidt independence criterion (HSIC). The resulting test costs O(m2), where m is the sample size. We demonstrate that this test outperforms established contingency table and functional correlation-based tests, and that this advantage is greater for multivariate data. Finally, we show the HSIC test also applies to text (and to structured data more generally), for which no other independence test presently exists.", "year": 2007, "referenceCount": 24, "citationCount": 662, "influentialCitationCount": 113, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1708497", "name": "A. Gretton"}, {"authorId": "1693668", "name": "K. Fukumizu"}, {"authorId": "37077406", "name": "C. Teo"}, {"authorId": "2143476916", "name": "Le Song"}, {"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}, {"authorId": "46234526", "name": "Alex Smola"}]}, {"paperId": "52a5a6290c0ba767ecbf3c7a665c8ac0768f37c7", "url": "https://www.semanticscholar.org/paper/52a5a6290c0ba767ecbf3c7a665c8ac0768f37c7", "title": "Hyperopt: a Python library for model selection and hyperparameter optimization", "abstract": "Sequential model-based optimization (also known as Bayesian optimization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. This paper also gives an overview of Hyperopt-Sklearn, a software project that provides automatic algorithm configuration of the Scikit-learn machine learning library. Following Auto-Weka, we take the view that the choice of classifier and even the choice of preprocessing module can be taken together to represent a single large hyperparameter optimization problem. We use Hyperopt to define a search space that encompasses many standard components (e.g. SVM, RF, KNN, PCA, TFIDF) and common patterns of composing them together. We demonstrate, using search algorithms in Hyperopt and standard benchmarking data sets (MNIST, 20-newsgroups, convex shapes), that searching this space is practical and effective. In particular, we improve on best-known scores for the model space for both MNIST and convex shapes. The paper closes with some discussion of ongoing and future work.", "year": 2015, "referenceCount": 23, "citationCount": 492, "influentialCitationCount": 48, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32837403", "name": "J. Bergstra"}, {"authorId": "4489012", "name": "Brent Komer"}, {"authorId": "1690396", "name": "C. Eliasmith"}, {"authorId": "40657572", "name": "Daniel L. K. Yamins"}, {"authorId": "2042941", "name": "D. Cox"}]}, {"paperId": "31f5864ada5fb08b69da74b6d5ad99e385dcc737", "url": "https://www.semanticscholar.org/paper/31f5864ada5fb08b69da74b6d5ad99e385dcc737", "title": "Sentence Simplification with Deep Reinforcement Learning", "abstract": "Sentence simplification aims to make sentences easier to read and understand. Most recent approaches draw on insights from machine translation to learn simplification rewrites from monolingual corpora of complex and simple sentences. We address the simplification problem with an encoder-decoder model coupled with a deep reinforcement learning framework. Our model, which we call DRESS (as shorthand for Deep REinforcement Sentence Simplification), explores the space of possible simplifications while learning to optimize a reward function that encourages outputs which are simple, fluent, and preserve the meaning of the input. Experiments on three datasets demonstrate that our model outperforms competitive simplification systems.", "year": 2017, "referenceCount": 60, "citationCount": 277, "influentialCitationCount": 56, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "37409910", "name": "Xingxing Zhang"}, {"authorId": "1747893", "name": "Mirella Lapata"}]}, {"paperId": "b564107c412bce2cae9f4793769e014f1dd8c16a", "url": "https://www.semanticscholar.org/paper/b564107c412bce2cae9f4793769e014f1dd8c16a", "title": "Machine learning in space: extending our reach", "abstract": null, "year": 2011, "referenceCount": 17, "citationCount": 22, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2299685", "name": "A. McGovern"}, {"authorId": "6541629", "name": "K. Wagstaff"}]}, {"paperId": "1baee4ae5e5eaf75e322b53afa3cbdea89dcc2d0", "url": "https://www.semanticscholar.org/paper/1baee4ae5e5eaf75e322b53afa3cbdea89dcc2d0", "title": "Efficient and Robust Feature Selection via Joint \u21132, 1-Norms Minimization", "abstract": "Feature selection is an important component of many machine learning applications. Especially in many bioinformatics tasks, efficient and robust feature selection methods are desired to extract meaningful features and eliminate noisy ones. In this paper, we propose a new robust feature selection method with emphasizing joint l2,1-norm minimization on both loss function and regularization. The l2,1-norm based loss function is robust to outliers in data points and the l2,1-norm regularization selects features across all data points with joint sparsity. An efficient algorithm is introduced with proved convergence. Our regression based objective makes the feature selection process more efficient. Our method has been applied into both genomic and proteomic biomarkers discovery. Extensive empirical studies are performed on six data sets to demonstrate the performance of our feature selection method.", "year": 2010, "referenceCount": 27, "citationCount": 1594, "influentialCitationCount": 151, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144962210", "name": "F. Nie"}, {"authorId": "1748032", "name": "Heng Huang"}, {"authorId": "144380249", "name": "Xiao Cai"}, {"authorId": "1737469", "name": "C. Ding"}]}, {"paperId": "449f288b6cfcb3ca36258f91262b7efb46308f57", "url": "https://www.semanticscholar.org/paper/449f288b6cfcb3ca36258f91262b7efb46308f57", "title": "Machine learning in concrete strength simulations: Multi-nation data analytics", "abstract": null, "year": 2014, "referenceCount": 59, "citationCount": 193, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "39578061", "name": "Jui-Sheng Chou"}, {"authorId": "1707262", "name": "Chih-Fong Tsai"}, {"authorId": "2872533", "name": "A. Pham"}, {"authorId": "2819038", "name": "Yu-Hsin Lu"}]}, {"paperId": "91f0114a4471e4d92f599fb31501bd96652057df", "url": "https://www.semanticscholar.org/paper/91f0114a4471e4d92f599fb31501bd96652057df", "title": "TF-ICF: A New Term Weighting Scheme for Clustering Dynamic Data Streams", "abstract": "In this paper, we propose a new term weighting scheme called term frequency-inverse corpus frequency (TF-ICF). It does not require term frequency information from other documents within the document collection and thus, it enables us to generate the document vectors of N streaming documents in linear time. In the context of a machine learning application, unsupervised document clustering, we evaluated the effectiveness of the proposed approach in comparison to five widely used term weighting schemes through extensive experimentation. Our results show that TF-ICF can produce document clusters that are of comparable quality as those generated by the widely recognized term weighting schemes and it is significantly faster than those methods", "year": 2006, "referenceCount": 18, "citationCount": 156, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32315323", "name": "J. Reed"}, {"authorId": "145847372", "name": "Y. Jiao"}, {"authorId": "1771895", "name": "T. Potok"}, {"authorId": "34162720", "name": "Brian A. Klump"}, {"authorId": "1398136390", "name": "M.T. Elmore"}, {"authorId": "1725194", "name": "A. Hurson"}]}, {"paperId": "ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee", "url": "https://www.semanticscholar.org/paper/ac91892a8a6b6c3e97aa92b6fa8d54b42cade0ee", "title": "Learning scheduling algorithms for data processing clusters", "abstract": "Efficiently scheduling data processing jobs on distributed compute clusters requires complex algorithms. Current systems use simple, generalized heuristics and ignore workload characteristics, since developing and tuning a scheduling policy for each workload is infeasible. In this paper, we show that modern machine learning techniques can generate highly-efficient policies automatically. Decima uses reinforcement learning (RL) and neural networks to learn workload-specific scheduling algorithms without any human instruction beyond a high-level objective, such as minimizing average job completion time. However, off-the-shelf RL techniques cannot handle the complexity and scale of the scheduling problem. To build Decima, we had to develop new representations for jobs' dependency graphs, design scalable RL models, and invent RL training methods for dealing with continuous stochastic job arrivals. Our prototype integration with Spark on a 25-node cluster shows that Decima improves average job completion time by at least 21% over hand-tuned scheduling heuristics, achieving up to 2x improvement during periods of high cluster load.", "year": 2018, "referenceCount": 91, "citationCount": 351, "influentialCitationCount": 36, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2512621", "name": "Hongzi Mao"}, {"authorId": "1962485", "name": "Malte Schwarzkopf"}, {"authorId": "2043402", "name": "S. Venkatakrishnan"}, {"authorId": "40071013", "name": "Zili Meng"}, {"authorId": "79404966", "name": "M. Alizadeh"}]}, {"paperId": "b415e836d447ea9efb7629a1de67cd2a6f9e7ba8", "url": "https://www.semanticscholar.org/paper/b415e836d447ea9efb7629a1de67cd2a6f9e7ba8", "title": "Machine Learning in Agriculture: A Comprehensive Updated Review", "abstract": "The digital transformation of agriculture has evolved various aspects of management into artificial intelligent systems for the sake of making value from the ever-increasing data originated from numerous sources. A subset of artificial intelligence, namely machine learning, has a considerable potential to handle numerous challenges in the establishment of knowledge-based farming systems. The present study aims at shedding light on machine learning in agriculture by thoroughly reviewing the recent scholarly literature based on keywords\u2019 combinations of \u201cmachine learning\u201d along with \u201ccrop management\u201d, \u201cwater management\u201d, \u201csoil management\u201d, and \u201clivestock management\u201d, and in accordance with PRISMA guidelines. Only journal papers were considered eligible that were published within 2018\u20132020. The results indicated that this topic pertains to different disciplines that favour convergence research at the international level. Furthermore, crop management was observed to be at the centre of attention. A plethora of machine learning algorithms were used, with those belonging to Artificial Neural Networks being more efficient. In addition, maize and wheat as well as cattle and sheep were the most investigated crops and animals, respectively. Finally, a variety of sensors, attached on satellites and unmanned ground and aerial vehicles, have been utilized as a means of getting reliable input data for the data analyses. It is anticipated that this study will constitute a beneficial guide to all stakeholders towards enhancing awareness of the potential advantages of using machine learning in agriculture and contributing to a more systematic research on this topic.", "year": 2021, "referenceCount": 435, "citationCount": 72, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "88679617", "name": "L. Benos"}, {"authorId": "50742688", "name": "A. Tagarakis"}, {"authorId": "2106650552", "name": "Georgios Dolias"}, {"authorId": "2076743", "name": "R. Berruto"}, {"authorId": "1942212", "name": "D. Kateris"}, {"authorId": "2345652", "name": "D. Bochtis"}]}, {"paperId": "c98beaf3a7de8b71151c5c49763694637ee91bc6", "url": "https://www.semanticscholar.org/paper/c98beaf3a7de8b71151c5c49763694637ee91bc6", "title": "Automatic feature selection in neuroevolution", "abstract": "Feature selection is the process of finding the set of inputs to a machine learning algorithm that will yield the best performance. Developing a way to solve this problem automatically would make current machine learning methods much more useful. Previous efforts to automate feature selection rely on expensive meta-learning or are applicable only when labeled training data is available. This paper presents a novel method called FS-NEAT which extends the NEAT neuroevolution method to automatically determine an appropriate set of inputs for the networks it evolves. By learning the network's inputs, topology, and weights simultaneously, FS-NEAT addresses the feature selection problem without relying on meta-learning or labeled data. Initial experiments in an autonomous car racing simulation demonstrate that FS-NEAT can learn better and faster than regular NEAT. In addition, the networks it evolves are smaller and require fewer inputs. Furthermore, FS-NEAT's performance remains robust even as the feature selection task it faces is made increasingly difficult.", "year": 2005, "referenceCount": 17, "citationCount": 119, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1766767", "name": "Shimon Whiteson"}, {"authorId": "144848112", "name": "P. Stone"}, {"authorId": "1846883", "name": "Kenneth O. Stanley"}, {"authorId": "1686788", "name": "R. Miikkulainen"}, {"authorId": "2686687", "name": "Nate Kohl"}]}, {"paperId": "e1d7e18915bd8991373404ea157c3599e493122f", "url": "https://www.semanticscholar.org/paper/e1d7e18915bd8991373404ea157c3599e493122f", "title": "A Survey on Compiler Autotuning using Machine Learning", "abstract": "Since the mid-1990s, researchers have been trying to use machine-learning-based approaches to solve a number of different compiler optimization problems. These techniques primarily enhance the quality of the obtained results and, more importantly, make it feasible to tackle two main compiler optimization problems: optimization selection (choosing which optimizations to apply) and phase-ordering (choosing the order of applying optimizations). The compiler optimization space continues to grow due to the advancement of applications, increasing number of compiler optimizations, and new target architectures. Generic optimization passes in compilers cannot fully leverage newly introduced optimizations and, therefore, cannot keep up with the pace of increasing options. This survey summarizes and classifies the recent advances in using machine learning for the compiler optimization field, particularly on the two major problems of (1) selecting the best optimizations, and (2) the phase-ordering of optimizations. The survey highlights the approaches taken so far, the obtained results, the fine-grain classification among different approaches, and finally, the influential papers of the field.", "year": 2018, "referenceCount": 308, "citationCount": 131, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2206135", "name": "Amir H. Ashouri"}, {"authorId": "46315768", "name": "W. Killian"}, {"authorId": "1874233", "name": "J. Cavazos"}, {"authorId": "144570312", "name": "G. Palermo"}, {"authorId": "1784619", "name": "C. Silvano"}]}, {"paperId": "127f400f7a0e1eb63020ef15849a76c33158c277", "url": "https://www.semanticscholar.org/paper/127f400f7a0e1eb63020ef15849a76c33158c277", "title": "Inductive and Bayesian learning in medical diagnosis", "abstract": "Abstract Although successful in medical diagnostic problems, inductive learning systems were not widely accepted in medical practice. In this paper two different approaches to machine learning in medical applications are compared: the system for inductive learning of decision trees Assistant, and the naive Bayesian classifier. Both methodologies were tested in four medical diagnostic problems: localization of primary tumor, prognostics of recurrence of breast cancer, diagnosis of thyroid diseases, and rheumatology. The accuracy of automatically acquired diagnostic knowledge from stored data records is compared, and the interpretation of the knowledge and the explanation ability of the classification process of each system is discussed. Surprisingly, the naive Bayesian classifier is superior to Assistant in classification accuracy and explanation ability, while the interpretation of the acquired knowledge seems to be equally valuable. In addition, two extensions to naive Bayesian classifier are briefly des...", "year": 1993, "referenceCount": 48, "citationCount": 334, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143986204", "name": "I. Kononenko"}]}, {"paperId": "9884d18f265f9178ff9862d53cacbbc9957ddc4c", "url": "https://www.semanticscholar.org/paper/9884d18f265f9178ff9862d53cacbbc9957ddc4c", "title": "Advances in Variational Inference", "abstract": "Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.", "year": 2017, "referenceCount": 266, "citationCount": 391, "influentialCitationCount": 37, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "37588498", "name": "C. Zhang"}, {"authorId": "2784486", "name": "Judith B\u00fctepage"}, {"authorId": "1704879", "name": "H. Kjellstr\u00f6m"}, {"authorId": "1783468", "name": "S. Mandt"}]}, {"paperId": "1cb4265110d1aa009fddbba5ec9e400befc411c0", "url": "https://www.semanticscholar.org/paper/1cb4265110d1aa009fddbba5ec9e400befc411c0", "title": "Mining time-changing data streams", "abstract": "Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a stationary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying processes generating them changed during this time, sometimes radically. Although a number of algorithms have been proposed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes questionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.", "year": 2001, "referenceCount": 110, "citationCount": 1735, "influentialCitationCount": 156, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1740077", "name": "Geoff Hulten"}, {"authorId": "145029414", "name": "Laurie Spencer"}, {"authorId": "1740213", "name": "Pedro M. Domingos"}]}, {"paperId": "5dc53e50148b01fe8b9536eb79fa6b1dce924174", "url": "https://www.semanticscholar.org/paper/5dc53e50148b01fe8b9536eb79fa6b1dce924174", "title": "Promising Artificial Intelligence\u2010Machine Learning\u2010Deep Learning Algorithms in Ophthalmology", "abstract": "Abstract: The lifestyle of modern society has changed significantly with the emergence of artificial intelligence (AI), machine learning (ML), and deep learning (DL) technologies in recent years. Artificial intelligence is a multidimensional technology with various components such as advanced algorithms, ML and DL. Together, AI, ML, and DL are expected to provide automated devices to ophthalmologists for early diagnosis and timely treatment of ocular disorders in the near future. In fact, AI, ML, and DL have been used in ophthalmic setting to validate the diagnosis of diseases, read images, perform corneal topographic mapping and intraocular lens calculations. Diabetic retinopathy (DR), age\u2010related macular degeneration (AMD), and glaucoma are the 3 most common causes of irreversible blindness on a global scale. Ophthalmic imaging provides a way to diagnose and objectively detect the progression of a number of pathologies including DR, AMD, glaucoma, and other ophthalmic disorders. There are 2 methods of imaging used as diagnostic methods in ophthalmic practice: fundus digital photography and optical coherence tomography (OCT). Of note, OCT has become the most widely used imaging modality in ophthalmology settings in the developed world. Changes in population demographics and lifestyle, extension of average lifespan, and the changing pattern of chronic diseases such as obesity, diabetes, DR, AMD, and glaucoma create a rising demand for such images. Furthermore, the limitation of availability of retina specialists and trained human graders is a major problem in many countries. Consequently, given the current population growth trends, it is inevitable that analyzing such images is time\u2010consuming, costly, and prone to human error. Therefore, the detection and treatment of DR, AMD, glaucoma, and other ophthalmic disorders through unmanned automated applications system in the near future will be inevitable. We provide an overview of the potential impact of the current AI, ML, and DL methods and their applications on the early detection and treatment of DR, AMD, glaucoma, and other ophthalmic diseases.", "year": 2019, "referenceCount": 94, "citationCount": 77, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "14380190", "name": "L. Balyen"}, {"authorId": "2474754", "name": "T. Peto"}]}, {"paperId": "711c7d1b80719c34b6b97f5ba572f7872e4c4902", "url": "https://www.semanticscholar.org/paper/711c7d1b80719c34b6b97f5ba572f7872e4c4902", "title": "Machine Learning-Assisted Discovery of Solid Li-Ion Conducting Materials", "abstract": "We discover many new crystalline solid materials with fast single crystal Li ion conductivity at room temperature, discovered through density functional theory simulations guided by machine learning-based methods. The discovery of new solid Li superionic conductors is of critical importance to the development of safe all-solid-state Li-ion batteries. With a predictive universal structure\u2013property relationship for fast ion conduction not well understood, the search for new solid Li ion conductors has relied largely on trial-and-error computational and experimental searches over the last several decades. In this work, we perform a guided search of materials space with a machine learning (ML)-based prediction model for material selection and density functional theory molecular dynamics (DFT-MD) simulations for calculating ionic conductivity. These materials are screened from over 12\u202f000 experimentally synthesized and characterized candidates with very diverse structures and compositions. When compared to a r...", "year": 2018, "referenceCount": 30, "citationCount": 132, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Materials Science"], "authors": [{"authorId": "3797152", "name": "Austin D. Sendek"}, {"authorId": "8132903", "name": "E. D. Cubuk"}, {"authorId": "51958630", "name": "Evan R. Antoniuk"}, {"authorId": "15464038", "name": "Gowoon Cheon"}, {"authorId": "144241783", "name": "Yi Cui"}, {"authorId": "145908744", "name": "E. Reed"}]}, {"paperId": "5122eef3f21a8ea0285927fa7fe33fa7a3dadfae", "url": "https://www.semanticscholar.org/paper/5122eef3f21a8ea0285927fa7fe33fa7a3dadfae", "title": "${{\\rm E}^{2}}{\\rm LMs}$ : Ensemble Extreme Learning Machines for Hyperspectral Image Classification", "abstract": "Extreme learning machine (ELM) has attracted attentions in pattern recognition field due to its remarkable advantages such as fast operation, straightforward solution, and strong generalization. However, the performance of ELM for high-dimensional data, such as hyperspectral image, is still an open problem. Therefore, in this paper, we introduce ELM for hyperspectral image classification. Furthermore, in order to overcome the drawbacks of ELM caused by the randomness of input weights and bias, two new algorithms of ensemble extreme learning machines (Bagging-based and AdaBoost-based ELMs) are proposed for the classification task. In order to illustrate the performance of the proposed algorithms, support vector machines (SVMs) are used for evaluation and comparison. Experimental results with real hyperspectral images collected by reflective optics spectrographic image system (ROSIS) and airborne visible/infrared imaging spectrometer (AVIRIS) indicate that the proposed ensemble algorithms produce excellent classification performance in different scenarios with respect to spectral and spectral-spatial feature sets.", "year": 2014, "referenceCount": 41, "citationCount": 145, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "40087450", "name": "A. Samat"}, {"authorId": "1882431", "name": "Peijun Du"}, {"authorId": "47129972", "name": "Sicong Liu"}, {"authorId": "46276037", "name": "Jun Yu Li"}, {"authorId": "2150208546", "name": "Liang Cheng"}]}, {"paperId": "d38e8631bba0720becdaf7b89f79d9f9dca45d82", "url": "https://www.semanticscholar.org/paper/d38e8631bba0720becdaf7b89f79d9f9dca45d82", "title": "Inferring Algorithmic Patterns with Stack-Augmented Recurrent Nets", "abstract": "Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.", "year": 2015, "referenceCount": 43, "citationCount": 360, "influentialCitationCount": 36, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2319608", "name": "Armand Joulin"}, {"authorId": "2047446108", "name": "Tomas Mikolov"}]}, {"paperId": "b69fe5a837277ddbea5215d6bacd3a902e9d11ce", "url": "https://www.semanticscholar.org/paper/b69fe5a837277ddbea5215d6bacd3a902e9d11ce", "title": "Fifty years of computer analysis in chest imaging: rule-based, machine learning, deep learning", "abstract": null, "year": 2017, "referenceCount": 62, "citationCount": 76, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "123637526", "name": "B. van Ginneken"}]}, {"paperId": "dbca9dbe14e9933515d2005dc1163ae2c24d9afd", "url": "https://www.semanticscholar.org/paper/dbca9dbe14e9933515d2005dc1163ae2c24d9afd", "title": "signSGD with Majority Vote is Communication Efficient and Fault Tolerant", "abstract": "Training neural networks on large datasets can be accelerated by distributing the workload over a network of machines. As datasets grow ever larger, networks of hundreds or thousands of machines become economically viable. The time cost of communicating gradients limits the effectiveness of using such large machine counts, as may the increased chance of network faults. We explore a particularly simple algorithm for robust, communication-efficient learning---signSGD. Workers transmit only the sign of their gradient vector to a server, and the overall update is decided by a majority vote. This algorithm uses 32\u00d7 less communication per iteration than full-precision, distributed SGD. Under natural conditions verified by experiment, we prove that signSGD converges in the large and mini-batch settings, establishing convergence for a parameter regime of Adam as a byproduct. Aggregating sign gradients by majority vote means that no individual worker has too much power. We prove that unlike SGD, majority vote is robust when up to 50% of workers behave adversarially. The class of adversaries we consider includes as special cases those that invert or randomise their gradient estimate. On the practical side, we built our distributed training system in Pytorch. Benchmarking against the state of the art collective communications library (NCCL), our framework---with the parameter server housed entirely on one machine---led to a 25% reduction in time for training resnet50 on Imagenet when using 15 AWS p3.2xlarge machines.", "year": 2018, "referenceCount": 0, "citationCount": 134, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2075339320", "name": "Jeremy Bernstein"}, {"authorId": "48019143", "name": "Jiawei Zhao"}, {"authorId": "3371922", "name": "K. Azizzadenesheli"}, {"authorId": "2047844", "name": "Anima Anandkumar"}]}, {"paperId": "0f03eb384cc4fef927d1160eab9e4da95e13148a", "url": "https://www.semanticscholar.org/paper/0f03eb384cc4fef927d1160eab9e4da95e13148a", "title": "Learning Pattern Classification Tasks with Imbalanced Data Sets", "abstract": "This chapter is concerned with the class imbalance problem, which has been recognised as a crucial problem in machine learning and data mining. The problem occurs when there are significantly fewer training instances of one class compared to another class.", "year": 2009, "referenceCount": 4, "citationCount": 177, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35451784", "name": "Giang Hoang Nguyen"}, {"authorId": "1692174", "name": "A. Bouzerdoum"}, {"authorId": "1690887", "name": "S. L. Phung"}]}, {"paperId": "08463339ca5e29cb4049da570518271ca375500e", "url": "https://www.semanticscholar.org/paper/08463339ca5e29cb4049da570518271ca375500e", "title": "Multimodal Learning Analytics and Education Data Mining: Using computational technologies to measure complex learning tasks", "abstract": "New high-frequency multimodal data collection technologies and machine learning analysis techniques could offer new insights into learning, especially when students have the opportunity to generate unique, personalized artifacts, such as computer programs, robots, and solutions engineering challenges. To date most of the work on learning analytics and educational data mining has been focused on online courses and cognitive tutors, both of which provide a high degree of structure to the tasks, and are restricted to interactions that occur in front of a computer screen. In this paper, we argue that multimodal learning analytics can offer new insights into students\u2019 learning trajectories in more complex and open-ended learning environments. We present several examples of this work and its educational application.", "year": 2016, "referenceCount": 67, "citationCount": 220, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3181883", "name": "Paulo Blikstein"}, {"authorId": "145862204", "name": "M. Worsley"}]}, {"paperId": "29b72183b618c6a87b583fa0d03f3279900fbf25", "url": "https://www.semanticscholar.org/paper/29b72183b618c6a87b583fa0d03f3279900fbf25", "title": "An Introduction to Nonlinear Dimensionality Reduction by Maximum Variance Unfolding", "abstract": "Many problems in AI are simplified by clever representations of sensory or symbolic input. How to discover such representations automatically, from large amounts of unlabeled data, remains a fundamental challenge. The goal of statistical methods for dimensionality reduction is to detect and discover low dimensional structure in high dimensional data. In this paper, we review a recently proposed algorithm-- maximum, variance unfolding--for learning faithful low dimensional representations of high dimensional data. The algorithm relies on modem tools in convex optimization that are proving increasingly useful in many areas of machine learning.", "year": 2006, "referenceCount": 12, "citationCount": 288, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "7446832", "name": "Kilian Q. Weinberger"}, {"authorId": "1796044", "name": "L. Saul"}]}, {"paperId": "8345a0f98f436b85db22a6b5eb0165da0bcab91a", "url": "https://www.semanticscholar.org/paper/8345a0f98f436b85db22a6b5eb0165da0bcab91a", "title": "BLAZE: Blazing Fast Privacy-Preserving Machine Learning", "abstract": "Machine learning tools have illustrated their potential in many significant sectors such as healthcare and finance, to aide in deriving useful inferences. The sensitive and confidential nature of the data, in such sectors, raise natural concerns for the privacy of data. This motivated the area of Privacy-preserving Machine Learning (PPML) where privacy of the data is guaranteed. Typically, ML techniques require large computing power, which leads clients with limited infrastructure to rely on the method of Secure Outsourced Computation (SOC). In SOC setting, the computation is outsourced to a set of specialized and powerful cloud servers and the service is availed on a pay-per-use basis. In this work, we explore PPML techniques in the SOC setting for widely used ML algorithms-- Linear Regression, Logistic Regression, and Neural Networks. \nWe propose BLAZE, a blazing fast PPML framework in the three server setting tolerating one malicious corruption over a ring (\\Z{\\ell}). BLAZE achieves the stronger security guarantee of fairness (all honest servers get the output whenever the corrupt server obtains the same). Leveraging an input-independent preprocessing phase, BLAZE has a fast input-dependent online phase relying on efficient PPML primitives such as: (i) A dot product protocol for which the communication in the online phase is independent of the vector size, the first of its kind in the three server setting; (ii) A method for truncation that shuns evaluating expensive circuit for Ripple Carry Adders (RCA) and achieves a constant round complexity. This improves over the truncation method of ABY3 (Mohassel et al., CCS 2018) that uses RCA and consumes a round complexity that is of the order of the depth of RCA. \nAn extensive benchmarking of BLAZE for the aforementioned ML algorithms over a 64-bit ring in both WAN and LAN settings shows massive improvements over ABY3.", "year": 2020, "referenceCount": 62, "citationCount": 81, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144736003", "name": "A. Patra"}, {"authorId": "153456203", "name": "Ajith Suresh"}]}, {"paperId": "fec86faa56764658b128eb1bf10c3ea359907924", "url": "https://www.semanticscholar.org/paper/fec86faa56764658b128eb1bf10c3ea359907924", "title": "Practical real-time intrusion detection using machine learning approaches", "abstract": null, "year": 2011, "referenceCount": 23, "citationCount": 196, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2732580", "name": "Phurivit Sangkatsanee"}, {"authorId": "2828008", "name": "N. Wattanapongsakorn"}, {"authorId": "1778979", "name": "C. Charnsripinyo"}]}, {"paperId": "b37db9eebeed31f53c24ccd79d09a515f29446aa", "url": "https://www.semanticscholar.org/paper/b37db9eebeed31f53c24ccd79d09a515f29446aa", "title": "Parameter optimization of machining processes using teaching\u2013learning-based optimization algorithm", "abstract": null, "year": 2013, "referenceCount": 38, "citationCount": 119, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "3430204", "name": "P. J. Pawar"}, {"authorId": "145894751", "name": "R. Rao"}]}, {"paperId": "58f6be50d3e372af372e7d4b77bb80c017661d4d", "url": "https://www.semanticscholar.org/paper/58f6be50d3e372af372e7d4b77bb80c017661d4d", "title": "Stratified Sampling Meets Machine Learning", "abstract": "This paper solves a specialized regression problem to obtain sampling probabilities for records in databases. The goal is to sample a small set of records over which evaluating aggregate queries can be done both efficiently and accurately. We provide a principled and provable solution for this problem; it is parameterless and requires no data insights. Unlike standard regression problems, the loss is inversely proportional to the regressed-to values. Moreover, a cost zero solution always exists and can only be excluded by hard budget constraints. A unique form of regularization is also needed. We provide an efficient and simple regularized Empirical Risk Minimization (ERM) algorithm along with a theoretical generalization result. Our extensive experimental results significantly improve over both uniform sampling and standard stratified sampling which are de-facto the industry standards.", "year": 2016, "referenceCount": 23, "citationCount": 27, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2941680", "name": "Edo Liberty"}, {"authorId": "49464494", "name": "Kevin J. Lang"}, {"authorId": "102308676", "name": "K. Shmakov"}]}, {"paperId": "9d7bbf669ce96e022379bc99f8b9aa6083400829", "url": "https://www.semanticscholar.org/paper/9d7bbf669ce96e022379bc99f8b9aa6083400829", "title": "One-Level Storage System", "abstract": "After a brief survey of the basic Atlas machine, the paper describes an automatic system which in principle can be applied to any combination of two storage systems so that the combination can be regarded by the machine user as a single level. The actual system described relates to a fast core store-drum combination. The effect of the system on instruction times is illustrated, and the tape transfer system is also introduced since it fits basically in through the same hardware. The scheme incorporates a ``learning'' program, a technique which can be of greater importance in future computers.", "year": 1962, "referenceCount": 7, "citationCount": 293, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Business"], "authors": [{"authorId": "34704748", "name": "T. Kilburn"}, {"authorId": "29334852", "name": "D. Edwards"}, {"authorId": "97926494", "name": "M. Lanigan"}, {"authorId": "24348125", "name": "F. Sumner"}]}, {"paperId": "cd31c9540edcd2c14a670f8e7ba92a805f540c7d", "url": "https://www.semanticscholar.org/paper/cd31c9540edcd2c14a670f8e7ba92a805f540c7d", "title": "Detecting sequence signals in targeting peptides using deep learning", "abstract": "During the development of TargetP 2.0, a state-of-the-art method to predict targeting signal, we find a previously overlooked biological signal for subcellular targeting using the output from a deep learning method. In bioinformatics, machine learning methods have been used to predict features embedded in the sequences. In contrast to what is generally assumed, machine learning approaches can also provide new insights into the underlying biology. Here, we demonstrate this by presenting TargetP 2.0, a novel state-of-the-art method to identify N-terminal sorting signals, which direct proteins to the secretory pathway, mitochondria, and chloroplasts or other plastids. By examining the strongest signals from the attention layer in the network, we find that the second residue in the protein, that is, the one following the initial methionine, has a strong influence on the classification. We observe that two-thirds of chloroplast and thylakoid transit peptides have an alanine in position 2, compared with 20% in other plant proteins. We also note that in fungi and single-celled eukaryotes, less than 30% of the targeting peptides have an amino acid that allows the removal of the N-terminal methionine compared with 60% for the proteins without targeting peptide. The importance of this feature for predictions has not been highlighted before.", "year": 2019, "referenceCount": 42, "citationCount": 304, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Biology", "Medicine"], "authors": [{"authorId": "147017703", "name": "J. J. Almagro Armenteros"}, {"authorId": "2054499712", "name": "M. Salvatore"}, {"authorId": "2728851", "name": "O. Emanuelsson"}, {"authorId": "1724252", "name": "O. Winther"}, {"authorId": "3300090", "name": "G. von Heijne"}, {"authorId": "2471717", "name": "A. Elofsson"}, {"authorId": "145416108", "name": "H. Nielsen"}]}, {"paperId": "80ab54c73fdd518310a0d52cf3632b3b5a668ad8", "url": "https://www.semanticscholar.org/paper/80ab54c73fdd518310a0d52cf3632b3b5a668ad8", "title": "Rethinking statistical learning theory: learning using statistical invariants", "abstract": null, "year": 2018, "referenceCount": 10, "citationCount": 38, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50560492", "name": "V. Vapnik"}, {"authorId": "143719748", "name": "R. Izmailov"}]}, {"paperId": "236da67d020b4dd3937e8089348d047b656ef128", "url": "https://www.semanticscholar.org/paper/236da67d020b4dd3937e8089348d047b656ef128", "title": "Machine learning for Big Data analytics in plants.", "abstract": null, "year": 2014, "referenceCount": 88, "citationCount": 184, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Biology", "Medicine"], "authors": [{"authorId": "49679382", "name": "Chuang Ma"}, {"authorId": "2144616171", "name": "Hao Zhang"}, {"authorId": "35238157", "name": "Xiangfeng Wang"}]}, {"paperId": "fc82c16b8123fa4928eb66bd5aeb61a100715dc8", "url": "https://www.semanticscholar.org/paper/fc82c16b8123fa4928eb66bd5aeb61a100715dc8", "title": "A study of distance-based machine learning algorithms", "abstract": "Distance-based algorithms are machine learning algorithms that classify queries by computing distances between these queries and a number of internally stored exemplars. Exemplars that are closest to the query have the largest influence on the classification assigned to the query. Two specific distance-based algorithms, the nearest neighbor algorithm and the nearest-hyperrectangle algorithm, are studied in detail. \nIt is shown that the k-nearest neighbor algorithm (kNN) outperforms the first nearest neighbor algorithm only under certain conditions. Data sets must contain moderate amounts of noise. Training examples from the different classes must belong to clusters that allow an increase in the value of k without reaching into clusters of other classes. Methods for choosing the value of k for kNN are investigated. It shown that one-fold cross-validation on a restricted number of values for k suffices for best performance. It is also shown that for best performance the votes of the k-nearest neighbors of a query should be weighted in inverse proportion to their distances from the query. \nPrincipal component analysis is shown to reduce the number of relevant dimensions substantially in several domains. Two methods for learning feature weights for a weighted Euclidean distance metric are proposed. These methods improve the performance of kNN and NN in a variety of domains. \nThe nearest-hyperrectangle algorithm (NGE) is found to give predictions that are substantially inferior to those given by kNN in a variety of domains. Experiments performed to understand this inferior performance led to the discovery of several improvements to NGE. Foremost of these is BNGE, a batch algorithm that avoids construction of overlapping hyperrectangles from different classes. Although it is generally superior to NGE, BNGE is still significantly inferior to kNN in a variety of domains. Hence, a hybrid algorithm (KBNGE), that uses BNGE in parts of the input space that can be represented by a single hyperrectangle and kNN otherwise, is introduced. \nThe primary contributions of this dissertation are (a) several improvements to existing distance-based algorithms, (b) several new distance-based algorithms, and (c) an experimentally supported understanding of the conditions under which various distance-based algorithms are likely to give good performance.", "year": 1994, "referenceCount": 0, "citationCount": 146, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "144299726", "name": "Thomas G. Dietterich"}, {"authorId": "1782539", "name": "D. Wettschereck"}]}, {"paperId": "1eb6449ae040f051120e4d44348a0f68af9c36e8", "url": "https://www.semanticscholar.org/paper/1eb6449ae040f051120e4d44348a0f68af9c36e8", "title": "Deep neural network based malware detection using two dimensional binary program features", "abstract": "In this paper we introduce a deep neural network based malware detection system that Invincea has developed, which achieves a usable detection rate at an extremely low false positive rate and scales to real world training example volumes on commodity hardware. We show that our system achieves a 95% detection rate at 0.1% false positive rate (FPR), based on more than 400,000 software binaries sourced directly from our customers and internal malware databases. In addition, we describe a non-parametric method for adjusting the classifier's scores to better represent expected precision in the deployment environment. Our results demonstrate that it is now feasible to quickly train and deploy a low resource, highly accurate machine learning classification model, with false positive rates that approach traditional labor intensive expert rule based malware detection, while also detecting previously unseen malware missed by these traditional approaches. Since machine learning models tend to improve with larger datasizes, we foresee deep neural network classification models gaining in importance as part of a layered network defense strategy in coming years.", "year": 2015, "referenceCount": 38, "citationCount": 480, "influentialCitationCount": 41, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39617543", "name": "Joshua Saxe"}, {"authorId": "2040708537", "name": "Konstantin Berlin"}]}, {"paperId": "f3ff9ec3ac5d42781b3f223e88edd2755bb32f30", "url": "https://www.semanticscholar.org/paper/f3ff9ec3ac5d42781b3f223e88edd2755bb32f30", "title": "High-throughput screening of bimetallic catalysts enabled by machine learning", "abstract": "We present a holistic machine-learning framework for rapid screening of bimetallic catalysts with the aid of the descriptor-based kinetic analysis. A catalyst database, which contains the adsorption energies of *CO and *OH on {111}-terminated model alloy surfaces and fingerprint features of active sites from density functional theory calculations with the semi-local generalized gradient approximation (GGA), is established and used in optimizing the structural and weight parameters of artificial neural networks. The fingerprint descriptors, rooted at the d-band chemisorption theory and its recent developments, include the sp-band and d-band characteristics of an adsorption site together with tabulated properties of host-metal atoms. Using methanol electro-oxidation as the model reaction, the machine-learning model trained with the existing dataset of \u223c1000 idealized alloy surfaces can capture complex, non-linear adsorbate/metal interactions with the RMSE \u223c 0.2 eV and shows predictive power in exploring the immense chemical space of bimetallic catalysts. Feature importance analysis sheds light on the underlying factors that govern the adsorbate/metal interactions and provides the physical origin of bimetallics in breaking energy-scaling constraints of *CO and *OH, the two most commonly used reactivity descriptors in heterogeneous catalysis.", "year": 2017, "referenceCount": 53, "citationCount": 132, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Chemistry"], "authors": [{"authorId": "145315502", "name": "Zheng Li"}, {"authorId": "47673457", "name": "Siwen Wang"}, {"authorId": "36170819", "name": "W. S. Chin"}, {"authorId": "69898797", "name": "Luke E K Achenie"}, {"authorId": "48329840", "name": "H. Xin"}]}, {"paperId": "dbb74d7bfd3cafde5148358ded4f493a46b9d2b7", "url": "https://www.semanticscholar.org/paper/dbb74d7bfd3cafde5148358ded4f493a46b9d2b7", "title": "Machine Learning Feature Selection Methods for Landslide Susceptibility Mapping", "abstract": null, "year": 2013, "referenceCount": 97, "citationCount": 177, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "52007194", "name": "N. Micheletti"}, {"authorId": "32021796", "name": "L. Foresti"}, {"authorId": "2027143957", "name": "Sylvain Robert"}, {"authorId": "36683108", "name": "M. Leuenberger"}, {"authorId": "49575974", "name": "A. Pedrazzini"}, {"authorId": "2096523", "name": "M. Jaboyedoff"}, {"authorId": "143940227", "name": "M. Kanevski"}]}, {"paperId": "86bf4e4eb3087fff0dcaf9d0c0bc00c36fd2d11b", "url": "https://www.semanticscholar.org/paper/86bf4e4eb3087fff0dcaf9d0c0bc00c36fd2d11b", "title": "Towards human-guided machine learning", "abstract": "Automated Machine Learning (AutoML) systems are emerging that automatically search for possible solutions from a large space of possible kinds of models. Although fully automated machine learning is appropriate for many applications, users often have knowledge that supplements and constraints the available data and solutions. This paper proposes human-guided machine learning (HGML) as a hybrid approach where a user interacts with an AutoML system and tasks it to explore different problem settings that reflect the user's knowledge about the data available. We present: 1) a task analysis of HGML that shows the tasks that a user would want to carry out, 2) a characterization of two scientific publications, one in neuroscience and one in political science, in terms of how the authors would search for solutions using an AutoML system, 3) requirements for HGML based on those characterizations, and 4) an assessment of existing AutoML systems in terms of those requirements.", "year": 2019, "referenceCount": 44, "citationCount": 50, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145526918", "name": "Y. Gil"}, {"authorId": "143755437", "name": "James Honaker"}, {"authorId": "2118928002", "name": "Shikhar Gupta"}, {"authorId": "2146277142", "name": "Yibo Ma"}, {"authorId": "1405498945", "name": "Vito D'Orazio"}, {"authorId": "1398926410", "name": "D. Garijo"}, {"authorId": "2066310557", "name": "Shruti Gadewar"}, {"authorId": "50513690", "name": "Qifan Yang"}, {"authorId": "1721831", "name": "N. Jahanshad"}]}, {"paperId": "c6769c7e33ec93b6ffae08b7a885fcb34407ab86", "url": "https://www.semanticscholar.org/paper/c6769c7e33ec93b6ffae08b7a885fcb34407ab86", "title": "Radiomics and machine learning may accurately predict the grade and histological subtype in meningiomas using conventional and diffusion tensor imaging", "abstract": null, "year": 2018, "referenceCount": 63, "citationCount": 99, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "49819192", "name": "Y. Park"}, {"authorId": "81798146", "name": "Jong-Kap Oh"}, {"authorId": "35526457", "name": "S. You"}, {"authorId": "152742026", "name": "Kyunghwa Han"}, {"authorId": "3805725", "name": "S. Ahn"}, {"authorId": "49146907", "name": "Y. Choi"}, {"authorId": "122550132", "name": "Jong-Hee Chang"}, {"authorId": "121333746", "name": "Se Hoon Kim"}, {"authorId": "47089801", "name": "Seung-Koo Lee"}]}, {"paperId": "b88aff3b7bb19035ed3e420ff3cbc50bf9fe2df5", "url": "https://www.semanticscholar.org/paper/b88aff3b7bb19035ed3e420ff3cbc50bf9fe2df5", "title": "Tensor Train decomposition on TensorFlow (T3F)", "abstract": "Tensor Train decomposition is used across many branches of machine learning, but until now it lacked an implementation with GPU support, batch processing, automatic differentiation, and versatile functionality for Riemannian optimization framework, which takes in account the underlying manifold structure in order to construct efficient optimization methods. In this work, we propose a library that aims to fix it and makes machine learning papers that rely on Tensor Train decomposition easier to implement. The library includes 92% test coverage, examples, and API reference documentation.", "year": 2018, "referenceCount": 35, "citationCount": 46, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2050212830", "name": "Alexander Novikov"}, {"authorId": "7991830", "name": "Pavel Izmailov"}, {"authorId": "10662951", "name": "Valentin Khrulkov"}, {"authorId": "73776617", "name": "Michael Figurnov"}, {"authorId": "1738205", "name": "I. Oseledets"}]}, {"paperId": "39785d36f7b822efdff0179784600cc7b1a3a897", "url": "https://www.semanticscholar.org/paper/39785d36f7b822efdff0179784600cc7b1a3a897", "title": "Heart Disease Prediction using Machine Learning Techniques", "abstract": "Nowadays, heart disease has become a major disease among the people irrespective of the age. We are seeing this even in children dying due to the heart disease. If we can predict this even before they die, there may be huge chances of surviving. Everybody has various qualities of beat rate (pulse rate) and circulatory strain (blood pressure). We are living in a period of data. Due to the rise in the technology, the amount of data that is generated is increasing daily. Some terabytes of data are being produced and stored. For example, the huge amount of data about the patients is produced in the hospitals such as chest pain, heart rate, blood pressure, pulse rate etc. If we can get this data and apply some machine learning techniques, we can reduce the probability of people dying. In this paper we have done survey using different classification and grouping strategies, for example, KNN, Decision tree classifier, Gaussian Na\u00efve Bayes, Support vector machine, Linear regression, Logistic regression, Random forest classifier, Random forest regression, linear descriptive analysis. We have taken the 14 attributes that are present in the dataset as an input and applying on the dataset which is taken from the UCI repository to develop and accurate model of predicting the heart disease contains colossal (huge) therapeutic (medical) information. In the proposed research, the exhibition of the conclusion model is acquired by using utilizing classification strategies. In this paper proposed an accuracy model to predict whether a person has coronary disease or not. This is implemented by comparing the accuracies of different machine-learning strategies such as KNN, Decision tree classifier, Gaussian Na\u00efve Bayes, SVM, Logistic regression, Random forest classifier, Linear regression, Random forest regression, linear descriptive analysis", "year": 2019, "referenceCount": 7, "citationCount": 77, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "118449591", "name": "Sharyu U. Kamble"}, {"authorId": "115069316", "name": "Vaishnavi S. Jawanjal"}, {"authorId": "115078285", "name": "Pooja P. Velapure"}, {"authorId": "2067344215", "name": "Priya K. Jadhav"}, {"authorId": "115443461", "name": "Sanjivani S. Kadam"}]}, {"paperId": "2cebf0212380f83b7171fb5660f842c8d7043f60", "url": "https://www.semanticscholar.org/paper/2cebf0212380f83b7171fb5660f842c8d7043f60", "title": "Identifying the Best Machine Learning Algorithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge", "abstract": "Gliomas are the most common primary brain malignancies, with different degrees of aggressiveness, variable prognosis and various heterogeneous histologic sub-regions, i.e., peritumoral edematous/invaded tissue, necrotic core, active and non-enhancing core. This intrinsic heterogeneity is also portrayed in their radio-phenotype, as their sub-regions are depicted by varying intensity profiles disseminated across multi-parametric magnetic resonance imaging (mpMRI) scans, reflecting varying biological properties. Their heterogeneous shape, extent, and location are some of the factors that make these tumors difficult to resect, and in some cases inoperable. The amount of resected tumoris a factor also considered in longitudinal scans, when evaluating the apparent tumor for potential diagnosis of progression. Furthermore, there is mounting evidence that accurate segmentation of the various tumor sub-regions can offer the basis for quantitative image analysis towards prediction of patient overall survival. This study assesses thestate-of-the-art machine learning (ML) methods used for brain tumor image analysis in mpMRI scans, during the last seven instances of the International Brain Tumor Segmentation (BraTS) challenge, i.e., 2012-2018. Specifically, we focus on i) evaluating segmentations of the various glioma sub-regions in pre-operative mpMRI scans, ii) assessing potential tumor progression by virtue of longitudinal growth of tumor sub-regions, beyond use of the RECIST/RANO criteria, and iii) predicting the overall survival from pre-operative mpMRI scans of patients that underwent gross tota lresection. Finally, we investigate the challenge of identifying the best ML algorithms for each of these tasks, considering that apart from being diverse on each instance of the challenge, the multi-institutional mpMRI BraTS dataset has also been a continuously evolving/growing dataset.", "year": 2018, "referenceCount": 109, "citationCount": 904, "influentialCitationCount": 52, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3199900", "name": "S. Bakas"}, {"authorId": "21119833", "name": "M. Reyes"}, {"authorId": "153194548", "name": "A. Jakab"}, {"authorId": "2067693441", "name": "S. Bauer"}, {"authorId": "2736816", "name": "M. Rempfler"}, {"authorId": "1842515", "name": "A. Crimi"}, {"authorId": "2099904817", "name": "R. Shinohara"}, {"authorId": "2018562628", "name": "Christoph Berger"}, {"authorId": "47286636", "name": "S. Ha"}, {"authorId": "3363368", "name": "Martin Rozycki"}, {"authorId": "1774950", "name": "M. Prastawa"}, {"authorId": "46822867", "name": "E. Alberts"}, {"authorId": "1959705", "name": "Jana Lipkov\u00e1"}, {"authorId": "37845261", "name": "J. Freymann"}, {"authorId": "40078837", "name": "J. Kirby"}, {"authorId": "1981194", "name": "M. Bilello"}, {"authorId": "1398249653", "name": "H. Fathallah-Shaykh"}, {"authorId": "1800611", "name": "R. Wiest"}, {"authorId": "38095391", "name": "J. Kirschke"}, {"authorId": "2364183", "name": "B. Wiestler"}, {"authorId": "2153726", "name": "R. Colen"}, {"authorId": "6287014", "name": "Aikaterini Kotrotsou"}, {"authorId": "50398915", "name": "Pamela J. LaMontagne"}, {"authorId": "145454295", "name": "D. Marcus"}, {"authorId": "49574776", "name": "Mikhail Milchenko"}, {"authorId": "117346498", "name": "Arash Nazeri"}, {"authorId": "2034063990", "name": "M. Weber"}, {"authorId": "4883630", "name": "A. Mahajan"}, {"authorId": "1455206803", "name": "Ujjwal Baid"}, {"authorId": "2082471952", "name": "Dongjin Kwon"}, {"authorId": "2938767", "name": "M. Agarwal"}, {"authorId": "2109936179", "name": "M. Alam"}, {"authorId": "1803435", "name": "A. Albiol"}, {"authorId": "35658720", "name": "A. Albiol"}, {"authorId": "34319048", "name": "Alex Varghese"}, {"authorId": "143723820", "name": "T. A. Tuan"}, {"authorId": "1699104", "name": "T. Arbel"}, {"authorId": "40585581", "name": "Aaron Avery"}, {"authorId": "51936673", "name": "B. Pranjal"}, {"authorId": "51007396", "name": "Subhashis Banerjee"}, {"authorId": "1411393839", "name": "Thomas Batchelder"}, {"authorId": "1902312", "name": "N. Batmanghelich"}, {"authorId": "40799943", "name": "E. Battistella"}, {"authorId": "2685024", "name": "M. Bendszus"}, {"authorId": "48512882", "name": "E. Benson"}, {"authorId": "40076935", "name": "J. Bernal"}, {"authorId": "2395747", "name": "G. Biros"}, {"authorId": "144511905", "name": "M. Cabezas"}, {"authorId": "40340011", "name": "Siddhartha Chandra"}, {"authorId": "2048602248", "name": "Yi-Ju Chang"}, {"authorId": "2053413480", "name": "et al."}]}, {"paperId": "10142991f2e481b1c7c92b34db5e0770c7668baf", "url": "https://www.semanticscholar.org/paper/10142991f2e481b1c7c92b34db5e0770c7668baf", "title": "Benchmarking Automatic Machine Learning Frameworks", "abstract": "AutoML serves as the bridge between varying levels of expertise when designing machine learning systems and expedites the data science process. A wide range of techniques is taken to address this, however there does not exist an objective comparison of these techniques. We present a benchmark of current open source AutoML solutions using open source datasets. We test auto-sklearn, TPOT, auto_ml, and H2O's AutoML solution against a compiled set of regression and classification datasets sourced from OpenML and find that auto-sklearn performs the best across classification datasets and TPOT performs the best across regression datasets.", "year": 2018, "referenceCount": 4, "citationCount": 57, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "51227018", "name": "Adithya Balaji"}, {"authorId": "2069302626", "name": "A. Allen"}]}, {"paperId": "cfe7643e4f54cd2ce5b9a9478cf0a55dafb65d46", "url": "https://www.semanticscholar.org/paper/cfe7643e4f54cd2ce5b9a9478cf0a55dafb65d46", "title": "Machine Learning in High Energy Physics Community White Paper", "abstract": "Machine learning is an important applied research area in particle physics, beginning with applications to high-level physics analysis in the 1990s and 2000s, followed by an explosion of applications in particle and event identification and reconstruction in the 2010s. In this document we discuss promising future research and development areas in machine learning in particle physics with a roadmap for their implementation, software and hardware resource requirements, collaborative initiatives with the data science community, academia and industry, and training the particle physics community in data science. The main objective of the document is to connect and motivate these areas of research and development with the physics drivers of the High-Luminosity Large Hadron Collider and future neutrino experiments and identify the resource needs for their implementation. Additionally we identify areas where collaboration with external communities will be of great benefit.", "year": 2018, "referenceCount": 70, "citationCount": 130, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "authors": [{"authorId": "2555574", "name": "K. Albertsson"}, {"authorId": "2076048438", "name": "Piero Altoe"}, {"authorId": "145839220", "name": "D. Anderson"}, {"authorId": "46422123", "name": "M. Andrews"}, {"authorId": "26856321", "name": "Juan Pedro Araque Espinosa"}, {"authorId": "145435602", "name": "A. Aurisano"}, {"authorId": "4194523", "name": "L. Basara"}, {"authorId": "95703949", "name": "A. Bevan"}, {"authorId": "73991716", "name": "W. Bhimji"}, {"authorId": "2434951", "name": "D. Bonacorsi"}, {"authorId": "2177162564", "name": "P. Calafiura"}, {"authorId": "16253351", "name": "M. Campanelli"}, {"authorId": "32012717", "name": "Louis Capps"}, {"authorId": "30092039", "name": "F. Carminati"}, {"authorId": "2674972", "name": "S. Carrazza"}, {"authorId": "33024340", "name": "T. Childers"}, {"authorId": "3913447", "name": "E. Coniavitis"}, {"authorId": "11638962", "name": "K. Cranmer"}, {"authorId": "88976857", "name": "C. David"}, {"authorId": "2083433937", "name": "D. Davis"}, {"authorId": "1389378372", "name": "Javier Mauricio Duarte"}, {"authorId": "39024091", "name": "M. Erdmann"}, {"authorId": "51131571", "name": "J. Eschle"}, {"authorId": "6266455", "name": "A. Farbin"}, {"authorId": "51112700", "name": "M. Feickert"}, {"authorId": "145847058", "name": "N. Castro"}, {"authorId": "46268700", "name": "C. Fitzpatrick"}, {"authorId": "152233126", "name": "M. Floris"}, {"authorId": "152344273", "name": "A. Forti"}, {"authorId": "1389704694", "name": "J. Garra-Tico"}, {"authorId": "51110034", "name": "J. Gemmler"}, {"authorId": "102421475", "name": "M. Girone"}, {"authorId": "40955844", "name": "P. Glaysher"}, {"authorId": "14794022", "name": "S. Gleyzer"}, {"authorId": "145135665", "name": "V. Gligorov"}, {"authorId": "5778473", "name": "T. Golling"}, {"authorId": "51122440", "name": "Jonas Graw"}, {"authorId": "121643590", "name": "L. Gray"}, {"authorId": "144172679", "name": "D. Greenwood"}, {"authorId": "2058993880", "name": "T. Hacker"}, {"authorId": "144660463", "name": "J. Harvey"}, {"authorId": "94071841", "name": "B. Hegner"}, {"authorId": "24060472", "name": "L. Heinrich"}, {"authorId": "5043628", "name": "B. Hooberman"}, {"authorId": "46823312", "name": "J. Junggeburth"}, {"authorId": "11413078", "name": "M. Kagan"}, {"authorId": "144036260", "name": "M. Kane"}, {"authorId": "137998986", "name": "K. Kanishchev"}, {"authorId": "26936798", "name": "P. Karpinski"}, {"authorId": "4956058", "name": "Z. Kassabov"}, {"authorId": "40237549", "name": "Gaurav Kaul"}, {"authorId": "2956298", "name": "D. Kcira"}, {"authorId": "143926707", "name": "T. Keck"}, {"authorId": "144021890", "name": "A. Klimentov"}, {"authorId": "2080118", "name": "J. Kowalkowski"}, {"authorId": "31228673", "name": "Luke Kreczko"}, {"authorId": "152460087", "name": "A. Kurepin"}, {"authorId": "50874586", "name": "R. Kutschke"}, {"authorId": "2037763389", "name": "V. Kuznetsov"}, {"authorId": "51126328", "name": "Nicolas K\u00f6hler"}, {"authorId": "145830397", "name": "I. Lakomov"}, {"authorId": "1716753", "name": "K. Lannon"}, {"authorId": "32169062", "name": "M. Lassnig"}, {"authorId": "5241012", "name": "A. Limosani"}, {"authorId": "1881041", "name": "Gilles Louppe"}, {"authorId": "51122292", "name": "A. Mangu"}, {"authorId": "152172681", "name": "P. Mato"}, {"authorId": "51116386", "name": "N. Meenakshi"}, {"authorId": "46269977", "name": "H. Meinhard"}, {"authorId": "51050611", "name": "D. Menasce"}, {"authorId": "49778710", "name": "L. Moneta"}, {"authorId": "4494046", "name": "S. Moortgat"}, {"authorId": "48339925", "name": "M. Neubauer"}, {"authorId": "93596685", "name": "H. Newman"}, {"authorId": "1820941511", "name": "Hans Pabst"}, {"authorId": "35550664", "name": "Michela Paganini"}, {"authorId": "102306732", "name": "M. Paulini"}, {"authorId": "51116496", "name": "G. Perdue"}, {"authorId": "2095496051", "name": "Uzziel Perez"}, {"authorId": "51052756", "name": "A. Picazio"}, {"authorId": "3379619", "name": "J. Pivarski"}, {"authorId": "104308394", "name": "H. Prosper"}, {"authorId": "3393638", "name": "F. Psihas"}, {"authorId": "40189320", "name": "A. Radovic"}, {"authorId": "51129171", "name": "R. Reece"}, {"authorId": "51051191", "name": "Aurelius Rinkevicius"}, {"authorId": "2065078099", "name": "Eduardo Rodrigues"}, {"authorId": "14509605", "name": "J. Rorie"}, {"authorId": "3200334", "name": "D. Rousseau"}, {"authorId": "51127504", "name": "A. Sauers"}, {"authorId": "145245769", "name": "S. Schramm"}, {"authorId": "69907043", "name": "A. Schwartzman"}, {"authorId": "2259410", "name": "H. Severini"}, {"authorId": "4765849", "name": "P. Seyfert"}, {"authorId": "51133662", "name": "Filip Siroky"}, {"authorId": "1404979544", "name": "Konstantin Skazytkin"}, {"authorId": "2556099", "name": "M. Sokoloff"}, {"authorId": "34481035", "name": "G. Stewart"}, {"authorId": "25571773", "name": "B. Stienen"}, {"authorId": "40947541", "name": "I. Stockdale"}, {"authorId": "51119692", "name": "G. Strong"}, {"authorId": "51050561", "name": "S. Thais"}, {"authorId": "8865907", "name": "K. Tomko"}, {"authorId": "1735099", "name": "E. Upfal"}, {"authorId": "145191410", "name": "E. Usai"}, {"authorId": "1763161", "name": "A. Ustyuzhanin"}, {"authorId": "1387650493", "name": "M. Vala"}, {"authorId": "3425469", "name": "S. Vallecorsa"}, {"authorId": "51052911", "name": "M. Verzetti"}, {"authorId": "1382790348", "name": "X. Vilas\u00eds-Cardona"}, {"authorId": "52630992", "name": "J. Vlimant"}, {"authorId": "8426071", "name": "I. Vukotic"}, {"authorId": "2116230104", "name": "Sean Wang"}, {"authorId": "1815031", "name": "G. Watts"}, {"authorId": "2116400713", "name": "Michael Williams"}, {"authorId": "2143794603", "name": "Wenjing Wu"}, {"authorId": "1388409121", "name": "S. Wunsch"}, {"authorId": "2069219352", "name": "O. Zapata"}]}]}