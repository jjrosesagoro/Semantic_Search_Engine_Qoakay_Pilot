{"total": 5120627, "offset": 2100, "next": 2200, "data": [{"paperId": "a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "url": "https://www.semanticscholar.org/paper/a7976c2bacfbb194ddbe7fd10c2e50a545cf4081", "title": "LSTM: A Search Space Odyssey", "abstract": "Several variants of the long short-term memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful functional ANalysis Of VAriance framework. In total, we summarize the results of 5400 experimental runs ( $\\approx 15$  years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.", "year": 2015, "referenceCount": 52, "citationCount": 3682, "influentialCitationCount": 286, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "3035541", "name": "Klaus Greff"}, {"authorId": "2100612", "name": "R. Srivastava"}, {"authorId": "2865775", "name": "J. Koutn\u00edk"}, {"authorId": "1714059", "name": "Bas R. Steunebrink"}, {"authorId": "145341374", "name": "J. Schmidhuber"}]}, {"paperId": "f3b24107715729163e8c3211a1cf232a128b56a0", "url": "https://www.semanticscholar.org/paper/f3b24107715729163e8c3211a1cf232a128b56a0", "title": "Machine learning of linear differential equations using Gaussian processes", "abstract": null, "year": 2017, "referenceCount": 61, "citationCount": 328, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145401977", "name": "M. Raissi"}, {"authorId": "3410970", "name": "P. Perdikaris"}, {"authorId": "1720124", "name": "G. Karniadakis"}]}, {"paperId": "1435c09304f24a1310e6ede11bf5dfb4dbe78c3d", "url": "https://www.semanticscholar.org/paper/1435c09304f24a1310e6ede11bf5dfb4dbe78c3d", "title": "Should We Really Use Post-Hoc Tests Based on Mean-Ranks?", "abstract": "The statistical comparison of multiple algorithms over multiple data sets is fundamental in machine learning. This is typically carried out by the Friedman test. When the Friedman test rejects the null hypothesis, multiple comparisons are carried out to establish which are the significant differences among algorithms. The multiple comparisons are usually performed using the mean-ranks test. The aim of this technical note is to discuss the inconsistencies of the mean-ranks post-hoc test with the goal of discouraging its use in machine learning as well as in medicine, psychology, etc. We show that the outcome of the mean-ranks test depends on the pool of algorithms originally included in the experiment. In other words, the outcome of the comparison between algorithms A and B depends also on the performance of the other algorithms included in the original experiment. This can lead to paradoxical situations. For instance the difference between A and B could be declared significant if the pool comprises algorithms C, D, E and not significant if the pool comprises algorithms F, G, H. To overcome these issues, we suggest instead to perform the multiple comparison using a test whose outcome only depends on the two algorithms being compared, such as the sign-test or the Wilcoxon signed-rank test.", "year": 2015, "referenceCount": 23, "citationCount": 219, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics", "Physics", "Biology"], "authors": [{"authorId": "1792368", "name": "A. Benavoli"}, {"authorId": "1680295", "name": "Giorgio Corani"}, {"authorId": "32441746", "name": "F. Mangili"}]}, {"paperId": "965a6451ba5eca2dc17d0bacac4cabb551edb050", "url": "https://www.semanticscholar.org/paper/965a6451ba5eca2dc17d0bacac4cabb551edb050", "title": "Memory and the Computational Brain: Why Cognitive Science will Transform Neuroscience", "abstract": "Preface. 1. Information. Shannon's Theory of Communication. Measuring Information. Efficient Coding. Information and the Brain. Digital and Analog Signals. Appendix: The Information Content of Rare Versus Common Events and Signals. 2. Bayesian Updating. Bayes' Theorem and Our Intuitions About Evidence. Using Bayes' Rule. Summary. 3. Functions. Functions of One Argument. Composition and Decomposition of Functions. Functions of More than One Argument. The Limits to Functional Decomposition. Functions Can Map to Multi-Part Outputs. Mapping to Multiple-Element Outputs Does Not Increase Expressive Power. Defining Particular Functions. Summary: Physical/Neurobiological Implications of Facts about Functions. 4. Representations. Some Simple Examples. Notation. The Algebraic Representation of Geometry. 5. Symbols. Physical Properties of Good Symbols. Symbol Taxonomy. Summary. 6. Procedures. Algorithms. Procedures, Computation, and Symbols. Coding and Procedures. Two Senses of Knowing. A Geometric Example. 7. Computation. Formalizing Procedures. The Turing Machine. Turing Machine for the Successor Function. Turing Machines for f is -even Turing Machines for f + Minimal Memory Structure. General Purpose Computer. Summary. 8. Architectures. One-Dimensional Look-Up Tables (If-Then Implementation). Adding State Memory: Finite-State Machines. Adding Register Memory. Summary. 9. Data Structures. Finding Information in Memory. An Illustrative Example. Procedures and the Coding of Data Structures. The Structure of the Read-Only Biological Memory. 10. Computing with Neurons. Transducers and Conductors. Synapses and the Logic Gates. The Slowness of It All. The Time-Scale Problem. Synaptic Plasticity. Recurrent Loops in Which Activity Reverberates. 11. The Nature of Learning. Learning As Rewiring. Synaptic Plasticity and the Associative Theory of Learning. Why Associations Are Not Symbols. Distributed Coding. Learning As the Extraction and Preservation of Useful Information. Updating an Estimate of One's Location. 12. Learning Time and Space. Computational Accessibility. Learning the Time of Day. Learning Durations. Episodic Memory. 13. The Modularity of Learning. Example 1: Path Integration. Example 2: Learning the Solar Ephemeris. Example 3: \"Associative\" Learning. Summary. 14. Dead Reckoning in a Neural Network. Reverberating Circuits as Read/Write Memory Mechanisms. Implementing Combinatorial Operations by Table-Look-Up. The Full Model. The Ontogeny of the Connections? How Realistic is the Model? Lessons to be Drawn. Summary. 15. Neural Models of Interval Timing. Timing an Interval on First Encounter. Dworkin's Paradox. Neurally Inspired Models. The Deeper Problems. 16. The Molecular Basis of Memory. The Need to Separate Theory of Memory from Theory of Learning. The Coding Question. A Cautionary Tale. Why Not Synaptic Conductance? A Molecular or Sub-Molecular Mechanism? Bringing the Data to the Computational Machinery. Is It Universal? References. Glossary. Index.", "year": 2009, "referenceCount": 172, "citationCount": 217, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2646034", "name": "C. Gallistel"}, {"authorId": "2093373961", "name": "A. King"}]}, {"paperId": "14e97585f51e3cd866dba0c9ff30c2289b6e47c7", "url": "https://www.semanticscholar.org/paper/14e97585f51e3cd866dba0c9ff30c2289b6e47c7", "title": "A review of machine learning", "abstract": "This paper reviews Machine Learning (ML), and extends and complements previous work (Kocabas, 1991; Kalkanis and Conroy, 1991). Although this paper focuses on inductive learning, it at least touches on a great many aspects of ML in general. In addition, incremental induction is also reviewed. Therefore, a general review of ML is presented, but specific detail which has been covered previously is omitted, although other relevant references are noted, and later material is commented upon.", "year": 1997, "referenceCount": 177, "citationCount": 88, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2076590622", "name": "David M. Dutton"}, {"authorId": "32056453", "name": "G. V. Conroy"}]}, {"paperId": "8a2045c0eadec75b2cf5b5b3353e01b7973bda67", "url": "https://www.semanticscholar.org/paper/8a2045c0eadec75b2cf5b5b3353e01b7973bda67", "title": "When Does Machine Learning FAIL? Generalized Transferability for Evasion and Poisoning Attacks", "abstract": "Attacks against machine learning systems represent a growing threat as highlighted by the abundance of attacks proposed lately. However, attacks often make unrealistic assumptions about the knowledge and capabilities of adversaries. To evaluate this threat systematically, we propose the FAIL attacker model, which describes the adversary's knowledge and control along four dimensions. The FAIL model allows us to consider a wide range of weaker adversaries that have limited control and incomplete knowledge of the features, learning algorithms and training instances utilized. Within this framework, we evaluate the generalized transferability of a known evasion attack and we design StingRay, a targeted poisoning attack that is broadly applicable---it is practical against 4 machine learning applications, which use 3 different learning algorithms, and it can bypass 2 existing defenses. Our evaluation provides deeper insights into the transferability of poison and evasion samples across models and suggests promising directions for investigating defenses against this threat.", "year": 2018, "referenceCount": 61, "citationCount": 174, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "31765629", "name": "Octavian Suciu"}, {"authorId": "4951026", "name": "R. Marginean"}, {"authorId": "40898242", "name": "Yigitcan Kaya"}, {"authorId": "1722360", "name": "Hal Daum\u00e9"}, {"authorId": "3343194", "name": "T. Dumitras"}]}, {"paperId": "462ab1edefcf4723b39df5438231c8badc4e9e55", "url": "https://www.semanticscholar.org/paper/462ab1edefcf4723b39df5438231c8badc4e9e55", "title": "Interpretable Machine Learning", "abstract": null, "year": 2020, "referenceCount": 70, "citationCount": 89, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50621691", "name": "Christoph Molnar"}]}, {"paperId": "cae9d90524cccac5081666985d5d055b71697cee", "url": "https://www.semanticscholar.org/paper/cae9d90524cccac5081666985d5d055b71697cee", "title": "Tensor Comprehensions: Framework-Agnostic High-Performance Machine Learning Abstractions", "abstract": "Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions offering both imperative and declarative styles, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]", "year": 2018, "referenceCount": 96, "citationCount": 313, "influentialCitationCount": 42, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1800919", "name": "Nicolas Vasilache"}, {"authorId": "2054833547", "name": "O. Zinenko"}, {"authorId": "2097631216", "name": "Theodoros Theodoridis"}, {"authorId": "47316088", "name": "Priya Goyal"}, {"authorId": "2375710", "name": "Zach DeVito"}, {"authorId": "16521420", "name": "William S. Moses"}, {"authorId": "1772079", "name": "Sven Verdoolaege"}, {"authorId": "144187067", "name": "Andrew Adams"}, {"authorId": "145509576", "name": "Albert Cohen"}]}, {"paperId": "597a73c173b921b6adf17835ace73bfdb2de7111", "url": "https://www.semanticscholar.org/paper/597a73c173b921b6adf17835ace73bfdb2de7111", "title": "Modeling virtualized applications using machine learning techniques", "abstract": "With the growing adoption of virtualized datacenters and cloud hosting services, the allocation and sizing of resources such as CPU, memory, and I/O bandwidth for virtual machines (VMs) is becoming increasingly important. Accurate performance modeling of an application would help users in better VM sizing, thus reducing costs. It can also benefit cloud service providers who can offer a new charging model based on the VMs' performance instead of their configured sizes. In this paper, we present techniques to model the performance of a VM-hosted application as a function of the resources allocated to the VM and the resource contention it experiences. To address this multi-dimensional modeling problem, we propose and refine the use of two machine learning techniques: artificial neural network (ANN) and support vector machine (SVM). We evaluate these modeling techniques using five virtualized applications from the RUBiS and Filebench suite of benchmarks and demonstrate that their median and 90th percentile prediction errors are within 4.36% and 29.17% respectively. These results are substantially better than regression based approaches as well as direct applications of machine learning techniques without our refinements. We also present a simple and effective approach to VM sizing and empirically demonstrate that it can deliver optimal results for 65% of the sizing problems that we studied and produces close-to-optimal sizes for the remaining 35%.", "year": 2012, "referenceCount": 40, "citationCount": 128, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "3070722", "name": "Sajib Kundu"}, {"authorId": "1709495", "name": "R. Rangaswami"}, {"authorId": "2063996022", "name": "Ajay Gulati"}, {"authorId": "145914105", "name": "Ming Zhao"}, {"authorId": "145058524", "name": "K. Dutta"}]}, {"paperId": "af0c9cc6361f17a1e5ef7234216a8d9fb4723d79", "url": "https://www.semanticscholar.org/paper/af0c9cc6361f17a1e5ef7234216a8d9fb4723d79", "title": "Classification of Mental Tasks from Eeg Signals Using Extreme Learning Machine", "abstract": "In this paper, a recently developed machine learning algorithm referred to as Extreme Learning Machine (ELM) is used to classify five mental tasks from different subjects using electroencephalogram (EEG) signals available from a well-known database. Performance of ELM is compared in terms of training time and classification accuracy with a Backpropagation Neural Network (BPNN) classifier and also Support Vector Machines (SVMs). For SVMs, the comparisons have been made for both 1-against-1 and 1-against-all methods. Results show that ELM needs an order of magnitude less training time compared with SVMs and two orders of magnitude less compared with BPNN. The classification accuracy of ELM is similar to that of SVMs and BPNN. The study showed that smoothing of the classifiers' outputs can significantly improve their classification accuracies.", "year": 2006, "referenceCount": 25, "citationCount": 233, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2972859", "name": "Nan-Ying Liang"}, {"authorId": "1800678", "name": "P. Saratchandran"}, {"authorId": "145678691", "name": "G. Huang"}, {"authorId": "145411276", "name": "N. Sundararajan"}]}, {"paperId": "a4e088a3d92f9299b6a8060c21a1082ecb251902", "url": "https://www.semanticscholar.org/paper/a4e088a3d92f9299b6a8060c21a1082ecb251902", "title": "Hyperbolic Entailment Cones for Learning Hierarchical Embeddings", "abstract": "Learning graph representations via low-dimensional embeddings that preserve relevant network properties is an important class of problems in machine learning. We here present a novel method to embed directed acyclic graphs. Following prior work, we first advocate for using hyperbolic spaces which provably model tree-like structures better than Euclidean geometry. Second, we view hierarchical relations as partial orders defined using a family of nested geodesically convex cones. We prove that these entailment cones admit an optimal shape with a closed form expression both in the Euclidean and hyperbolic spaces. Moreover, they canonically define the embedding learning process. Experiments show significant improvements of our method over strong recent baselines both in terms of representational capacity and generalization.", "year": 2018, "referenceCount": 37, "citationCount": 164, "influentialCitationCount": 26, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1882451", "name": "O. Ganea"}, {"authorId": "9940262", "name": "Gary B\u00e9cigneul"}, {"authorId": "143936663", "name": "Thomas Hofmann"}]}, {"paperId": "9ceae85a0bd4231cd2efe14884c40b7bc04d3dac", "url": "https://www.semanticscholar.org/paper/9ceae85a0bd4231cd2efe14884c40b7bc04d3dac", "title": "Accounting for Variance in Machine Learning Benchmarks", "abstract": "Strong empirical evidence that one machine-learning algorithm A outperforms another one B ideally calls for multiple trials optimizing the learning pipeline over sources of variation such as data sampling, augmentation, parameter initialization, and hyperparameters choices. This is prohibitively expensive, and corners are cut to reach conclusions. We model the whole benchmarking process, revealing that variance due to data sampling, parameter initialization and hyperparameter choice impact markedly the results. We analyze the predominant comparison methods used today in the light of this variance. We show a counter-intuitive result that adding more sources of variation to an imperfect estimator approaches better the ideal estimator at a 51 \u00d7 reduction in compute cost. Building on these results, we study the error rate of detecting improvements, on \ufb01ve different deep-learning tasks/architectures. This study leads us to propose recommendations for performance comparisons.", "year": 2021, "referenceCount": 71, "citationCount": 51, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2900675", "name": "Xavier Bouthillier"}, {"authorId": "2097176461", "name": "Pierre Delaunay"}, {"authorId": "1760944", "name": "Mirko Bronzi"}, {"authorId": "49860096", "name": "Assya Trofimov"}, {"authorId": "2051891988", "name": "B. Nichyporuk"}, {"authorId": "2073094612", "name": "Justin Szeto"}, {"authorId": "2051892157", "name": "Naz Sepah"}, {"authorId": "34885007", "name": "Edward Raff"}, {"authorId": "51054293", "name": "Kanika Madan"}, {"authorId": "2961618", "name": "Vikram S. Voleti"}, {"authorId": "3127597", "name": "S. Kahou"}, {"authorId": "1748421", "name": "Vincent Michalski"}, {"authorId": "1862138", "name": "Dmitriy Serdyuk"}, {"authorId": "1699104", "name": "T. Arbel"}, {"authorId": "98109738", "name": "Chris Pal"}, {"authorId": "3025780", "name": "G. Varoquaux"}, {"authorId": "120247189", "name": "Pascal Vincent"}]}, {"paperId": "998e78d2d7bccee1e8665e0aca991ab1cef7bc31", "url": "https://www.semanticscholar.org/paper/998e78d2d7bccee1e8665e0aca991ab1cef7bc31", "title": "Quantum Machine Learning in Chemical Compound Space", "abstract": "Rather than numerically solving the computationally demanding equations of quantum or statistical mechanics, machine learning methods can infer approximate solutions, interpolating previously acquired property data sets of molecules and materials. The case is made for quantum machine learning: An inductive molecular modeling approach which can be applied to quantum chemistry problems.", "year": 2018, "referenceCount": 397, "citationCount": 109, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "11615881", "name": "O. A. V. Lilienfeld"}]}, {"paperId": "bfbd10ebffc9494423770a5bd30ebd0f9cbce66d", "url": "https://www.semanticscholar.org/paper/bfbd10ebffc9494423770a5bd30ebd0f9cbce66d", "title": "Device Placement Optimization with Reinforcement Learning", "abstract": "The past few years have witnessed a growth in size and computational requirements for training and inference with neural networks. Currently, a common approach to address these requirements is to use a heterogeneous distributed environment with a mixture of hardware devices such as CPUs and GPUs. Importantly, the decision of placing parts of the neural models on devices is often made by human experts based on simple heuristics and intuitions. In this paper, we propose a method which learns to optimize device placement for TensorFlow computational graphs. Key to our method is the use of a sequence-to-sequence model to predict which subsets of operations in a TensorFlow graph should run on which of the available devices. The execution time of the predicted placements is then used as the reward signal to optimize the parameters of the sequence-to-sequence model. Our main result is that on Inception-V3 for ImageNet classification, and on RNN LSTM, for language modeling and neural machine translation, our model finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods.", "year": 2017, "referenceCount": 49, "citationCount": 339, "influentialCitationCount": 36, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1861312", "name": "Azalia Mirhoseini"}, {"authorId": "143950636", "name": "Hieu Pham"}, {"authorId": "2827616", "name": "Quoc V. Le"}, {"authorId": "32163737", "name": "Benoit Steiner"}, {"authorId": "47223513", "name": "Rasmus Larsen"}, {"authorId": "2116451595", "name": "Yuefeng Zhou"}, {"authorId": "2116960562", "name": "Naveen Kumar"}, {"authorId": "144739074", "name": "Mohammad Norouzi"}, {"authorId": "1751569", "name": "Samy Bengio"}, {"authorId": "48448318", "name": "J. Dean"}]}, {"paperId": "1e13454c3388ccc2ae44ff307979256b09bed1bf", "url": "https://www.semanticscholar.org/paper/1e13454c3388ccc2ae44ff307979256b09bed1bf", "title": "On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization", "abstract": "Recent developments on large-scale distributed machine learning applications, e.g., deep neural networks, benefit enormously from the advances in distributed non-convex optimization techniques, e.g., distributed Stochastic Gradient Descent (SGD). A series of recent works study the linear speedup property of distributed SGD variants with reduced communication. The linear speedup property enable us to scale out the computing capability by adding more computing nodes into our system. The reduced communication complexity is desirable since communication overhead is often the performance bottleneck in distributed systems. Recently, momentum methods are more and more widely adopted in training machine learning models and can often converge faster and generalize better. For example, many practitioners use distributed SGD with momentum to train deep neural networks with big data. However, it remains unclear whether any distributed momentum SGD possesses the same linear speedup property as distributed SGD and has reduced communication complexity. This paper fills the gap by considering a distributed communication efficient momentum SGD method and proving its linear speedup property.", "year": 2019, "referenceCount": 43, "citationCount": 222, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "38713459", "name": "Hao Yu"}, {"authorId": "144723884", "name": "Rong Jin"}, {"authorId": "2140181029", "name": "Sen Yang"}]}, {"paperId": "ded6403e634a9fa620dc071c71149ca5632e217b", "url": "https://www.semanticscholar.org/paper/ded6403e634a9fa620dc071c71149ca5632e217b", "title": "NetMHCpan-3.0; improved prediction of binding to MHC class I molecules integrating information from multiple receptor and peptide length datasets", "abstract": null, "year": 2016, "referenceCount": 32, "citationCount": 387, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Biology"], "authors": [{"authorId": "143898643", "name": "M. Nielsen"}, {"authorId": "2930314", "name": "M. Andreatta"}]}, {"paperId": "35c2814858423862353e100c6e7979c58187fdcf", "url": "https://www.semanticscholar.org/paper/35c2814858423862353e100c6e7979c58187fdcf", "title": "High-Throughput Machine-Learning-Driven Synthesis of Full-Heusler Compounds", "abstract": "A machine-learning model has been trained to discover Heusler compounds, which are intermetallics exhibiting diverse physical properties attractive for applications in thermoelectric and spintronic materials. Improving these properties requires knowledge of crystal structures, which occur in three subtle variations (Heusler, inverse Heusler, and CsCl-type structures) that are difficult, and at times impossible, to distinguish by diffraction techniques. Compared to alternative approaches, this Heusler discovery engine performs exceptionally well, making fast and reliable predictions of the occurrence of Heusler vs non-Heusler compounds for an arbitrary combination of elements with no structural input on over 400\u202f000 candidates. The model has a true positive rate of 0.94 (and false positive rate of 0.01). It is also valuable for data sanitizing, by flagging questionable entries in crystallographic databases. It was applied to screen candidates with the formula AB2C and predict the existence of 12 novel gall...", "year": 2016, "referenceCount": 39, "citationCount": 218, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Materials Science"], "authors": [{"authorId": "15739628", "name": "A. Oliynyk"}, {"authorId": "4995443", "name": "Erin Antono"}, {"authorId": "15227654", "name": "Taylor D. Sparks"}, {"authorId": "13336968", "name": "Leila Ghadbeigi"}, {"authorId": "10019044", "name": "M. Gaultois"}, {"authorId": "8839934", "name": "B. Meredig"}, {"authorId": "34052585", "name": "A. Mar"}]}, {"paperId": "c636a2dd242908fe2e598a1077c0c57bfdea8633", "url": "https://www.semanticscholar.org/paper/c636a2dd242908fe2e598a1077c0c57bfdea8633", "title": "ReasoNet: Learning to Stop Reading in Machine Comprehension", "abstract": "Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset.", "year": 2016, "referenceCount": 38, "citationCount": 295, "influentialCitationCount": 24, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1752875", "name": "Yelong Shen"}, {"authorId": "2421691", "name": "Po-Sen Huang"}, {"authorId": "1800422", "name": "Jianfeng Gao"}, {"authorId": "2109136147", "name": "Weizhu Chen"}]}, {"paperId": "cef4d6ec6505d3d3dcbc9365802947dda107dba2", "url": "https://www.semanticscholar.org/paper/cef4d6ec6505d3d3dcbc9365802947dda107dba2", "title": "Top-down induction of decision trees classifiers - a survey", "abstract": "Decision trees are considered to be one of the most popular approaches for representing classifiers. Researchers from various disciplines such as statistics, machine learning, pattern recognition, and data mining considered the issue of growing a decision tree from available data. This paper presents an updated survey of current methods for constructing decision tree classifiers in a top-down manner. The paper suggests a unified algorithmic framework for presenting these algorithms and describes the various splitting criteria and pruning methodologies.", "year": 2005, "referenceCount": 89, "citationCount": 727, "influentialCitationCount": 32, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1732091", "name": "L. Rokach"}, {"authorId": "1770487", "name": "O. Maimon"}]}, {"paperId": "f0b9b5d6f79d62305f47ef560512bf53d17e38e2", "url": "https://www.semanticscholar.org/paper/f0b9b5d6f79d62305f47ef560512bf53d17e38e2", "title": "Kernel Mean Embedding of Distributions: A Review and Beyonds", "abstract": "A Hilbert space embedding of a distribution---in short, a kernel mean embedding---has recently emerged as a powerful tool for machine learning and inference. The basic idea behind this framework is to map distributions into a reproducing kernel Hilbert space (RKHS) in which the whole arsenal of kernel methods can be extended to probability measures. It can be viewed as a generalization of the original \"feature map\" common to support vector machines (SVMs) and other kernel methods. While initially closely associated with the latter, it has meanwhile found application in fields ranging from kernel machines and probabilistic modeling to statistical inference, causal discovery, and deep learning. The goal of this survey is to give a comprehensive review of existing work and recent advances in this research area, and to discuss the most challenging issues and open problems that could lead to new research directions. The survey begins with a brief introduction to the RKHS and positive definite kernels which forms the backbone of this survey, followed by a thorough discussion of the Hilbert space embedding of marginal distributions, theoretical guarantees, and a review of its applications. The embedding of distributions enables us to apply RKHS methods to probability measures which prompts a wide range of applications such as kernel two-sample testing, independent testing, and learning on distributional data. Next, we discuss the Hilbert space embedding for conditional distributions, give theoretical insights, and review some applications. The conditional mean embedding enables us to perform sum, product, and Bayes' rules---which are ubiquitous in graphical model, probabilistic inference, and reinforcement learning---in a non-parametric way. We then discuss relationships between this framework and other related areas. Lastly, we give some suggestions on future research directions.", "year": 2016, "referenceCount": 309, "citationCount": 451, "influentialCitationCount": 48, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2276351", "name": "Krikamol Muandet"}, {"authorId": "1693668", "name": "K. Fukumizu"}, {"authorId": "3313411", "name": "Bharath K. Sriperumbudur"}, {"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}]}, {"paperId": "441fbfdcc77187c9f9c41166b5fd42de04de1427", "url": "https://www.semanticscholar.org/paper/441fbfdcc77187c9f9c41166b5fd42de04de1427", "title": "Deep learning in remote sensing: a review", "abstract": "Standing at the paradigm shift towards data-intensive science, machine learning techniques are becoming increasingly important. In particular, as a major breakthrough in the field, deep learning has proven as an extremely powerful tool in many fields. Shall we embrace deep learning as the key to all? Or, should we resist a 'black-box' solution? There are controversial opinions in the remote sensing community. In this article, we analyze the challenges of using deep learning for remote sensing data analysis, review the recent advances, and provide resources to make deep learning in remote sensing ridiculously simple to start with. More importantly, we advocate remote sensing scientists to bring their expertise into deep learning, and use it as an implicit general model to tackle unprecedented large-scale influential challenges, such as climate change and urbanization.", "year": 2017, "referenceCount": 200, "citationCount": 394, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "46875441", "name": "Xiaoxiang Zhu"}, {"authorId": "2977931", "name": "D. Tuia"}, {"authorId": "35041003", "name": "Lichao Mou"}, {"authorId": "39943835", "name": "Gui-Song Xia"}, {"authorId": "9802604", "name": "Liang-pei Zhang"}, {"authorId": "2152480620", "name": "Feng Xu"}, {"authorId": "2033171", "name": "F. Fraundorfer"}]}, {"paperId": "b20e117d5d05cba6dd070fe50afbb8a71d9a4618", "url": "https://www.semanticscholar.org/paper/b20e117d5d05cba6dd070fe50afbb8a71d9a4618", "title": "Deep learning for neuroimaging: a validation study", "abstract": "Deep learning methods have recently made notable advances in the tasks of classification and representation learning. These tasks are important for brain imaging and neuroscience discovery, making the methods attractive for porting to a neuroimager's toolbox. Success of these methods is, in part, explained by the flexibility of deep learning models. However, this flexibility makes the process of porting to new areas a difficult parameter optimization problem. In this work we demonstrate our results (and feasible parameter ranges) in application of deep learning methods to structural and functional brain imaging data. These methods include deep belief networks and their building block the restricted Boltzmann machine. We also describe a novel constraint-based approach to visualizing high dimensional data. We use it to analyze the effect of parameter choices on data transformations. Our results show that deep learning methods are able to learn physiologically important representations and detect latent relations in neuroimaging data.", "year": 2013, "referenceCount": 48, "citationCount": 497, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "2122479", "name": "S. Plis"}, {"authorId": "40482726", "name": "R. Devon Hjelm"}, {"authorId": "145124475", "name": "R. Salakhutdinov"}, {"authorId": "144048760", "name": "V. Calhoun"}]}, {"paperId": "176f1d608b918eec8dc4b75e7b6e0acaba84a447", "url": "https://www.semanticscholar.org/paper/176f1d608b918eec8dc4b75e7b6e0acaba84a447", "title": "Adversarial Learning for Neural Dialogue Generation", "abstract": "We apply adversarial training to open-domain dialogue generation, training a system to produce sequences that are indistinguishable from human-generated dialogue utterances. We cast the task as a reinforcement learning problem where we jointly train two systems: a generative model to produce response sequences, and a discriminator\u2014analagous to the human evaluator in the Turing test\u2014 to distinguish between the human-generated dialogues and the machine-generated ones. In this generative adversarial network approach, the outputs from the discriminator are used to encourage the system towards more human-like dialogue. Further, we investigate models for adversarial evaluation that uses success in fooling an adversary as a dialogue evaluation metric, while avoiding a number of potential pitfalls. Experimental results on several metrics, including adversarial evaluation, demonstrate that the adversarially-trained system generates higher-quality responses than previous baselines", "year": 2017, "referenceCount": 52, "citationCount": 791, "influentialCitationCount": 112, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "49298465", "name": "Jiwei Li"}, {"authorId": "145768639", "name": "Will Monroe"}, {"authorId": "10238549", "name": "Tianlin Shi"}, {"authorId": "152857609", "name": "S\u00e9bastien Jean"}, {"authorId": "1863425", "name": "Alan Ritter"}, {"authorId": "1746807", "name": "Dan Jurafsky"}]}, {"paperId": "737a59b9f7f45885bbae959ca6f254170bd2ade3", "url": "https://www.semanticscholar.org/paper/737a59b9f7f45885bbae959ca6f254170bd2ade3", "title": "Quantitative Phase Imaging and Artificial Intelligence: A Review", "abstract": "Recent advances in quantitative phase imaging (QPI) and artificial intelligence (AI) have opened up the possibility of an exciting frontier. The fast and label-free nature of QPI enables the rapid generation of large-scale and uniform-quality imaging data in two, three, and four dimensions. Subsequently, the AI-assisted interrogation of QPI data using data-driven machine learning techniques results in a variety of biomedical applications. Also, machine learning enhances QPI itself. Herein, we review the synergy between QPI and machine learning with a particular focus on deep learning. Furthermore, we provide practical guidelines and perspectives for further development.", "year": 2018, "referenceCount": 211, "citationCount": 118, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics"], "authors": [{"authorId": "144344831", "name": "YoungJu Jo"}, {"authorId": "2453689", "name": "Hyungjoon Cho"}, {"authorId": "2152576625", "name": "Sang Yun Lee"}, {"authorId": "27696356", "name": "Gunho Choi"}, {"authorId": "145289485", "name": "Geon Kim"}, {"authorId": "153842738", "name": "Hyun-Seok Min"}, {"authorId": "3220236", "name": "Yongkeun Park"}]}, {"paperId": "af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2", "url": "https://www.semanticscholar.org/paper/af3f67b6639a50fd094e1467a2f3b6b8fef7c7c2", "title": "Transformers: State-of-the-Art Natural Language Processing", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.", "year": 2019, "referenceCount": 42, "citationCount": 3391, "influentialCitationCount": 131, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50335211", "name": "Thomas Wolf"}, {"authorId": "1380459402", "name": "Lysandre Debut"}, {"authorId": "51918868", "name": "Victor Sanh"}, {"authorId": "40811585", "name": "Julien Chaumond"}, {"authorId": "40899333", "name": "Clement Delangue"}, {"authorId": "1382164294", "name": "Anthony Moi"}, {"authorId": "1382164165", "name": "Pierric Cistac"}, {"authorId": "1382164170", "name": "T. Rault"}, {"authorId": "2185329", "name": "R\u00e9mi Louf"}, {"authorId": "97662964", "name": "Morgan Funtowicz"}, {"authorId": "1383218348", "name": "Jamie Brew"}]}, {"paperId": "e8d92bfb6d71e7863a03a011252f553902c009c2", "url": "https://www.semanticscholar.org/paper/e8d92bfb6d71e7863a03a011252f553902c009c2", "title": "The Big Data Newsvendor: Practical Insights from Machine Learning", "abstract": "In Ban and Rudin\u2019s (2018) \u201cThe Big Data Newsvendor: Practical Insights from Machine Learning,\u201d the authors take an innovative machine-learning approach to a classic problem solved by almost every company, every day, for inventory management. By allowing companies to use large amounts of data to predict the correct answers to decisions directly, they avoid intermediate questions, such as \u201chow many customers will we get tomorrow?\u201d and instead can tell the company how much inventory to stock for these customers. This has implications for almost all other decision-making problems considered in operations research, which has traditionally considered data estimation separately from the decision optimization. Their proposed methods are shown to work both analytically and empirically with the latter explored in a hospital nurse staffing example in which the best one-step, feature-based newsvendor algorithm (the kernel-weights optimization method) is shown to beat the best-practice benchmark by 24% in the out-of-sample cost at a fraction of the speed.", "year": 2019, "referenceCount": 88, "citationCount": 218, "influentialCitationCount": 31, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "103243641", "name": "Gah-Yi Ban"}, {"authorId": "48395540", "name": "C. Rudin"}]}, {"paperId": "31bb481c1beb48ff23749989c5a88969e506e398", "url": "https://www.semanticscholar.org/paper/31bb481c1beb48ff23749989c5a88969e506e398", "title": "Extreme learning machine and its applications", "abstract": null, "year": 2014, "referenceCount": 66, "citationCount": 149, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145613107", "name": "Shifei Ding"}, {"authorId": "29279000", "name": "Xinzheng Xu"}, {"authorId": "2066146121", "name": "Ru Nie"}]}, {"paperId": "503dc3243d206ac86ac13bdc2bf38d1b3272b962", "url": "https://www.semanticscholar.org/paper/503dc3243d206ac86ac13bdc2bf38d1b3272b962", "title": "High-Performance Extreme Learning Machines: A Complete Toolbox for Big Data Applications", "abstract": "This paper presents a complete approach to a successful utilization of a high-performance extreme learning machines (ELMs) Toolbox for Big Data. It summarizes recent advantages in algorithmic performance; gives a fresh view on the ELM solution in relation to the traditional linear algebraic performance; and reaps the latest software and hardware performance achievements. The results are applicable to a wide range of machine learning problems and thus provide a solid ground for tackling numerous Big Data challenges. The included toolbox is targeted at enabling the full potential of ELMs to the widest range of users.", "year": 2015, "referenceCount": 49, "citationCount": 207, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2798381", "name": "Anton Akusok"}, {"authorId": "3347255", "name": "Kaj-Mikael Bj\u00f6rk"}, {"authorId": "3194493", "name": "Y. Mich\u00e9"}, {"authorId": "1731049", "name": "A. Lendasse"}]}, {"paperId": "28f734ca252df1709a1314197838c781a7248c59", "url": "https://www.semanticscholar.org/paper/28f734ca252df1709a1314197838c781a7248c59", "title": "Combining Statistical Learning with a Knowledge-Based Approach - A Case Study in Intensive Care Monitoring", "abstract": "The paper describes a case study in combining different methods for acquiring medical knowledge. Given a huge amount of noisy, high dimensional numerical time series data describing patients in intensive care, the support vector machine is used to learn when and how to change the dose of which drug. Given medical knowledge about and expertise in clinical decision making, a first-order logic knowledge base about effects of therapeutical interventions has been built. As a preprocessing mechanism it uses another statistical method. The integration of numerical and knowledge-based procedures eases the task of validation in two ways. On one hand, the knowledge base is validated with respect to past patients records. On the other hand, medical interventions that are recommended by learning results are justified by the knowledge base.", "year": 1999, "referenceCount": 10, "citationCount": 418, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1752599", "name": "K. Morik"}, {"authorId": "2406679", "name": "Peter Brockhausen"}, {"authorId": "1680188", "name": "T. Joachims"}]}, {"paperId": "b3d61c603378341b2d7769ab01012111127ead31", "url": "https://www.semanticscholar.org/paper/b3d61c603378341b2d7769ab01012111127ead31", "title": "A machine learning information retrieval approach to protein fold recognition", "abstract": "MOTIVATION\nRecognizing proteins that have similar tertiary structure is the key step of template-based protein structure prediction methods. Traditionally, a variety of alignment methods are used to identify similar folds, based on sequence similarity and sequence-structure compatibility. Although these methods are complementary, their integration has not been thoroughly exploited. Statistical machine learning methods provide tools for integrating multiple features, but so far these methods have been used primarily for protein and fold classification, rather than addressing the retrieval problem of fold recognition-finding a proper template for a given query protein.\n\n\nRESULTS\nHere we present a two-stage machine learning, information retrieval, approach to fold recognition. First, we use alignment methods to derive pairwise similarity features for query-template protein pairs. We also use global profile-profile alignments in combination with predicted secondary structure, relative solvent accessibility, contact map and beta-strand pairing to extract pairwise structural compatibility features. Second, we apply support vector machines to these features to predict the structural relevance (i.e. in the same fold or not) of the query-template pairs. For each query, the continuous relevance scores are used to rank the templates. The FOLDpro approach is modular, scalable and effective. Compared with 11 other fold recognition methods, FOLDpro yields the best results in almost all standard categories on a comprehensive benchmark dataset. Using predictions of the top-ranked template, the sensitivity is approximately 85, 56, and 27% at the family, superfamily and fold levels respectively. Using the 5 top-ranked templates, the sensitivity increases to 90, 70, and 48%.", "year": 2006, "referenceCount": 123, "citationCount": 225, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "39609402", "name": "Jianlin Cheng"}, {"authorId": "144902513", "name": "P. Baldi"}]}, {"paperId": "9f63cc53122173598c80c367d03014d1ed49be9f", "url": "https://www.semanticscholar.org/paper/9f63cc53122173598c80c367d03014d1ed49be9f", "title": "From signals to knowledge: A conceptual model for multimodal learning analytics", "abstract": "Multimodality in learning analytics and learning science is under the spotlight. The landscape of sensors and wearable trackers that can be used for learning support is evolving rapidly, as well as data collection and analysis methods. Multimodal data can now be collected and processed in real time at an unprecedented scale. With sensors, it is possible to capture observable events of the learning process such as learner's behaviour and the learning context. The learning process, however, consists also of latent attributes, such as the learner's cognitions or emotions. These attributes are unobservable to sensors and need to be elicited by human\u2010driven interpretations. We conducted a literature survey of experiments using multimodal data to frame the young research field of multimodal learning analytics. The survey explored the multimodal data used in related studies (the input space) and the learning theories selected (the hypothesis space). The survey led to the formulation of the Multimodal Learning Analytics Model whose main objectives are of (O1) mapping the use of multimodal data to enhance the feedback in a learning context; (O2) showing how to combine machine learning with multimodal data; and (O3) aligning the terminology used in the field of machine learning and learning science", "year": 2018, "referenceCount": 58, "citationCount": 88, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3413100", "name": "D. D. Mitri"}, {"authorId": "40405611", "name": "J. Schneider"}, {"authorId": "143989442", "name": "M. Specht"}, {"authorId": "1714952", "name": "H. Drachsler"}]}, {"paperId": "f4cb568fa8bbab1afcc87b62538f6052c770a660", "url": "https://www.semanticscholar.org/paper/f4cb568fa8bbab1afcc87b62538f6052c770a660", "title": "Incorporating prior information in machine learning by creating virtual examples", "abstract": "One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training-set size. We show that in some contexts this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well motivated. The process of creating virtual examples in real-world pattern recognition tasks is highly nontrivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea.", "year": 1998, "referenceCount": 55, "citationCount": 307, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1770745", "name": "P. Niyogi"}, {"authorId": "1804489", "name": "F. Girosi"}, {"authorId": "1685292", "name": "T. Poggio"}]}, {"paperId": "f9163156eeba67762a7441db48fe6720106137cd", "url": "https://www.semanticscholar.org/paper/f9163156eeba67762a7441db48fe6720106137cd", "title": "Survey on deep learning with class imbalance", "abstract": null, "year": 2019, "referenceCount": 117, "citationCount": 867, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2115231138", "name": "Justin M. Johnson"}, {"authorId": "1725285", "name": "T. Khoshgoftaar"}]}, {"paperId": "770990076b4dd6bd9078ec6fe3d9680bcf766dde", "url": "https://www.semanticscholar.org/paper/770990076b4dd6bd9078ec6fe3d9680bcf766dde", "title": "IoT Security Techniques Based on Machine Learning", "abstract": "Internet of things (IoT) that integrate a variety of devices into networks to provide advanced and intelligent services have to protect user privacy and address attacks such as spoofing attacks, denial of service attacks, jamming and eavesdropping. In this article, we investigate the attack model for IoT systems, and review the IoT security solutions based on machine learning techniques including supervised learning, unsupervised learning and reinforcement learning. We focus on the machine learning based IoT authentication, access control, secure offloading and malware detection schemes to protect data privacy. In this article, we discuss the challenges that need to be addressed to implement these machine learning based security schemes in practical IoT systems.", "year": 2018, "referenceCount": 37, "citationCount": 79, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "33821409", "name": "Liang Xiao"}, {"authorId": "14423947", "name": "Xiaoyue Wan"}, {"authorId": "47062683", "name": "Xiaozhen Lu"}, {"authorId": "38200671", "name": "Yanyong Zhang"}, {"authorId": "92148538", "name": "Di Wu"}]}, {"paperId": "9e18f56bfee171cd53702ad028a1be957683eeaa", "url": "https://www.semanticscholar.org/paper/9e18f56bfee171cd53702ad028a1be957683eeaa", "title": "Asynchronous Byzantine Machine Learning (the case of SGD)", "abstract": "Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce \\emph{Kardam}, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against $\\frac{1}{3}$ Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than $f/n$, where $f$ is the number of Byzantine failures tolerated and $n$ the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.", "year": 2018, "referenceCount": 36, "citationCount": 81, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "11008382", "name": "Georgios Damaskinos"}, {"authorId": "9623412", "name": "El Mahdi El Mhamdi"}, {"authorId": "1727558", "name": "R. Guerraoui"}, {"authorId": "40051618", "name": "Rhicheek Patra"}, {"authorId": "3035974", "name": "Mahsa Taziki"}]}, {"paperId": "af1841e1db6579f1f1777a59c7e9e4658d2ac466", "url": "https://www.semanticscholar.org/paper/af1841e1db6579f1f1777a59c7e9e4658d2ac466", "title": "PATE-GAN: Generating Synthetic Data with Differential Privacy Guarantees", "abstract": "Machine learning has the potential to assist many communities in using the large datasets that are becoming more and more available. Unfortunately, much of that potential is not being realized because it would require sharing data in a way that compromises privacy. In this paper, we investigate a method for ensuring (differential) privacy of the generator of the Generative Adversarial Nets (GAN) framework. The resulting model can be used for generating synthetic data on which algorithms can be trained and validated, and on which competitions can be conducted, without compromising the privacy of the original dataset. Our method modifies the Private Aggregation of Teacher Ensembles (PATE) framework and applies it to GANs. Our modified framework (which we call PATE-GAN) allows us to tightly bound the influence of any individual sample on the model, resulting in tight differential privacy guarantees and thus an improved performance over models with the same guarantees. We also look at measuring the quality of synthetic data from a new angle; we assert that for the synthetic data to be useful for machine learning researchers, the relative performance of two algorithms (trained and tested) on the synthetic dataset should be the same as their relative performance (when trained and tested) on the original dataset. Our experiments, on various datasets, demonstrate that PATE-GAN consistently outperforms the stateof-the-art method with respect to this and other notions of synthetic data quality.", "year": 2018, "referenceCount": 29, "citationCount": 287, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "37996637", "name": "James Jordon"}, {"authorId": "2144029", "name": "Jinsung Yoon"}, {"authorId": "1729969", "name": "M. Schaar"}]}, {"paperId": "4e4c56efc132ec1506cf7edb1077608b2483354e", "url": "https://www.semanticscholar.org/paper/4e4c56efc132ec1506cf7edb1077608b2483354e", "title": "On breast cancer detection: an application of machine learning algorithms on the wisconsin diagnostic dataset", "abstract": "This paper presents a comparison of six machine learning (ML) algorithms: GRU-SVM[1], Linear Regression, Multilayer Perceptron (MLP), Nearest Neighbor (NN) search, Softmax Regression, and Support Vector Machine (SVM) on the Wisconsin Diagnostic Breast Cancer (WDBC) dataset[2] by measuring their classification test accuracy, and their sensitivity and specificity values. The said dataset consists of features which were computed from digitized images of FNA tests on a breast mass[2]. For the implementation of the ML algorithms, the dataset was partitioned in the following fashion: 70% for training phase, and 30% for the testing phase. The hyper-parameters used for all the classifiers were manually assigned. Results show that all the presented ML algorithms performed well (all exceeded 90% test accuracy) on the classification task. The MLP algorithm stands out among the implemented algorithms with a test accuracy of \u224899.04%.", "year": 2017, "referenceCount": 20, "citationCount": 140, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "26412983", "name": "Abien Fred Agarap"}]}, {"paperId": "5794141889d0e994c3103b0aaab08a18222c9c43", "url": "https://www.semanticscholar.org/paper/5794141889d0e994c3103b0aaab08a18222c9c43", "title": "Support vector machine classification and validation of cancer tissue samples using microarray expression data", "abstract": "MOTIVATION\nDNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.\n\n\nRESULTS\nWe demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment results for 97,802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed. The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.\n\n\nAVAILABILITY\nThe SVM software is available at http://www.cs. columbia.edu/ approximately bgrundy/svm.", "year": 2000, "referenceCount": 33, "citationCount": 2478, "influentialCitationCount": 103, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Biology", "Medicine"], "authors": [{"authorId": "1716986", "name": "T. Furey"}, {"authorId": "1685083", "name": "N. Cristianini"}, {"authorId": "143857271", "name": "Nigel P. Duffy"}, {"authorId": "34700222", "name": "D. Bednarski"}, {"authorId": "5049147", "name": "M. Schummer"}, {"authorId": "1733689", "name": "D. Haussler"}]}, {"paperId": "35bcf0efc70975eff7dd4bb95856cdee66c6a159", "url": "https://www.semanticscholar.org/paper/35bcf0efc70975eff7dd4bb95856cdee66c6a159", "title": "Region-based Image Annotation using Asymmetrical Support Vector Machine-based Multiple-Instance Learning", "abstract": "In region-based image annotation, keywords are usually associated with images instead of individual regions in the training data set. This poses a major challenge for any learning strategy. In this paper, we formulate image annotation as a supervised learning problem under Multiple-Instance Learning (MIL) framework. We present a novel Asymmetrical Support Vector Machine-based MIL algorithm (ASVM-MIL), which extends the conventional Support Vector Machine (SVM) to the MIL setting by introducing asymmetrical loss functions for false positives and false negatives. The proposed ASVM-MIL algorithm is evaluated on both image annotation data sets and the benchmark MUSK data sets.", "year": 2006, "referenceCount": 17, "citationCount": 175, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1723700", "name": "Changbo Yang"}, {"authorId": "144964053", "name": "Ming Dong"}, {"authorId": "48840512", "name": "Jing Hua"}]}, {"paperId": "ea5288e280babc95c4c22c7b1bdbf26b43585e94", "url": "https://www.semanticscholar.org/paper/ea5288e280babc95c4c22c7b1bdbf26b43585e94", "title": "Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems", "abstract": "Several researchers have argued that a machine \nlearning system\u2019s interpretability should be defined \nin relation to a specific agent or task: we \nshould not ask if the system is interpretable, but \nto whom is it interpretable. We describe a model \nintended to help answer this question, by identifying \ndifferent roles that agents can fulfill in relation \nto the machine learning system. We illustrate the \nuse of our model in a variety of scenarios, exploring \nhow an agent\u2019s role influences its goals, \nand the implications for defining interpretability. \nFinally, we make suggestions for how our model \ncould be useful to interpretability researchers, system \ndevelopers, and regulatory bodies auditing \nmachine learning systems.", "year": 2018, "referenceCount": 31, "citationCount": 107, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50998197", "name": "Richard J. Tomsett"}, {"authorId": "2215679", "name": "Dave Braines"}, {"authorId": "51021483", "name": "Daniel Harborne"}, {"authorId": "1762890", "name": "A. Preece"}, {"authorId": "144387904", "name": "Supriyo Chakraborty"}]}, {"paperId": "51c1519a57a65351a713a3d74f8d477105df0ec3", "url": "https://www.semanticscholar.org/paper/51c1519a57a65351a713a3d74f8d477105df0ec3", "title": "Prior Knowledge in Support Vector Kernels", "abstract": "We explore methods for incorporating prior knowledge about a problem at hand in Support Vector learning machines. We show that both invariances under group transformations and prior knowledge about locality in images can be incorporated by constructing appropriate kernel functions.", "year": 1997, "referenceCount": 8, "citationCount": 356, "influentialCitationCount": 23, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}, {"authorId": "2812486", "name": "P. Simard"}, {"authorId": "46234526", "name": "Alex Smola"}, {"authorId": "50560492", "name": "V. Vapnik"}]}, {"paperId": "10069b0c06e83fbb6bf837f78adb28b3efaf2ace", "url": "https://www.semanticscholar.org/paper/10069b0c06e83fbb6bf837f78adb28b3efaf2ace", "title": "A Perspective on Using Machine Learning in 3D Bioprinting", "abstract": "Recently, three-dimensional (3D) printing technologies have been widely applied in industry and our daily lives. The term 3D bioprinting has been coined to describe 3D printing at the biomedical level. Machine learning is currently becoming increasingly active and has been used to improve 3D printing processes, such as process optimization, dimensional accuracy analysis, manufacturing defect detection, and material property prediction. However, few studies have been found to use machine learning in 3D bioprinting processes. In this paper, related machine learning methods used in 3D printing are briefly reviewed and a perspective on how machine learning can also benefit 3D bioprinting is discussed. We believe that machine learning can significantly affect the future development of 3D bioprinting and hope this paper can inspire some ideas on how machine learning can be used to improve 3D bioprinting.", "year": 2020, "referenceCount": 39, "citationCount": 71, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "93384147", "name": "Chunling Yu"}, {"authorId": "3401459", "name": "Jingchao Jiang"}]}, {"paperId": "d8ad3afdf4480c646169b916e81573bd6eef6b17", "url": "https://www.semanticscholar.org/paper/d8ad3afdf4480c646169b916e81573bd6eef6b17", "title": "Machine learning methods for wind turbine condition monitoring: A review", "abstract": null, "year": 2019, "referenceCount": 156, "citationCount": 324, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2719706", "name": "Adrian Stetco"}, {"authorId": "51967241", "name": "F. Dinmohammadi"}, {"authorId": "47039303", "name": "Xingyu Zhao"}, {"authorId": "1718021", "name": "V. Robu"}, {"authorId": "143836960", "name": "D. Flynn"}, {"authorId": "144988937", "name": "M. Barnes"}, {"authorId": "145025539", "name": "J. Keane"}, {"authorId": "2144507", "name": "G. Nenadic"}]}, {"paperId": "fbf5f098b9378b08a3d5bcd2fab97998d8efad9d", "url": "https://www.semanticscholar.org/paper/fbf5f098b9378b08a3d5bcd2fab97998d8efad9d", "title": "An Evaluation of Machine Learning-Based Methods for Detection of Phishing Sites", "abstract": null, "year": 2008, "referenceCount": 23, "citationCount": 92, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2974755", "name": "Daisuke Miyamoto"}, {"authorId": "2926294", "name": "H. Hazeyama"}, {"authorId": "1751653", "name": "Y. Kadobayashi"}]}, {"paperId": "72d3ddf1f7210d7e70144bbc09f770ec411fe909", "url": "https://www.semanticscholar.org/paper/72d3ddf1f7210d7e70144bbc09f770ec411fe909", "title": "Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence", "abstract": "Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical machine learning and scalable general-purpose graphics processing unit (GPU) computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.", "year": 2020, "referenceCount": 249, "citationCount": 127, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2562040", "name": "S. Raschka"}, {"authorId": "2055466552", "name": "Joshua Patterson"}, {"authorId": "1491910676", "name": "Corey J. Nolet"}]}, {"paperId": "719e4e1328be9487b33a13dc38b6120993999ed5", "url": "https://www.semanticscholar.org/paper/719e4e1328be9487b33a13dc38b6120993999ed5", "title": "Introduction to expert systems", "abstract": "From the Publisher: \nThe third edition of Peter Jackson's book, Introduction to Expert Systems, updates the technological base of expert systems research and embeds those results in the context of a wide variety of application areas. The earlier chapters take a more practical approach to the basic topics than the previous editions, while the later chapters introduce new topic areas, such as case-based reasoning, connectionist systems, and hybrid systems. Results in related areas, such as machine learning and reasoning with uncertainty are also accorded a thorough treatment.", "year": 1986, "referenceCount": 10, "citationCount": 1233, "influentialCitationCount": 36, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143791005", "name": "Peter Jackson"}]}, {"paperId": "235d581134ef34887a75c6f35a0a2ff23409f40d", "url": "https://www.semanticscholar.org/paper/235d581134ef34887a75c6f35a0a2ff23409f40d", "title": "A Comparison of a Machine Learning Model with EuroSCORE II in Predicting Mortality after Elective Cardiac Surgery: A Decision Curve Analysis", "abstract": "Background The benefits of cardiac surgery are sometimes difficult to predict and the decision to operate on a given individual is complex. Machine Learning and Decision Curve Analysis (DCA) are recent methods developed to create and evaluate prediction models. Methods and finding We conducted a retrospective cohort study using a prospective collected database from December 2005 to December 2012, from a cardiac surgical center at University Hospital. The different models of prediction of mortality in-hospital after elective cardiac surgery, including EuroSCORE II, a logistic regression model and a machine learning model, were compared by ROC and DCA. Of the 6,520 patients having elective cardiac surgery with cardiopulmonary bypass, 6.3% died. Mean age was 63.4 years old (standard deviation 14.4), and mean EuroSCORE II was 3.7 (4.8) %. The area under ROC curve (IC95%) for the machine learning model (0.795 (0.755\u20130.834)) was significantly higher than EuroSCORE II or the logistic regression model (respectively, 0.737 (0.691\u20130.783) and 0.742 (0.698\u20130.785), p < 0.0001). Decision Curve Analysis showed that the machine learning model, in this monocentric study, has a greater benefit whatever the probability threshold. Conclusions According to ROC and DCA, machine learning model is more accurate in predicting mortality after elective cardiac surgery than EuroSCORE II. These results confirm the use of machine learning methods in the field of medical prediction.", "year": 2017, "referenceCount": 37, "citationCount": 102, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "4352053", "name": "J. Allyn"}, {"authorId": "4812633", "name": "N. Allou"}, {"authorId": "145994468", "name": "P. Augustin"}, {"authorId": "48402713", "name": "I. Philip"}, {"authorId": "5702371", "name": "O. Martinet"}, {"authorId": "2093806764", "name": "Myriem Belghiti"}, {"authorId": "6390424", "name": "S. Provench\u00e8re"}, {"authorId": "6542497", "name": "P. Montravers"}, {"authorId": "5720810", "name": "C. Ferdynus"}]}, {"paperId": "264efad57d38a3146c93ea243f377650f2ed31c1", "url": "https://www.semanticscholar.org/paper/264efad57d38a3146c93ea243f377650f2ed31c1", "title": "Source localization in an ocean waveguide using supervised machine learning", "abstract": "Source localization in ocean acoustics is posed as a machine learning problem in which data-driven methods learn source ranges directly from observed acoustic data. The pressure received by a vertical linear array is preprocessed by constructing a normalized sample covariance matrix and used as the input for three machine learning methods: feed-forward neural networks (FNN), support vector machines (SVM), and random forests (RF). The range estimation problem is solved both as a classification problem and as a regression problem by these three machine learning algorithms. The results of range estimation for the Noise09 experiment are compared for FNN, SVM, RF, and conventional matched-field processing and demonstrate the potential of machine learning for underwater source localization.", "year": 2017, "referenceCount": 57, "citationCount": 89, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics", "Medicine"], "authors": [{"authorId": "8792988", "name": "Haiqiang Niu"}, {"authorId": "1929240", "name": "P. Gerstoft"}, {"authorId": "145245006", "name": "Emma Reeves"}]}, {"paperId": "1c6c0d39d377a465834ed608ae23f48a79ab8eac", "url": "https://www.semanticscholar.org/paper/1c6c0d39d377a465834ed608ae23f48a79ab8eac", "title": "A Very Brief Introduction to Machine Learning With Applications to Communication Systems", "abstract": "Given the unprecedented availability of data and computing resources, there is widespread renewed interest in applying data-driven machine learning methods to problems for which the development of conventional engineering solutions is challenged by modeling or algorithmic deficiencies. This tutorial-style paper starts by addressing the questions of why and when such techniques can be useful. It then provides a high-level introduction to the basics of supervised and unsupervised learning. For both supervised and unsupervised learning, exemplifying applications to communication networks are discussed by distinguishing tasks carried out at the edge and at the cloud segments of the network at different layers of the protocol stack, with an emphasis on the physical layer.", "year": 2018, "referenceCount": 82, "citationCount": 285, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1705869", "name": "O. Simeone"}]}, {"paperId": "1fa9ed2bea208511ae698a967875e943049f16b6", "url": "https://www.semanticscholar.org/paper/1fa9ed2bea208511ae698a967875e943049f16b6", "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing", "abstract": "Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. \\textit{Transformers} is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. \\textit{Transformers} is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at \\url{this https URL}.", "year": 2019, "referenceCount": 79, "citationCount": 3291, "influentialCitationCount": 279, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50335211", "name": "Thomas Wolf"}, {"authorId": "1380459402", "name": "Lysandre Debut"}, {"authorId": "51918868", "name": "Victor Sanh"}, {"authorId": "40811585", "name": "Julien Chaumond"}, {"authorId": "40899333", "name": "Clement Delangue"}, {"authorId": "1382164294", "name": "Anthony Moi"}, {"authorId": "1382164165", "name": "Pierric Cistac"}, {"authorId": "1382164170", "name": "T. Rault"}, {"authorId": "2185329", "name": "R\u00e9mi Louf"}, {"authorId": "97662964", "name": "Morgan Funtowicz"}, {"authorId": "1383218348", "name": "Jamie Brew"}]}, {"paperId": "fd2ceb79bd0e384ff304292424f653413e2adef6", "url": "https://www.semanticscholar.org/paper/fd2ceb79bd0e384ff304292424f653413e2adef6", "title": "Unsupervised Process Monitoring and Fault Diagnosis with Machine Learning Methods", "abstract": null, "year": 2013, "referenceCount": 207, "citationCount": 97, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "68992903", "name": "C. Aldrich"}, {"authorId": "2207032", "name": "L. Auret"}]}, {"paperId": "bae497bc97aa81811591d9fade76c12540a894b4", "url": "https://www.semanticscholar.org/paper/bae497bc97aa81811591d9fade76c12540a894b4", "title": "Classifying depression patients and normal subjects using machine learning techniques", "abstract": null, "year": 2011, "referenceCount": 27, "citationCount": 324, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Mathematics", "Computer Science"], "authors": [{"authorId": "1978796", "name": "Behshad Hosseinifard"}, {"authorId": "1731232", "name": "M. Moradi"}, {"authorId": "144906653", "name": "R. Rostami"}]}, {"paperId": "19930147204c97be4d0964e166e8fe72ac1d6c3d", "url": "https://www.semanticscholar.org/paper/19930147204c97be4d0964e166e8fe72ac1d6c3d", "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness", "abstract": "The most prevalent notions of fairness in machine learning are statistical definitions: they fix a small collection of pre-defined groups, and then ask for parity of some statistic of the classifier across these groups. Constraints of this form are susceptible to intentional or inadvertent \"fairness gerrymandering\", in which a classifier appears to be fair on each individual group, but badly violates the fairness constraint on one or more structured subgroups defined over the protected attributes. We propose instead to demand statistical notions of fairness across exponentially (or infinitely) many subgroups, defined by a structured class of functions over the protected attributes. This interpolates between statistical definitions of fairness and recently proposed individual notions of fairness, but raises several computational challenges. It is no longer clear how to audit a fixed classifier to see if it satisfies such a strong definition of fairness. We prove that the computational problem of auditing subgroup fairness for both equality of false positive rates and statistical parity is equivalent to the problem of weak agnostic learning, which means it is computationally hard in the worst case, even for simple structured subclasses. \nWe then derive two algorithms that provably converge to the best fair classifier, given access to oracles which can solve the agnostic learning problem. The algorithms are based on a formulation of subgroup fairness as a two-player zero-sum game between a Learner and an Auditor. Our first algorithm provably converges in a polynomial number of steps. Our second algorithm enjoys only provably asymptotic convergence, but has the merit of simplicity and faster per-step computation. We implement the simpler algorithm using linear regression as a heuristic oracle, and show that we can effectively both audit and learn fair classifiers on real datasets.", "year": 2017, "referenceCount": 60, "citationCount": 480, "influentialCitationCount": 47, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "81338045", "name": "M. Kearns"}, {"authorId": "5880154", "name": "Seth Neel"}, {"authorId": "1682008", "name": "Aaron Roth"}, {"authorId": "1768074", "name": "Zhiwei Steven Wu"}]}, {"paperId": "b913a9546c748e4796380dd62039d6adb16255f2", "url": "https://www.semanticscholar.org/paper/b913a9546c748e4796380dd62039d6adb16255f2", "title": "Introduction: Paradigms for Machine Learning", "abstract": null, "year": 1989, "referenceCount": 8, "citationCount": 115, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143712374", "name": "J. Carbonell"}]}, {"paperId": "8ad79c3d22bff461cd5f83ee3e7eb013ea300661", "url": "https://www.semanticscholar.org/paper/8ad79c3d22bff461cd5f83ee3e7eb013ea300661", "title": "On Changing Continuous Attributes into Ordered Discrete Attributes", "abstract": null, "year": 1991, "referenceCount": 17, "citationCount": 512, "influentialCitationCount": 32, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144356748", "name": "J. Catlett"}]}, {"paperId": "c56e758ba18066a8cdc333f15dfdb7ea6af4d283", "url": "https://www.semanticscholar.org/paper/c56e758ba18066a8cdc333f15dfdb7ea6af4d283", "title": "Man vs. computer: Benchmarking machine learning algorithms for traffic sign recognition", "abstract": null, "year": 2012, "referenceCount": 37, "citationCount": 1031, "influentialCitationCount": 200, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "69539592", "name": "J. Stallkamp"}, {"authorId": "2502317", "name": "Marc Schlipsing"}, {"authorId": "2743486", "name": "J. Salmen"}, {"authorId": "1748824", "name": "C. Igel"}]}, {"paperId": "8b6e8d71d17a1fb5a7172c5d44ebba1cf06a7dd4", "url": "https://www.semanticscholar.org/paper/8b6e8d71d17a1fb5a7172c5d44ebba1cf06a7dd4", "title": "Shallow and Deep Networks Intrusion Detection System: A Taxonomy and Survey", "abstract": "Intrusion detection has attracted a considerable interest from researchers and industries. The community, after many years of research, still faces the problem of building reliable and efficient IDS that are capable of handling large quantities of data, with changing patterns in real time situations. The work presented in this manuscript classifies intrusion detection systems (IDS). Moreover, a taxonomy and survey of shallow and deep networks intrusion detection systems is presented based on previous and current works. This taxonomy and survey reviews machine learning techniques and their performance in detecting anomalies. Feature selection which influences the effectiveness of machine learning (ML) IDS is discussed to explain the role of feature selection in the classification and training phase of ML IDS. Finally, a discussion of the false and true positive alarm rates is presented to help researchers model reliable and efficient machine learning based intrusion detection systems.", "year": 2017, "referenceCount": 164, "citationCount": 184, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "8801229", "name": "Elike Hodo"}, {"authorId": "2778050", "name": "X. Bellekens"}, {"authorId": "153072596", "name": "Andrew W. Hamilton"}, {"authorId": "2954675", "name": "C. Tachtatzis"}, {"authorId": "153890027", "name": "Robert C. Atkinson"}]}, {"paperId": "bb12329fb5beaed74fa6c9c201ee2c8b7ba3c885", "url": "https://www.semanticscholar.org/paper/bb12329fb5beaed74fa6c9c201ee2c8b7ba3c885", "title": "Sliced Wasserstein Kernels for Probability Distributions", "abstract": "Optimal transport distances, otherwise known as Wasserstein distances, have recently drawn ample attention in computer vision and machine learning as powerful discrepancy measures for probability distributions. The recent developments on alternative formulations of the optimal transport have allowed for faster solutions to the problem and have revamped their practical applications in machine learning. In this paper, we exploit the widely used kernel methods and provide a family of provably positive definite kernels based on the Sliced Wasserstein distance and demonstrate the benefits of these kernels in a variety of learning tasks. Our work provides a new perspective on the application of optimal transport flavored distances through kernel methods in machine learning tasks.", "year": 2015, "referenceCount": 48, "citationCount": 116, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2062432", "name": "S. Kolouri"}, {"authorId": "2087067777", "name": "Yang Zou"}, {"authorId": "144703687", "name": "G. Rohde"}]}, {"paperId": "df60f442cf26a727a3a4b70a2ed7f36402a1bcf5", "url": "https://www.semanticscholar.org/paper/df60f442cf26a727a3a4b70a2ed7f36402a1bcf5", "title": "The Mathematical Foundations of Learning Machines", "abstract": "This book is about machines that learn to discover hidden relationships in data. A constant sfream of data bombards our senses and millions of sensory channels carry information into our brains. Brains are also learning machines that condition, combine, parse, and store data. Is it possible to learn something about learning by observing the style of computation used by brains? This is the motivation for research into computational devices that today are called \"neural networks.\" Neural networks are nonlinear dynamical systems with many degrees of freedom that can be used to solve computational problems. The mathematical foundations for learning in this class of machines was laid by agroup of researchers in the 1940s and 1950s. The achievement documented in this book is the thorough study of one of the simplest members of this class, feedforward networks with one layer of m w ~ a b l e weights connecting input units to output units. In a sense, these might be called reflex machines. The knee-jerk reflex, for example, is mediated by synaptic connections from the sensory recepton in your knee directly onto rnotoneurons in your spinal cord that in turn activate leg muscles. There are limits to how much computation can be accomplished by such reflexes, and these limits have been carefully delineated in this book. Just as more complex creatures evolved by layering control loops on the primitive reflexes, network models have also evolved in recent years and now have achieved vastly greater capabilities than reflex machines by making use of multilayered architectures with feedback connections. Nonetheless, recent work could not have been accomplished without building on these foundations. vii viil INTRODUCTION INTRODUCTION ix Despite the early promise of research on neural networks, there was a period of about 20 years, from the mid 1960s to the mid 1980s. when interest in neural networks as computational devices and models of human behavior waned in favor of models based on symbol processing. There are many reasons for this, some of them now evident in this book. Still, Learning Machines was an underground classic among the neural network modelers who were active during this \"dark age\" and deserves to be better known to the generation that is \"relearning\" what was once known about statistical learning machines. The intuitive geometric explanations and the mathematical foundations in this monograph are as invaluable today as they were when it was first written. Nilsson is \u2026", "year": 1990, "referenceCount": 68, "citationCount": 152, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144497046", "name": "N. Nilsson"}]}, {"paperId": "c21b644926a1391d403b0a74e003a0e593de4464", "url": "https://www.semanticscholar.org/paper/c21b644926a1391d403b0a74e003a0e593de4464", "title": "Digital mammographic tumor classification using transfer learning from deep convolutional neural networks", "abstract": "Abstract. Convolutional neural networks (CNNs) show potential for computer-aided diagnosis (CADx) by learning features directly from the image data instead of using analytically extracted features. However, CNNs are difficult to train from scratch for medical images due to small sample sizes and variations in tumor presentations. Instead, transfer learning can be used to extract tumor information from medical images via CNNs originally pretrained for nonmedical tasks, alleviating the need for large datasets. Our database includes 219 breast lesions (607 full-field digital mammographic images). We compared support vector machine classifiers based on the CNN-extracted image features and our prior computer-extracted tumor features in the task of distinguishing between benign and malignant breast lesions. Five-fold cross validation (by lesion) was conducted with the area under the receiver operating characteristic (ROC) curve as the performance metric. Results show that classifiers based on CNN-extracted features (with transfer learning) perform comparably to those using analytically extracted features [area under the ROC curve (AUC)=0.81]. Further, the performance of ensemble classifiers based on both types was significantly better than that of either classifier type alone (AUC=0.86 versus 0.81, p=0.022). We conclude that transfer learning can improve current CADx methods while also providing standalone classifiers without large datasets, facilitating machine-learning methods in radiomics and precision medicine.", "year": 2016, "referenceCount": 18, "citationCount": 373, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Engineering"], "authors": [{"authorId": "7257452", "name": "B. Huynh"}, {"authorId": "87030808", "name": "Hui Li"}, {"authorId": "144692532", "name": "M. Giger"}]}, {"paperId": "1a7423f3c3386ec9e73a2e51a11f2afc61753cb7", "url": "https://www.semanticscholar.org/paper/1a7423f3c3386ec9e73a2e51a11f2afc61753cb7", "title": "A fast pruned-extreme learning machine for classification problem", "abstract": null, "year": 2008, "referenceCount": 22, "citationCount": 340, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2774247", "name": "Hai-Jun Rong"}, {"authorId": "144848119", "name": "Y. Ong"}, {"authorId": "144362750", "name": "A. Tan"}, {"authorId": "2920520", "name": "Zexuan Zhu"}]}, {"paperId": "71d9222b39d0539322b6083de6790fc630129b9f", "url": "https://www.semanticscholar.org/paper/71d9222b39d0539322b6083de6790fc630129b9f", "title": "Why Are We Using Black Box Models in AI When We Don\u2019t Need To? A Lesson From An Explainable AI Competition", "abstract": "In 2018, a landmark challenge in artificial intelligence (AI) took place, namely, the Explainable Machine Learning Challenge. The goal of the competition was to create a complicated black box model for the dataset and explain how it worked. One team did not follow the rules. Instead of sending in a black box, they created a model that was fully interpretable. This leads to the question of whether the real world of machine learning is similar to the Explainable Machine Learning Challenge, where black box models are used even when they are not needed. We discuss this team\u2019s thought processes during the competition and their implications, which reach far beyond the competition itself.Keywords: interpretability, explainability, machine learning, finance", "year": 2019, "referenceCount": 13, "citationCount": 128, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48395540", "name": "C. Rudin"}, {"authorId": "46546677", "name": "Joanna Radin"}]}, {"paperId": "4ec47080fcb2967cbedf2b9d821d7b458aff8799", "url": "https://www.semanticscholar.org/paper/4ec47080fcb2967cbedf2b9d821d7b458aff8799", "title": "Bayesian Network Refinement Via Machine Learning Approach", "abstract": "An approach to refining Bayesian network structures from new data is developed. Most previous work has only considered the refinement of the network's conditional probability parameters and has not addressed the issue of refining the network's structure. We tackle this problem by a machine learning approach based on a formalism known as the minimum description length (MDL) principle. The MDL principle is well suited to this task since it can perform tradeoffs between the accuracy, simplicity, and closeness to the existent structure. Another salient feature of this refinement approach is the capability of refining a network structure using partially specified data. Moreover, a localization scheme is developed for efficient computation of the description lengths since direct evaluation involves exponential time resources.", "year": 1998, "referenceCount": 25, "citationCount": 59, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144594306", "name": "Wai Lam"}]}, {"paperId": "82b8cf061d53c32ab5a403543d1c440bc502416a", "url": "https://www.semanticscholar.org/paper/82b8cf061d53c32ab5a403543d1c440bc502416a", "title": "Learning to Disentangle Factors of Variation with Manifold Interaction", "abstract": "Many latent factors of variation interact to generate sensory data; for example, pose, morphology and expression in face images. In this work, we propose to learn manifold coordinates for the relevant factors of variation and to model their joint interaction. Many existing feature learning algorithms focus on a single task and extract features that are sensitive to the task-relevant factors and invariant to all others. However, models that just extract a single set of invariant features do not exploit the relationships among the latent factors. To address this, we propose a higher-order Boltzmann machine that incorporates multiplicative interactions among groups of hidden units that each learn to encode a distinct factor of variation. Furthermore, we propose correspondence-based training strategies that allow effective disentangling. Our model achieves state-of-the-art emotion recognition and face verification performance on the Toronto Face Database. We also demonstrate disentangled features learned on the CMU Multi-PIE dataset.", "year": 2014, "referenceCount": 39, "citationCount": 238, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144828948", "name": "Scott E. Reed"}, {"authorId": "1729571", "name": "Kihyuk Sohn"}, {"authorId": "145489055", "name": "Y. Zhang"}, {"authorId": "1697141", "name": "Honglak Lee"}]}, {"paperId": "d1800b8c7ea7cda4248eccad995b242b8e403ce0", "url": "https://www.semanticscholar.org/paper/d1800b8c7ea7cda4248eccad995b242b8e403ce0", "title": "Coadaptive Brain\u2013Machine Interface via Reinforcement Learning", "abstract": "This paper introduces and demonstrates a novel brain-machine interface (BMI) architecture based on the concepts of reinforcement learning (RL), coadaptation, and shaping. RL allows the BMI control algorithm to learn to complete tasks from interactions with the environment, rather than an explicit training signal. Coadaption enables continuous, synergistic adaptation between the BMI control algorithm and BMI user working in changing environments. Shaping is designed to reduce the learning curve for BMI users attempting to control a prosthetic. Here, we present the theory and in vivo experimental paradigm to illustrate how this BMI learns to complete a reaching task using a prosthetic arm in a 3-D workspace based on the user's neuronal activity. This semisupervised learning framework does not require user movements. We quantify BMI performance in closed-loop brain control over six to ten days for three rats as a function of increasing task difficulty. All three subjects coadapted with their BMI control algorithms to control the prosthetic significantly above chance at each level of difficulty.", "year": 2009, "referenceCount": 61, "citationCount": 156, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "46851752", "name": "J. DiGiovanna"}, {"authorId": "2538494", "name": "B. Mahmoudi"}, {"authorId": "144371682", "name": "J. Fortes"}, {"authorId": "143961030", "name": "J. Pr\u00edncipe"}, {"authorId": "34732826", "name": "Justin C. Sanchez"}]}, {"paperId": "c317b5739afc119a1563c31f1e8335db5f760119", "url": "https://www.semanticscholar.org/paper/c317b5739afc119a1563c31f1e8335db5f760119", "title": "Bioinformatics: The Machine Learning Approach", "abstract": "In this book Pierre Baldi and Soren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.", "year": 2002, "referenceCount": 0, "citationCount": 86, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2003079", "name": "G. Stolovitzky"}]}, {"paperId": "fb6e85d35fc7b10c16d465fc71748d819672ad24", "url": "https://www.semanticscholar.org/paper/fb6e85d35fc7b10c16d465fc71748d819672ad24", "title": "MILEPOST GCC: machine learning based research compiler", "abstract": "Tuning hardwired compiler optimizations for rapidly evolving hardware makes porting an optimizing compiler for each new platform extremely challenging. Our radical approach is to develop a modular, extensible, self-optimizing compiler that automatically learns the best optimization heuristics based on the behavior of the platform. In this paper we describe MILEPOST 1 GCC, a machine-learning-based compiler that automatically adjusts its optimization heuristics to improve the execution time, code size, or compilation time of specific programs on different architectures. Our preliminary experimental results show that it is possible to considerably reduce execution time of the MiBench benchmark suite on a range of platforms entirely automatically.", "year": 2008, "referenceCount": 39, "citationCount": 129, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2043821", "name": "G. Fursin"}, {"authorId": "2052818", "name": "Cupertino Miranda"}, {"authorId": "1731764", "name": "O. Temam"}, {"authorId": "3305063", "name": "Mircea Namolaru"}, {"authorId": "1388775854", "name": "E. Yom-Tov"}, {"authorId": "38737579", "name": "A. Zaks"}, {"authorId": "1795886", "name": "Bilha Mendelson"}, {"authorId": "30561807", "name": "Edwin V. Bonilla"}, {"authorId": "153031524", "name": "J. Thomson"}, {"authorId": "2476647", "name": "H. Leather"}, {"authorId": "145715698", "name": "Christopher K. I. Williams"}, {"authorId": "1401533251", "name": "M. O\u2019Boyle"}, {"authorId": "38759952", "name": "Phil Barnard"}, {"authorId": "40580165", "name": "Elton Ashton"}, {"authorId": "2630186", "name": "E. Courtois"}, {"authorId": "1697641", "name": "F. Bodin"}]}, {"paperId": "b28290e2e03434f146dc4a3dcac77ca7dc89725c", "url": "https://www.semanticscholar.org/paper/b28290e2e03434f146dc4a3dcac77ca7dc89725c", "title": "Classification of Heart Disease Using Support Vector Machine", "abstract": "Diabetes mellitus is one of the most serious health challenges in both developing and developed countries. According to the International Diabetes Federation, there are 285 million diabetic people worldwide. This total is expected to rise to 380 million within 20 years. Due to its importance, a design of classifier for the detection of Diabetes disease with optimal cost and better performance is the need of the age. The Pima Indian diabetic database at the UCI machine learning laboratory has become a standard for testing data mining algorithms to see their prediction accuracy in diabetes data classification. The proposed method uses Support Vector Machine (SVM), a machine learning method as the classifier for diagnosis of diabetes. The machine learning method focus on classifying diabetes disease from high dimensional medical dataset. The experimental results obtained show that support vector machine can be successfully used for diagnosing diabetes disease.", "year": 2019, "referenceCount": 21, "citationCount": 123, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "41227523", "name": "V. Kirankumar"}, {"authorId": "1470750357", "name": "S. Ramasubbareddy"}, {"authorId": "80472802", "name": "G. Kannayaram"}, {"authorId": "2131609325", "name": "K. Nikhil Kumar"}]}, {"paperId": "c3625cf5d6ebfafd493754a9ae09b2f5fe8dc285", "url": "https://www.semanticscholar.org/paper/c3625cf5d6ebfafd493754a9ae09b2f5fe8dc285", "title": "Nonparametric statistical analysis for multiple comparison of machine learning regression algorithms", "abstract": "In the paper we present some guidelines for the application of nonparametric statistical tests and post-hoc procedures devised to perform multiple comparisons of machine learning algorithms. We emphasize that it is necessary to distinguish between pairwise and multiple comparison tests. We show that the pairwise Wilcoxon test, when employed to multiple comparisons, will lead to overoptimistic conclusions. We carry out intensive normality examination employing ten different tests showing that the output of machine learning algorithms for regression problems does not satisfy normality requirements. We conduct experiments on nonparametric statistical tests and post-hoc procedures designed for multiple 1\u00d7N and N \u00d7N comparisons with six different neural regression algorithms over 29 benchmark regression data sets. Our investigation proves the usefulness and strength of multiple comparison statistical procedures to analyse and select machine learning algorithms.", "year": 2012, "referenceCount": 149, "citationCount": 142, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2212679", "name": "B. Trawinski"}, {"authorId": "40090963", "name": "Magdalena Smetek"}, {"authorId": "2455338", "name": "Zbigniew Telec"}, {"authorId": "3019750", "name": "T. Lasota"}]}, {"paperId": "c5bd660b5ddc5878639483f8d0ad80cc4f1e4b6d", "url": "https://www.semanticscholar.org/paper/c5bd660b5ddc5878639483f8d0ad80cc4f1e4b6d", "title": "The Machine Learning landscape of top taggers", "abstract": "Based on the established task of identifying boosted, hadronically\ndecaying top quarks, we compare a wide range of modern machine learning\napproaches. Unlike most established methods they rely on low-level\ninput, for instance calorimeter output. While their network\narchitectures are vastly different, their performance is comparatively\nsimilar. In general, we find that these new approaches are extremely\npowerful and great fun.", "year": 2019, "referenceCount": 85, "citationCount": 124, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics"], "authors": [{"authorId": "152493927", "name": "G. Kasieczka"}, {"authorId": "3578208", "name": "T. Plehn"}, {"authorId": "80350870", "name": "A. Butter"}, {"authorId": "40986375", "name": "D. Debnath"}, {"authorId": "101427650", "name": "M. Fairbairn"}, {"authorId": "5668057", "name": "W. Fedorko"}, {"authorId": "150948997", "name": "C. Gay"}, {"authorId": "41201711", "name": "L. Gouskos"}, {"authorId": "88942968", "name": "P. Komiske"}, {"authorId": "2097884773", "name": "S. Leiss"}, {"authorId": "2365933", "name": "A. Lister"}, {"authorId": "47546998", "name": "S. Macaluso"}, {"authorId": "3946259", "name": "E. Metodiev"}, {"authorId": "47147349", "name": "L. Moore"}, {"authorId": "3085579", "name": "B. Nachman"}, {"authorId": "144868535", "name": "K. Nordstrom"}, {"authorId": "10778348", "name": "J. Pearkes"}, {"authorId": "143760570", "name": "H. Qu"}, {"authorId": "152547984", "name": "Y. Rath"}, {"authorId": "2066867462", "name": "M. Riegler"}, {"authorId": "145727985", "name": "D. Shih"}, {"authorId": "145040707", "name": "J. Thompson"}, {"authorId": "143611673", "name": "S. Varma"}]}, {"paperId": "a3bb9649b19b34ca21651bdd885ecec2ea5a8615", "url": "https://www.semanticscholar.org/paper/a3bb9649b19b34ca21651bdd885ecec2ea5a8615", "title": "A Guide to Monte Carlo Simulations in Statistical Physics", "abstract": "Dealing with all aspects of Monte Carlo simulation of complex physical systems encountered in condensed matter physics and statistical mechanics, this book provides an introduction to computer simulations in physics. The 5th edition contains extensive new material describing numerous powerful algorithms and methods that represent recent developments in the field. New topics such as active matter and machine learning are also introduced. Throughout, there are many applications, examples, recipes, case studies, and exercises to help the reader fully comprehend the material. This book is ideal for graduate students and researchers, both in academia and industry, who want to learn techniques that have become a third tool of physical science, complementing experiment and analytical theory.", "year": 2000, "referenceCount": 85, "citationCount": 3242, "influentialCitationCount": 181, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Chemistry", "Computer Science", "Mathematics"], "authors": [{"authorId": "93170840", "name": "D. Landau"}, {"authorId": "144469417", "name": "K. Binder"}]}, {"paperId": "91fe28e54a27b2360f6979b484805871d29eef7d", "url": "https://www.semanticscholar.org/paper/91fe28e54a27b2360f6979b484805871d29eef7d", "title": "Machine Learning and Knowledge Discovery in Databases", "abstract": null, "year": 2015, "referenceCount": 27, "citationCount": 129, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1720178", "name": "J. Balc\u00e1zar"}, {"authorId": "1705764", "name": "F. Bonchi"}, {"authorId": "1682878", "name": "A. Gionis"}, {"authorId": "69343681", "name": "M. Sebag"}]}, {"paperId": "83e9565cede81b2b88a9fa241833135da142f4d3", "url": "https://www.semanticscholar.org/paper/83e9565cede81b2b88a9fa241833135da142f4d3", "title": "Parallelized Stochastic Gradient Descent", "abstract": "With the increase in available data parallel machine learning has become an increasingly pressing problem. In this paper we present the first parallel stochastic gradient descent algorithm including a detailed analysis and experimental evidence. Unlike prior work on parallel optimization algorithms [5, 7] our variant comes with parallel acceleration guarantees and it poses no overly tight latency constraints, which might only be available in the multicore setting. Our analysis introduces a novel proof technique \u2014 contractive mappings to quantify the speed of convergence of parameter distributions to their asymptotic limits. As a side effect this answers the question of how quickly stochastic gradient descent algorithms reach the asymptotically normal regime [1, 8].", "year": 2010, "referenceCount": 12, "citationCount": 1207, "influentialCitationCount": 174, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "8195063", "name": "Martin A. Zinkevich"}, {"authorId": "2965406", "name": "Markus Weimer"}, {"authorId": "46234526", "name": "Alex Smola"}, {"authorId": "47681372", "name": "Lihong Li"}]}, {"paperId": "d2aa99aec727af4779c7b805fca06db56faf40fc", "url": "https://www.semanticscholar.org/paper/d2aa99aec727af4779c7b805fca06db56faf40fc", "title": "A Method of Moments for Mixture Models and Hidden Markov Models", "abstract": "Mixture models are a fundamental tool in applied statistics and machine learning for treating data taken from multiple subpopulations. The current practice for estimating the parameters of such models relies on local search heuristics (e.g., the EM algorithm) which are prone to failure, and existing consistent methods are unfavorable due to their high computational and sample complexity which typically scale exponentially with the number of mixture components. This work develops an efficient method of moments approach to parameter estimation for a broad class of high-dimensional mixture models with many components, including multi-view mixtures of Gaussians (such as mixtures of axis-aligned Gaussians) and hidden Markov models. The new method leads to rigorous unsupervised learning results for mixture models that were not achieved by previous works; and, because of its simplicity, it offers a viable alternative to EM for practical deployment.", "year": 2012, "referenceCount": 43, "citationCount": 317, "influentialCitationCount": 58, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2047844", "name": "Anima Anandkumar"}, {"authorId": "143724861", "name": "Daniel J. Hsu"}, {"authorId": "144695232", "name": "S. Kakade"}]}, {"paperId": "26d93995cadcee9af33c14528c2d843888b14bc3", "url": "https://www.semanticscholar.org/paper/26d93995cadcee9af33c14528c2d843888b14bc3", "title": "SVM incremental learning, adaptation and optimization", "abstract": "The objective of machine learning is to identify a model that yields good generalization performance. This involves repeatedly selecting a hypothesis class, searching the hypothesis class by minimizing a given objective function over the model's parameter space, and evaluating the generalization performance of the resulting model. This search can be computationally intensive as training data continuously arrives, or as one needs to tune hyperparameters in the hypothesis class and the objective function. In this paper, we present a framework for exact incremental learning and adaptation of support vector machine (SVM) classifiers. The approach is general and allows one to learn and unlearn individual or multiple examples, adapt the current SVM to changes in regularization and kernel parameters, and evaluate generalization performance through exact leave-one-out error estimation.", "year": 2003, "referenceCount": 25, "citationCount": 268, "influentialCitationCount": 29, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "18555072", "name": "C. Diehl"}, {"authorId": "113751801", "name": "G. Cauwenberghs"}]}, {"paperId": "34818236c2f6eec5dbc23f8660dceb5998e5b333", "url": "https://www.semanticscholar.org/paper/34818236c2f6eec5dbc23f8660dceb5998e5b333", "title": "Machine learning: applications of artificial intelligence to imaging and diagnosis", "abstract": null, "year": 2018, "referenceCount": 40, "citationCount": 132, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "32624132", "name": "J. A. Nichols"}, {"authorId": "1398620165", "name": "Hsien W. Herbert Chan"}, {"authorId": "47052177", "name": "M. Baker"}]}, {"paperId": "3e4d892fa31ef958abaaabb9a66513096d60f8e8", "url": "https://www.semanticscholar.org/paper/3e4d892fa31ef958abaaabb9a66513096d60f8e8", "title": "Detection of Online Fake News Using N-Gram Analysis and Machine Learning Techniques", "abstract": null, "year": 2017, "referenceCount": 17, "citationCount": 274, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "73460788", "name": "H. Ahmed"}, {"authorId": "143812950", "name": "I. Traor\u00e9"}, {"authorId": "12906109", "name": "Sherif Saad"}]}, {"paperId": "dc2b3571907434dbe70cb9674ef16b18286f2cc9", "url": "https://www.semanticscholar.org/paper/dc2b3571907434dbe70cb9674ef16b18286f2cc9", "title": "Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework", "abstract": "We present a comprehensive framework for augmenting turbulence models with physics-informed machine learning, illustrating a complete workflow from identification of input/output to prediction of mean velocities. The learned model has Galilean invariance and coordinate rotational invariance.", "year": 2018, "referenceCount": 63, "citationCount": 216, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "47876141", "name": "Jinlong Wu"}, {"authorId": "41193045", "name": "Heng Xiao"}, {"authorId": "50524368", "name": "E. Paterson"}]}, {"paperId": "0e1f153576c7f9f2628cdd34a1067c4d26bdc096", "url": "https://www.semanticscholar.org/paper/0e1f153576c7f9f2628cdd34a1067c4d26bdc096", "title": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning", "abstract": null, "year": 2019, "referenceCount": 133, "citationCount": 502, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1699054", "name": "W. Samek"}, {"authorId": "144535526", "name": "G. Montavon"}, {"authorId": "1687524", "name": "A. Vedaldi"}, {"authorId": "145579972", "name": "L. K. Hansen"}, {"authorId": "145034054", "name": "K. M\u00fcller"}]}, {"paperId": "896191c053a5c98f2478cb20246ae447ebcd6e38", "url": "https://www.semanticscholar.org/paper/896191c053a5c98f2478cb20246ae447ebcd6e38", "title": "Optimal Mass Transport: Signal processing and machine-learning applications", "abstract": "Transport-based techniques for signal and data analysis have recently received increased interest. Given their ability to provide accurate generative models for signal intensities and other data distributions, they have been used in a variety of applications, including content-based retrieval, cancer detection, image superresolution, and statistical machine learning, to name a few, and they have been shown to produce state-of-the-art results. Moreover, the geometric characteristics of transport-related metrics have inspired new kinds of algorithms for interpreting the meaning of data distributions. Here, we provide a practical overview of the mathematical underpinnings of mass transport-related methods, including numerical implementation, as well as a review, with demonstrations, of several applications. Software accompanying this article is available from [43].", "year": 2017, "referenceCount": 47, "citationCount": 235, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "2062432", "name": "S. Kolouri"}, {"authorId": "2115290905", "name": "Se Rim Park"}, {"authorId": "2999959", "name": "M. Thorpe"}, {"authorId": "2318292", "name": "D. Slep\u010dev"}, {"authorId": "144703687", "name": "G. Rohde"}]}, {"paperId": "160bd7cc365e55c966fea5e4624a8f736a7d979d", "url": "https://www.semanticscholar.org/paper/160bd7cc365e55c966fea5e4624a8f736a7d979d", "title": "TABLA: A unified template-based framework for accelerating statistical machine learning", "abstract": "A growing number of commercial and enterprise systems increasingly rely on compute-intensive Machine Learning (ML) algorithms. While the demand for these compute-intensive applications is growing, the performance benefits from general-purpose platforms are diminishing. Field Programmable Gate Arrays (FPGAs) provide a promising path forward to accommodate the needs of machine learning algorithms and represent an intermediate point between the efficiency of ASICs and the programmability of general-purpose processors. However, acceleration with FPGAs still requires long development cycles and extensive expertise in hardware design. To tackle this challenge, instead of designing an accelerator for a machine learning algorithm, we present TABLA, a framework that generates accelerators for a class of machine learning algorithms. The key is to identify the commonalities across a wide range of machine learning algorithms and utilize this commonality to provide a high-level abstraction for programmers. TABLA leverages the insight that many learning algorithms can be expressed as a stochastic optimization problem. Therefore, learning becomes solving an optimization problem using stochastic gradient descent that minimizes an objective function over the training data. The gradient descent solver is fixed while the objective function changes for different learning algorithms. TABLA provides a template-based framework to accelerate this class of learning algorithms. Therefore, a developer can specify the learning task by only expressing the gradient of the objective function using our high-level language. Tabla then automatically generates the synthesizable implementation of the accelerator for FPGA realization using a set of hand-optimized templates. We use Tabla to generate accelerators for ten different learning tasks targeted at a Xilinx Zynq FPGA platform. We rigorously compare the benefits of FPGA acceleration to multi-core CPUs (ARM Cortex A15 and Xeon E3) and many-core GPUs (Tegra K1, GTX 650 Ti, and Tesla K40) using real hardware measurements. TABLA-generated accelerators provide 19.4x and 2.9x average speedup over the ARM and Xeon processors, respectively. These accelerators provide 17.57x, 20.2x, and 33.4x higher Performance-per-Watt in comparison to Tegra, GTX 650 Ti and Tesla, respectively. These benefits are achieved while the programmers write less than 50 lines of code.", "year": 2016, "referenceCount": 63, "citationCount": 142, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "33278013", "name": "Divya Mahajan"}, {"authorId": "3101741", "name": "Jongse Park"}, {"authorId": "2053398", "name": "Emmanuel Amaro"}, {"authorId": "32646274", "name": "Hardik Sharma"}, {"authorId": "2112229", "name": "A. Yazdanbakhsh"}, {"authorId": "2117059930", "name": "J. Kim"}, {"authorId": "1696563", "name": "H. Esmaeilzadeh"}]}, {"paperId": "7380e343dd4547e21d5118b16daf03d021d98c4e", "url": "https://www.semanticscholar.org/paper/7380e343dd4547e21d5118b16daf03d021d98c4e", "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation", "abstract": "As machine learning algorithms are increasingly applied to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers can explain how such algorithms arrived at their predictions. In recent years, a number of image saliency methods have been developed to summarize where highly complex neural networks \u201clook\u201d in an image for evidence for their predictions. However, these techniques are limited by their heuristic nature and architectural constraints. In this paper, we make two main contributions: First, we propose a general framework for learning different kinds of explanations for any black box algorithm. Second, we specialise the framework to find the part of an image most responsible for a classifier decision. Unlike previous works, our method is model-agnostic and testable because it is grounded in explicit and interpretable image perturbations.", "year": 2017, "referenceCount": 24, "citationCount": 1041, "influentialCitationCount": 137, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "25576460", "name": "Ruth C. Fong"}, {"authorId": "1687524", "name": "A. Vedaldi"}]}, {"paperId": "7a09464f26e18a25a948baaa736270bfb84b5e12", "url": "https://www.semanticscholar.org/paper/7a09464f26e18a25a948baaa736270bfb84b5e12", "title": "On-line Q-learning using connectionist systems", "abstract": "Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete nite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of di erent algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Di erence algorithm (Sutton 1988), including a new algorithm (Modi ed Connectionist Q-Learning), and Q( ) (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each trial before updating can occur. On-line updating is found to be more robust to the choice of training parameters than backward replay, and also enables the algorithms to be used in continuously operating systems where no end of trial conditions occur. We compare the performance of these algorithms on a realistic robot navigation problem, where a simulated mobile robot is trained to guide itself to a goal position in the presence of obstacles. The robot must rely on limited sensory feedback from its surroundings, and make decisions that can be generalised to arbitrary layouts of obstacles. These simulations show that on-line learning algorithms are less sensitive to the choice of training parameters than backward replay, and that the alternative update rules of MCQ-L and Q( ) are more robust than standard Q-learning updates. 1", "year": 1994, "referenceCount": 11, "citationCount": 1469, "influentialCitationCount": 267, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3466704", "name": "Gavin Adrian Rummery"}, {"authorId": "145387873", "name": "M. Niranjan"}]}, {"paperId": "3eb594bdc7057858a7bcd6243947c1944e89e2e3", "url": "https://www.semanticscholar.org/paper/3eb594bdc7057858a7bcd6243947c1944e89e2e3", "title": "Adversarial Machine Learning - Industry Perspectives", "abstract": "Based on interviews with 28 organizations, we found that industry practitioners are not equipped with tactical and strategic tools to protect, detect and respond to attacks on their Machine Learning (ML) systems. We leverage the insights from the interviews and we enumerate the gaps in perspective in securing machine learning systems when viewed in the context of traditional software security development. We write this paper from the perspective of two personas: developers/ML engineers and security incident responders who are tasked with securing ML systems as they are designed, developed and deployed ML systems. The goal of this paper is to engage researchers to revise and amend the Security Development Lifecycle for industrial-grade software in the adversarial ML era.", "year": 2020, "referenceCount": 46, "citationCount": 84, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2117776576", "name": "R. Kumar"}, {"authorId": "2040692968", "name": "Magnus Nystr\u00f6m"}, {"authorId": "153623755", "name": "J. Lambert"}, {"authorId": "2054944467", "name": "Andrew Marshall"}, {"authorId": "3293531", "name": "Mario Goertzel"}, {"authorId": "1492009487", "name": "Andi Comissoneru"}, {"authorId": "40539088", "name": "Matt Swann"}, {"authorId": "1492004512", "name": "Sharon Xia"}]}, {"paperId": "bd71831c69191c26a2a091f35509b1a80a4b1b64", "url": "https://www.semanticscholar.org/paper/bd71831c69191c26a2a091f35509b1a80a4b1b64", "title": "Reliable Classifications with Machine Learning", "abstract": null, "year": 2002, "referenceCount": 25, "citationCount": 82, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1719813", "name": "M. Kukar"}, {"authorId": "143986204", "name": "I. Kononenko"}]}, {"paperId": "412965e0537f259167fe6d8d692c41c67f3119ee", "url": "https://www.semanticscholar.org/paper/412965e0537f259167fe6d8d692c41c67f3119ee", "title": "KDEEP: Protein-Ligand Absolute Binding Affinity Prediction via 3D-Convolutional Neural Networks", "abstract": "Accurately predicting protein-ligand binding affinities is an important problem in computational chemistry since it can substantially accelerate drug discovery for virtual screening and lead optimization. We propose here a fast machine-learning approach for predicting binding affinities using state-of-the-art 3D-convolutional neural networks and compare this approach to other machine-learning and scoring methods using several diverse data sets. The results for the standard PDBbind (v.2016) core test-set are state-of-the-art with a Pearson's correlation coefficient of 0.82 and a RMSE of 1.27 in pK units between experimental and predicted affinity, but accuracy is still very sensitive to the specific protein used. KDEEP is made available via PlayMolecule.org for users to test easily their own protein-ligand complexes, with each prediction taking a fraction of a second. We believe that the speed, performance, and ease of use of KDEEP makes it already an attractive scoring function for modern computational chemistry pipelines.", "year": 2018, "referenceCount": 0, "citationCount": 381, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "83680162", "name": "J. Jim\u00e9nez"}, {"authorId": "19207644", "name": "M. \u0160kali\u010d"}, {"authorId": "1389490573", "name": "Gerard Mart\u00ednez-Rosell"}, {"authorId": "144186698", "name": "G. D. Fabritiis"}]}, {"paperId": "b01a7ec32097a1f57fb80ef4073bc8a487d241fb", "url": "https://www.semanticscholar.org/paper/b01a7ec32097a1f57fb80ef4073bc8a487d241fb", "title": "Using corpora in machine-learning chatbot systems", "abstract": "A chatbot is a machine conversation system which interacts with human users via natural conversational language. Software to machine-learn conversational patterns from a transcribed dialogue corpus has been used to generate a range of chatbots speaking various languages and sublanguages including varieties of English, as well as French, Arabic and Afrikaans. This paper presents a program to learn from spoken transcripts of the Dialogue Diversity Corpus of English, the Minnesota French Corpus, the Corpus of Spoken Afrikaans, the Qur\u2019an Arabic-English parallel corpus, and the British National Corpus of English; we discuss the problems which arose during learning and testing. Two main goals were achieved from the automation process. One was the ability to generate different versions of the chatbot in different languages, bringing chatbot technology to languages with few if any NLP resources: the corpus-based learning techniques transferred straightforwardly to develop chatbots for Afrikaans and Qur\u2019anic Arabic. The second achievement was the ability to learn a very large number of categories within a short time, saving effort and errors in doing such work manually: we generated more than one million AIML categories or conversation-rules from the BNC corpus, 20 times the size of existing AIML rule-sets, and probably the biggest AI Knowledge-Base ever.", "year": 2005, "referenceCount": 42, "citationCount": 136, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697090", "name": "B. A. Shawar"}, {"authorId": "144214753", "name": "E. Atwell"}]}, {"paperId": "13ee6d92db3d842fabaa4e43d3385d454f7f2a0b", "url": "https://www.semanticscholar.org/paper/13ee6d92db3d842fabaa4e43d3385d454f7f2a0b", "title": "explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning", "abstract": "We propose a framework for interactive and explainable machine learning that enables users to (1) understand machine learning models; (2) diagnose model limitations using different explainable AI methods; as well as (3) refine and optimize the models. Our framework combines an iterative XAI pipeline with eight global monitoring and steering mechanisms, including quality monitoring, provenance tracking, model comparison, and trust building. To operationalize the framework, we present explAIner, a visual analytics system for interactive and explainable machine learning that instantiates all phases of the suggested pipeline within the commonly used TensorBoard environment. We performed a user-study with nine participants across different expertise levels to examine their perception of our workflow and to collect suggestions to fill the gap between our system and framework. The evaluation confirms that our tightly integrated system leads to an informed machine learning process while disclosing opportunities for further extensions.", "year": 2019, "referenceCount": 89, "citationCount": 102, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "74515603", "name": "Thilo Spinner"}, {"authorId": "34865259", "name": "Udo Schlegel"}, {"authorId": "2054386555", "name": "H. Sch\u00e4fer"}, {"authorId": "1401917601", "name": "Mennatallah El-Assady"}]}, {"paperId": "36a76da134045c6e8ea68102b3f972a0496454cd", "url": "https://www.semanticscholar.org/paper/36a76da134045c6e8ea68102b3f972a0496454cd", "title": "Flow-shop scheduling with a learning effect", "abstract": null, "year": 2005, "referenceCount": 29, "citationCount": 154, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2109720245", "name": "J.-B. Wang"}, {"authorId": "2917227", "name": "Z. Xia"}]}, {"paperId": "65bf178effbf3abe806926899c79924a80502f8c", "url": "https://www.semanticscholar.org/paper/65bf178effbf3abe806926899c79924a80502f8c", "title": "Proximal Algorithms in Statistics and Machine Learning", "abstract": "In this paper we develop proximal methods for statistical learning. Proximal point algorithms are useful in statistics and machine learning for obtaining optimization solutions for composite functions. Our approach exploits closed-form solutions of proximal operators and envelope representations based on the Moreau, Forward-Backward, Douglas-Rachford and Half-Quadratic envelopes. Envelope representations lead to novel proximal algorithms for statistical optimisation of composite objective functions which include both non-smooth and non-convex objectives. We illustrate our methodology with regularized Logistic and Poisson regression and non-convex bridge penalties with a fused lasso norm. We provide a discussion of convergence of non-descent algorithms with acceleration and for non-convex functions. Finally, we provide directions for future research.", "year": 2015, "referenceCount": 101, "citationCount": 116, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2085638", "name": "Nicholas G. Polson"}, {"authorId": "145516664", "name": "James G. Scott"}, {"authorId": "39153647", "name": "Brandon T. Willard"}]}, {"paperId": "5fd088f89e4a3edded62345b0527f84cd19c8f19", "url": "https://www.semanticscholar.org/paper/5fd088f89e4a3edded62345b0527f84cd19c8f19", "title": "Machine Learning for Drug-Target Interaction Prediction", "abstract": "Identifying drug-target interactions will greatly narrow down the scope of search of candidate medications, and thus can serve as the vital first step in drug discovery. Considering that in vitro experiments are extremely costly and time-consuming, high efficiency computational prediction methods could serve as promising strategies for drug-target interaction (DTI) prediction. In this review, our goal is to focus on machine learning approaches and provide a comprehensive overview. First, we summarize a brief list of databases frequently used in drug discovery. Next, we adopt a hierarchical classification scheme and introduce several representative methods of each category, especially the recent state-of-the-art methods. In addition, we compare the advantages and limitations of methods in each category. Lastly, we discuss the remaining challenges and future outlook of machine learning in DTI prediction. This article may provide a reference and tutorial insights on machine learning-based DTI prediction for future researchers.", "year": 2018, "referenceCount": 75, "citationCount": 116, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "2118231178", "name": "Ruolan Chen"}, {"authorId": "46522058", "name": "Xiangrong Liu"}, {"authorId": "6693702", "name": "Shuting Jin"}, {"authorId": "49719427", "name": "Jiawei Lin"}, {"authorId": "2108516616", "name": "Juan Liu"}]}, {"paperId": "c2c6a7973db72b92337a0b2d17feba79ed926bb7", "url": "https://www.semanticscholar.org/paper/c2c6a7973db72b92337a0b2d17feba79ed926bb7", "title": "Facies classification using machine learning", "abstract": "There has been much excitement recently about big data and the dire need for data scientists who possess the ability to extract meaning from it. Geoscientists, meanwhile, have been doing science with voluminous data for years, without needing to brag about how big it is. But now that large, complex data sets are widely available, there has been a proliferation of tools and techniques for analyzing them. Many free and open-source packages now exist that provide powerful additions to the geoscientist9s toolbox, much of which used to be only available in proprietary (and expensive) software platforms.", "year": 2016, "referenceCount": 2, "citationCount": 106, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "72427015", "name": "B. Hall"}]}, {"paperId": "6ca8fa3ba89ed69128c7f6f03e07f3aa42446596", "url": "https://www.semanticscholar.org/paper/6ca8fa3ba89ed69128c7f6f03e07f3aa42446596", "title": "Activity sensing in the wild: a field trial of ubifit garden", "abstract": "Recent advances in small inexpensive sensors, low-power processing, and activity modeling have enabled applications that use on-body sensing and machine learning to infer people's activities throughout everyday life. To address the growing rate of sedentary lifestyles, we have developed a system, UbiFit Garden, which uses these technologies and a personal, mobile display to encourage physical activity. We conducted a 3-week field trial in which 12 participants used the system and report findings focusing on their experiences with the sensing and activity inference. We discuss key implications for systems that use on-body sensing and activity inference to encourage physical activity.", "year": 2008, "referenceCount": 29, "citationCount": 1235, "influentialCitationCount": 61, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1802071", "name": "Sunny Consolvo"}, {"authorId": "144541497", "name": "David W. McDonald"}, {"authorId": "1688145", "name": "Tammy R Toscos"}, {"authorId": "2335746", "name": "Mike Y. Chen"}, {"authorId": "145904884", "name": "Jon E. Froehlich"}, {"authorId": "40376159", "name": "B. Harrison"}, {"authorId": "2035680", "name": "P. Klasnja"}, {"authorId": "144230355", "name": "A. LaMarca"}, {"authorId": "2069633999", "name": "Louis Legrand"}, {"authorId": "48541767", "name": "R. Libby"}, {"authorId": "143949898", "name": "I. Smith"}, {"authorId": "9522307", "name": "J. Landay"}]}, {"paperId": "1aa295c00c37e7a3b4dc56ec2e40793b1617bcb3", "url": "https://www.semanticscholar.org/paper/1aa295c00c37e7a3b4dc56ec2e40793b1617bcb3", "title": "Pose Machines: Articulated Pose Estimation via Inference Machines", "abstract": null, "year": 2014, "referenceCount": 35, "citationCount": 231, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "20569810", "name": "V. Ramakrishna"}, {"authorId": "51505748", "name": "Daniel Munoz"}, {"authorId": "145670946", "name": "M. Hebert"}, {"authorId": "1756566", "name": "J. Bagnell"}, {"authorId": "1774867", "name": "Yaser Sheikh"}]}, {"paperId": "395de0bd3837fdf4b4b5e5f04835bcc69c279481", "url": "https://www.semanticscholar.org/paper/395de0bd3837fdf4b4b5e5f04835bcc69c279481", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and other recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa on GLUE and SQuAD, and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 3.5 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also replicate other pretraining schemes within the BART framework, to understand their effect on end-task performance.", "year": 2019, "referenceCount": 34, "citationCount": 3465, "influentialCitationCount": 875, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "35084211", "name": "M. Lewis"}, {"authorId": "11323179", "name": "Yinhan Liu"}, {"authorId": "39589154", "name": "Naman Goyal"}, {"authorId": "2320509", "name": "Marjan Ghazvininejad"}, {"authorId": "113947684", "name": "Abdelrahman Mohamed"}, {"authorId": "39455775", "name": "Omer Levy"}, {"authorId": "1759422", "name": "Veselin Stoyanov"}, {"authorId": "1982950", "name": "Luke Zettlemoyer"}]}, {"paperId": "2077d0f30507d51a0d3bbec4957d55e817d66a59", "url": "https://www.semanticscholar.org/paper/2077d0f30507d51a0d3bbec4957d55e817d66a59", "title": "Fields of Experts: a framework for learning image priors", "abstract": "We develop a framework for learning generic, expressive image priors that capture the statistics of natural scenes and can be used for a variety of machine vision tasks. The approach extends traditional Markov random field (MRF) models by learning potential functions over extended pixel neighborhoods. Field potentials are modeled using a Products-of-Experts framework that exploits nonlinear functions of many linear filter responses. In contrast to previous MRF approaches all parameters, including the linear filters themselves, are learned from training data. We demonstrate the capabilities of this Field of Experts model with two example applications, image denoising and image inpainting, which are implemented using a simple, approximate inference scheme. While the model is trained on a generic image database and is not tuned toward a specific application, we obtain results that compete with and even outperform specialized techniques.", "year": 2005, "referenceCount": 29, "citationCount": 1086, "influentialCitationCount": 117, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145920814", "name": "S. Roth"}, {"authorId": "2105795", "name": "Michael J. Black"}]}, {"paperId": "789c0aa09ad004400a07074e0c1ecdca207dccf4", "url": "https://www.semanticscholar.org/paper/789c0aa09ad004400a07074e0c1ecdca207dccf4", "title": "Dynamic ensemble extreme learning machine based on sample entropy", "abstract": null, "year": 2012, "referenceCount": 39, "citationCount": 122, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39375300", "name": "Jun-Hai Zhai"}, {"authorId": "2146236373", "name": "Hong-Yu Xu"}, {"authorId": "2154495143", "name": "Xizhao Wang"}]}, {"paperId": "899db02ba28ef2479b5bea3e51627685be5b3865", "url": "https://www.semanticscholar.org/paper/899db02ba28ef2479b5bea3e51627685be5b3865", "title": "Deep Recurrent Neural Networks for Human Activity Recognition", "abstract": "Adopting deep learning methods for human activity recognition has been effective in extracting discriminative features from raw input sequences acquired from body-worn sensors. Although human movements are encoded in a sequence of successive samples in time, typical machine learning methods perform recognition tasks without exploiting the temporal correlations between input data samples. Convolutional neural networks (CNNs) address this issue by using convolutions across a one-dimensional temporal sequence to capture dependencies among input data. However, the size of convolutional kernels restricts the captured range of dependencies between data samples. As a result, typical models are unadaptable to a wide range of activity-recognition configurations and require fixed-length input windows. In this paper, we propose the use of deep recurrent neural networks (DRNNs) for building recognition models that are capable of capturing long-range dependencies in variable-length input sequences. We present unidirectional, bidirectional, and cascaded architectures based on long short-term memory (LSTM) DRNNs and evaluate their effectiveness on miscellaneous benchmark datasets. Experimental results show that our proposed models outperform methods employing conventional machine learning, such as support vector machine (SVM) and k-nearest neighbors (KNN). Additionally, the proposed models yield better performance than other deep learning techniques, such as deep believe networks (DBNs) and CNNs.", "year": 2017, "referenceCount": 32, "citationCount": 257, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "30050286", "name": "Abdulmajid Murad"}, {"authorId": "1739676", "name": "Jae-Young Pyun"}]}, {"paperId": "b42c38975f3d3f8bfe4b0c1a6e576c3e297cec38", "url": "https://www.semanticscholar.org/paper/b42c38975f3d3f8bfe4b0c1a6e576c3e297cec38", "title": "Stochastic Gradient Push for Distributed Deep Learning", "abstract": "Distributed data-parallel algorithms aim to accelerate the training of deep neural networks by parallelizing the computation of large mini-batch gradient updates across multiple nodes. Approaches that synchronize nodes using exact distributed averaging (e.g., via AllReduce) are sensitive to stragglers and communication delays. The PushSum gossip algorithm is robust to these issues, but only performs approximate distributed averaging. This paper studies Stochastic Gradient Push (SGP), which combines PushSum with stochastic gradient updates. We prove that SGP converges to a stationary point of smooth, non-convex objectives at the same sub-linear rate as SGD, and that all nodes achieve consensus. We empirically validate the performance of SGP on image classification (ResNet-50, ImageNet) and machine translation (Transformer, WMT'16 En-De) workloads. Our code will be made publicly available.", "year": 2018, "referenceCount": 42, "citationCount": 211, "influentialCitationCount": 45, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "38698856", "name": "Mahmoud Assran"}, {"authorId": "1941365", "name": "Nicolas Loizou"}, {"authorId": "2482072", "name": "Nicolas Ballas"}, {"authorId": "2066127975", "name": "Michael G. Rabbat"}]}, {"paperId": "4fd5672a30ec6a620556ccf8dc1d9e76fc67a5bf", "url": "https://www.semanticscholar.org/paper/4fd5672a30ec6a620556ccf8dc1d9e76fc67a5bf", "title": "Identification of novel antibacterial peptides by chemoinformatics and machine learning.", "abstract": "The rise of antibiotic resistant pathogens is one of the most pressing global health issues. Discovery of new classes of antibiotics has not kept pace; new agents often suffer from cross-resistance to existing agents of similar structure. Short, cationic peptides with antimicrobial activity are essential to the host defenses of many organisms and represent a promising new class of antimicrobials. This paper reports the successful in silico screening for potent antibiotic peptides using a combination of QSAR and machine learning techniques. On the basis of initial high-throughput measurements of activity of over 1400 random peptides, artificial neural network models were built using QSAR descriptors and subsequently used to screen an in silico library of approximately 100,000 peptides. In vitro validation of the modeling showed 94% accuracy in identifying highly active peptides. The best peptides identified through screening were found to have activities comparable or superior to those of four conventional antibiotics and superior to the peptide most advanced in clinical development against a broad array of multiresistant human pathogens.", "year": 2009, "referenceCount": 32, "citationCount": 242, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Biology", "Medicine"], "authors": [{"authorId": "2160852", "name": "C. Fjell"}, {"authorId": "15888696", "name": "H. Jenssen"}, {"authorId": "2458164", "name": "K. Hilpert"}, {"authorId": "2059932271", "name": "Warren A. Cheung"}, {"authorId": "4550592", "name": "N. Pant\u00e9"}, {"authorId": "143696638", "name": "R. Hancock"}, {"authorId": "145230547", "name": "A. Cherkasov"}]}]}