{"total": 5120627, "offset": 1400, "next": 1500, "data": [{"paperId": "64ad89855f829d0587e0f77d1be52030616c67cb", "url": "https://www.semanticscholar.org/paper/64ad89855f829d0587e0f77d1be52030616c67cb", "title": "Sentiment analysis in twitter using machine learning techniques", "abstract": "Sentiment analysis deals with identifying and classifying opinions or sentiments expressed in source text. Social media is generating a vast amount of sentiment rich data in the form of tweets, status updates, blog posts etc. Sentiment analysis of this user generated data is very useful in knowing the opinion of the crowd. Twitter sentiment analysis is difficult compared to general sentiment analysis due to the presence of slang words and misspellings. The maximum limit of characters that are allowed in Twitter is 140. Knowledge base approach and Machine learning approach are the two strategies used for analyzing sentiments from the text. In this paper, we try to analyze the twitter posts about electronic products like mobiles, laptops etc using Machine Learning approach. By doing sentiment analysis in a specific domain, it is possible to identify the effect of domain information in sentiment classification. We present a new feature vector for classifying the tweets as positive, negative and extract peoples' opinion about products.", "year": 2013, "referenceCount": 18, "citationCount": 316, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9388856", "name": "M. Neethu"}, {"authorId": "144357907", "name": "R. Rajasree"}]}, {"paperId": "36aa0d0936b2cf128c646c36a1981807b5a27aaf", "url": "https://www.semanticscholar.org/paper/36aa0d0936b2cf128c646c36a1981807b5a27aaf", "title": "On Kernel-Target Alignment", "abstract": null, "year": 2001, "referenceCount": 24, "citationCount": 1080, "influentialCitationCount": 145, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1685083", "name": "N. Cristianini"}, {"authorId": "1404459229", "name": "J. Shawe-Taylor"}, {"authorId": "1766703", "name": "A. Elisseeff"}, {"authorId": "145536952", "name": "J. Kandola"}]}, {"paperId": "831edc3d67457db83da40d260e93bfd7559347ae", "url": "https://www.semanticscholar.org/paper/831edc3d67457db83da40d260e93bfd7559347ae", "title": "Dyna, an integrated architecture for learning, planning, and reacting", "abstract": "Dyna is an AI architecture that integrates learning, planning, and reactive execution. Learning methods are used in Dyna both for compiling planning results and for updating a model of the effects of the agent's actions on the world. Planning is incremental and can use the probabilistic and ofttimes incorrect world models generated by learning processes. Execution is fully reactive in the sense that no planning intervenes between perception and action. Dyna relies on machine learning methods for learning from examples---these are among the basic building blocks making up the architecture---yet is not tied to any particular method. This paper briefly introduces Dyna and discusses its strengths and weaknesses with respect to other architectures.", "year": 1990, "referenceCount": 29, "citationCount": 721, "influentialCitationCount": 64, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1699645", "name": "R. Sutton"}]}, {"paperId": "c3c52164751a7c116657c8e95790d84aa7308e43", "url": "https://www.semanticscholar.org/paper/c3c52164751a7c116657c8e95790d84aa7308e43", "title": "Pattern Recognition & Machine Learning", "abstract": null, "year": 2016, "referenceCount": 11, "citationCount": 120, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2006062040", "name": "Y. Anzai"}]}, {"paperId": "5f1024a28c977c9555b77ef2b7c8cada2ae3b36b", "url": "https://www.semanticscholar.org/paper/5f1024a28c977c9555b77ef2b7c8cada2ae3b36b", "title": "Machine Learning: Hands-On for Developers and Technical Professionals", "abstract": "Dig deep into the data with a hands-on guide to machine learning Machine Learning: Hands-On for Developers and Technical Professionals provides hands-on instruction and fully-coded working examples for the most common machine learning techniques used by developers and technical professionals. The book contains a breakdown of each ML variant, explaining how it works and how it is used within certain industries, allowing readers to incorporate the presented techniques into their own work as they follow along. A core tenant of machine learning is a strong focus on data preparation, and a full exploration of the various types of learning algorithms illustrates how the proper tools can help any developer extract information and insights from existing data. The book includes a full complement of Instructor's Materials to facilitate use in the classroom, making this resource useful for students and as a professional reference.", "year": 2014, "referenceCount": 0, "citationCount": 128, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51011457", "name": "J. Bell"}]}, {"paperId": "bd2e5b8f07a4b4675f4b5ab8273ab10eb80704a9", "url": "https://www.semanticscholar.org/paper/bd2e5b8f07a4b4675f4b5ab8273ab10eb80704a9", "title": "Fast and Scalable Local Kernel Machines", "abstract": "A computationally efficient approach to local learning with kernel methods is presented. The Fast Local Kernel Support Vector Machine (FaLK-SVM) trains a set of local SVMs on redundant neighbourhoods in the training set and an appropriate model for each query point is selected at testing time according to a proximity strategy. Supported by a recent result by Zakai and Ritov (2009) relating consistency and localizability, our approach achieves high classification accuracies by dividing the separation function in local optimisation problems that can be handled very efficiently from the computational viewpoint. The introduction of a fast local model selection further speeds-up the learning process. Learning and complexity bounds are derived for FaLK-SVM, and the empirical evaluation of the approach (with data sets up to 3 million points) showed that it is much faster and more accurate and scalable than state-of-the-art accurate and approximated SVM solvers at least for non high-dimensional data sets. More generally, we show that locality can be an important factor to sensibly speed-up learning approaches and kernel methods, differently from other recent techniques that tend to dismiss local information in order to improve scalability.", "year": 2010, "referenceCount": 84, "citationCount": 93, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47429813", "name": "N. Segata"}, {"authorId": "2930903", "name": "E. Blanzieri"}]}, {"paperId": "f1b1d5da2b18bb3740a24ff3c75c19c82feae375", "url": "https://www.semanticscholar.org/paper/f1b1d5da2b18bb3740a24ff3c75c19c82feae375", "title": "Data Mining and Machine Learning Techniques for the Identification of Mutagenicity Inducing Substructures and Structure Activity Relationships of Noncongeneric Compounds", "abstract": "This paper explores the utility of data mining and machine learning algorithms for the induction of mutagenicity structure-activity relationships (SARs) from noncongeneric data sets. We compare (i) a newly developed algorithm (MOLFEA) for the generation of descriptors (molecular fragments) for noncongeneric compounds with traditional SAR approaches (molecular properties) and (ii) different machine learning algorithms for the induction of SARs from these descriptors. In addition we investigate the optimal parameter settings for these programs and give an exemplary interpretation of the derived models. The predictive accuracies of models using MOLFEA derived descriptors is approximately 10-15%age points higher than those using molecular properties alone. Using both types of descriptors together does not improve the derived models. From the applied machine learning techniques the rule learner PART and support vector machines gave the best results, although the differences between the learning algorithms are only marginal. We were able to achieve predictive accuracies up to 78% for 10-fold cross-validation. The resulting models are relatively easy to interpret and usable for predictive as well as for explanatory purposes.", "year": 2004, "referenceCount": 27, "citationCount": 240, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "3323969", "name": "C. Helma"}, {"authorId": "2070118303", "name": "Tobias Cramer"}, {"authorId": "145471896", "name": "Stefan Kramer"}, {"authorId": "1740042", "name": "L. D. Raedt"}]}, {"paperId": "c5ee696025d1651e8e8cae7f399ef11c3dc31976", "url": "https://www.semanticscholar.org/paper/c5ee696025d1651e8e8cae7f399ef11c3dc31976", "title": "Big Data Meets Quantum Chemistry Approximations: The \u0394-Machine Learning Approach.", "abstract": "Chemically accurate and comprehensive studies of the virtual space of all possible molecules are severely limited by the computational cost of quantum chemistry. We introduce a composite strategy that adds machine learning corrections to computationally inexpensive approximate legacy quantum methods. After training, highly accurate predictions of enthalpies, free energies, entropies, and electron correlation energies are possible, for significantly larger molecular sets than used for training. For thermochemical properties of up to 16k isomers of C7H10O2 we present numerical evidence that chemical accuracy can be reached. We also predict electron correlation energy in post Hartree-Fock methods, at the computational cost of Hartree-Fock, and we establish a qualitative relationship between molecular entropy and electron correlation. The transferability of our approach is demonstrated, using semiempirical quantum chemistry and machine learning models trained on 1 and 10% of 134k organic molecules, to reproduce enthalpies of all remaining molecules at density functional theory level of accuracy.", "year": 2015, "referenceCount": 50, "citationCount": 415, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Medicine"], "authors": [{"authorId": "6781829", "name": "R. Ramakrishnan"}, {"authorId": "5378288", "name": "Pavlo O. Dral"}, {"authorId": "48041657", "name": "M. Rupp"}, {"authorId": "7847508", "name": "O. A. von Lilienfeld"}]}, {"paperId": "24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe", "url": "https://www.semanticscholar.org/paper/24d21ecaeb2d2ecc20e26a5e3f5128247704ccfe", "title": "Swarm Learning for decentralized and confidential clinical machine learning", "abstract": null, "year": 2021, "referenceCount": 50, "citationCount": 158, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "1441363881", "name": "Stefanie Warnat-Herresthal"}, {"authorId": "1491641853", "name": "Hartmut Schultze"}, {"authorId": "39126788", "name": "K. Shastry"}, {"authorId": "2530831", "name": "Sathyanarayanan Manamohan"}, {"authorId": "3040289", "name": "Saikat Mukherjee"}, {"authorId": "2067125343", "name": "Vishesh Garg"}, {"authorId": "7765742", "name": "Ravi Sarveswara"}, {"authorId": "48648791", "name": "K. H\u00e4ndler"}, {"authorId": "5953896", "name": "P. Pickkers"}, {"authorId": "31528430", "name": "N. Aziz"}, {"authorId": "8297290", "name": "S. Ktena"}, {"authorId": "12668727", "name": "F. Tran"}, {"authorId": "2220233", "name": "M. Bitzer"}, {"authorId": "32478749", "name": "S. Ossowski"}, {"authorId": "3818521", "name": "N. Casadei"}, {"authorId": "6317095", "name": "C. Herr"}, {"authorId": "51327074", "name": "Daniel Petersheim"}, {"authorId": "4829125", "name": "U. Behrends"}, {"authorId": "37417418", "name": "Fabian Kern"}, {"authorId": "4846639", "name": "T. Fehlmann"}, {"authorId": "10371954", "name": "P. Schommers"}, {"authorId": "4764355", "name": "C. Lehmann"}, {"authorId": "30137838", "name": "M. Augustin"}, {"authorId": "5775372", "name": "J. Rybniker"}, {"authorId": "1963213", "name": "J. Altm\u00fcller"}, {"authorId": "150247497", "name": "N. Mishra"}, {"authorId": "1820956545", "name": "J. P. Bernardes"}, {"authorId": "5634144", "name": "B. Kr\u00e4mer"}, {"authorId": "145567472", "name": "L. Bonaguro"}, {"authorId": "1397963301", "name": "J. Schulte-Schrepping"}, {"authorId": "5342859", "name": "Elena De Domenico"}, {"authorId": "1780746912", "name": "Christian Siever"}, {"authorId": "50333718", "name": "Michael Kraut"}, {"authorId": "153555977", "name": "Milind Desai"}, {"authorId": "1780578787", "name": "Bruno Monnet"}, {"authorId": "2128923813", "name": "M. Saridaki"}, {"authorId": "144307851", "name": "C. Siegel"}, {"authorId": "49572483", "name": "A. Drews"}, {"authorId": "1780708283", "name": "Melanie Nuesch-Germano"}, {"authorId": "4505843", "name": "H. Theis"}, {"authorId": "5172855", "name": "J. Heyckendorf"}, {"authorId": "2172440244", "name": "S. Schreiber"}, {"authorId": "1398942621", "name": "S. Kim-Hellmuth"}, {"authorId": "6316085", "name": "J. Nattermann"}, {"authorId": "6273548", "name": "D. Skowasch"}, {"authorId": "6177275", "name": "I. Kurth"}, {"authorId": "145122867", "name": "A. Keller"}, {"authorId": "6664655", "name": "R. Bals"}, {"authorId": "1701925", "name": "P. N\u00fcrnberg"}, {"authorId": "2070590037", "name": "O. Riess"}, {"authorId": "144883195", "name": "P. Rosenstiel"}, {"authorId": "145123820", "name": "M. Netea"}, {"authorId": "2051758169", "name": "F. Theis"}, {"authorId": "145807368", "name": "S. Mukherjee"}, {"authorId": "145598514", "name": "Michael Backes"}, {"authorId": "143690088", "name": "A. Aschenbrenner"}, {"authorId": "4796855", "name": "T. Ulas"}, {"authorId": "145189483", "name": "M. Breteler"}, {"authorId": "1381717153", "name": "E. Giamarellos\u2010Bourboulis"}, {"authorId": "4713521", "name": "M. Kox"}, {"authorId": "144332474", "name": "M. Becker"}, {"authorId": "34936176", "name": "S. Cheran"}, {"authorId": "2546502", "name": "M. Woodacre"}, {"authorId": "17836661", "name": "E. L. Goh"}, {"authorId": "2356294", "name": "J. Schultze"}]}, {"paperId": "c393c48a2d634acb1daee3567eaaea733a1224fb", "url": "https://www.semanticscholar.org/paper/c393c48a2d634acb1daee3567eaaea733a1224fb", "title": "Efficient Convolution Kernels for Dependency and Constituent Syntactic Trees", "abstract": null, "year": 2006, "referenceCount": 28, "citationCount": 499, "influentialCitationCount": 66, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1719404", "name": "Alessandro Moschitti"}]}, {"paperId": "9bc86d59b2a78f91a9ffd8b8ffa4d5c3a1d3a787", "url": "https://www.semanticscholar.org/paper/9bc86d59b2a78f91a9ffd8b8ffa4d5c3a1d3a787", "title": "Design of efficient molecular organic light-emitting diodes by a high-throughput virtual screening and experimental approach.", "abstract": null, "year": 2016, "referenceCount": 49, "citationCount": 583, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Materials Science", "Medicine"], "authors": [{"authorId": "1398336096", "name": "Rafael G\u00f3mez-Bombarelli"}, {"authorId": "1422175619", "name": "J. Aguilera-Iparraguirre"}, {"authorId": "145916942", "name": "Timothy D. Hirzel"}, {"authorId": "1704657", "name": "D. Duvenaud"}, {"authorId": "1683298", "name": "D. Maclaurin"}, {"authorId": "1402965139", "name": "M. Blood-Forsythe"}, {"authorId": "47633881", "name": "H. Chae"}, {"authorId": "12955864", "name": "Markus Einzinger"}, {"authorId": "48074866", "name": "Dong-Gwang Ha"}, {"authorId": "47353855", "name": "Tony C. Wu"}, {"authorId": "48135233", "name": "Georgios Markopoulos"}, {"authorId": "12954232", "name": "S. Jeon"}, {"authorId": "153579776", "name": "Hosuk Kang"}, {"authorId": "144392756", "name": "H. Miyazaki"}, {"authorId": "4129389", "name": "Masaki Numata"}, {"authorId": "49899968", "name": "Sunghan Kim"}, {"authorId": "2513153", "name": "Wenliang Huang"}, {"authorId": "1400339993", "name": "Seong Ik Hong"}, {"authorId": "144812597", "name": "M. Baldo"}, {"authorId": "1722180", "name": "Ryan P. Adams"}, {"authorId": "1380248954", "name": "Al\u00e1n Aspuru-Guzik"}]}, {"paperId": "32c709cf5d6ba1b5a729b4871c3129bb1bf578bf", "url": "https://www.semanticscholar.org/paper/32c709cf5d6ba1b5a729b4871c3129bb1bf578bf", "title": "Quantum machine learning in high energy physics", "abstract": "Machine learning has been used in high energy physics (HEP) for a long time, primarily at the analysis level with supervised classification. Quantum computing was postulated in the early 1980s as way to perform computations that would not be tractable with a classical computer. With the advent of noisy intermediate-scale quantum computing devices, more quantum algorithms are being developed with the aim at exploiting the capacity of the hardware for machine learning applications. An interesting question is whether there are ways to apply quantum machine learning to HEP. This paper reviews the first generation of ideas that use quantum machine learning on problems in HEP and provide an outlook on future applications.", "year": 2020, "referenceCount": 176, "citationCount": 46, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Physics"], "authors": [{"authorId": "47368669", "name": "W. Guan"}, {"authorId": "51116496", "name": "G. Perdue"}, {"authorId": "52133091", "name": "Arthur Pesah"}, {"authorId": "3048564", "name": "M. Schuld"}, {"authorId": "94071871", "name": "K. Terashi"}, {"authorId": "3425469", "name": "S. Vallecorsa"}, {"authorId": "52630992", "name": "J. Vlimant"}]}, {"paperId": "f9de494da473d8a2e90ed331d9ab6c8a39d8737d", "url": "https://www.semanticscholar.org/paper/f9de494da473d8a2e90ed331d9ab6c8a39d8737d", "title": "Survey of Machine Learning Techniques in Drug Discovery.", "abstract": "BACKGROUND\nDrug discovery, which is the process of discovering new candidate medications, is very important for pharmaceutical industries. At its current stage, discovering new drugs is still a very expensive and time-consuming process, requiring Phases I, II and III for clinical trials. Recently, machine learning techniques in Artificial Intelligence (AI), especially the deep learning techniques which allow a computational model to generate multiple layers, have been widely applied and achieved state-of-the-art performance in different fields, such as speech recognition, image classification, bioinformatics, etc. One very important application of these AI techniques is in the field of drug discovery.\n\n\nMETHODS\nWe did a large-scale literature search on existing scientific websites (e.g, ScienceDirect, Arxiv) and startup companies to understand current status of machine learning techniques in drug discovery.\n\n\nRESULTS\nOur experiments demonstrated that there are different patterns in machine learning fields and drug discovery fields. For example, keywords like prediction, brain, discovery, and treatment are usually in drug discovery fields. Also, the total number of papers published in drug discovery fields with machine learning techniques is increasing every year.\n\n\nCONCLUSION\nThe main focus of this survey is to understand the current status of machine learning techniques in the drug discovery field within both academic and industrial settings, and discuss its potential future applications. Several interesting patterns for machine learning techniques in drug discovery fields are discussed in this survey.", "year": 2019, "referenceCount": 74, "citationCount": 126, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "51183421", "name": "Natalie Stephenson"}, {"authorId": "51204603", "name": "Emily Shane"}, {"authorId": "2056317579", "name": "Jessica Chase"}, {"authorId": "2066120681", "name": "Jason Rowland"}, {"authorId": "2064613774", "name": "David Ries"}, {"authorId": "2065880952", "name": "Nicola Justice"}, {"authorId": "2159189616", "name": "Jie Zhang"}, {"authorId": "145822563", "name": "Leong Chan"}, {"authorId": "33082702", "name": "Renzhi Cao"}]}, {"paperId": "2e47b92db5f7887eef6b3750f8482f9441251236", "url": "https://www.semanticscholar.org/paper/2e47b92db5f7887eef6b3750f8482f9441251236", "title": "Noninvasive brain-actuated control of a mobile robot by human EEG", "abstract": "Brain activity recorded noninvasively is sufficient to control a mobile robot if advanced robotics is used in combination with asynchronous electroencephalogram (EEG) analysis and machine learning techniques. Until now brain-actuated control has mainly relied on implanted electrodes, since EEG-based systems have been considered too slow for controlling rapid and complex sequences of movements. We show that two human subjects successfully moved a robot between several rooms by mental control only, using an EEG-based brain-machine interface that recognized three mental states. Mental control was comparable to manual control on the same task with a performance ratio of 0.74.", "year": 2004, "referenceCount": 27, "citationCount": 717, "influentialCitationCount": 21, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1716694", "name": "J. Mill\u00e1n"}, {"authorId": "2315711", "name": "F. Renkens"}, {"authorId": "1694040", "name": "J. Mouri\u00f1o"}, {"authorId": "1708945", "name": "W. Gerstner"}]}, {"paperId": "ee9995f5bf81578b0f9622e26f777cc16c10dc48", "url": "https://www.semanticscholar.org/paper/ee9995f5bf81578b0f9622e26f777cc16c10dc48", "title": "Machine learning and medical education", "abstract": null, "year": 2018, "referenceCount": 16, "citationCount": 118, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1887282", "name": "V. Kolachalama"}, {"authorId": "6295799", "name": "Priya S Garg"}]}, {"paperId": "021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "url": "https://www.semanticscholar.org/paper/021fc345d40d3e6332cd2ef276e2eaa5e71102e4", "title": "Speeding up Convolutional Neural Networks with Low Rank Expansions", "abstract": "The focus of this paper is speeding up the application of convolutional neural networks. While delivering impressive results across a range of computer vision and machine learning tasks, these networks are computationally demanding, limiting their deployability. Convolutional layers generally consume the bulk of the processing time, and so in this work we present two simple schemes for drastically speeding up these layers. This is achieved by exploiting cross-channel or filter redundancy to construct a low rank basis of filters that are rank-1 in the spatial domain. Our methods are architecture agnostic, and can be easily applied to existing CPU and GPU convolutional frameworks for tuneable speedup performance. We demonstrate this with a real world network designed for scene text character recognition [15], showing a possible 2.5\u00d7 speedup with no loss in accuracy, and 4.5\u00d7 speedup with less than 1% drop in accuracy, still achieving state-of-the-art on standard benchmarks.", "year": 2014, "referenceCount": 42, "citationCount": 1223, "influentialCitationCount": 84, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3093886", "name": "Max Jaderberg"}, {"authorId": "1687524", "name": "A. Vedaldi"}, {"authorId": "1688869", "name": "Andrew Zisserman"}]}, {"paperId": "06cadde4cd846a721cbe49be65e95d2cdd4b8780", "url": "https://www.semanticscholar.org/paper/06cadde4cd846a721cbe49be65e95d2cdd4b8780", "title": "Data Mining, Fourth Edition: Practical Machine Learning Tools and Techniques", "abstract": "Data Mining: Practical Machine Learning Tools and Techniques, Fourth Edition, offers a thorough grounding in machine learning concepts, along with practical advice on applying these tools and techniques in real-world data mining situations. This highly anticipated fourth edition of the most acclaimed work on data mining and machine learning teaches readers everything they need to know to get going, from preparing inputs, interpreting outputs, evaluating results, to the algorithmic methods at the heart of successful data mining approaches. Extensive updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including substantial new chapters on probabilistic methods and on deep learning. Accompanying the book is a new version of the popular WEKA machine learning software from the University of Waikato. Authors Witten, Frank, Hall, and Pal include today's techniques coupled with the methods at the leading edge of contemporary research. Please visit the book companion website at http://www.cs.waikato.ac.nz/ml/weka/book.html It contains Powerpoint slides for Chapters 1-12. This is a very comprehensive teaching resource, with many PPT slides covering each chapter of the bookOnline Appendix on the Weka workbench; again a very comprehensive learning aid for the open source software that goes with the book.", "year": 2016, "referenceCount": 1, "citationCount": 174, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9419406", "name": "I. Witten"}, {"authorId": "143713826", "name": "Eibe Frank"}, {"authorId": "118860642", "name": "M. Hall"}, {"authorId": "98109738", "name": "Chris Pal"}]}, {"paperId": "e42159082e7e3c9a82c820c31dccb5cf0988acbb", "url": "https://www.semanticscholar.org/paper/e42159082e7e3c9a82c820c31dccb5cf0988acbb", "title": "Data mining: machine learning, statistics, and databases", "abstract": "Knowledge discovery in databases and data mining aim at semiautomatic tools for the analysis of large data sets. We give an overview of the area and present some of the research issues, especially from the database angle.", "year": 1996, "referenceCount": 25, "citationCount": 238, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1712654", "name": "H. Mannila"}]}, {"paperId": "3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127", "url": "https://www.semanticscholar.org/paper/3abcd0ffc54c3a16c9dc5e5d3ea59eaa43070127", "title": "Preventing undesirable behavior of intelligent machines", "abstract": "Making well-behaved algorithms Machine learning algorithms are being used in an ever-increasing number of applications, and many of these applications affect quality of life. Yet such algorithms often exhibit undesirable behavior, from various types of bias to causing financial loss or delaying medical diagnoses. In standard machine learning approaches, the burden of avoiding this harmful behavior is placed on the user of the algorithm, who most often is not a computer scientist. Thomas et al. introduce a general framework for algorithm design in which this burden is shifted from the user to the designer of the algorithm. The researchers illustrate the benefits of their approach using examples in gender fairness and diabetes management. Science, this issue p. 999 A machine learning algorithm design framework shifts the burden of avoiding undesirable behavior from user to designer. Intelligent machines using machine learning algorithms are ubiquitous, ranging from simple data analysis and pattern recognition tools to complex systems that achieve superhuman performance on various tasks. Ensuring that they do not exhibit undesirable behavior\u2014that they do not, for example, cause harm to humans\u2014is therefore a pressing problem. We propose a general and flexible framework for designing machine learning algorithms. This framework simplifies the problem of specifying and regulating undesirable behavior. To show the viability of this framework, we used it to create machine learning algorithms that precluded the dangerous behavior caused by standard machine learning algorithms in our experiments. Our framework for designing machine learning algorithms simplifies the safe and responsible application of machine learning.", "year": 2019, "referenceCount": 209, "citationCount": 101, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "143640165", "name": "P. Thomas"}, {"authorId": "1424529299", "name": "Bruno Castro da Silva"}, {"authorId": "1730590", "name": "A. Barto"}, {"authorId": "34617935", "name": "S. Giguere"}, {"authorId": "2932798", "name": "Yuriy Brun"}, {"authorId": "2563117", "name": "Emma Brunskill"}]}, {"paperId": "1bd6e929ed8384ea2212d50ab3c103ec018cc9fd", "url": "https://www.semanticscholar.org/paper/1bd6e929ed8384ea2212d50ab3c103ec018cc9fd", "title": "A Bayesian/Information Theoretic Model of Learning to Learn via Multiple Task Sampling", "abstract": null, "year": 1997, "referenceCount": 24, "citationCount": 410, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47392513", "name": "Jonathan Baxter"}]}, {"paperId": "7ac58400e5063bed9b7c35f87e44ddb917ccf357", "url": "https://www.semanticscholar.org/paper/7ac58400e5063bed9b7c35f87e44ddb917ccf357", "title": "PyTorch-BigGraph: A Large-scale Graph Embedding System", "abstract": "Graph embedding methods produce unsupervised node features from graphs that can then be used for a variety of machine learning tasks. Modern graphs, particularly in industrial applications, contain billions of nodes and trillions of edges, which exceeds the capability of existing embedding systems. We present PyTorch-BigGraph (PBG), an embedding system that incorporates several modifications to traditional multi-relation embedding systems that allow it to scale to graphs with billions of nodes and trillions of edges. PBG uses graph partitioning to train arbitrarily large embeddings on either a single machine or in a distributed environment. We demonstrate comparable performance with existing embedding systems on common benchmarks, while allowing for scaling to arbitrarily large graphs and parallelization on multiple machines. We train and evaluate embeddings on several large social network graphs as well as the full Freebase dataset, which contains over 100 million nodes and 2 billion edges.", "year": 2019, "referenceCount": 43, "citationCount": 241, "influentialCitationCount": 51, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1977806", "name": "Adam Lerer"}, {"authorId": "51183248", "name": "Ledell Yu Wu"}, {"authorId": "144246357", "name": "Jiajun Shen"}, {"authorId": "47733973", "name": "Timoth\u00e9e Lacroix"}, {"authorId": "90076842", "name": "Luca Wehrstedt"}, {"authorId": "2069222826", "name": "Abhijit Bose"}, {"authorId": "1970752", "name": "A. Peysakhovich"}]}, {"paperId": "d21c796c6af518da905752a64d54b541384eba12", "url": "https://www.semanticscholar.org/paper/d21c796c6af518da905752a64d54b541384eba12", "title": "Semisupervised Learning for Computational Linguistics", "abstract": "The rapid advancement in the theoretical understanding of statistical and machine learning methods for semisupervised learning has made it difficult for nonspecialists to keep up to date in the field. Providing a broad, accessible treatment of the theory as well as linguistic applications, Semisupervised Learning for Computational Linguistics offers self-contained coverage of semisupervised methods that includes background material on supervised and unsupervised learning. The book presents a brief history of semisupervised learning and its place in the spectrum of learning methods before moving on to discuss well-known natural language processing methods, such as self-training and co-training. It then centers on machine learning techniques, including the boundary-oriented methods of perceptrons, boosting, support vector machines (SVMs), and the null-category noise model. In addition, the book covers clustering, the expectation-maximization (EM) algorithm, related generative methods, and agreement methods. It concludes with the graph-based method of label propagation as well as a detailed discussion of spectral methods. Taking an intuitive approach to the material, this lucid book facilitates the application of semisupervised learning methods to natural language processing and provides the framework and motivation for a more systematic study of machine learning.", "year": 2007, "referenceCount": 8, "citationCount": 191, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35551590", "name": "Steven P. Abney"}]}, {"paperId": "d05d86db86a4ac0d95e6dcd951b42a9651939793", "url": "https://www.semanticscholar.org/paper/d05d86db86a4ac0d95e6dcd951b42a9651939793", "title": "Deep Learning Approach for Intelligent Intrusion Detection System", "abstract": "Machine learning techniques are being widely used to develop an intrusion detection system (IDS) for detecting and classifying cyberattacks at the network-level and the host-level in a timely and automatic manner. However, many challenges arise since malicious attacks are continually changing and are occurring in very large volumes requiring a scalable solution. There are different malware datasets available publicly for further research by cyber security community. However, no existing study has shown the detailed analysis of the performance of various machine learning algorithms on various publicly available datasets. Due to the dynamic nature of malware with continuously changing attacking methods, the malware datasets available publicly are to be updated systematically and benchmarked. In this paper, a deep neural network (DNN), a type of deep learning model, is explored to develop a flexible and effective IDS to detect and classify unforeseen and unpredictable cyberattacks. The continuous change in network behavior and rapid evolution of attacks makes it necessary to evaluate various datasets which are generated over the years through static and dynamic approaches. This type of study facilitates to identify the best algorithm which can effectively work in detecting future cyberattacks. A comprehensive evaluation of experiments of DNNs and other classical machine learning classifiers are shown on various publicly available benchmark malware datasets. The optimal network parameters and network topologies for DNNs are chosen through the following hyperparameter selection methods with KDDCup 99 dataset. All the experiments of DNNs are run till 1,000 epochs with the learning rate varying in the range [0.01\u20130.5]. The DNN model which performed well on KDDCup 99 is applied on other datasets, such as NSL-KDD, UNSW-NB15, Kyoto, WSN-DS, and CICIDS 2017, to conduct the benchmark. Our DNN model learns the abstract and high-dimensional feature representation of the IDS data by passing them into many hidden layers. Through a rigorous experimental testing, it is confirmed that DNNs perform well in comparison with the classical machine learning classifiers. Finally, we propose a highly scalable and hybrid DNNs framework called scale-hybrid-IDS-AlertNet which can be used in real-time to effectively monitor the network traffic and host-level events to proactively alert possible cyberattacks.", "year": 2019, "referenceCount": 85, "citationCount": 546, "influentialCitationCount": 24, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51162051", "name": "R. Vinayakumar"}, {"authorId": "2474250", "name": "M. Alazab"}, {"authorId": "144456263", "name": "K. Soman"}, {"authorId": "2916235", "name": "P. Poornachandran"}, {"authorId": "1399133087", "name": "Ameer Al-Nemrat"}, {"authorId": "145710905", "name": "S. Venkatraman"}]}, {"paperId": "899746ba46279cf2b842615ed38d998f9c4654df", "url": "https://www.semanticscholar.org/paper/899746ba46279cf2b842615ed38d998f9c4654df", "title": "A PAC-Bayesian bound for Lifelong Learning", "abstract": "Transfer learning has received a lot of attention in the machine learning community over the last years, and several effective algorithms have been developed. However, relatively little is known about their theoretical properties, especially in the setting of lifelong learning, where the goal is to transfer information to tasks for which no data have been observed so far. \n \nIn this work we study lifelong learning from a theoretical perspective. Our main result is a PAC-Bayesian generalization bound that offers a unified view on existing paradigms for transfer learning, such as the transfer of parameters or the transfer of low-dimensional representations. We also use the bound to derive two principled lifelong learning algorithms, and we show that these yield results comparable with existing methods.", "year": 2013, "referenceCount": 31, "citationCount": 171, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2989149", "name": "Anastasia Pentina"}, {"authorId": "1787591", "name": "Christoph H. Lampert"}]}, {"paperId": "b96ce653b41f9850a88452aaa959eda875c2a482", "url": "https://www.semanticscholar.org/paper/b96ce653b41f9850a88452aaa959eda875c2a482", "title": "Parallel machine scheduling with a learning effect", "abstract": null, "year": 2001, "referenceCount": 9, "citationCount": 219, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1732723", "name": "Gur Mosheiov"}]}, {"paperId": "4d8a2fcf9b43efd2636d5f90e0d35b98194c0025", "url": "https://www.semanticscholar.org/paper/4d8a2fcf9b43efd2636d5f90e0d35b98194c0025", "title": "Algorithmic Learning in a Random World", "abstract": null, "year": 2006, "referenceCount": 1, "citationCount": 808, "influentialCitationCount": 214, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "1740486", "name": "W. Gasarch"}, {"authorId": "1734090", "name": "B. Pierce"}, {"authorId": "2082283956", "name": "dorandomness by Shparlinski"}]}, {"paperId": "632a3e8971f6a26cf127a03689c28399d2fce7d8", "url": "https://www.semanticscholar.org/paper/632a3e8971f6a26cf127a03689c28399d2fce7d8", "title": "Domain Adaptation Problems: A DASVM Classification Technique and a Circular Validation Strategy", "abstract": "This paper addresses pattern classification in the framework of domain adaptation by considering methods that solve problems in which training data are assumed to be available only for a source domain different (even if related) from the target domain of (unlabeled) test data. Two main novel contributions are proposed: 1) a domain adaptation support vector machine (DASVM) technique which extends the formulation of support vector machines (SVMs) to the domain adaptation framework and 2) a circular indirect accuracy assessment strategy for validating the learning of domain adaptation classifiers when no true labels for the target--domain instances are available. Experimental results, obtained on a series of two-dimensional toy problems and on two real data sets related to brain computer interface and remote sensing applications, confirmed the effectiveness and the reliability of both the DASVM technique and the proposed circular validation strategy.", "year": 2010, "referenceCount": 51, "citationCount": 545, "influentialCitationCount": 63, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1698844", "name": "L. Bruzzone"}, {"authorId": "1681367", "name": "M. Marconcini"}]}, {"paperId": "a6f7d1a828e2bb553b5a89efbf76e50f84c969c2", "url": "https://www.semanticscholar.org/paper/a6f7d1a828e2bb553b5a89efbf76e50f84c969c2", "title": "Natural Language Annotation for Machine Learning", "abstract": "Create your own natural language training corpus for machine learning. This example-driven book walks you through the annotation cycle, from selecting an annotation task and creating the annotation specification to designing the guidelines, creating a \"gold standard\" corpus, and then beginning the actual data creation with the annotation process. Systems exist for analyzing existing corpora, but making a new corpus can be extremely complex. To help you build a foundation for your own machine learning goals, this easy-to-use guide includes case studies that demonstrate four different annotation tasks in detail. You'll also learn how to use a lightweight software package for annotating texts and adjudicating the annotations. This book is a perfect companion to O'Reilly's Natural Language Processing with Python, which describes how to use existing corpora with the Natural Language Toolkit.", "year": 2012, "referenceCount": 10, "citationCount": 172, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1707726", "name": "J. Pustejovsky"}, {"authorId": "4301541", "name": "Amber Stubbs"}]}, {"paperId": "6f55f2234a002c69001df8ef9108cb86f1dfa506", "url": "https://www.semanticscholar.org/paper/6f55f2234a002c69001df8ef9108cb86f1dfa506", "title": "Accurate Uncertainties for Deep Learning Using Calibrated Regression", "abstract": "Methods for reasoning under uncertainty are a key building block of accurate and reliable machine learning systems. Bayesian methods provide a general framework to quantify uncertainty. However, because of model misspecification and the use of approximate inference, Bayesian uncertainty estimates are often inaccurate -- for example, a 90% credible interval may not contain the true outcome 90% of the time. Here, we propose a simple procedure for calibrating any regression algorithm; when applied to Bayesian and probabilistic models, it is guaranteed to produce calibrated uncertainty estimates given enough data. Our procedure is inspired by Platt scaling and extends previous work on classification. We evaluate this approach on Bayesian linear regression, feedforward, and recurrent neural networks, and find that it consistently outputs well-calibrated credible intervals while improving performance on time series forecasting and model-based reinforcement learning tasks.", "year": 2018, "referenceCount": 34, "citationCount": 349, "influentialCitationCount": 51, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145712106", "name": "Volodymyr Kuleshov"}, {"authorId": "51130926", "name": "Nathan Fenner"}, {"authorId": "2490652", "name": "S. Ermon"}]}, {"paperId": "7ec3898439c2032c47762784085c3b54cae20523", "url": "https://www.semanticscholar.org/paper/7ec3898439c2032c47762784085c3b54cae20523", "title": "Diagnosing Breast Cancer Based on Support Vector Machines", "abstract": "The Support Vector Machine (SVM) classification algorithm, recently developed from the machine learning community, was used to diagnose breast cancer. At the same time, the SVM was compared to several machine learning techniques currently used in this field. The classification task involves predicting the state of diseases, using data obtained from the UCI machine learning repository. SVM outperformed k-means cluster and two artificial neural networks on the whole. It can be concluded that nine samples could be mislabeled from the comparison of several machine learning techniques.", "year": 2003, "referenceCount": 20, "citationCount": 117, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science", "Chemistry"], "authors": [{"authorId": "46936235", "name": "Huanxiang Liu"}, {"authorId": "1731291", "name": "Ruisheng Zhang"}, {"authorId": "145123382", "name": "F. Luan"}, {"authorId": "2652197", "name": "X. Yao"}, {"authorId": "2139847", "name": "Mancang Liu"}, {"authorId": "145248454", "name": "Zhide Hu"}, {"authorId": "27524105", "name": "B. Fan"}]}, {"paperId": "a3846ae9a5bc6f7230442e0684aeb62eabf3c341", "url": "https://www.semanticscholar.org/paper/a3846ae9a5bc6f7230442e0684aeb62eabf3c341", "title": "Machine learning modelling for predicting soil liquefaction susceptibility", "abstract": "Abstract. This study describes two machine learning techniques applied to predict liquefaction susceptibility of soil based on the standard penetration test (SPT) data from the 1999 Chi-Chi, Taiwan earthquake. The first machine learning technique which uses Artificial Neural Network (ANN) based on multi-layer perceptions (MLP) that are trained with Levenberg-Marquardt backpropagation algorithm. The second machine learning technique uses the Support Vector machine (SVM) that is firmly based on the theory of statistical learning theory, uses classification technique. ANN and SVM have been developed to predict liquefaction susceptibility using corrected SPT [(N1)60] and cyclic stress ratio (CSR). Further, an attempt has been made to simplify the models, requiring only the two parameters [(N1)60 and peck ground acceleration (amax/g)], for the prediction of liquefaction susceptibility. The developed ANN and SVM models have also been applied to different case histories available globally. The paper also highlights the capability of the SVM over the ANN models.", "year": 2011, "referenceCount": 41, "citationCount": 138, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "3258641", "name": "P. Samui"}, {"authorId": "145447732", "name": "T. Sitharam"}]}, {"paperId": "1d298139fc73654db7d8d936b07fa9e85bba008c", "url": "https://www.semanticscholar.org/paper/1d298139fc73654db7d8d936b07fa9e85bba008c", "title": "DeepTox: Toxicity Prediction using Deep Learning", "abstract": "The Tox21 Data Challenge has been the largest effort of the scientific community to compare computational methods for toxicity prediction. This challenge comprised 12,000 environmental chemicals and drugs which were measured for 12 different toxic effects by specifically designed assays. We participated in this challenge to assess the performance of Deep Learning in computational toxicity prediction. Deep Learning has already revolutionized image processing, speech recognition, and language understanding but has not yet been applied to computational toxicity. Deep Learning is founded on novel algorithms and architectures for artificial neural networks together with the recent availability of very fast computers and massive datasets. It discovers multiple levels of distributed representations of the input, with higher levels representing more abstract concepts. We hypothesized that the construction of a hierarchy of chemical features gives Deep Learning the edge over other toxicity prediction methods. Furthermore, Deep Learning naturally enables multi-task learning, that is, learning of all toxic effects in one neural network and thereby learning of highly informative chemical features. In order to utilize Deep Learning for toxicity prediction, we have developed the DeepTox pipeline. First, DeepTox normalizes the chemical representations of the compounds. Then it computes a large number of chemical descriptors that are used as input to machine learning methods. In its next step, DeepTox trains models, evaluates them, and combines the best of them to ensembles. Finally, DeepTox predicts the toxicity of new compounds. In the Tox21 Data Challenge, DeepTox had the highest performance of all computational methods winning the grand challenge, the nuclear receptor panel, the stress response panel, and six single assays (teams ``Bioinf@JKU''). We found that Deep Learning excelled in toxicity prediction and outperformed many other computational approaches like naive Bayes, support vector machines, and random forests.", "year": 2016, "referenceCount": 102, "citationCount": 517, "influentialCitationCount": 23, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "144831680", "name": "Andreas Mayr"}, {"authorId": "1994964", "name": "G. Klambauer"}, {"authorId": "2465270", "name": "Thomas Unterthiner"}, {"authorId": "3308557", "name": "S. Hochreiter"}]}, {"paperId": "c7f948eaf0665fba5a34337fb3004698920c4a49", "url": "https://www.semanticscholar.org/paper/c7f948eaf0665fba5a34337fb3004698920c4a49", "title": "Are we really making much progress? A worrying analysis of recent neural recommendation approaches", "abstract": "Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area.", "year": 2019, "referenceCount": 54, "citationCount": 384, "influentialCitationCount": 31, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51237994", "name": "Maurizio Ferrari Dacrema"}, {"authorId": "1709519", "name": "P. Cremonesi"}, {"authorId": "1705282", "name": "D. Jannach"}]}, {"paperId": "4fe855079443b097587fe42bcff972fc825b8c26", "url": "https://www.semanticscholar.org/paper/4fe855079443b097587fe42bcff972fc825b8c26", "title": "Machine Learning: ECML 2007, 18th European Conference on Machine Learning, Warsaw, Poland, September 17-21, 2007, Proceedings", "abstract": null, "year": 2007, "referenceCount": 0, "citationCount": 164, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1713683", "name": "J. Kok"}]}, {"paperId": "03cb4e2cb669d3f6344a733e622f07909f87ff0a", "url": "https://www.semanticscholar.org/paper/03cb4e2cb669d3f6344a733e622f07909f87ff0a", "title": "An introduction to genetic algorithms", "abstract": "From the Publisher: \n \n\"This is the best general book on Genetic Algorithms written to date. It covers background, history, and motivation; it selects important, informative examples of applications and discusses the use of Genetic Algorithms in scientific models; and it gives a good account of the status of the theory of Genetic Algorithms. Best of all the book presents its material in clear, straightforward, felicitous prose, accessible to anyone with a college-level scientific background. If you want a broad, solid understanding of Genetic Algorithms -- where they came from, what's being done with them, and where they are going -- this is the book. \n-- John H. Holland, Professor, Computer Science and Engineering, and Professor of Psychology, The University of Michigan; External Professor, the Santa Fe Institute. \nGenetic algorithms have been used in science and engineering as adaptive algorithms for solving practical problems and as computational models of natural evolutionary systems. This brief, accessible introduction describes some of the most interesting research in the field and also enables readers to implement and experiment with genetic algorithms on their own. It focuses in depth on a small set of important and interesting topics -- particularly in machine learning, scientific modeling, and artificial life -- and reviews a broad span of research, including the work of Mitchell and her colleagues. \nThe descriptions of applications and modeling projects stretch beyond the strict boundaries of computer science to include dynamical systems theory, game theory, molecular biology, ecology, evolutionary biology, and population genetics, underscoring the exciting \"general purpose\" nature of genetic algorithms as search methods that can be employed across disciplines. \nAn Introduction to Genetic Algorithms is accessible to students and researchers in any scientific discipline. It includes many thought and computer exercises that build on and reinforce the reader's understanding of the text. \nThe first chapter introduces genetic algorithms and their terminology and describes two provocative applications in detail. The second and third chapters look at the use of genetic algorithms in machine learning (computer programs, data analysis and prediction, neural networks) and in scientific models (interactions among learning, evolution, and culture; sexual selection; ecosystems; evolutionary activity). Several approaches to the theory of genetic algorithms are discussed in depth in the fourth chapter. The fifth chapter takes up implementation, and the last chapter poses some currently unanswered questions and surveys prospects for the future of evolutionary computation.", "year": 1996, "referenceCount": 193, "citationCount": 4906, "influentialCitationCount": 99, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144380037", "name": "Melanie Mitchell"}]}, {"paperId": "05e5a4c51b6df1ffa8d9b76f88ad0d2c92f4627a", "url": "https://www.semanticscholar.org/paper/05e5a4c51b6df1ffa8d9b76f88ad0d2c92f4627a", "title": "Extracting Hidden Context", "abstract": null, "year": 1998, "referenceCount": 39, "citationCount": 196, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "19455562", "name": "M. Harries"}, {"authorId": "1804253", "name": "C. Sammut"}, {"authorId": "34286654", "name": "K. Horn"}]}, {"paperId": "6ef10dfe5cc86df24a09df0a64853bcbb029c0f0", "url": "https://www.semanticscholar.org/paper/6ef10dfe5cc86df24a09df0a64853bcbb029c0f0", "title": "Machine learning for power system disturbance and cyber-attack discrimination", "abstract": "Power system disturbances are inherently complex and can be attributed to a wide range of sources, including both natural and man-made events. Currently, the power system operators are heavily relied on to make decisions regarding the causes of experienced disturbances and the appropriate course of action as a response. In the case of cyber-attacks against a power system, human judgment is less certain since there is an overt attempt to disguise the attack and deceive the operators as to the true state of the system. To enable the human decision maker, we explore the viability of machine learning as a means for discriminating types of power system disturbances, and focus specifically on detecting cyber-attacks where deception is a core tenet of the event. We evaluate various machine learning methods as disturbance discriminators and discuss the practical implications for deploying machine learning systems as an enhancement to existing power system architectures.", "year": 2014, "referenceCount": 45, "citationCount": 191, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "30974836", "name": "Raymond Borges Hink"}, {"authorId": "3091321", "name": "J. Beaver"}, {"authorId": "31839130", "name": "M. Buckner"}, {"authorId": "143819011", "name": "T. Morris"}, {"authorId": "3081252", "name": "U. Adhikari"}, {"authorId": "2660113", "name": "S. Pan"}]}, {"paperId": "857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb", "url": "https://www.semanticscholar.org/paper/857b1c3f171afb3cdf9df23d23e5d0cdfaa83efb", "title": "Learning perturbation sets for robust machine learning", "abstract": "Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.", "year": 2020, "referenceCount": 64, "citationCount": 59, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "51026953", "name": "Eric Wong"}, {"authorId": "145116464", "name": "J. Z. Kolter"}]}, {"paperId": "cfacdf62b0f3e50993a6a55250127d694a2efdce", "url": "https://www.semanticscholar.org/paper/cfacdf62b0f3e50993a6a55250127d694a2efdce", "title": "The city as a machine for learning.", "abstract": "Abstract Despite its centrality to urban politics, economies and life, learning remains a neglected and undertheorised domain in urban geography. In this paper, I address this by exploring a politics of learning through two key sites: first, tactical learning; second, urban learning forums. I offer a conception of learning based on three processes: translation , or the relational distributions through which learning is produced as a sociomaterial epistemology of displacement and change; coordination , or the construction of functional systems that enable learning as a means of linking different forms of knowledge, coping with complexity and facilitating adaptation; and dwelling , or the education of attention through which learning operates as a way of seeing and inhabiting the world. I then consider this conception of learning in relation to tactical learning, i.e. the resources marginal groups use to cope with, negotiate and resist in the city, and urban learning forums, i.e. the possibilities for progressive forms of learning between different constituencies in the city. I conclude with an outline of a critical urbanism of learning.", "year": 2011, "referenceCount": 76, "citationCount": 168, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "51937792", "name": "Colin McFarlane"}]}, {"paperId": "c6919dd8f8e72ff3e1fbc0072d64bde0a19c2a8e", "url": "https://www.semanticscholar.org/paper/c6919dd8f8e72ff3e1fbc0072d64bde0a19c2a8e", "title": "Genetics-Based Machine Learning for Rule Induction: State of the Art, Taxonomy, and Comparative Study", "abstract": "The classification problem can be addressed by numerous techniques and algorithms which belong to different paradigms of machine learning. In this paper, we are interested in evolutionary algorithms, the so-called genetics-based machine learning algorithms. In particular, we will focus on evolutionary approaches that evolve a set of rules, i.e., evolutionary rule-based systems, applied to classification tasks, in order to provide a state of the art in this field. This paper has a double aim: to present a taxonomy of the genetics-based machine learning approaches for rule induction, and to develop an empirical analysis both for standard classification and for classification with imbalanced data sets. We also include a comparative study of the genetics-based machine learning (GBML) methods with some classical non-evolutionary algorithms, in order to observe the suitability and high potential of the search performed by evolutionary algorithms and the behavior of the GBML algorithms in contrast to the classical approaches, in terms of classification accuracy.", "year": 2010, "referenceCount": 111, "citationCount": 147, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143741583", "name": "Alberto Fern\u00e1ndez"}, {"authorId": "39558258", "name": "S. Garc\u00eda"}, {"authorId": "145748883", "name": "J. Luengo"}, {"authorId": "1399458171", "name": "Ester Bernad\u00f3-Mansilla"}, {"authorId": "1699069", "name": "F. Herrera"}]}, {"paperId": "2f6a2926edcc3b4d620d4fdb46100ed4eeab1cc4", "url": "https://www.semanticscholar.org/paper/2f6a2926edcc3b4d620d4fdb46100ed4eeab1cc4", "title": "Machine Learning Techniques for Optical Performance Monitoring From Directly Detected PDM-QAM Signals", "abstract": "Linear signal processing algorithms are effective in dealing with linear transmission channel and linear signal detection, whereas the nonlinear signal processing algorithms, from the machine learning community, are effective in dealing with nonlinear transmission channel and nonlinear signal detection. In this paper, a brief overview of the various machine learning methods and their application in optical communication is presented and discussed. Moreover, supervised machine learning methods, such as neural networks and support vector machine, are experimentally demonstrated for in-band optical signal to noise ratio estimation and modulation format classification, respectively. The proposed methods accurately evaluate optical signals employing up to 64 quadrature amplitude modulation, at 32 Gbd, using only directly detected data.", "year": 2017, "referenceCount": 26, "citationCount": 127, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3613063", "name": "Jakob Thrane"}, {"authorId": "30888444", "name": "Jesper Wass"}, {"authorId": "3630976", "name": "M. Piels"}, {"authorId": "2537849", "name": "J. Diniz"}, {"authorId": "2109214146", "name": "R. Jones"}, {"authorId": "2735803", "name": "D. Zibar"}]}, {"paperId": "e481de52378f366d75fa78cff438d1f37842f0aa", "url": "https://www.semanticscholar.org/paper/e481de52378f366d75fa78cff438d1f37842f0aa", "title": "Survey of review spam detection using machine learning techniques", "abstract": null, "year": 2015, "referenceCount": 50, "citationCount": 309, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2066467014", "name": "M. Crawford"}, {"authorId": "1725285", "name": "T. Khoshgoftaar"}, {"authorId": "2920464", "name": "Joseph D. Prusa"}, {"authorId": "2099109", "name": "Aaron N. Richter"}, {"authorId": "2573982", "name": "Hamzah Al Najada"}]}, {"paperId": "3326d0b269d123290c68fbacec67c30a5d0a4da7", "url": "https://www.semanticscholar.org/paper/3326d0b269d123290c68fbacec67c30a5d0a4da7", "title": "Machine Learning for Spatial Environmental Data: Theory, Applications, and Software", "abstract": "PREFACE LEARNING FROM GEOSPATIAL DATA Problems and important concepts of machine learning Machine learning algorithms for geospatial data Contents of the book Software description Short review of the literature EXPLORATORY SPATIAL DATA ANALYSIS PRESENTATION OF DATA AND CASE STUDIES Exploratory spatial data analysis Data pre-processing Spatial correlations: Variography Presentation of data k-Nearest neighbours algorithm: a benchmark model for regression and classification Conclusions to chapter GEOSTATISTICS Spatial predictions Geostatistical conditional simulations Spatial classification Software Conclusions ARTIFICIAL NEURAL NETWORKS Introduction Radial basis function neural networks General regression neural networks Probabilistic neural networks Self-organising maps Gaussian mixture models and mixture density network Conclusions SUPPORT VECTOR MACHINES AND KERNEL METHODS Introduction to statistical learning theory Support vector classification Spatial data classification with SVM Support vector regression Advanced topics in kernel methods REFERENCES INDEX", "year": 2009, "referenceCount": 0, "citationCount": 173, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143940227", "name": "M. Kanevski"}, {"authorId": "1766763", "name": "A. Pozdnoukhov"}, {"authorId": "49073726", "name": "V. Timonin"}]}, {"paperId": "83b58081d8351b88c3bc42e3e2043727f707edf0", "url": "https://www.semanticscholar.org/paper/83b58081d8351b88c3bc42e3e2043727f707edf0", "title": "Ontology Learning from Text: Methods, Evaluation and Applications", "abstract": "This volume brings together ontology learning, knowledge acquisition and other related topics. It presents current research in ontology learning, addressing three perspectives. The first perspective looks at methodologies that have been proposed to automatically extract information from texts and to give a structured organization to such knowledge, including approaches based on machine learning techniques. Then there are evaluation methods for ontology learning, aiming at defining procedures and metrics for a quantitative evaluation of the ontology learning task; and finally application scenarios that make ontology learning a challenging area in the context of real applications such as bio-informatics. According to the three perspectives mentioned above, the book is divided into three sections, each including a selection of papers addressing respectively the methods, the applications and the evaluation of ontology learning approaches.", "year": 2005, "referenceCount": 5, "citationCount": 540, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1712352", "name": "B. Magnini"}, {"authorId": "3338131", "name": "P. Buitelaar"}, {"authorId": "1748977", "name": "P. Cimiano"}]}, {"paperId": "a2b5abb2e8ef4935c5e651fde5e3f3f007c5d9f9", "url": "https://www.semanticscholar.org/paper/a2b5abb2e8ef4935c5e651fde5e3f3f007c5d9f9", "title": "Evolutionary Computation: Towards a New Philosophy of Machine Intelligence", "abstract": "From the Publisher: \nIn this revised and significantly expanded second edition, distinguished scientist David B. Fogel presents the latest advances in both the theory and practice of evolutionary computation to help you keep pace with developments in this fast-changing field.. \"In-depth and updated, Evolutionary Computation shows you how to use simulated evolution to achieve machine intelligence. You will gain current insights into the history of evolutionary computation and the newest theories shaping research. Fogel carefully reviews the \"no free lunch theorem\" and discusses new theoretical findings that challenge some of the mathematical foundations of simulated evolution. This second edition also presents the latest game-playing techniques that combine evolutionary algorithms with neural networks, including their success in playing competitive checkers. Chapter by chapter, this comprehensive book highlights the relationship between learning and intelligence.. \"Evolutionary Computation features an unparalleled integration of history with state-of-the-art theory and practice for engineers, professors, and graduate students of evolutionary computation and computer science who need to keep up-to-date in this developing field.", "year": 1995, "referenceCount": 0, "citationCount": 2099, "influentialCitationCount": 73, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144330013", "name": "D. Fogel"}]}, {"paperId": "db8c04925ff115c9ee50112df7c0884cb3b6c484", "url": "https://www.semanticscholar.org/paper/db8c04925ff115c9ee50112df7c0884cb3b6c484", "title": "Machine learning attacks against the Asirra CAPTCHA", "abstract": "The Asirra CAPTCHA [7], proposed at ACM CCS 2007, relies on the problem of distinguishing images of cats and dogs (a task that humans are very good at). The security of Asirra is based on the presumed difficulty of classifying these images automatically. In this paper, we describe a classifier which is 82.7% accurate in telling apart the images of cats and dogs used in Asirra. This classifier is a combination of support-vector machine classifiers trained on color and texture features extracted from images. Our classifier allows us to solve a 12-image Asirra challenge automatically with probability 10.3%. This probability of success is significantly higher than the estimate of 0.2% given in [7] for machine vision attacks. Our results suggest caution against deploying Asirra without safeguards. We also investigate the impact of our attacks on the partial credit and token bucket algorithms proposed in [7]. The partial credit algorithm weakens Asirra considerably and we recommend against its use. The token bucket algorithm helps mitigate the impact of our attacks and allows Asirra to be deployed in a way that maintains an appealing balance between usability and security. One contribution of our work is to inform the choice of safeguard parameters in Asirra deployments.", "year": 2009, "referenceCount": 22, "citationCount": 231, "influentialCitationCount": 23, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2779068", "name": "P. Golle"}]}, {"paperId": "05ac8302865161061098f81337d0e52924949f24", "url": "https://www.semanticscholar.org/paper/05ac8302865161061098f81337d0e52924949f24", "title": "Machine Learning and Image Interpretation", "abstract": null, "year": 1997, "referenceCount": 8, "citationCount": 68, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1861262", "name": "T. Caelli"}, {"authorId": "1766762", "name": "W. Bischof"}]}, {"paperId": "d443ff1955e9d544063693ce29016f94b98e6ab3", "url": "https://www.semanticscholar.org/paper/d443ff1955e9d544063693ce29016f94b98e6ab3", "title": "Emotional state classification from EEG data using machine learning approach", "abstract": null, "year": 2014, "referenceCount": 78, "citationCount": 489, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2108202258", "name": "Xiao-Wei Wang"}, {"authorId": "2064196383", "name": "Dan Nie"}, {"authorId": "1715839", "name": "Bao-Liang Lu"}]}, {"paperId": "a6f835ca6e12245a835ab6074bc6ec2c3c60b85a", "url": "https://www.semanticscholar.org/paper/a6f835ca6e12245a835ab6074bc6ec2c3c60b85a", "title": "One Pixel Attack for Fooling Deep Neural Networks", "abstract": "Recent research has revealed that the output of deep neural networks (DNNs) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution (DE). It requires less adversarial information (a black-box attack) and can fool more types of networks due to the inherent features of DE. The results show that 67.97% of the natural images in Kaggle CIFAR-10 test dataset and 16.04% of the ImageNet (ILSVRC 2012) test images can be perturbed to at least one target class by modifying just one pixel with 74.03% and 22.91% confidence on average. We also show the same vulnerability on the original CIFAR-10 dataset. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks. Besides, we also illustrate an important application of DE (or broadly speaking, evolutionary computation) in the domain of adversarial machine learning: creating tools that can effectively generate low-cost adversarial attacks against neural networks for evaluating robustness.", "year": 2017, "referenceCount": 67, "citationCount": 1526, "influentialCitationCount": 78, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1730754", "name": "Jiawei Su"}, {"authorId": "145197293", "name": "Danilo Vasconcellos Vargas"}, {"authorId": "145106127", "name": "K. Sakurai"}]}, {"paperId": "9719e16aa0aedbffae3ec19e926ace54cfd1e4fd", "url": "https://www.semanticscholar.org/paper/9719e16aa0aedbffae3ec19e926ace54cfd1e4fd", "title": "Mechanism design via machine learning", "abstract": "We use techniques from sample-complexity in machine learning to reduce problems of incentive-compatible mechanism design to standard algorithmic questions, for a wide variety of revenue-maximizing pricing problems. Our reductions imply that for these problems, given an optimal (or /spl beta/-approximation) algorithm for the standard algorithmic problem, we can convert it into a (1 + /spl epsi/)-approximation (or /spl beta/(1 +/spl epsi/)-approximation) for the incentive-compatible mechanism design problem, so long as the number of bidders is sufficiently large as a function of an appropriate measure of complexity of the comparison class of solutions. We apply these results to the problem of auctioning a digital good, the attribute auction problem, and to the problem of item-pricing in unlimited-supply combinatorial auctions. From a learning perspective, these settings present several challenges: in particular the loss function is discontinuous and asymmetric, and the range of bidders' valuations may be large.", "year": 2005, "referenceCount": 35, "citationCount": 123, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1745410", "name": "Maria-Florina Balcan"}, {"authorId": "1690967", "name": "A. Blum"}, {"authorId": "2346641", "name": "Jason D. Hartline"}, {"authorId": "144830983", "name": "Y. Mansour"}]}, {"paperId": "2f058d60303b795975da917e44afc5f6336caf42", "url": "https://www.semanticscholar.org/paper/2f058d60303b795975da917e44afc5f6336caf42", "title": "Machine Learning and Formal Concept Analysis", "abstract": null, "year": 2004, "referenceCount": 44, "citationCount": 121, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1765540", "name": "S. Kuznetsov"}]}, {"paperId": "643fb5aeccce0351cac89bb8e446c8d7e48a3d99", "url": "https://www.semanticscholar.org/paper/643fb5aeccce0351cac89bb8e446c8d7e48a3d99", "title": "The limits and potentials of deep learning for robotics", "abstract": "The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics.", "year": 2018, "referenceCount": 141, "citationCount": 370, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering", "Mathematics"], "authors": [{"authorId": "1771913", "name": "Niko S\u00fcnderhauf"}, {"authorId": "1717724", "name": "O. Brock"}, {"authorId": "2613438", "name": "W. Scheirer"}, {"authorId": "2315504", "name": "R. Hadsell"}, {"authorId": "145197953", "name": "D. Fox"}, {"authorId": "40548044", "name": "J. Leitner"}, {"authorId": "1803115", "name": "B. Upcroft"}, {"authorId": "1689992", "name": "P. Abbeel"}, {"authorId": "1725973", "name": "W. Burgard"}, {"authorId": "1809144", "name": "Michael Milford"}, {"authorId": "1714296", "name": "Peter Corke"}]}, {"paperId": "66405753664af1895830f147365cc5903388dea1", "url": "https://www.semanticscholar.org/paper/66405753664af1895830f147365cc5903388dea1", "title": "A review of machine learning approaches to Spam filtering", "abstract": null, "year": 2009, "referenceCount": 121, "citationCount": 491, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3327532", "name": "Thiago S. Guzella"}, {"authorId": "2891406", "name": "W. Caminhas"}]}, {"paperId": "6772f7ffccc5d320ef6f067c62c8e63083072892", "url": "https://www.semanticscholar.org/paper/6772f7ffccc5d320ef6f067c62c8e63083072892", "title": "Financial time series forecasting with machine learning techniques: a survey", "abstract": "Stock index forecasting is vital for making informed investment decisions. This paper surveys recent literature in the domain of machine learning techniques and artificial intelligence used to forecast stock market movements. The publications are categorised according to the machine learning technique used, the forecasting timeframe, the input variables used, and the evaluation techniques employed. It is found that there is a consensus between researchers stressing the importance of stock index forecasting. Artificial Neural Networks (ANNs) are identified to be the dominant machine learning technique in this area. We conclude with possible future research directions.", "year": 2010, "referenceCount": 49, "citationCount": 146, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2479403", "name": "Bjoern Krollner"}, {"authorId": "1794293", "name": "B. Vanstone"}, {"authorId": "1768198", "name": "G. Finnie"}]}, {"paperId": "71064f8e69187208710a50b589caaf554a86aec0", "url": "https://www.semanticscholar.org/paper/71064f8e69187208710a50b589caaf554a86aec0", "title": "Dimension Reduction in Text Classification with Support Vector Machines", "abstract": "Support vector machines (SVMs) have been recognized as one of the most successful classification methods for many applications including text classification. Even though the learning ability and computational complexity of training in support vector machines may be independent of the dimension of the feature space, reducing computational complexity is an essential issue to efficiently handle a large number of terms in practical applications of text classification. In this paper, we adopt novel dimension reduction methods to reduce the dimension of the document vectors dramatically. We also introduce decision functions for the centroid-based classification algorithm and support vector classifiers to handle the classification problem where a document may belong to multiple classes. Our substantial experimental results show that with several dimension reduction methods that are designed particularly for clustered data, higher efficiency for both training and testing can be achieved without sacrificing prediction accuracy of text classification even when the dimension of the input space is significantly reduced.", "year": 2005, "referenceCount": 44, "citationCount": 280, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2118020225", "name": "Hyunsoo Kim"}, {"authorId": "2085719", "name": "P. Howland"}, {"authorId": "1685928", "name": "Haesun Park"}]}, {"paperId": "4135da4cc0ef70917e45ae4436e7a6411077325c", "url": "https://www.semanticscholar.org/paper/4135da4cc0ef70917e45ae4436e7a6411077325c", "title": "A Review of Machine Learning and Deep Learning Applications", "abstract": "Machine learning is one of the fields in the modern computing world. A plenty of research has been undertaken to make machines intelligent. Learning is a natural human behavior which has been made an essential aspect of the machines as well. There are various techniques devised for the same. Traditional machine learning algorithms have been applied in many application areas. Researchers have put many efforts to improve the accuracy of that machinelearning algorithms. Another dimension was given thought which leads to deep learning concept. Deep learning is a subset of machine learning. So far few applications of deep learning have been explored. This is definitely going to cater to solving issues in several new application domains, sub-domains using deep learning. A review of these past and future application domains, sub-domains, and applications of machine learning and deep learning are illustrated in this paper.", "year": 2018, "referenceCount": 15, "citationCount": 83, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "108021414", "name": "Pramila Shinde"}, {"authorId": "47973421", "name": "Seema Shah"}]}, {"paperId": "cf958f6b406035f1d24859c2be49c657fb4156bf", "url": "https://www.semanticscholar.org/paper/cf958f6b406035f1d24859c2be49c657fb4156bf", "title": "Machine Learning in the Air", "abstract": "Thanks to the recent advances in processing speed, data acquisition and storage, machine learning (ML) is penetrating every facet of our lives, and transforming research in many areas in a fundamental manner. Wireless communications is another success story \u2013 ubiquitous in our lives, from handheld devices to wearables, smart homes, and automobiles. While recent years have seen a flurry of research activity in exploiting ML tools for various wireless communication problems, the impact of these techniques in practical communication systems and standards is yet to be seen. In this paper, we review some of the major promises and challenges of ML in wireless communication systems, focusing mainly on the physical layer. We present some of the most striking recent accomplishments that ML techniques have achieved with respect to classical approaches, and point to promising research directions where ML is likely to make the biggest impact in the near future. We also highlight the complementary problem of designing physical layer techniques to enable distributed ML at the wireless network edge, which further emphasizes the need to understand and connect ML with fundamental concepts in wireless communications.", "year": 2019, "referenceCount": 104, "citationCount": 131, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1727814", "name": "Deniz G\u00fcnd\u00fcz"}, {"authorId": "1904955", "name": "Paul de Kerret"}, {"authorId": "73776482", "name": "N. Sidiropoulos"}, {"authorId": "1710234", "name": "D. Gesbert"}, {"authorId": "145920767", "name": "C. Murthy"}, {"authorId": "1729969", "name": "M. Schaar"}]}, {"paperId": "a267c8dcc631728e947aaac86d9baf4f73f9a975", "url": "https://www.semanticscholar.org/paper/a267c8dcc631728e947aaac86d9baf4f73f9a975", "title": "Use of machine learning techniques for educational proposes: a decision support system for forecasting students\u2019 grades", "abstract": null, "year": 2012, "referenceCount": 60, "citationCount": 173, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697867", "name": "S. Kotsiantis"}]}, {"paperId": "539c39bd2c583dba0f9aee514b1c17318f9f0233", "url": "https://www.semanticscholar.org/paper/539c39bd2c583dba0f9aee514b1c17318f9f0233", "title": "Learning Theory and Kernel Machines", "abstract": null, "year": 2003, "referenceCount": 41, "citationCount": 141, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}]}, {"paperId": "c43025c429b1fbf6f1379f61801a1b40834d62e7", "url": "https://www.semanticscholar.org/paper/c43025c429b1fbf6f1379f61801a1b40834d62e7", "title": "Convolutional networks and applications in vision", "abstract": "Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or \"features\")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.", "year": 2010, "referenceCount": 54, "citationCount": 1699, "influentialCitationCount": 120, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1688882", "name": "Yann LeCun"}, {"authorId": "2645384", "name": "K. Kavukcuoglu"}, {"authorId": "2256269", "name": "C. Farabet"}]}, {"paperId": "dc418bccc06fdf3873b0241c97af4219dfca794a", "url": "https://www.semanticscholar.org/paper/dc418bccc06fdf3873b0241c97af4219dfca794a", "title": "Machine Learning in a Quantum World", "abstract": null, "year": 2006, "referenceCount": 25, "citationCount": 132, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1765537", "name": "Esma A\u00efmeur"}, {"authorId": "1771605", "name": "G. Brassard"}, {"authorId": "1777382", "name": "S. Gambs"}]}, {"paperId": "dc601a35addd196fd51f2999be486bf1fa647951", "url": "https://www.semanticscholar.org/paper/dc601a35addd196fd51f2999be486bf1fa647951", "title": "On the Class Imbalance Problem", "abstract": "The class imbalance problem has been recognized in many practical domains and a hot topic of machine learning in recent years. In such a problem, almost all the examples are labeled as one class, while far fewer examples are labeled as the other class, usually the more important class. In this case, standard machine learning algorithms tend to be overwhelmed by the majority class and ignore the minority class since traditional classifiers seeking an accurate performance over a full range of instances. This paper reviewed academic activities special for the class imbalance problem firstly. Then investigated various remedies in four different levels according to learning phases. Following surveying evaluation metrics and some other related factors, this paper showed some future directions at last.", "year": 2008, "referenceCount": 57, "citationCount": 397, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3193005", "name": "X. Guo"}, {"authorId": "102446355", "name": "Yilong Yin"}, {"authorId": "50819223", "name": "Cailing Dong"}, {"authorId": "1728122", "name": "Gongping Yang"}, {"authorId": "2057809", "name": "Guang-Tong Zhou"}]}, {"paperId": "0fe97c88452d8d8603d9ba883a0721da46ba84f4", "url": "https://www.semanticscholar.org/paper/0fe97c88452d8d8603d9ba883a0721da46ba84f4", "title": "The Set Covering Machine", "abstract": "We extend the classical algorithms of Valiant and Haussler for learning compact conjunctions and disjunctions of Boolean attributes to allow features that are constructed from the data and to allow a trade-off between accuracy and complexity. The result is a general-purpose learning machine, suitable for practical learning tasks, that we call the set covering machine. We present a version of the set covering machine that uses data-dependent balls for its set of features and compare its performance with the support vector machine. By extending a technique pioneered by Littlestone and Warmuth, we bound its generalization error as a function of the amount of data compression it achieves during training. In experiments with real-world learning tasks, the bound is shown to be extremely tight and to provide an effective guide for model selection.", "year": 2003, "referenceCount": 21, "citationCount": 141, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143858557", "name": "M. Marchand"}, {"authorId": "1404459229", "name": "J. Shawe-Taylor"}]}, {"paperId": "d76228d085d1232b74ff443f249898b6dcc3306c", "url": "https://www.semanticscholar.org/paper/d76228d085d1232b74ff443f249898b6dcc3306c", "title": "TFX: A TensorFlow-Based Production-Scale Machine Learning Platform", "abstract": "Creating and maintaining a platform for reliably producing and deploying machine learning models requires careful orchestration of many components---a learner for generating models based on training data, modules for analyzing and validating both data as well as models, and finally infrastructure for serving models in production. This becomes particularly challenging when data changes over time and fresh models need to be produced continuously. Unfortunately, such orchestration is often done ad hoc using glue code and custom scripts developed by individual teams for specific use cases, leading to duplicated effort and fragile systems with high technical debt. We present TensorFlow Extended (TFX), a TensorFlow-based general-purpose machine learning platform implemented at Google. By integrating the aforementioned components into one platform, we were able to standardize the components, simplify the platform configuration, and reduce the time to production from the order of months to weeks, while providing platform stability that minimizes disruptions. We present the case study of one deployment of TFX in the Google Play app store, where the machine learning models are refreshed continuously as new data arrive. Deploying TFX led to reduced custom code, faster experiment cycles, and a 2% increase in app installs resulting from improved data and model analysis.", "year": 2017, "referenceCount": 24, "citationCount": 315, "influentialCitationCount": 41, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2064178911", "name": "Denis Baylor"}, {"authorId": "1798895", "name": "Eric Breck"}, {"authorId": "2061550", "name": "Heng-Tze Cheng"}, {"authorId": "22640071", "name": "Noah Fiedel"}, {"authorId": "22420870", "name": "Chuan Yu Foo"}, {"authorId": "50730596", "name": "Zakaria Haque"}, {"authorId": "40269586", "name": "Salem Haykal"}, {"authorId": "37413761", "name": "M. Ispir"}, {"authorId": "20048351", "name": "Vihan Jain"}, {"authorId": "40338695", "name": "L. Koc"}, {"authorId": "1766347", "name": "C. Koo"}, {"authorId": "40015409", "name": "Lukasz Lew"}, {"authorId": "22684493", "name": "Clemens Mewald"}, {"authorId": "2063982692", "name": "A. Modi"}, {"authorId": "1763100", "name": "Neoklis Polyzotis"}, {"authorId": "18654369", "name": "Sukriti Ramesh"}, {"authorId": "1473253195", "name": "Sudip Roy"}, {"authorId": "3288247", "name": "Steven Euijong Whang"}, {"authorId": "35078078", "name": "M. Wicke"}, {"authorId": "2224123", "name": "Jarek Wilkiewicz"}, {"authorId": "2150904662", "name": "Xin Zhang"}, {"authorId": "8195063", "name": "Martin A. Zinkevich"}]}, {"paperId": "fc4a8062d5b8dfd34e61f46ebedade56e01ef001", "url": "https://www.semanticscholar.org/paper/fc4a8062d5b8dfd34e61f46ebedade56e01ef001", "title": "Machine learning aided Android malware classification", "abstract": null, "year": 2017, "referenceCount": 35, "citationCount": 239, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143860899", "name": "Nikola Milosevic"}, {"authorId": "2011284", "name": "A. Dehghantanha"}, {"authorId": "116022805", "name": "K. Choo"}]}, {"paperId": "8d35230fec724398bed3f5939e9fa6a94f55a785", "url": "https://www.semanticscholar.org/paper/8d35230fec724398bed3f5939e9fa6a94f55a785", "title": "Differential Privacy and Machine Learning: a Survey and Review", "abstract": "The objective of machine learning is to extract useful information from data, while privacy is preserved by concealing information. Thus it seems hard to reconcile these competing interests. However, they frequently must be balanced when mining sensitive data. For example, medical research represents an important application where it is necessary both to extract useful information and protect patient privacy. One way to resolve the conflict is to extract general characteristics of whole populations without disclosing the private information of individuals. \nIn this paper, we consider differential privacy, one of the most popular and powerful definitions of privacy. We explore the interplay between machine learning and differential privacy, namely privacy-preserving machine learning algorithms and learning-based data release mechanisms. We also describe some theoretical results that address what can be learned differentially privately and upper bounds of loss functions for differentially private algorithms. \nFinally, we present some open questions, including how to incorporate public data, how to deal with missing data in private datasets, and whether, as the number of observed samples grows arbitrarily large, differentially private machine learning algorithms can be achieved at no cost to utility as compared to corresponding non-differentially private algorithms.", "year": 2014, "referenceCount": 65, "citationCount": 186, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40387717", "name": "Zhanglong Ji"}, {"authorId": "32219137", "name": "Zachary Chase Lipton"}, {"authorId": "1722831", "name": "C. Elkan"}]}, {"paperId": "dbc7401e3e75c40d3c720e7db3c906d48bd742d7", "url": "https://www.semanticscholar.org/paper/dbc7401e3e75c40d3c720e7db3c906d48bd742d7", "title": "Deep Autoencoding Gaussian Mixture Model for Unsupervised Anomaly Detection", "abstract": "Unsupervised anomaly detection on multior high-dimensional data is of great importance in both fundamental machine learning research and industrial applications, for which density estimation lies at the core. Although previous approaches based on dimensionality reduction followed by density estimation have made fruitful progress, they mainly suffer from decoupled model learning with inconsistent optimization goals and incapability of preserving essential information in the low-dimensional space. In this paper, we present a Deep Autoencoding Gaussian Mixture Model (DAGMM) for unsupervised anomaly detection. Our model utilizes a deep autoencoder to generate a low-dimensional representation and reconstruction error for each input data point, which is further fed into a Gaussian Mixture Model (GMM). Instead of using decoupled two-stage training and the standard Expectation-Maximization (EM) algorithm, DAGMM jointly optimizes the parameters of the deep autoencoder and the mixture model simultaneously in an end-to-end fashion, leveraging a separate estimation network to facilitate the parameter learning of the mixture model. The joint optimization, which well balances autoencoding reconstruction, density estimation of latent representation, and regularization, helps the autoencoder escape from less attractive local optima and further reduce reconstruction errors, avoiding the need of pre-training. Experimental results on several public benchmark datasets show that, DAGMM significantly outperforms state-of-the-art anomaly detection techniques, and achieves up to 14% improvement based on the standard F1 score.", "year": 2018, "referenceCount": 34, "citationCount": 787, "influentialCitationCount": 148, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2991105", "name": "Bo Zong"}, {"authorId": "2091568319", "name": "Qi Song"}, {"authorId": "2984407", "name": "Martin Renqiang Min"}, {"authorId": "92186360", "name": "Wei Cheng"}, {"authorId": "2470254", "name": "C. Lumezanu"}, {"authorId": "1797221", "name": "Dae-ki Cho"}, {"authorId": "2145225543", "name": "Haifeng Chen"}]}, {"paperId": "693914b7f38c19585e35668fd626aecf62d4c5e7", "url": "https://www.semanticscholar.org/paper/693914b7f38c19585e35668fd626aecf62d4c5e7", "title": "Text Mining Infrastructure in R", "abstract": "During the last decade text mining has become a widely used discipline utilizing statistical and machine learning methods. We present the tm package which provides a framework for text mining applications within R. We give a survey on text mining facilities in R and explain how typical application tasks can be carried out using our framework. We present techniques for count-based analysis methods, text clustering, text classification and string kernels.", "year": 2008, "referenceCount": 89, "citationCount": 1049, "influentialCitationCount": 33, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2711164", "name": "Ingo Feinerer"}, {"authorId": "1764952", "name": "K. Hornik"}, {"authorId": "153939986", "name": "David Meyer"}]}, {"paperId": "d4ea93fe6cb61d6ec55b39980d0e23c8ce24c53c", "url": "https://www.semanticscholar.org/paper/d4ea93fe6cb61d6ec55b39980d0e23c8ce24c53c", "title": "SystemML: Declarative Machine Learning on Spark", "abstract": "The rising need for custom machine learning (ML) algorithms and the growing data sizes that require the exploitation of distributed, data-parallel frameworks such as MapReduce or Spark, pose significant productivity challenges to data scientists. Apache SystemML addresses these challenges through declarative ML by (1) increasing the productivity of data scientists as they are able to express custom algorithms in a familiar domain-specific language covering linear algebra primitives and statistical functions, and (2) transparently running these ML algorithms on distributed, data-parallel frameworks by applying cost-based compilation techniques to generate efficient, low-level execution plans with in-memory single-node and large-scale distributed operations. This paper describes SystemML on Apache Spark, end to end, including insights into various optimizer and runtime techniques as well as performance characteristics. We also share lessons learned from porting SystemML to Spark and declarative ML in general. Finally, SystemML is open-source, which allows the database community to leverage it as a testbed for further research.", "year": 2016, "referenceCount": 48, "citationCount": 183, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40237241", "name": "Matthias Boehm"}, {"authorId": "144477225", "name": "Michael W. Dusenberry"}, {"authorId": "3455068", "name": "D. Eriksson"}, {"authorId": "3351526", "name": "A. Evfimievski"}, {"authorId": "2010184", "name": "F. Manshadi"}, {"authorId": "2451612", "name": "Niketan Pansare"}, {"authorId": "1698945", "name": "B. Reinwald"}, {"authorId": "47265115", "name": "Frederick Reiss"}, {"authorId": "40655309", "name": "P. Sen"}, {"authorId": "50729888", "name": "Arvind Surve"}, {"authorId": "1947100", "name": "S. Tatikonda"}]}, {"paperId": "5512b6eb20f2388b6b08fa9bb2fd0302f8ec21b7", "url": "https://www.semanticscholar.org/paper/5512b6eb20f2388b6b08fa9bb2fd0302f8ec21b7", "title": "Application of machine learning to proteomics data: classification and biomarker identification in postgenomics biology.", "abstract": "Mass spectrometry is an analytical technique for the characterization of biological samples and is increasingly used in omics studies because of its targeted, nontargeted, and high throughput abilities. However, due to the large datasets generated, it requires informatics approaches such as machine learning techniques to analyze and interpret relevant data. Machine learning can be applied to MS-derived proteomics data in two ways. First, directly to mass spectral peaks and second, to proteins identified by sequence database searching, although relative protein quantification is required for the latter. Machine learning has been applied to mass spectrometry data from different biological disciplines, particularly for various cancers. The aims of such investigations have been to identify biomarkers and to aid in diagnosis, prognosis, and treatment of specific diseases. This review describes how machine learning has been applied to proteomics tandem mass spectrometry data. This includes how it can be used to identify proteins suitable for use as biomarkers of disease and for classification of samples into disease or treatment groups, which may be applicable for diagnostics. It also includes the challenges faced by such investigations, such as prediction of proteins present, protein quantification, planning for the use of machine learning, and small sample sizes.", "year": 2013, "referenceCount": 108, "citationCount": 164, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Biology", "Medicine"], "authors": [{"authorId": "33070697", "name": "A. Swan"}, {"authorId": "47159255", "name": "A. Mobasheri"}, {"authorId": "6825262", "name": "D. Allaway"}, {"authorId": "143945092", "name": "S. Liddell"}, {"authorId": "1697337", "name": "J. Bacardit"}]}, {"paperId": "3558dc0a0139d6a3dd48458d207c497db027ab1b", "url": "https://www.semanticscholar.org/paper/3558dc0a0139d6a3dd48458d207c497db027ab1b", "title": "Thirty Years of Machine Learning: The Road to Pareto-Optimal Wireless Networks", "abstract": "Future wireless networks have a substantial potential in terms of supporting a broad range of complex compelling applications both in military and civilian fields, where the users are able to enjoy high-rate, low-latency, low-cost and reliable information services. Achieving this ambitious goal requires new radio techniques for adaptive learning and intelligent decision making because of the complex heterogeneous nature of the network structures and wireless services. Machine learning (ML) algorithms have great success in supporting big data analytics, efficient parameter estimation and interactive decision making. Hence, in this article, we review the thirty-year history of ML by elaborating on supervised learning, unsupervised learning, reinforcement learning and deep learning. Furthermore, we investigate their employment in the compelling applications of wireless networks, including heterogeneous networks (HetNets), cognitive radios (CR), Internet of Things (IoT), machine to machine networks (M2M), and so on. This article aims for assisting the readers in clarifying the motivation and methodology of the various ML algorithms, so as to invoke them for hitherto unexplored services as well as scenarios of future wireless networks.", "year": 2019, "referenceCount": 380, "citationCount": 230, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145210497", "name": "JingJing Wang"}, {"authorId": "1750017", "name": "Chunxiao Jiang"}, {"authorId": "50024622", "name": "Haijun Zhang"}, {"authorId": "145659296", "name": "Yong Ren"}, {"authorId": "66073306", "name": "Kwang-Cheng Chen"}, {"authorId": "1730180", "name": "L. Hanzo"}]}, {"paperId": "848e3ed9b9b14f648b63ea66cbf2a5663bd22fdd", "url": "https://www.semanticscholar.org/paper/848e3ed9b9b14f648b63ea66cbf2a5663bd22fdd", "title": "The Ladder: A Reliable Leaderboard for Machine Learning Competitions", "abstract": "The organizer of a machine learning competition faces the problem of maintaining an accurate leaderboard that faithfully represents the quality of the best submission of each competing team. What makes this estimation problem particularly challenging is its sequential and adaptive nature. As participants are allowed to repeatedly evaluate their submissions on the leaderboard, they may begin to overfit to the holdout data that supports the leaderboard. Few theoretical results give actionable advice on how to design a reliable leaderboard. Existing approaches therefore often resort to poorly understood heuristics such as limiting the bit precision of answers and the rate of resubmission. \n \nIn this work, we introduce a notion of leaderboard accuracy tailored to the format of a competition. We introduce a natural algorithm called the Ladder and demonstrate that it simultaneously supports strong theoretical guarantees in a fully adaptive model of estimation, withstands practical adversarial attacks, and achieves high utility on real submission files from an actual competition hosted by Kaggle. \n \nNotably, we are able to sidestep a powerful recent hardness result for adaptive risk estimation that rules out algorithms such as ours under a seemingly very similar notion of accuracy. On a practical note, we provide a completely parameter-free variant of our algorithm that can be deployed in a real competition with no tuning required whatsoever.", "year": 2015, "referenceCount": 15, "citationCount": 116, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1690967", "name": "A. Blum"}, {"authorId": "1775622", "name": "Moritz Hardt"}]}, {"paperId": "28d82be56e54df4b28a17908de17480d699e1409", "url": "https://www.semanticscholar.org/paper/28d82be56e54df4b28a17908de17480d699e1409", "title": "A Machine Learning Approach to Android Malware Detection", "abstract": "With the recent emergence of mobile platforms capable of executing increasingly complex software and the rising ubiquity of using mobile platforms in sensitive applications such as banking, there is a rising danger associated with malware targeted at mobile devices. The problem of detecting such malware presents unique challenges due to the limited resources avalible and limited privileges granted to the user, but also presents unique opportunity in the required metadata attached to each application. In this article, we present a machine learning-based system for the detection of malware on Android devices. Our system extracts a number of features and trains a One-Class Support Vector Machine in an offline (off-device) manner, in order to leverage the higher computing power of a server or cluster of servers.", "year": 2012, "referenceCount": 44, "citationCount": 321, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2200163", "name": "Justin Sahs"}, {"authorId": "145155297", "name": "L. Khan"}]}, {"paperId": "30b2a3422332a76663110beae4bfc4d74763f4a0", "url": "https://www.semanticscholar.org/paper/30b2a3422332a76663110beae4bfc4d74763f4a0", "title": "A kernel method for multi-labelled classification", "abstract": "This article presents a Support Vector Machine (SVM) like learning system to handle multi-label problems. Such problems are usually decomposed into many two-class problems but the expressive power of such a system can be weak [5, 7]. We explore a new direct approach. It is based on a large margin ranking system that shares a lot of common properties with SVMs. We tested it on a Yeast gene functional classification problem with positive results.", "year": 2001, "referenceCount": 9, "citationCount": 1296, "influentialCitationCount": 163, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1766703", "name": "A. Elisseeff"}, {"authorId": "145183709", "name": "J. Weston"}]}, {"paperId": "51891710e30da33c4ced4ae7daee1593e0cb5cc4", "url": "https://www.semanticscholar.org/paper/51891710e30da33c4ced4ae7daee1593e0cb5cc4", "title": "Machine Learning: The High Interest Credit Card of Technical Debt", "abstract": "Machine learning offers a fantastically powerful toolkit for building complex systems quickly. This paper argues that it is dangerous to think of these quick wins as coming for free. Using the framework of technical debt, we note that it is remarkably easy to incur massive ongoing maintenance costs at the system level when applying machine learning. The goal of this paper is highlight several machine learning specific risk factors and design patterns to be avoided or refactored where possible. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns. 1 Machine Learning and Complex Systems Real world software engineers are often faced with the challenge of moving quickly to ship new products or services, which can lead to a dilemma between speed of execution and quality of engineering. The concept of technical debt was first introduced by Ward Cunningham in 1992 as a way to help quantify the cost of such decisions. Like incurring fiscal debt, there are often sound strategic reasons to take on technical debt. Not all debt is necessarily bad, but technical debt does tend to compound. Deferring the work to pay it off results in increasing costs, system brittleness, and reduced rates of innovation. Traditional methods of paying off technical debt include refactoring, increasing coverage of unit tests, deleting dead code, reducing dependencies, tightening APIs, and improving documentation [4]. The goal of these activities is not to add new functionality, but to make it easier to add future improvements, be cheaper to maintain, and reduce the likelihood of bugs. One of the basic arguments in this paper is that machine learning packages have all the basic code complexity issues as normal code, but also have a larger system-level complexity that can create hidden debt. Thus, refactoring these libraries, adding better unit tests, and associated activity is time well spent but does not necessarily address debt at a systems level. In this paper, we focus on the system-level interaction between machine learning code and larger systems as an area where hidden technical debt may rapidly accumulate. At a system-level, a machine learning model may subtly erode abstraction boundaries. It may be tempting to re-use input signals in ways that create unintended tight coupling of otherwise disjoint systems. Machine learning packages may often be treated as black boxes, resulting in large masses of \u201cglue code\u201d or calibration layers that can lock in assumptions. Changes in the external world may make models or input signals change behavior in unintended ways, ratcheting up maintenance cost and the burden of any debt. Even monitoring that the system as a whole is operating as intended may be difficult without careful design.", "year": 2014, "referenceCount": 10, "citationCount": 260, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Business"], "authors": [{"authorId": "1733143", "name": "D. Sculley"}, {"authorId": "144510728", "name": "Gary Holt"}, {"authorId": "145973657", "name": "D. Golovin"}, {"authorId": "143698521", "name": "Eugene Davydov"}, {"authorId": "2054375101", "name": "Todd Phillips"}, {"authorId": "49236095", "name": "D. Ebner"}, {"authorId": "2055477158", "name": "Vinay Chaudhary"}, {"authorId": "2114084357", "name": "Michael Young"}]}, {"paperId": "319a3a51457b5fcaa179576de11305a045177eaa", "url": "https://www.semanticscholar.org/paper/319a3a51457b5fcaa179576de11305a045177eaa", "title": "Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing", "abstract": null, "year": 1996, "referenceCount": 95, "citationCount": 162, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1736513", "name": "S. Wermter"}, {"authorId": "1691993", "name": "E. Riloff"}, {"authorId": "2576323", "name": "G. Scheler"}]}, {"paperId": "5396e9858d70f09bf42f251621b5f2816a9cb660", "url": "https://www.semanticscholar.org/paper/5396e9858d70f09bf42f251621b5f2816a9cb660", "title": "Cover trees for nearest neighbor", "abstract": "We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer & Lee, 2004b). If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality, as defined in (Karger & Ruhl, 2002), the cover tree data structure can be constructed in O (c6n log n) time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O (c12 log n) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.", "year": 2006, "referenceCount": 29, "citationCount": 845, "influentialCitationCount": 94, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2624289", "name": "A. Beygelzimer"}, {"authorId": "144695232", "name": "S. Kakade"}, {"authorId": "144162125", "name": "J. Langford"}]}, {"paperId": "365aea921d0290eb260d7cda7cc33b286cdca495", "url": "https://www.semanticscholar.org/paper/365aea921d0290eb260d7cda7cc33b286cdca495", "title": "Machine Learning for Sociology", "abstract": "Machine learning is a field at the intersection of statistics and computer science that uses algorithms to extract information and knowledge from data. Its applications increasingly find their way into economics, political science, and sociology. We offer a brief introduction to this vast toolbox and illustrate its current uses in the social sciences, including distilling measures from new data sources, such as text and images; characterizing population heterogeneity; improving causal inference; and offering predictions to aid policy decisions and theory development. We argue that, in addition to serving similar purposes in sociology, machine learning tools can speak to long-standing questions on the limitations of the linear modeling framework, the criteria for evaluating empirical findings, transparency around the context of discovery, and the epistemological core of the discipline.", "year": 2019, "referenceCount": 0, "citationCount": 92, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "143749392", "name": "M. Molina"}, {"authorId": "4347244", "name": "F. Garip"}]}, {"paperId": "e939b13aa7440810d420267ee7575276843e1a74", "url": "https://www.semanticscholar.org/paper/e939b13aa7440810d420267ee7575276843e1a74", "title": "Interactive and interpretable machine learning models for human machine collaboration", "abstract": "I envision a system that enables successful collaborations between humans and machine learning models by harnessing the relative strength to accomplish what neither can do alone. Machine learning techniques and humans have skills that complement each other \u2014 machine learning techniques are good at computation on data at the lowest level of granularity, whereas people are better at abstracting knowledge from their experience, and transferring the knowledge across domains. The goal of this thesis is to develop a framework for human-in-the-loop machine learning that enables people to interact effectively with machine learning models to make better decisions, without requiring in-depth knowledge about machine learning techniques. Many of us interact with machine learning systems everyday. Systems that mine data for product recommendations, for example, are ubiquitous. However these systems compute their output without end-user involvement, and there are typically no life or death consequences in the case the machine learning result is not acceptable to the user. In contrast, domains where decisions can have serious consequences (e.g., emergency response panning, medical decision-making), require the incorporation of human experts\u2019 domain knowledge. These systems also must be transparent to earn experts\u2019 trust and be adopted in their workflow. The challenge addressed in this thesis is that traditional machine learning systems are not designed to extract domain experts\u2019 knowledge from natural workflow, or to provide pathways for the human domain expert to directly interact with the algorithm to interject their knowledge or to better understand the system output. For machine learning systems to make a real-world impact in these important domains, these systems must be able to communicate with highly skilled human experts to leverage their judgment and expertise, and share useful information or patterns from the data. In this thesis, I bridge this gap by building human-in-the-loop machine learning models and systems that compute and communicate machine learning results in ways that are compatible with the human decision-making process, and that can readily incorporate human experts\u2019 domain knowledge. I start by building a machine learning model that infers human teams\u2019 planning decisions from the structured form of natural language of team meetings. I show that the model can infer a human teams\u2019 final plan with 86% accuracy", "year": 2015, "referenceCount": 135, "citationCount": 83, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "3351164", "name": "Been Kim"}]}, {"paperId": "abf5478c24664a1380b7e213a3ab1c4af54775d0", "url": "https://www.semanticscholar.org/paper/abf5478c24664a1380b7e213a3ab1c4af54775d0", "title": "Rapid Learning or Feature Reuse? Towards Understanding the Effectiveness of MAML", "abstract": "An important research direction in machine learning has centered around developing meta-learning algorithms to tackle few-shot learning. An especially successful algorithm has been Model Agnostic Meta-Learning (MAML), a method that consists of two optimization loops, with the outer loop finding a meta-initialization, from which the inner loop can efficiently learn new tasks. Despite MAML's popularity, a fundamental open question remains -- is the effectiveness of MAML due to the meta-initialization being primed for rapid learning (large, efficient changes in the representations) or due to feature reuse, with the meta initialization already containing high quality features? We investigate this question, via ablation studies and analysis of the latent representations, finding that feature reuse is the dominant factor. This leads to the ANIL (Almost No Inner Loop) algorithm, a simplification of MAML where we remove the inner loop for all but the (task-specific) head of a MAML-trained network. ANIL matches MAML's performance on benchmark few-shot image classification and RL and offers computational improvements over MAML. We further study the precise contributions of the head and body of the network, showing that performance on the test tasks is entirely determined by the quality of the learned features, and we can remove even the head of the network (the NIL algorithm). We conclude with a discussion of the rapid learning vs feature reuse question for meta-learning algorithms more broadly.", "year": 2019, "referenceCount": 42, "citationCount": 357, "influentialCitationCount": 85, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "31411877", "name": "Aniruddh Raghu"}, {"authorId": "40297238", "name": "M. Raghu"}, {"authorId": "1751569", "name": "Samy Bengio"}, {"authorId": "1689108", "name": "Oriol Vinyals"}]}, {"paperId": "e1b41796b752d6fb6032bbad2f8998302209d79b", "url": "https://www.semanticscholar.org/paper/e1b41796b752d6fb6032bbad2f8998302209d79b", "title": "A survey on ensemble learning", "abstract": null, "year": 2020, "referenceCount": 238, "citationCount": 327, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "152147830", "name": "Xibin Dong"}, {"authorId": "144861834", "name": "Zhiwen Yu"}, {"authorId": "47415977", "name": "Wenming Cao"}, {"authorId": "39596382", "name": "Yifan Shi"}, {"authorId": "144598359", "name": "Qianli Ma"}]}, {"paperId": "a67f1dcf735a1c52708a9a5392ff585658dbc8ce", "url": "https://www.semanticscholar.org/paper/a67f1dcf735a1c52708a9a5392ff585658dbc8ce", "title": "Bundle Methods for Machine Learning", "abstract": "We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/\u220a) steps to \u220a precision for general convex problems and in O(log(1/\u220a)) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach.", "year": 2007, "referenceCount": 17, "citationCount": 146, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "46234526", "name": "Alex Smola"}, {"authorId": "145713876", "name": "S. Vishwanathan"}, {"authorId": "2827616", "name": "Quoc V. Le"}]}, {"paperId": "4ba593a27a3e10cb44ae80726a55274f1e3c8b87", "url": "https://www.semanticscholar.org/paper/4ba593a27a3e10cb44ae80726a55274f1e3c8b87", "title": "Explanation-Based Learning: An Alternative View", "abstract": null, "year": 1986, "referenceCount": 41, "citationCount": 122, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1802807", "name": "G. DeJong"}, {"authorId": "1797655", "name": "R. Mooney"}]}, {"paperId": "2ab5330706c6b12942a212b6d72ed1221b578729", "url": "https://www.semanticscholar.org/paper/2ab5330706c6b12942a212b6d72ed1221b578729", "title": "Laplacian Support Vector Machines Trained in the Primal", "abstract": "In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classification. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation. In particular, training a LapSVM in the primal can be efficiently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from O(n3) to O(kn2), where n is the combined number of labeled and unlabeled examples and k is empirically evaluated to be significantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach.", "year": 2009, "referenceCount": 36, "citationCount": 355, "influentialCitationCount": 50, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1760309", "name": "S. Melacci"}, {"authorId": "145520115", "name": "Mikhail Belkin"}]}, {"paperId": "943c61449223aee07abcd6547f480a729c5f2699", "url": "https://www.semanticscholar.org/paper/943c61449223aee07abcd6547f480a729c5f2699", "title": "Closed-loop separation control using machine learning", "abstract": "We present the first closed-loop separation control experiment using a novel, model-free strategy based on genetic programming, which we call \u2018machine learning control\u2019. The goal is to reduce the recirculation zone of backward-facing step flow at $\\mathit{Re}_{h}=1350$ manipulated by a slotted jet and optically sensed by online particle image velocimetry. The feedback control law is optimized with respect to a cost functional based on the recirculation area and a penalization of the actuation. This optimization is performed employing genetic programming. After 12 generations comprised of 500 individuals, the algorithm converges to a feedback law which reduces the recirculation zone by 80 %. This machine learning control is benchmarked against the best periodic forcing which excites Kelvin\u2013Helmholtz vortices. The machine learning control yields a new actuation mechanism resonating with the low-frequency flapping mode instability. This feedback control performs similarly to periodic forcing at the design condition but outperforms periodic forcing when the Reynolds number is varied by a factor two. The current study indicates that machine learning control can effectively explore and optimize new feedback actuation mechanisms in numerous experimental applications.", "year": 2014, "referenceCount": 57, "citationCount": 162, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Computer Science"], "authors": [{"authorId": "144932679", "name": "N. Gautier"}, {"authorId": "12914771", "name": "Thomas Duriez"}, {"authorId": "3064326", "name": "J. Aider"}, {"authorId": "144875384", "name": "B. R. Noack"}, {"authorId": "2019562", "name": "M. Segond"}, {"authorId": "145515929", "name": "Markus Abel"}]}, {"paperId": "259b2f471629e88b7fe5fb19c6d7c73de26b8d10", "url": "https://www.semanticscholar.org/paper/259b2f471629e88b7fe5fb19c6d7c73de26b8d10", "title": "Deep Learning Accurately Predicts Estrogen Receptor Status in Breast Cancer Metabolomics Data", "abstract": "Metabolomics holds the promise as a new technology to diagnose highly heterogeneous diseases. Conventionally, metabolomics data analysis for diagnosis is done using various statistical and machine learning based classification methods. However, it remains unknown if deep neural network, a class of increasingly popular machine learning methods, is suitable to classify metabolomics data. Here we use a cohort of 271 breast cancer tissues, 204 positive estrogen receptor (ER+) and 67 negative estrogen receptor (ER-), to test the accuracies of autoencoder, a deep learning (DL) framework, as well as six widely used machine learning models, namely Random Forest (RF), Support Vector Machines (SVM), Recursive Partitioning and Regression Trees (RPART), Linear Discriminant Analysis (LDA), Prediction Analysis for Microarrays (PAM), and Generalized Boosted Models (GBM). DL framework has the highest area under the curve (AUC) of 0.93 in classifying ER+/ER-patients, compared to the other six machine learning algorithms. Furthermore, the biological interpretation of the first hidden layer reveals eight commonly enriched significant metabolomics pathways (adjusted P-value<0.05) that cannot be discovered by other machine learning methods. Among them, protein digestion & absorption and ATP-binding cassette (ABC) transporters pathways are also confirmed in integrated analysis between metabolomics and gene expression data in these samples. In summary, deep learning method shows advantages for metabolomics based breast cancer ER status classification, with both the highest prediction accurcy (AUC=0.93) and better revelation of disease biology. We encourage the adoption of autoencoder based deep learning method in the metabolomics research community for classification.", "year": 2017, "referenceCount": 85, "citationCount": 141, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Biology", "Computer Science", "Medicine"], "authors": [{"authorId": "2083360167", "name": "F. Alakwaa"}, {"authorId": "1713066", "name": "Kumardeep Chaudhary"}, {"authorId": "65975586", "name": "L. Garmire"}]}, {"paperId": "91e2b87f884b54847489d1ad156c144ce830fc25", "url": "https://www.semanticscholar.org/paper/91e2b87f884b54847489d1ad156c144ce830fc25", "title": "Metalearners for estimating heterogeneous treatment effects using machine learning", "abstract": "Significance Estimating and analyzing heterogeneous treatment effects is timely, yet challenging. We introduce a unifying framework for many conditional average treatment effect estimators, and we propose a metalearner, the X-learner, which can adapt to structural properties, such as the smoothness and sparsity of the underlying treatment effect. We present its favorable properties, using theory and simulations. We apply it, using random forests, to two field experiments in political science, where it is shown to be easy to use and to produce results that are interpretable. There is growing interest in estimating and analyzing heterogeneous treatment effects in experimental and observational studies. We describe a number of metaalgorithms that can take advantage of any supervised learning or regression method in machine learning and statistics to estimate the conditional average treatment effect (CATE) function. Metaalgorithms build on base algorithms\u2014such as random forests (RFs), Bayesian additive regression trees (BARTs), or neural networks\u2014to estimate the CATE, a function that the base algorithms are not designed to estimate directly. We introduce a metaalgorithm, the X-learner, that is provably efficient when the number of units in one treatment group is much larger than in the other and can exploit structural properties of the CATE function. For example, if the CATE function is linear and the response functions in treatment and control are Lipschitz-continuous, the X-learner can still achieve the parametric rate under regularity conditions. We then introduce versions of the X-learner that use RF and BART as base learners. In extensive simulation studies, the X-learner performs favorably, although none of the metalearners is uniformly the best. In two persuasion field experiments from political science, we demonstrate how our X-learner can be used to target treatment regimes and to shed light on underlying mechanisms. A software package is provided that implements our methods.", "year": 2017, "referenceCount": 41, "citationCount": 365, "influentialCitationCount": 68, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Mathematics"], "authors": [{"authorId": "49962158", "name": "S\u00f6ren R. K\u00fcnzel"}, {"authorId": "7359134", "name": "J. Sekhon"}, {"authorId": "1678771", "name": "P. Bickel"}, {"authorId": "144923779", "name": "Bin Yu"}]}, {"paperId": "a6d37438ac29c681f7bffc53726d09d8801dbc4e", "url": "https://www.semanticscholar.org/paper/a6d37438ac29c681f7bffc53726d09d8801dbc4e", "title": "Machine Learning Directed Search for Ultraincompressible, Superhard Materials.", "abstract": "In the pursuit of materials with exceptional mechanical properties, a machine-learning model is developed to direct the synthetic efforts toward compounds with high hardness by predicting the elastic moduli as a proxy. This approach screens 118\u202f287 compounds compiled in crystal structure databases for the materials with the highest bulk and shear moduli determined by support vector machine regression. Following these models, a ternary rhenium tungsten carbide and a quaternary molybdenum tungsten borocarbide are selected and synthesized at ambient pressure. High-pressure diamond anvil cell measurements corroborate the machine-learning prediction of the bulk modulus with less than 10% error, as well as confirm the ultraincompressible nature of both compounds. Subsequent Vickers microhardness measurements reveal that each compound also has an extremely high hardness exceeding the superhard threshold of 40 GPa at low loads (0.49 N). These results show the effectiveness of materials development through state-of-the-art machine-learning techniques by identifying functional inorganic materials.", "year": 2018, "referenceCount": 45, "citationCount": 157, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine", "Chemistry"], "authors": [{"authorId": "14646016", "name": "Aria Mansouri Tehrani"}, {"authorId": "15739628", "name": "A. Oliynyk"}, {"authorId": "144232488", "name": "M. Parry"}, {"authorId": "51079099", "name": "Zeshan Rizvi"}, {"authorId": "51083375", "name": "S. Couper"}, {"authorId": "2113696995", "name": "F. Lin"}, {"authorId": "4503342", "name": "L. Miyagi"}, {"authorId": "15227654", "name": "Taylor D. Sparks"}, {"authorId": "13191312", "name": "Jakoah Brgoch"}]}, {"paperId": "9815ac82b583ea50c03d4791a88520515b22e024", "url": "https://www.semanticscholar.org/paper/9815ac82b583ea50c03d4791a88520515b22e024", "title": "Mapping Species Distributions: Spatial Inference and Prediction", "abstract": "Part I. History and Ecological Basis of Species' Distribution Modeling: 1. Species distribution modeling 2. Why do we need species' distribution models? 3. Ecological understanding of species' distributions Part II. The Data Needed for Modeling Species' Distributions 4. Data for species' distribution models: the biological data 5. Data for species' distribution models: the environmental data Part III. An Overview of the Modeling Methods: 6. Statistical models - modern regression 7. Machine learning methods 8. Classification, similarity and other methods for presence-only data Part IV. Model Evaluation and Implementation: 9. Model evaluation 10. Implementation of species' distribution models.", "year": 2010, "referenceCount": 0, "citationCount": 1937, "influentialCitationCount": 248, "isOpenAccess": false, "fieldsOfStudy": ["Biology"], "authors": [{"authorId": "144735103", "name": "J. Franklin"}, {"authorId": "2109506351", "name": "Jennifer A. Miller"}]}, {"paperId": "3d0d5a9b5f360f666d189f0cdde5beb35105c8fc", "url": "https://www.semanticscholar.org/paper/3d0d5a9b5f360f666d189f0cdde5beb35105c8fc", "title": "Flock: Hybrid Crowd-Machine Learning Classifiers", "abstract": "We present hybrid crowd-machine learning classifiers: classification models that start with a written description of a learning goal, use the crowd to suggest predictive features and label data, and then weigh these features using machine learning to produce models that are accurate and use human-understandable features. These hybrid classifiers enable fast prototyping of machine learning models that can improve on both algorithm performance and human judgment, and accomplish tasks where automated feature extraction is not yet feasible. Flock, an interactive machine learning platform, instantiates this approach. To generate informative features, Flock asks the crowd to compare paired examples, an approach inspired by analogical encoding. The crowd's efforts can be focused on specific subsets of the input space where machine-extracted features are not predictive, or instead used to partition the input space and improve algorithm performance in subregions of the space. An evaluation on six prediction tasks, ranging from detecting deception to differentiating impressionist artists, demonstrated that aggregating crowd features improves upon both asking the crowd for a direct prediction and off-the-shelf machine learning features by over 10%. Further, hybrid systems that use both crowd-nominated and machine-extracted features can outperform those that use either in isolation.", "year": 2015, "referenceCount": 50, "citationCount": 115, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144500753", "name": "Justin Cheng"}, {"authorId": "145879842", "name": "Michael S. Bernstein"}]}, {"paperId": "5f08514db80861b0b3700620715de3759689e2fc", "url": "https://www.semanticscholar.org/paper/5f08514db80861b0b3700620715de3759689e2fc", "title": "Sublinear Optimization for Machine Learning", "abstract": "We give sub linear-time approximation algorithms for some optimization problems arising in machine learning, such as training linear classifiers and finding minimum enclosing balls. Our algorithms can be extended to some kernelized versions of these problems, such as SVDD, hard margin SVM, and $L_2$-SVM, for which sub linear-time algorithms were not known before. These new algorithms use a combination of a novel sampling techniques and a new multiplicative update algorithm. We give lower bounds which show the running times of many of our algorithms to be nearly best possible in the unit-cost RAM model. We also give implementations of our algorithms in the semi-streaming setting, obtaining the first low pass polylogarithmic space and sub linear time algorithms achieving arbitrary approximation factor.", "year": 2010, "referenceCount": 36, "citationCount": 118, "influentialCitationCount": 14, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2055593", "name": "K. Clarkson"}, {"authorId": "34840427", "name": "Elad Hazan"}, {"authorId": "143982862", "name": "David P. Woodruff"}]}, {"paperId": "b3f5151d6e488fb5b5e0c723e31cf0b4d9de0829", "url": "https://www.semanticscholar.org/paper/b3f5151d6e488fb5b5e0c723e31cf0b4d9de0829", "title": "Machine Learning Algorithms Outperform Conventional Regression Models in Predicting Development of Hepatocellular Carcinoma", "abstract": null, "year": 2013, "referenceCount": 33, "citationCount": 160, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3461764", "name": "A. Singal"}, {"authorId": "1849545", "name": "A. Mukherjee"}, {"authorId": "5474306", "name": "B. Elmunzer"}, {"authorId": "49200163", "name": "P. Higgins"}, {"authorId": "47820206", "name": "A. Lok"}, {"authorId": "2108715098", "name": "Ji Zhu"}, {"authorId": "144822511", "name": "J. Marrero"}, {"authorId": "5239295", "name": "A. Waljee"}]}, {"paperId": "a5319b1136149a2dd2b37eba90ef03d1f717a857", "url": "https://www.semanticscholar.org/paper/a5319b1136149a2dd2b37eba90ef03d1f717a857", "title": "Four Types of Learning Curves", "abstract": "If machines are learning to make decisions given a number of examples, the generalization error (t) is defined as the average probability that an incorrect decision is made for a new example by a machine when trained with t examples. The generalization error decreases as t increases, and the curve (t) is called a learning curve. The present paper uses the Bayesian approach to show that given the annealed approximation, learning curves can be classified into four asymptotic types. If the machine is deterministic with noiseless teacher signals, then (1) at-1 when the correct machine parameter is unique, and (2) at-2 when the set of the correct parameters has a finite measure. If the teacher signals are noisy, then (3) at-1/2 for a deterministic machine, and (4) c + at-1 for a stochastic machine.", "year": 1992, "referenceCount": 31, "citationCount": 144, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "144362425", "name": "S. Amari"}, {"authorId": "9585148", "name": "Naotake Fujita"}, {"authorId": "1724497", "name": "S. Shinomoto"}]}, {"paperId": "0cb7ebc80fadaa775c3ca64da8930384d910f00e", "url": "https://www.semanticscholar.org/paper/0cb7ebc80fadaa775c3ca64da8930384d910f00e", "title": "Predicting the Price of Bitcoin Using Machine Learning", "abstract": "The goal of this paper is to ascertain with what accuracy the direction of Bitcoin price in USD can be predicted. The price data is sourced from the Bitcoin Price Index. The task is achieved with varying degrees of success through the implementation of a Bayesian optimised recurrent neural network (RNN) and a Long Short Term Memory (LSTM) network. The LSTM achieves the highest classification accuracy of 52% and a RMSE of 8%. The popular ARIMA model for time series forecasting is implemented as a comparison to the deep learning models. As expected, the non-linear deep learning methods outperform the ARIMA forecast which performs poorly. Finally, both deep learning models are benchmarked on both a GPU and a CPU with the training time on the GPU outperforming the CPU implementation by 67.7%.", "year": 2018, "referenceCount": 56, "citationCount": 335, "influentialCitationCount": 26, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46391717", "name": "S. Mcnally"}, {"authorId": "2059704800", "name": "Jason Roche"}, {"authorId": "2452441", "name": "Simon Caton"}]}, {"paperId": "a69e5b952a3d334d88555ad767b29ddd52d67cad", "url": "https://www.semanticscholar.org/paper/a69e5b952a3d334d88555ad767b29ddd52d67cad", "title": "Machine Learning in Adversarial Settings", "abstract": "Recent advances in machine learning have led to innovative applications and services that use computational structures to reason about complex phenomenon. Over the past several years, the security and machine-learning communities have developed novel techniques for constructing adversarial samples--malicious inputs crafted to mislead (and therefore corrupt the integrity of) systems built on computationally learned models. The authors consider the underlying causes of adversarial samples and the future countermeasures that might mitigate them.", "year": 2016, "referenceCount": 8, "citationCount": 112, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144061974", "name": "P. Mcdaniel"}, {"authorId": "1967156", "name": "Nicolas Papernot"}, {"authorId": "144643812", "name": "Z. B. Celik"}]}, {"paperId": "b34fc78de28be598e21118d7cb9d84d63374addc", "url": "https://www.semanticscholar.org/paper/b34fc78de28be598e21118d7cb9d84d63374addc", "title": "Analysis of Dimensionality Reduction Techniques on Big Data", "abstract": "Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.", "year": 2020, "referenceCount": 44, "citationCount": 245, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "38608914", "name": "G. T. Reddy"}, {"authorId": "49247239", "name": "M. S. K. Reddy"}, {"authorId": "51017707", "name": "K. Lakshmanna"}, {"authorId": "46234199", "name": "Rajesh Kaluri"}, {"authorId": "32336300", "name": "D. Rajput"}, {"authorId": "144369609", "name": "Gautam Srivastava"}, {"authorId": "37022396", "name": "T. Baker"}]}, {"paperId": "9da1310a7158854a8070d9207d7042c9fff4f6ba", "url": "https://www.semanticscholar.org/paper/9da1310a7158854a8070d9207d7042c9fff4f6ba", "title": "Machine Learning: ECML 2003", "abstract": null, "year": 2003, "referenceCount": 19, "citationCount": 95, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1730104", "name": "N. Lavrac"}, {"authorId": "1767339", "name": "D. Gamberger"}, {"authorId": "1755851", "name": "H. Blockeel"}, {"authorId": "1735220", "name": "L. Todorovski"}]}, {"paperId": "fb144a1d31aec3b2bece6a59bd11a876a9fafb34", "url": "https://www.semanticscholar.org/paper/fb144a1d31aec3b2bece6a59bd11a876a9fafb34", "title": "Exploiting Open-Endedness to Solve Problems Through the Search for Novelty", "abstract": "This paper establishes a link between the challenge of solving highly ambitious problems in machine learning and the goal of reproducing the dynamics of open-ended evolution in artificial life. A major problem with the objective function in machine learning is that through deception it may actually prevent the objective from being reached. In a similar way, selection in evolution may sometimes act to discourage increasing complexity. This paper proposes a single idea that both overcomes the obstacle of deception and suggests a simple new approach to open-ended evolution: Instead of either explicitly seeking an objective or modeling a domain to capture the open-endedness of natural evolution, the idea is to simply search for novelty. Even in an objective-based problem, such novelty search ignores the objective and searches for behavioral novelty. Yet because many points in the search space collapse to the same point in behavior space, it turns out that the search for novelty is computationally feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. In fact, on the way up the ladder of complexity, the search is likely to encounter at least one solution. In this way, by decoupling the idea of open-ended search from only artificial life worlds, the raw search for novelty can be applied to real world problems. Counterintuitively, in the deceptive maze navigation task in this paper, novelty search significantly outperforms objective-based search, suggesting a surprising new approach to machine learning.", "year": 2008, "referenceCount": 39, "citationCount": 494, "influentialCitationCount": 90, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39799304", "name": "J. Lehman"}, {"authorId": "1846883", "name": "Kenneth O. Stanley"}]}, {"paperId": "bb66ddb6ff2d875d568873058bb1d92407acef86", "url": "https://www.semanticscholar.org/paper/bb66ddb6ff2d875d568873058bb1d92407acef86", "title": "Metric and Kernel Learning Using a Linear Transformation", "abstract": "Metric and kernel learning arise in several machine learning applications. However, most existing metric learning algorithms are limited to learning metrics over low-dimensional data, while existing kernel learning algorithms are often limited to the transductive setting and do not generalize to new data points. In this paper, we study the connections between metric learning and kernel learning that arise when studying metric learning as a linear transformation learning problem. In particular, we propose a general optimization framework for learning metrics via linear transformations, and analyze in detail a special case of our framework-that of minimizing the LogDet divergence subject to linear constraints. We then propose a general regularized framework for learning a kernel matrix, and show it to be equivalent to our metric learning framework. Our theoretical connections between metric and kernel learning have two main consequences: 1) the learned kernel matrix parameterizes a linear transformation kernel function and can be applied inductively to new data points, 2) our result yields a constructive method for kernelizing most existing Mahalanobis metric learning formulations. We demonstrate our learning approach by applying it to large-scale real world problems in computer vision, text mining and semi-supervised kernel dimensionality reduction.", "year": 2009, "referenceCount": 61, "citationCount": 189, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "48964143", "name": "Prateek Jain"}, {"authorId": "1692670", "name": "B. Kulis"}, {"authorId": "2111090524", "name": "Jason V. Davis"}, {"authorId": "1783667", "name": "I. Dhillon"}]}, {"paperId": "0fbad208057b68ec5dc71a28375ec298983dd68c", "url": "https://www.semanticscholar.org/paper/0fbad208057b68ec5dc71a28375ec298983dd68c", "title": "Performance evaluation of fuzzy classifier systems for multidimensional pattern classification problems", "abstract": "We examine the performance of a fuzzy genetics-based machine learning method for multidimensional pattern classification problems with continuous attributes. In our method, each fuzzy if-then rule is handled as an individual, and a fitness value is assigned to each rule. Thus, our method can be viewed as a classifier system. In this paper, we first describe fuzzy if-then rules and fuzzy reasoning for pattern classification problems. Then we explain a genetics-based machine learning method that automatically generates fuzzy if-then rules for pattern classification problems from numerical data. Because our method uses linguistic values with fixed membership functions as antecedent fuzzy sets, a linguistic interpretation of each fuzzy if-then rule is easily obtained. The fixed membership functions also lead to a simple implementation of our method as a computer program. The simplicity of implementation and the linguistic interpretation of the generated fuzzy if-then rules are the main characteristic features of our method. The performance of our method is evaluated by computer simulations on some well-known test problems. While our method involves no tuning mechanism of membership functions, it works very well in comparison with other classification methods such as nonfuzzy machine learning techniques and neural networks.", "year": 1999, "referenceCount": 66, "citationCount": 502, "influentialCitationCount": 37, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "10022373", "name": "H. Ishibuchi"}, {"authorId": "72328943", "name": "T. Nakashima"}, {"authorId": "1704824", "name": "T. Murata"}]}]}