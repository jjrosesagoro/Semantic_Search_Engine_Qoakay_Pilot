{"total": 5120627, "offset": 1100, "next": 1200, "data": [{"paperId": "1242d79573397094c5670f55e58c8333cced0beb", "url": "https://www.semanticscholar.org/paper/1242d79573397094c5670f55e58c8333cced0beb", "title": "Deep Learning: A Primer for Radiologists.", "abstract": "Deep learning is a class of machine learning methods that are gaining success and attracting interest in many domains, including computer vision, speech recognition, natural language processing, and playing games. Deep learning methods produce a mapping from raw inputs to desired outputs (eg, image classes). Unlike traditional machine learning methods, which require hand-engineered feature extraction from inputs, deep learning methods learn these features directly from data. With the advent of large datasets and increased computing power, these methods can produce models with exceptional performance. These models are multilayer artificial neural networks, loosely inspired by biologic neural systems. Weighted connections between nodes (neurons) in the network are iteratively adjusted based on example pairs of inputs and target outputs by back-propagating a corrective error signal through the network. For computer vision tasks, convolutional neural networks (CNNs) have proven to be effective. Recently, several clinical applications of CNNs have been proposed and studied in radiology for classification, detection, and segmentation tasks. This article reviews the key concepts of deep learning for clinical radiologists, discusses technical requirements, describes emerging applications in clinical radiology, and outlines limitations and future directions in this field. Radiologists should become familiar with the principles and potential applications of deep learning in medical imaging. \u00a9RSNA, 2017.", "year": 2017, "referenceCount": 28, "citationCount": 583, "influentialCitationCount": 14, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "38162399", "name": "G. Chartrand"}, {"authorId": "34842677", "name": "P. Cheng"}, {"authorId": "37627814", "name": "Eugene Vorontsov"}, {"authorId": "3325894", "name": "M. Drozdzal"}, {"authorId": "3622148", "name": "S. Turcotte"}, {"authorId": "1972076", "name": "C. Pal"}, {"authorId": "1781469", "name": "S. Kadoury"}, {"authorId": "2080173111", "name": "A. Tang"}]}, {"paperId": "47c528344fedb6cb67a38e43d095b41c34715330", "url": "https://www.semanticscholar.org/paper/47c528344fedb6cb67a38e43d095b41c34715330", "title": "Adaptive Federated Optimization", "abstract": "Federated learning is a distributed machine learning paradigm in which a large number of clients coordinate with a central server to learn a model without sharing their own training data. Due to the heterogeneity of the client datasets, standard federated optimization methods such as Federated Averaging (FedAvg) are often difficult to tune and exhibit unfavorable convergence behavior. In non-federated settings, adaptive optimization methods have had notable success in combating such issues. In this work, we propose federated versions of adaptive optimizers, including Adagrad, Adam, and Yogi, and analyze their convergence in the presence of heterogeneous data for general nonconvex settings. Our results highlight the interplay between client heterogeneity and communication efficiency. We also perform extensive experiments on these methods and show that the use of adaptive optimizers can significantly improve the performance of federated learning.", "year": 2020, "referenceCount": 52, "citationCount": 464, "influentialCitationCount": 123, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1981186", "name": "Sashank J. Reddi"}, {"authorId": "143676545", "name": "Zachary B. Charles"}, {"authorId": "1771307", "name": "M. Zaheer"}, {"authorId": "40449749", "name": "Zachary Garrett"}, {"authorId": "1387453057", "name": "Keith Rush"}, {"authorId": "32139366", "name": "Jakub Konecn\u00fd"}, {"authorId": "152663162", "name": "Sanjiv Kumar"}, {"authorId": "145057514", "name": "H. B. McMahan"}]}, {"paperId": "3bc4736f9b8512043ed47357a81f26b93a1204b6", "url": "https://www.semanticscholar.org/paper/3bc4736f9b8512043ed47357a81f26b93a1204b6", "title": "Semi-supervised learning with graphs", "abstract": "In traditional machine learning approaches to classification, one uses only a labeled set to train the classifier. Labeled instances however are often difficult, expensive, or time consuming to obtain, as they require the efforts of experienced human annotators. Meanwhile unlabeled data may be relatively easy to collect, but there has been few ways to use them. Semi-supervised learning addresses this problem by using large amount of unlabeled data, together with the labeled data, to build better classifiers. Because semi-supervised learning requires less human effort and gives higher accuracy, it is of great interest both in theory and in practice. \nWe present a series of novel semi-supervised learning approaches arising from a graph representation, where labeled and unlabeled instances are represented as vertices, and edges encode the similarity between instances. They address the following questions: How to use unlabeled data? (label propagation); What is the probabilistic interpretation? (Gaussian fields and harmonic functions); What if we can choose labeled data? (active learning); How to construct good graphs? (hyperparameter learning); How to work with kernel machines like SVM? (graph kernels); How to handle complex data like sequences? (kernel conditional random fields); How to handle scalability and induction? (harmonic mixtures). An extensive literature review is included at the end.", "year": 2005, "referenceCount": 136, "citationCount": 636, "influentialCitationCount": 63, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1832364", "name": "Xiaojin Zhu"}, {"authorId": "1739581", "name": "J. Lafferty"}, {"authorId": "145903504", "name": "R. Rosenfeld"}]}, {"paperId": "b09ad6ef6e63ddf2e1d07faa783adecce0cc2a77", "url": "https://www.semanticscholar.org/paper/b09ad6ef6e63ddf2e1d07faa783adecce0cc2a77", "title": "Coronavirus disease (COVID-19) cases analysis using machine-learning applications", "abstract": null, "year": 2021, "referenceCount": 40, "citationCount": 177, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "2100113493", "name": "Ameer Sardar Kwekha-Rashid"}, {"authorId": "66680362", "name": "H. N. Abduljabbar"}, {"authorId": "98174258", "name": "Bilal S. A. Alhayani"}]}, {"paperId": "d86c9623d56e469aec73a76758a829891b0b2a09", "url": "https://www.semanticscholar.org/paper/d86c9623d56e469aec73a76758a829891b0b2a09", "title": "Technical Report on the CleverHans v2.1.0 Adversarial Examples Library", "abstract": "CleverHans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. \nThis technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the CleverHans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.", "year": 2016, "referenceCount": 23, "citationCount": 349, "influentialCitationCount": 26, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1967156", "name": "Nicolas Papernot"}, {"authorId": "2978170", "name": "Fartash Faghri"}, {"authorId": "2483738", "name": "Nicholas Carlini"}, {"authorId": "153440022", "name": "Ian J. Goodfellow"}, {"authorId": "40285565", "name": "Reuben Feinman"}, {"authorId": "145714153", "name": "A. Kurakin"}, {"authorId": "3011497", "name": "Cihang Xie"}, {"authorId": "49738125", "name": "Yash Sharma"}, {"authorId": "31035595", "name": "Tom B. Brown"}, {"authorId": "39788470", "name": "Aurko Roy"}, {"authorId": "2139376", "name": "Alexander Matyasko"}, {"authorId": "2641675", "name": "Vahid Behzadan"}, {"authorId": "5641536", "name": "Karen Hambardzumyan"}, {"authorId": "2852303", "name": "Zhishuai Zhang"}, {"authorId": "144458540", "name": "Yi-Lin Juang"}, {"authorId": "2118215674", "name": "Zhi Li"}, {"authorId": "9541640", "name": "Ryan Sheatsley"}, {"authorId": "145437002", "name": "Abhibhav Garg"}, {"authorId": "9960452", "name": "J. Uesato"}, {"authorId": "145556052", "name": "W. Gierke"}, {"authorId": "3431029", "name": "Yinpeng Dong"}, {"authorId": "39835551", "name": "David Berthelot"}, {"authorId": "118838370", "name": "P. Hendricks"}, {"authorId": "19237612", "name": "Jonas Rauber"}, {"authorId": "103423330", "name": "Rujun Long"}, {"authorId": "144061974", "name": "P. Mcdaniel"}]}, {"paperId": "912597e993e8c42c7535751a09638c834b9af94e", "url": "https://www.semanticscholar.org/paper/912597e993e8c42c7535751a09638c834b9af94e", "title": "Local Receptive Fields Based Extreme Learning Machine", "abstract": "Extreme learning machine (ELM), which was originally proposed for \"generalized\" single-hidden layer feedforward neural networks (SLFNs), provides efficient unified learning solutions for the applications of feature learning, clustering, regression and classification. Different from the common understanding and tenet that hidden neurons of neural networks need to be iteratively adjusted during training stage, ELM theories show that hidden neurons are important but need not be iteratively tuned. In fact, all the parameters of hidden nodes can be independent of training samples and randomly generated according to any continuous probability distribution. And the obtained ELM networks satisfy universal approximation and classification capability. The fully connected ELM architecture has been extensively studied. However, ELM with local connections has not attracted much research attention yet. This paper studies the general architecture of locally connected ELM, showing that: 1) ELM theories are naturally valid for local connections, thus introducing local receptive fields to the input layer; 2) each hidden node in ELM can be a combination of several hidden nodes (a subnetwork), which is also consistent with ELM theories. ELM theories may shed a light on the research of different local receptive fields including true biological receptive fields of which the exact shapes and formula may be unknown to human beings. As a specific example of such general architectures, random convolutional nodes and a pooling structure are implemented in this paper. Experimental results on the NORB dataset, a benchmark for object recognition, show that compared with conventional deep learning solutions, the proposed local receptive fields based ELM (ELM-LRF) reduces the error rate from 6.5% to 2.7% and increases the learning speed up to 200 times.", "year": 2015, "referenceCount": 70, "citationCount": 300, "influentialCitationCount": 32, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145678691", "name": "G. Huang"}, {"authorId": "2612408", "name": "Zuo Bai"}, {"authorId": "2996971", "name": "L. L. C. Kasun"}, {"authorId": "1807914", "name": "C. Vong"}]}, {"paperId": "f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f", "url": "https://www.semanticscholar.org/paper/f1318d15d7d8ab626b92d0c70dbdc5b5d37e223f", "title": "HolStep: A Machine Learning Dataset for Higher-order Logic Theorem Proving", "abstract": "Large computer-understandable proofs consist of millions of intermediate logical steps. The vast majority of such steps originate from manually selected and manually guided heuristics applied to intermediate goals. So far, machine learning has generally not been used to filter or generate these steps. In this paper, we introduce a new dataset based on Higher-Order Logic (HOL) proofs, for the purpose of developing new machine learning-based theorem-proving strategies. We make this dataset publicly available under the BSD license. We propose various machine learning tasks that can be performed on this dataset, and discuss their significance for theorem proving. We also benchmark a set of simple baseline machine learning models suited for the tasks (including logistic regression, convolutional neural networks and recurrent neural networks). The results of our baseline models show the promise of applying machine learning to HOL theorem proving.", "year": 2017, "referenceCount": 50, "citationCount": 70, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1784106", "name": "C. Kaliszyk"}, {"authorId": "1565641737", "name": "Fran\u00e7ois Chollet"}, {"authorId": "2574060", "name": "Christian Szegedy"}]}, {"paperId": "61ce67533d2dd6605c907146658ccdbc4778a5d8", "url": "https://www.semanticscholar.org/paper/61ce67533d2dd6605c907146658ccdbc4778a5d8", "title": "Learning a Multi-View Stereo Machine", "abstract": "We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches as well as recent learning based methods.", "year": 2017, "referenceCount": 62, "citationCount": 355, "influentialCitationCount": 37, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145579476", "name": "Abhishek Kar"}, {"authorId": "2172959", "name": "Christian H\u00e4ne"}, {"authorId": "143751119", "name": "Jitendra Malik"}]}, {"paperId": "e767ea3f3875f096a70bbdac4847436a5e30800e", "url": "https://www.semanticscholar.org/paper/e767ea3f3875f096a70bbdac4847436a5e30800e", "title": "Comparison of Deep Learning With Multiple Machine Learning Methods and Metrics Using Diverse Drug Discovery Data Sets.", "abstract": "Machine learning methods have been applied to many data sets in pharmaceutical research for several decades. The relative ease and availability of fingerprint type molecular descriptors paired with Bayesian methods resulted in the widespread use of this approach for a diverse array of end points relevant to drug discovery. Deep learning is the latest machine learning algorithm attracting attention for many of pharmaceutical applications from docking to virtual screening. Deep learning is based on an artificial neural network with multiple hidden layers and has found considerable traction for many artificial intelligence applications. We have previously suggested the need for a comparison of different machine learning methods with deep learning across an array of varying data sets that is applicable to pharmaceutical research. End points relevant to pharmaceutical research include absorption, distribution, metabolism, excretion, and toxicity (ADME/Tox) properties, as well as activity against pathogens and drug discovery data sets. In this study, we have used data sets for solubility, probe-likeness, hERG, KCNQ1, bubonic plague, Chagas, tuberculosis, and malaria to compare different machine learning methods using FCFP6 fingerprints. These data sets represent whole cell screens, individual proteins, physicochemical properties as well as a data set with a complex end point. Our aim was to assess whether deep learning offered any improvement in testing when assessed using an array of metrics including AUC, F1 score, Cohen's kappa, Matthews correlation coefficient and others. Based on ranked normalized scores for the metrics or data sets Deep Neural Networks (DNN) ranked higher than SVM, which in turn was ranked higher than all the other machine learning methods. Visualizing these properties for training and test sets using radar type plots indicates when models are inferior or perhaps over trained. These results also suggest the need for assessing deep learning further using multiple metrics with much larger scale comparisons, prospective testing as well as assessment of different fingerprints and DNN architectures beyond those used.", "year": 2017, "referenceCount": 115, "citationCount": 197, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "46927010", "name": "A. Korotcov"}, {"authorId": "145694312", "name": "Valery Tkachenko"}, {"authorId": "8903363", "name": "Daniel P. Russo"}, {"authorId": "1887610", "name": "S. Ekins"}]}, {"paperId": "a42ca00fc188beb5586ad4c7108b70aeb5317da0", "url": "https://www.semanticscholar.org/paper/a42ca00fc188beb5586ad4c7108b70aeb5317da0", "title": "Auto-WEKA: combined selection and hyperparameter optimization of classification algorithms", "abstract": "Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance.", "year": 2012, "referenceCount": 43, "citationCount": 1223, "influentialCitationCount": 134, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143928655", "name": "C. Thornton"}, {"authorId": "144661829", "name": "F. Hutter"}, {"authorId": "2470869", "name": "H. Hoos"}, {"authorId": "1388404060", "name": "Kevin Leyton-Brown"}]}, {"paperId": "d75356e2bf674902a06a14bb55d18ee88af5b4bb", "url": "https://www.semanticscholar.org/paper/d75356e2bf674902a06a14bb55d18ee88af5b4bb", "title": "Machine Learning Methods That Economists Should Know About", "abstract": "We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.", "year": 2019, "referenceCount": 146, "citationCount": 339, "influentialCitationCount": 27, "isOpenAccess": false, "fieldsOfStudy": ["Economics", "Computer Science"], "authors": [{"authorId": "2631417", "name": "S. Athey"}, {"authorId": "47166531", "name": "G. Imbens"}]}, {"paperId": "f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10", "url": "https://www.semanticscholar.org/paper/f8b012720a2322dcf4ed9ac4d61d6be11d9ebd10", "title": "Concepts of Artificial Intelligence for Computer-Assisted Drug Discovery.", "abstract": "Artificial intelligence (AI), and, in particular, deep learning as a subcategory of AI, provides opportunities for the discovery and development of innovative drugs. Various machine learning approaches have recently (re)emerged, some of which may be considered instances of domain-specific AI which have been successfully employed for drug discovery and design. This review provides a comprehensive portrayal of these machine learning techniques and of their applications in medicinal chemistry. After introducing the basic principles, alongside some application notes, of the various machine learning algorithms, the current state-of-the art of AI-assisted pharmaceutical discovery is discussed, including applications in structure- and ligand-based virtual screening, de novo drug design, physicochemical and pharmacokinetic property prediction, drug repurposing, and related aspects. Finally, several challenges and limitations of the current methods are summarized, with a view to potential future directions for AI-assisted drug discovery and design.", "year": 2019, "referenceCount": 618, "citationCount": 286, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Chemistry", "Medicine"], "authors": [{"authorId": "2150439478", "name": "Xin Yang"}, {"authorId": "2115568943", "name": "Yifei Wang"}, {"authorId": "2060877077", "name": "R. Byrne"}, {"authorId": "144522872", "name": "G. Schneider"}, {"authorId": "144824105", "name": "Sheng-yong Yang"}]}, {"paperId": "94b295e711ec744583597432c19749a5cd61038e", "url": "https://www.semanticscholar.org/paper/94b295e711ec744583597432c19749a5cd61038e", "title": "Multicategory Support Vector Machines", "abstract": "Two-category support vector machines (SVM) have been very popular in the machine learning community for classification problems. Solving multicategory problems by a series of binary classifiers is quite common in the SVM paradigm; however, this approach may fail under various circumstances. We propose the multicategory support vector machine (MSVM), which extends the binary SVM to the multicategory case and has good theoretical properties. The proposed method provides a unifying framework when there are either equal or unequal misclassification costs. As a tuning criterion for the MSVM, an approximate leave-one-out cross-validation function, called Generalized Approximate Cross Validation, is derived, analogous to the binary case. The effectiveness of the MSVM is demonstrated through the applications to cancer classification using microarray data and cloud classification with satellite radiance profiles.", "year": 2004, "referenceCount": 153, "citationCount": 467, "influentialCitationCount": 31, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "4628585", "name": "Yoonkyung Lee"}, {"authorId": "2108066200", "name": "Yi Lin"}, {"authorId": "145733439", "name": "G. Wahba"}]}, {"paperId": "3f22c9462f8e588ce4210a304133e2265f41d913", "url": "https://www.semanticscholar.org/paper/3f22c9462f8e588ce4210a304133e2265f41d913", "title": "Artificial Intelligence and Machine Learning in Pathology: The Present Landscape of Supervised Methods", "abstract": "Increased interest in the opportunities provided by artificial intelligence and machine learning has spawned a new field of health-care research. The new tools under development are targeting many aspects of medical practice, including changes to the practice of pathology and laboratory medicine. Optimal design in these powerful tools requires cross-disciplinary literacy, including basic knowledge and understanding of critical concepts that have traditionally been unfamiliar to pathologists and laboratorians. This review provides definitions and basic knowledge of machine learning categories (supervised, unsupervised, and reinforcement learning), introduces the underlying concept of the bias-variance trade-off as an important foundation in supervised machine learning, and discusses approaches to the supervised machine learning study design along with an overview and description of common supervised machine learning algorithms (linear regression, logistic regression, Naive Bayes, k-nearest neighbor, support vector machine, random forest, convolutional neural networks).", "year": 2019, "referenceCount": 76, "citationCount": 127, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "31682040", "name": "H. Rashidi"}, {"authorId": "144026292", "name": "N. Tran"}, {"authorId": "83007327", "name": "Elham Vali Betts"}, {"authorId": "5788528", "name": "L. Howell"}, {"authorId": "2072877937", "name": "Ralph Green"}]}, {"paperId": "e2208a581439145f0709206ccfb0b763bbf05b55", "url": "https://www.semanticscholar.org/paper/e2208a581439145f0709206ccfb0b763bbf05b55", "title": "Deep Extreme Learning Machine and Its Application in EEG Classification", "abstract": "Recently, deep learning has aroused wide interest in machine learning fields. Deep learning is a multilayer perceptron artificial neural network algorithm. Deep learning has the advantage of approximating the complicated function and alleviating the optimization difficulty associated with deep models. Multilayer extreme learning machine (MLELM) is a learning algorithm of an artificial neural network which takes advantages of deep learning and extreme learning machine. Not only does MLELM approximate the complicated function but it also does not need to iterate during the training process. We combining with MLELM and extreme learning machine with kernel (KELM) put forward deep extreme learning machine (DELM) and apply it to EEG classification in this paper. This paper focuses on the application of DELM in the classification of the visual feedback experiment, using MATLAB and the second brain-computer interface (BCI) competition datasets. By simulating and analyzing the results of the experiments, effectiveness of the application of DELM in EEG classification is confirmed.", "year": 2015, "referenceCount": 29, "citationCount": 132, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2066430561", "name": "Shifei Ding"}, {"authorId": "144679550", "name": "Nan Zhang"}, {"authorId": "29279000", "name": "Xinzheng Xu"}, {"authorId": "49868446", "name": "Lili Guo"}, {"authorId": "2151811677", "name": "Jian Zhang"}]}, {"paperId": "5389977ca9f0fa3a628a4182d0f09c8cedef2421", "url": "https://www.semanticscholar.org/paper/5389977ca9f0fa3a628a4182d0f09c8cedef2421", "title": "Learning Machines", "abstract": "This book is about machines that learn to discover hidden relationships in data. A constant sfream of data bombards our senses and millions of sensory channels carry information into our brains. Brains are also learning machines that condition, combine, parse, and store data. Is it possible to learn something about learning by observing the style of computation used by brains? This is the motivation for research into computational devices that today are called \"neural networks.\" Neural networks are nonlinear dynamical systems with many degrees of freedom that can be used to solve computational problems. The mathematical foundations for learning in this class of machines was laid by agroup of researchers in the 1940s and 1950s. The achievement documented in this book is the thorough study of one of the simplest members of this class, feedforward networks with one layer of m w ~ a b l e weights connecting input units to output units. In a sense, these might be called reflex machines. The knee-jerk reflex, for example, is mediated by synaptic connections from the sensory recepton in your knee directly onto rnotoneurons in your spinal cord that in turn activate leg muscles. There are limits to how much computation can be accomplished by such reflexes, and these limits have been carefully delineated in this book. Just as more complex creatures evolved by layering control loops on the primitive reflexes, network models have also evolved in recent years and now have achieved vastly greater capabilities than reflex machines by making use of multilayered architectures with feedback connections. Nonetheless, recent work could not have been accomplished without building on these foundations. vii viil INTRODUCTION INTRODUCTION ix Despite the early promise of research on neural networks, there was a period of about 20 years, from the mid 1960s to the mid 1980s. when interest in neural networks as computational devices and models of human behavior waned in favor of models based on symbol processing. There are many reasons for this, some of them now evident in this book. Still, Learning Machines was an underground classic among the neural network modelers who were active during this \"dark age\" and deserves to be better known to the generation that is \"relearning\" what was once known about statistical learning machines. The intuitive geometric explanations and the mathematical foundations in this monograph are as invaluable today as they were when it was first written. Nilsson is \u2026", "year": 2020, "referenceCount": 87, "citationCount": 104, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2815219", "name": "M. Boman"}, {"authorId": "1689109", "name": "Magnus Sahlgren"}, {"authorId": "1810096", "name": "Olof G\u00f6rnerup"}, {"authorId": "2338283", "name": "D. Gillblad"}]}, {"paperId": "95615c6bce2123f12e39c3d9eb293ebb759501aa", "url": "https://www.semanticscholar.org/paper/95615c6bce2123f12e39c3d9eb293ebb759501aa", "title": "Machine learning, social learning and the governance of self-driving cars", "abstract": "Self-driving cars, a quintessentially \u2018smart\u2019 technology, are not born smart. The algorithms that control their movements are learning as the technology emerges. Self-driving cars represent a high-stakes test of the powers of machine learning, as well as a test case for social learning in technology governance. Society is learning about the technology while the technology learns about society. Understanding and governing the politics of this technology means asking \u2018Who is learning, what are they learning and how are they learning?\u2019 Focusing on the successes and failures of social learning around the much-publicized crash of a Tesla Model S in 2016, I argue that trajectories and rhetorics of machine learning in transport pose a substantial governance challenge. \u2018Self-driving\u2019 or \u2018autonomous\u2019 cars are misnamed. As with other technologies, they are shaped by assumptions about social needs, solvable problems, and economic opportunities. Governing these technologies in the public interest means improving social learning by constructively engaging with the contingencies of machine learning.", "year": 2017, "referenceCount": 181, "citationCount": 212, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Engineering", "Medicine"], "authors": [{"authorId": "3416633", "name": "J. Stilgoe"}]}, {"paperId": "c526ad0ea8b4cff9a671eb8a90ea98eb64ae17a7", "url": "https://www.semanticscholar.org/paper/c526ad0ea8b4cff9a671eb8a90ea98eb64ae17a7", "title": "Sparse Bayesian Extreme Learning Machine for Multi-classification", "abstract": "Extreme learning machine (ELM) has become a popular topic in machine learning in recent years. ELM is a new kind of single-hidden layer feedforward neural network with an extremely low computational cost. ELM, however, has two evident drawbacks: 1) the output weights solved by Moore-Penrose generalized inverse is a least squares minimization issue, which easily suffers from overfitting and 2) the accuracy of ELM is drastically sensitive to the number of hidden neurons so that a large model is usually generated. This brief presents a sparse Bayesian approach for learning the output weights of ELM in classification. The new model, called Sparse Bayesian ELM (SBELM), can resolve these two drawbacks by estimating the marginal likelihood of network outputs and automatically pruning most of the redundant hidden neurons during learning phase, which results in an accurate and compact model. The proposed SBELM is evaluated on wide types of benchmark classification problems, which verifies that the accuracy of SBELM model is relatively insensitive to the number of hidden neurons; and hence a much more compact model is always produced as compared with other state-of-the-art neural network classifiers.", "year": 2014, "referenceCount": 26, "citationCount": 171, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2353988", "name": "Jiahua Luo"}, {"authorId": "1807914", "name": "C. Vong"}, {"authorId": "48692831", "name": "P. Wong"}]}, {"paperId": "02227c94dd41fe0b439e050d377b0beb5d427cda", "url": "https://www.semanticscholar.org/paper/02227c94dd41fe0b439e050d377b0beb5d427cda", "title": "Reading Digits in Natural Images with Unsupervised Feature Learning", "abstract": "Detecting and reading text from natural images is a hard computer vision task that is central to a variety of emerging applications. Related problems like document character recognition have been widely studied by computer vision and machine learning researchers and are virtually solved for practical applications like reading handwritten digits. Reliably recognizing characters in more complex scenes like photographs, however, is far more difficult: the best existing methods lag well behind human performance on the same tasks. In this paper we attack the problem of recognizing digits in a real application using unsupervised feature learning methods: reading house numbers from street level photos. To this end, we introduce a new benchmark dataset for research use containing over 600,000 labeled digits cropped from Street View images. We then demonstrate the difficulty of recognizing these digits when the problem is approached with hand-designed features. Finally, we employ variants of two recently proposed unsupervised feature learning methods and find that they are convincingly superior on our benchmarks.", "year": 2011, "referenceCount": 28, "citationCount": 4433, "influentialCitationCount": 1682, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "34180232", "name": "Yuval Netzer"}, {"authorId": "2156632012", "name": "Tao Wang"}, {"authorId": "144638694", "name": "Adam Coates"}, {"authorId": "1726358", "name": "A. Bissacco"}, {"authorId": "144397975", "name": "Bo Wu"}, {"authorId": "34699434", "name": "A. Ng"}]}, {"paperId": "37b7f2901f3f82255d397ec88e3a6a9e9dfec74f", "url": "https://www.semanticscholar.org/paper/37b7f2901f3f82255d397ec88e3a6a9e9dfec74f", "title": "Facial Attractiveness: Beauty and the Machine", "abstract": "This work presents a novel study of the notion of facial attractiveness in a machine learning context. To this end, we collected human beauty ratings for data sets of facial images and used various techniques for learning the attractiveness of a face. The trained predictor achieves a significant correlation of 0.65 with the average human ratings. The results clearly show that facial beauty is a universal concept that a machine can learn. Analysis of the accuracy of the beauty prediction machine as a function of the size of the training data indicates that a machine producing human-like attractiveness rating could be obtained given a moderately larger data set.", "year": 2006, "referenceCount": 85, "citationCount": 240, "influentialCitationCount": 26, "isOpenAccess": false, "fieldsOfStudy": ["Psychology", "Computer Science", "Medicine"], "authors": [{"authorId": "3185543", "name": "Yael Eisenthal"}, {"authorId": "2088846", "name": "G. Dror"}, {"authorId": "1779370", "name": "E. Ruppin"}]}, {"paperId": "a5ee556c355392db1750df92ae2dc8867073e771", "url": "https://www.semanticscholar.org/paper/a5ee556c355392db1750df92ae2dc8867073e771", "title": "Improved Local Coordinate Coding using Local Tangents", "abstract": "Local Coordinate Coding (LCC), introduced in (Yu et al., 2009), is a high dimensional nonlinear learning method that explicitly takes advantage of the geometric structure of the data. Its successful use in the winning system of last year's Pascal image classification Challenge (Everingham, 2009) shows that the ability to integrate geometric information is critical for some real world machine learning applications. This paper further develops the idea of integrating geometry in machine learning by extending the original LCC method to include local tangent directions. These new correction terms lead to better approximation of high dimensional nonlinear functions when the underlying data manifold is locally relatively flat. The method significantly reduces the number of anchor points needed in LCC, which not only reduces computational cost, but also improves prediction performance. Experiments are included to demonstrate that this method is more effective than the original LCC method on some image classification tasks.", "year": 2010, "referenceCount": 16, "citationCount": 163, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144782042", "name": "Kai Yu"}, {"authorId": "49104973", "name": "Tong Zhang"}]}, {"paperId": "bb111921d2020af454ea492c97a663cbe724ce40", "url": "https://www.semanticscholar.org/paper/bb111921d2020af454ea492c97a663cbe724ce40", "title": "Machine learning in side-channel analysis: a first study", "abstract": null, "year": 2011, "referenceCount": 21, "citationCount": 256, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2192488", "name": "Gabriel Hospodar"}, {"authorId": "1709831", "name": "Benedikt Gierlichs"}, {"authorId": "2952692", "name": "E. D. Mulder"}, {"authorId": "1722064", "name": "I. Verbauwhede"}, {"authorId": "1704135", "name": "J. Vandewalle"}]}, {"paperId": "e59f25a68ed5f66bce4e0c14c026cfa7c9424fd4", "url": "https://www.semanticscholar.org/paper/e59f25a68ed5f66bce4e0c14c026cfa7c9424fd4", "title": "Evolutionary Cost-Sensitive Extreme Learning Machine", "abstract": "Conventional extreme learning machines (ELMs) solve a Moore\u2013Penrose generalized inverse of hidden layer activated matrix and analytically determine the output weights to achieve generalized performance, by assuming the same loss from different types of misclassification. The assumption may not hold in cost-sensitive recognition tasks, such as face recognition-based access control system, where misclassifying a stranger as a family member may result in more serious disaster than misclassifying a family member as a stranger. Though recent cost-sensitive learning can reduce the total loss with a given cost matrix that quantifies how severe one type of mistake against another, in many realistic cases, the cost matrix is unknown to users. Motivated by these concerns, this paper proposes an evolutionary cost-sensitive ELM, with the following merits: 1) to the best of our knowledge, it is the first proposal of ELM in evolutionary cost-sensitive classification scenario; 2) it well addresses the open issue of how to define the cost matrix in cost-sensitive learning tasks; and 3) an evolutionary backtracking search algorithm is induced for adaptive cost matrix optimization. Experiments in a variety of cost-sensitive tasks well demonstrate the effectiveness of the proposed approaches, with about 5%\u201310% improvements.", "year": 2015, "referenceCount": 77, "citationCount": 129, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "36685537", "name": "Lei Zhang"}, {"authorId": "66396175", "name": "D. Zhang"}]}, {"paperId": "6ce1922802169f757bbafc6e087cc274a867c763", "url": "https://www.semanticscholar.org/paper/6ce1922802169f757bbafc6e087cc274a867c763", "title": "Regularizing Neural Networks by Penalizing Confident Output Distributions", "abstract": "We systematically explore regularizing neural networks by penalizing low entropy output distributions. We show that penalizing low entropy output distributions, which has been shown to improve exploration in reinforcement learning, acts as a strong regularizer in supervised learning. Furthermore, we connect a maximum entropy based confidence penalty to label smoothing through the direction of the KL divergence. We exhaustively evaluate the proposed confidence penalty and label smoothing on 6 common benchmarks: image classification (MNIST and Cifar-10), language modeling (Penn Treebank), machine translation (WMT'14 English-to-German), and speech recognition (TIMIT and WSJ). We find that both label smoothing and the confidence penalty improve state-of-the-art models across benchmarks without modifying existing hyperparameters, suggesting the wide applicability of these regularizers.", "year": 2017, "referenceCount": 50, "citationCount": 793, "influentialCitationCount": 134, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2064737506", "name": "Gabriel Pereyra"}, {"authorId": "145499435", "name": "G. Tucker"}, {"authorId": "2292403", "name": "J. Chorowski"}, {"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId": "1695689", "name": "Geoffrey E. Hinton"}]}, {"paperId": "db68a79e59291b85e10300b79c43843b651aa195", "url": "https://www.semanticscholar.org/paper/db68a79e59291b85e10300b79c43843b651aa195", "title": "Support vector machines for classification in remote sensing", "abstract": "Support vector machines (SVM) represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of two experiments in which multi\u2010class SVMs are compared with maximum likelihood (ML) and artificial neural network (ANN) methods in terms of classification accuracy. The two land cover classification experiments use multispectral (Landsat\u20107 ETM+) and hyperspectral (DAIS) data, respectively, for test areas in eastern England and central Spain. Our results show that the SVM achieves a higher level of classification accuracy than either the ML or the ANN classifier, and that the SVM can be used with small training datasets and high\u2010dimensional data.", "year": 2005, "referenceCount": 14, "citationCount": 838, "influentialCitationCount": 40, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3639504", "name": "M. Pal"}, {"authorId": "34684035", "name": "P. Mather"}]}, {"paperId": "80c3c1cb86eeb9f2ab0934d6f914918889d34db7", "url": "https://www.semanticscholar.org/paper/80c3c1cb86eeb9f2ab0934d6f914918889d34db7", "title": "Tslearn, A Machine Learning Toolkit for Time Series Data", "abstract": "tslearn is a general-purpose Python machine learning library for time series that offers tools for pre-processing and feature extraction as well as dedicated models for clustering, classification and regression. It follows scikit-learn's Application Programming Interface for transformers and estimators, allowing the use of standard pipelines and model selection tools on top of tslearn objects. It is distributed under the BSD-2-Clause license, and its source code is available at https://github.com/tslearn-team/tslearn.", "year": 2020, "referenceCount": 22, "citationCount": 139, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2576923", "name": "R. Tavenard"}, {"authorId": "1572627978", "name": "J. Faouzi"}, {"authorId": "7876438", "name": "Gilles Vandewiele"}, {"authorId": "2083496889", "name": "Felix Divo"}, {"authorId": "2064934605", "name": "Guillaume Androz"}, {"authorId": "2109765", "name": "Chester Holtz"}, {"authorId": "2052418358", "name": "Marie Payne"}, {"authorId": "8657082", "name": "R. Yurchak"}, {"authorId": "35469144", "name": "M. Ru\u00dfwurm"}, {"authorId": "1402250456", "name": "Kushal Kolar"}, {"authorId": "102818944", "name": "E. Woods"}]}, {"paperId": "c0bc4ef587b4cbebd5839baeed95274fbf26c43a", "url": "https://www.semanticscholar.org/paper/c0bc4ef587b4cbebd5839baeed95274fbf26c43a", "title": "Applications of machine learning to diagnosis and treatment of neurodegenerative diseases", "abstract": null, "year": 2020, "referenceCount": 309, "citationCount": 114, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "5569230", "name": "Monika A. Myszczynska"}, {"authorId": "10753629", "name": "P. Ojamies"}, {"authorId": "2003831275", "name": "Alix M. B. Lacoste"}, {"authorId": "145243593", "name": "Daniel Neil"}, {"authorId": "1741702", "name": "Amir Saffari"}, {"authorId": "2059872991", "name": "R. Mead"}, {"authorId": "4080239", "name": "G. Hautbergue"}, {"authorId": "2602146", "name": "J. Holbrook"}, {"authorId": "6346151", "name": "L. Ferraiuolo"}]}, {"paperId": "9df3542f670302e101943a65362c4c6c0df727d9", "url": "https://www.semanticscholar.org/paper/9df3542f670302e101943a65362c4c6c0df727d9", "title": "Low-Rank Kernel Learning with Bregman Matrix Divergences", "abstract": "In this paper, we study low-rank matrix nearness problems, with a focus on learning low-rank positive semidefinite (kernel) matrices for machine learning applications. We propose efficient algorithms that scale linearly in the number of data points and quadratically in the rank of the input matrix. Existing algorithms for learning kernel matrices often scale poorly, with running times that are cubic in the number of data points. We employ Bregman matrix divergences as the measures of nearness---these divergences are natural for learning low-rank kernels since they preserve rank as well as positive semidefiniteness. Special cases of our framework yield faster algorithms for various existing learning problems, and experimental results demonstrate that our algorithms can effectively learn both low-rank and full-rank kernel matrices.", "year": 2009, "referenceCount": 49, "citationCount": 246, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1692670", "name": "B. Kulis"}, {"authorId": "2190189", "name": "M\u00e1ty\u00e1s A. Sustik"}, {"authorId": "1783667", "name": "I. Dhillon"}]}, {"paperId": "83996f9214d998ffb5ea8c2ea6104e96584f587b", "url": "https://www.semanticscholar.org/paper/83996f9214d998ffb5ea8c2ea6104e96584f587b", "title": "Enhanced Network Anomaly Detection Based on Deep Neural Networks", "abstract": "Due to the monumental growth of Internet applications in the last decade, the need for security of information network has increased manifolds. As a primary defense of network infrastructure, an intrusion detection system is expected to adapt to dynamically changing threat landscape. Many supervised and unsupervised techniques have been devised by researchers from the discipline of machine learning and data mining to achieve reliable detection of anomalies. Deep learning is an area of machine learning which applies neuron-like structure for learning tasks. Deep learning has profoundly changed the way we approach learning tasks by delivering monumental progress in different disciplines like speech processing, computer vision, and natural language processing to name a few. It is only relevant that this new technology must be investigated for information security applications. The aim of this paper is to investigate the suitability of deep learning approaches for anomaly-based intrusion detection system. For this research, we developed anomaly detection models based on different deep neural network structures, including convolutional neural networks, autoencoders, and recurrent neural networks. These deep models were trained on NSLKDD training data set and evaluated on both test data sets provided by NSLKDD, namely NSLKDDTest+ and NSLKDDTest21. All experiments in this paper are performed by authors on a GPU-based test bed. Conventional machine learning-based intrusion detection models were implemented using well-known classification techniques, including extreme learning machine, nearest neighbor, decision-tree, random-forest, support vector machine, naive-bays, and quadratic discriminant analysis. Both deep and conventional machine learning models were evaluated using well-known classification metrics, including receiver operating characteristics, area under curve, precision-recall curve, mean average precision and accuracy of classification. Experimental results of deep IDS models showed promising results for real-world application in anomaly detection systems.", "year": 2018, "referenceCount": 40, "citationCount": 217, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "38897515", "name": "Sheraz Naseer"}, {"authorId": "145854986", "name": "Y. Saleem"}, {"authorId": "144665620", "name": "S. Khalid"}, {"authorId": "9236457", "name": "Muhammad Khawar Bashir"}, {"authorId": "1845785166", "name": "Jihun Han"}, {"authorId": "145149104", "name": "M. Iqbal"}, {"authorId": "2149740935", "name": "Kijun Han"}]}, {"paperId": "9ecd3155ea1b5056c43c624a91e3d9523f303e74", "url": "https://www.semanticscholar.org/paper/9ecd3155ea1b5056c43c624a91e3d9523f303e74", "title": "Learning with non-positive kernels", "abstract": "In this paper we show that many kernel methods can be adapted to deal with indefinite kernels, that is, kernels which are not positive semidefinite. They do not satisfy Mercer's condition and they induce associated functional spaces called Reproducing Kernel Kre\u012dn Spaces (RKKS), a generalization of Reproducing Kernel Hilbert Spaces (RKHS).Machine learning in RKKS shares many \"nice\" properties of learning in RKHS, such as orthogonality and projection. However, since the kernels are indefinite, we can no longer minimize the loss, instead we stabilize it. We show a general representer theorem for constrained stabilization and prove generalization bounds by computing the Rademacher averages of the kernel class. We list several examples of indefinite kernels and investigate regularization methods to solve spline interpolation. Some preliminary experiments with indefinite kernels for spline smoothing are reported for truncated spectral factorization, Landweber-Fridman iterations, and MR-II.", "year": 2004, "referenceCount": 23, "citationCount": 256, "influentialCitationCount": 38, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1706780", "name": "Cheng Soon Ong"}, {"authorId": "143955806", "name": "X. Mary"}, {"authorId": "1794818", "name": "S. Canu"}, {"authorId": "46234526", "name": "Alex Smola"}]}, {"paperId": "9e7e1d962ab25ae00e6fb5713116967e9b64be50", "url": "https://www.semanticscholar.org/paper/9e7e1d962ab25ae00e6fb5713116967e9b64be50", "title": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "abstract": "Nonparametric kernel methods are widely used and proven to be successful in many statistical learning problems. Well-known examples include the kernel density estimate (KDE) for density estimation and the support vector machine (SVM) for classification. We propose a kernel classifier that optimizes the L2 or integrated squared error (ISE) of a \u201cdifference of densities\u201d. We focus on the Gaussian kernel, although the method applies to other kernels suitable for density estimation. Like a support vector machine (SVM), the classifier is sparse and results from solving a quadratic program. We provide statistical performance guarantees for the proposed L2 kernel classifier in the form of a finite sample oracle inequality, and strong consistency in the sense of both ISE and probability of error. A special case of our analysis applies to a previously introduced ISE-based method for kernel density estimation. For dimensionality greater than 15, the basic L2 kernel classifier performs poorly in practice. Thus, we extend the method through the introduction of a natural regularization parameter, which allows it to remain competitive with the SVM in high dimensions. Simulation results for both synthetic and real-world data are presented.", "year": 2022, "referenceCount": 31, "citationCount": 842, "influentialCitationCount": 82, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2061918", "name": "JooSeuk Kim"}, {"authorId": "40531965", "name": "C. Scott"}]}, {"paperId": "20244e1e5a051816159ce8c5d681a4903a2f7653", "url": "https://www.semanticscholar.org/paper/20244e1e5a051816159ce8c5d681a4903a2f7653", "title": "AXIS: Generating Explanations at Scale with Learnersourcing and Machine Learning", "abstract": "While explanations may help people learn by providing information about why an answer is correct, many problems on online platforms lack high-quality explanations. This paper presents AXIS (Adaptive eXplanation Improvement System), a system for obtaining explanations. AXIS asks learners to generate, revise, and evaluate explanations as they solve a problem, and then uses machine learning to dynamically determine which explanation to present to a future learner, based on previous learners' collective input. Results from a case study deployment and a randomized experiment demonstrate that AXIS elicits and identifies explanations that learners find helpful. Providing explanations from AXIS also objectively enhanced learning, when compared to the default practice where learners solved problems and received answers without explanations. The rated quality and learning benefit of AXIS explanations did not differ from explanations generated by an experienced instructor.", "year": 2016, "referenceCount": 26, "citationCount": 139, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1580222881", "name": "J. Williams"}, {"authorId": "1800981", "name": "Juho Kim"}, {"authorId": "1875900", "name": "Anna N. Rafferty"}, {"authorId": "2019633", "name": "Samuel G. Maldonado"}, {"authorId": "1770992", "name": "Krzysztof Z Gajos"}, {"authorId": "2598433", "name": "Walter S. Lasecki"}, {"authorId": "1686529", "name": "N. Heffernan"}]}, {"paperId": "f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556", "url": "https://www.semanticscholar.org/paper/f95f5b715eb0441ca4ee1b0fac6b4bcaaba65556", "title": "The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions", "abstract": null, "year": 2018, "referenceCount": 53, "citationCount": 1093, "influentialCitationCount": 124, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "3819689", "name": "P. Tschandl"}, {"authorId": "145770068", "name": "C. Rosendahl"}, {"authorId": "2236754", "name": "H. Kittler"}]}, {"paperId": "635cf1572f0194d900c11b6b34e41e3b85616863", "url": "https://www.semanticscholar.org/paper/635cf1572f0194d900c11b6b34e41e3b85616863", "title": "MLC++: a machine learning library in C++", "abstract": "We present MLC++, a library of C++ classes and tools for supervised machine learning. While MLC++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC++ is can attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC++ aims to solve, the design of MLC++, and the current functionality.<<ETX>>", "year": 1994, "referenceCount": 37, "citationCount": 224, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1726733", "name": "Ron Kohavi"}, {"authorId": "34929449", "name": "George H. John"}, {"authorId": "2057352235", "name": "Richard Long"}, {"authorId": "2061867819", "name": "David Manley"}, {"authorId": "30079006", "name": "Karl Pfleger"}]}, {"paperId": "3be7a0b80128dd2395d3230982dbd44d656d658e", "url": "https://www.semanticscholar.org/paper/3be7a0b80128dd2395d3230982dbd44d656d658e", "title": "Toward an Intelligent Edge: Wireless Communication Meets Machine Learning", "abstract": "The recent revival of AI is revolutionizing almost every branch of science and technology. Given the ubiquitous smart mobile gadgets and IoT devices, it is expected that a majority of intelligent applications will be deployed at the edge of wireless networks. This trend has generated strong interest in realizing an \"intelligent edge\" to support AI-enabled applications at various edge devices. Accordingly, a new research area, called edge learning, has emerged, which crosses and revolutionizes two disciplines: wireless communication and machine learning. A major theme in edge learning is to overcome the limited computing power, as well as limited data, at each edge device. This is accomplished by leveraging the mobile edge computing platform and exploiting the massive data distributed over a large number of edge devices. In such systems, learning from distributed data and communicating between the edge server and devices are two critical and coupled aspects, and their fusion poses many new research challenges. This article advocates a new set of design guidelines for wireless communication in edge learning, collectively called learning- driven communication. Illustrative examples are provided to demonstrate the effectiveness of these design guidelines. Unique research opportunities are identified.", "year": 2018, "referenceCount": 24, "citationCount": 281, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering", "Mathematics"], "authors": [{"authorId": "2957831", "name": "Guangxu Zhu"}, {"authorId": "10747375", "name": "Dongzhu Liu"}, {"authorId": "144894286", "name": "Yuqing Du"}, {"authorId": "1974803", "name": "Changsheng You"}, {"authorId": "31653740", "name": "Jun Zhang"}, {"authorId": "1697582", "name": "Kaibin Huang"}]}, {"paperId": "7da323e7103245eeaed32367c46abe3f4913df86", "url": "https://www.semanticscholar.org/paper/7da323e7103245eeaed32367c46abe3f4913df86", "title": "A survey of techniques for internet traffic classification using machine learning", "abstract": "The research community has begun looking for IP traffic classification techniques that do not rely on `well known\u00bf TCP or UDP port numbers, or interpreting the contents of packet payloads. New work is emerging on the use of statistical traffic characteristics to assist in the identification and classification process. This survey paper looks at emerging research into the application of Machine Learning (ML) techniques to IP traffic classification - an inter-disciplinary blend of IP networking and data mining techniques. We provide context and motivation for the application of ML techniques to IP traffic classification, and review 18 significant works that cover the dominant period from 2004 to early 2007. These works are categorized and reviewed according to their choice of ML strategies and primary contributions to the literature. We also discuss a number of key requirements for the employment of ML-based traffic classifiers in operational IP networks, and qualitatively critique the extent to which the reviewed works meet these requirements. Open issues and challenges in the field are also discussed.", "year": 2008, "referenceCount": 63, "citationCount": 1479, "influentialCitationCount": 83, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1716282", "name": "Thuy T. T. Nguyen"}, {"authorId": "145027304", "name": "G. Armitage"}]}, {"paperId": "a2403c1ce02120f7bd383e395b561ff7c64d52ec", "url": "https://www.semanticscholar.org/paper/a2403c1ce02120f7bd383e395b561ff7c64d52ec", "title": "A System for Massively Parallel Hyperparameter Tuning", "abstract": "Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.", "year": 2018, "referenceCount": 48, "citationCount": 170, "influentialCitationCount": 32, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "51517360", "name": "Liam Li"}, {"authorId": "40566417", "name": "Kevin G. Jamieson"}, {"authorId": "2435268", "name": "Afshin Rostamizadeh"}, {"authorId": "2308319", "name": "Ekaterina Gonina"}, {"authorId": "2086100700", "name": "Jonathan Ben-tzur"}, {"authorId": "1775622", "name": "Moritz Hardt"}, {"authorId": "9229182", "name": "B. Recht"}, {"authorId": "145532827", "name": "Ameet S. Talwalkar"}]}, {"paperId": "ba4cbda7a3a40a8d0917008e4d84890456b92038", "url": "https://www.semanticscholar.org/paper/ba4cbda7a3a40a8d0917008e4d84890456b92038", "title": "Probability machines: consistent probability estimation using nonparametric learning machines.", "abstract": "BACKGROUND\nMost machine learning approaches only provide a classification for binary responses. However, probabilities are required for risk estimation using individual patient characteristics. It has been shown recently that every statistical learning machine known to be consistent for a nonparametric regression problem is a probability machine that is provably consistent for this estimation problem.\n\n\nOBJECTIVES\nThe aim of this paper is to show how random forests and nearest neighbors can be used for consistent estimation of individual probabilities.\n\n\nMETHODS\nTwo random forest algorithms and two nearest neighbor algorithms are described in detail for estimation of individual probabilities. We discuss the consistency of random forests, nearest neighbors and other learning machines in detail. We conduct a simulation study to illustrate the validity of the methods. We exemplify the algorithms by analyzing two well-known data sets on the diagnosis of appendicitis and the diagnosis of diabetes in Pima Indians.\n\n\nRESULTS\nSimulations demonstrate the validity of the method. With the real data application, we show the accuracy and practicality of this approach. We provide sample code from R packages in which the probability estimation is already available. This means that all calculations can be performed using existing software.\n\n\nCONCLUSIONS\nRandom forest algorithms as well as nearest neighbor approaches are valid machine learning methods for estimating individual probabilities for binary responses. Freely available implementations are available in R and may be used for applications.", "year": 2011, "referenceCount": 36, "citationCount": 162, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1753856", "name": "J. Malley"}, {"authorId": "48740962", "name": "J. Kruppa"}, {"authorId": "2057370798", "name": "A. Dasgupta"}, {"authorId": "49547346", "name": "K. Malley"}, {"authorId": "144071896", "name": "A. Ziegler"}]}, {"paperId": "f5e04e45416dbfb66c3e132570f897d67f13d474", "url": "https://www.semanticscholar.org/paper/f5e04e45416dbfb66c3e132570f897d67f13d474", "title": "Using GPUs for machine learning algorithms", "abstract": "Using dedicated hardware to do machine learning typically ends up in disaster because of cost, obsolescence, and poor software. The popularization of graphic processing units (GPUs), which are now available on every PC, provides an attractive alternative. We propose a generic 2-layer fully connected neural network GPU implementation which yields over 3/spl times/ speedup for both training and testing with respect to a 3 GHz P4 CPU.", "year": 2005, "referenceCount": 12, "citationCount": 218, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3265929", "name": "Dave Steinkrau"}, {"authorId": "2812486", "name": "P. Simard"}, {"authorId": "144437216", "name": "I. Buck"}]}, {"paperId": "93cb06180743fa648d844b9e7883b62468921c84", "url": "https://www.semanticscholar.org/paper/93cb06180743fa648d844b9e7883b62468921c84", "title": "Pattern Recognition and Neural Networks", "abstract": "From the Publisher: \nPattern recognition has long been studied in relation to many different (and mainly unrelated) applications, such as remote sensing, computer vision, space research, and medical imaging. In this book Professor Ripley brings together two crucial ideas in pattern recognition; statistical methods and machine learning via neural networks. Unifying principles are brought to the fore, and the author gives an overview of the state of the subject. Many examples are included to illustrate real problems in pattern recognition and how to overcome them.This is a self-contained account, ideal both as an introduction for non-specialists readers, and also as a handbook for the more expert reader.", "year": 1996, "referenceCount": 113, "citationCount": 2856, "influentialCitationCount": 63, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2122942", "name": "B. Ripley"}]}, {"paperId": "16e22b6ab82096eab926228b83d20936c0c249dc", "url": "https://www.semanticscholar.org/paper/16e22b6ab82096eab926228b83d20936c0c249dc", "title": "Self-Adaptive Evolutionary Extreme Learning Machine", "abstract": null, "year": 2012, "referenceCount": 27, "citationCount": 232, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39874278", "name": "Jiuwen Cao"}, {"authorId": "145558868", "name": "Zhiping Lin"}, {"authorId": "145678691", "name": "G. Huang"}]}, {"paperId": "82d2170d0a40c87fdd06f825339bc3c71fe56508", "url": "https://www.semanticscholar.org/paper/82d2170d0a40c87fdd06f825339bc3c71fe56508", "title": "Unsupervised extreme learning machine with representational features", "abstract": null, "year": 2017, "referenceCount": 24, "citationCount": 77, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2066430561", "name": "Shifei Ding"}, {"authorId": "47899254", "name": "Nan Zhang"}, {"authorId": "2151811677", "name": "Jian Zhang"}, {"authorId": "29279000", "name": "Xinzheng Xu"}, {"authorId": "145000409", "name": "Zhongzhi Shi"}]}, {"paperId": "dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd", "url": "https://www.semanticscholar.org/paper/dab261b25ff8ccd2c9144a5cb3a46b39ac0ac4bd", "title": "Troubling Trends in Machine Learning Scholarship", "abstract": "Flawed scholarship threatens to mislead the public and stymie future research by compromising ML\u2019s intellectual foundations. Indeed, many of these problems have recurred cyclically throughout the history of AI and, more broadly, in scientific research. In 1976, Drew McDermott chastised the AI community for abandoning self-discipline, warning prophetically that \"if we can\u2019t criticize ourselves, someone else will save us the trouble.\" The current strength of machine learning owes to a large body of rigorous research to date, both theoretical and empirical. By promoting clear scientific thinking and communication, our community can sustain the trust and investment it currently enjoys.", "year": 2018, "referenceCount": 89, "citationCount": 224, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "32219137", "name": "Zachary Chase Lipton"}, {"authorId": "5164568", "name": "J. Steinhardt"}]}, {"paperId": "a43e61f7900c64e7f3bf18ef7ff5770295149b37", "url": "https://www.semanticscholar.org/paper/a43e61f7900c64e7f3bf18ef7ff5770295149b37", "title": "Machine Learning, a Probabilistic Perspective", "abstract": "ISBN-13: 978-0262018029 The book also introduces the notion of a Bayesian likelihood function (p.228), which \u201cdiffers slightly from that in classical statistics.\u201d The only difference I can spot is in the interpretation: Both functions of (\u03b8, x) are numerically the same. Overall, the chapter on Bayesian inference does not spend much time on prior specification. There is a section on conjugate priors that does not mention picking the hyperparameters. While improper priors are introduced as limits of proper priors and as conveying \u201cthe least amount of information about [the parameters]\u201d (p.236), the difficulty in using improper priors for hypothesis testing is not mentioned. Both Chib\u2019s method and the Savage-Dickey density ratio are suggested for the approximation of marginal likelihoods.", "year": 2014, "referenceCount": 0, "citationCount": 187, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145155783", "name": "C. Robert"}]}, {"paperId": "561b15685fb900593f43ac8f7e615a73bb4ff965", "url": "https://www.semanticscholar.org/paper/561b15685fb900593f43ac8f7e615a73bb4ff965", "title": "Artificial intelligence, machine learning and health systems", "abstract": "Artificial Intelligence and machine learning have the potential to be the catalyst for transformation of health systems to improve efficiency and effectiveness, create headroom for universal health coverage and improve outcomes.", "year": 2018, "referenceCount": 20, "citationCount": 209, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Psychology"], "authors": [{"authorId": "3264383", "name": "T. Panch"}, {"authorId": "1679873", "name": "Peter Szolovits"}, {"authorId": "40144701", "name": "R. Atun"}]}, {"paperId": "78947497cbbffc691aac3f590d972130259af9ce", "url": "https://www.semanticscholar.org/paper/78947497cbbffc691aac3f590d972130259af9ce", "title": "Distance Metric Learning for Large Margin Nearest Neighbor Classification", "abstract": "The accuracy of k-nearest neighbor (kNN) classification depends significantly on the metric used to compute distances between different examples. In this paper, we show how to learn a Mahalanobis distance metric for kNN classification from labeled examples. The Mahalanobis metric can equivalently be viewed as a global linear transformation of the input space that precedes kNN classification using Euclidean distances. In our approach, the metric is trained with the goal that the k-nearest neighbors always belong to the same class while examples from different classes are separated by a large margin. As in support vector machines (SVMs), the margin criterion leads to a convex optimization based on the hinge loss. Unlike learning in SVMs, however, our approach requires no modification or extension for problems in multiway (as opposed to binary) classification. In our framework, the Mahalanobis distance metric is obtained as the solution to a semidefinite program. On several data sets of varying size and difficulty, we find that metrics trained in this way lead to significant improvements in kNN classification. Sometimes these results can be further improved by clustering the training examples and learning an individual metric within each cluster. We show how to learn and combine these local metrics in a globally integrated manner.", "year": 2005, "referenceCount": 34, "citationCount": 5238, "influentialCitationCount": 741, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "7446832", "name": "Kilian Q. Weinberger"}, {"authorId": "1796044", "name": "L. Saul"}]}, {"paperId": "1d720f25d5d2c6dcaebfe108ead3036b7fe06940", "url": "https://www.semanticscholar.org/paper/1d720f25d5d2c6dcaebfe108ead3036b7fe06940", "title": "CoCoA: A General Framework for Communication-Efficient Distributed Optimization", "abstract": "The scale of modern datasets necessitates the development of efficient distributed optimization methods for machine learning. We present a general-purpose framework for distributed computing environments, CoCoA, that has an efficient communication scheme and is applicable to a wide variety of problems in machine learning and signal processing. We extend the framework to cover general non-strongly-convex regularizers, including L1-regularized problems like lasso, sparse logistic regression, and elastic net regularization, and show how earlier work can be derived as a special case. We provide convergence guarantees for the class of convex regularized loss minimization objectives, leveraging a novel approach in handling non-strongly-convex regularizers and non-smooth loss functions. The resulting framework has markedly improved performance over state-of-the-art methods, as we illustrate with an extensive set of experiments on real distributed datasets.", "year": 2016, "referenceCount": 80, "citationCount": 214, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145260024", "name": "Virginia Smith"}, {"authorId": "2820634", "name": "Simone Forte"}, {"authorId": "2008023", "name": "Chenxin Ma"}, {"authorId": "144696183", "name": "Martin Tak\u00e1c"}, {"authorId": "1694621", "name": "Michael I. Jordan"}, {"authorId": "2456863", "name": "Martin Jaggi"}]}, {"paperId": "806c2c4327a31fded64a5d673ab82b133194c234", "url": "https://www.semanticscholar.org/paper/806c2c4327a31fded64a5d673ab82b133194c234", "title": "Introduction to information retrieval", "abstract": "Class-tested and coherent, this textbook teaches classical and web information retrieval, including web search and the related areas of text classification and text clustering from basic concepts. It gives an up-to-date treatment of all aspects of the design and implementation of systems for gathering, indexing, and searching documents; methods for evaluating systems; and an introduction to the use of machine learning methods on text collections. All the important ideas are explained using examples and figures, making it perfect for introductory courses in information retrieval for advanced undergraduates and graduate students in computer science. Based on feedback from extensive classroom experience, the book has been carefully structured in order to make teaching more natural and effective. Slides and additional exercises (with solutions for lecturers) are also available through the book's supporting website to help course instructors prepare their lectures.", "year": 2008, "referenceCount": 0, "citationCount": 8486, "influentialCitationCount": 810, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144783904", "name": "Christopher D. Manning"}, {"authorId": "145503401", "name": "P. Raghavan"}, {"authorId": "144418438", "name": "Hinrich Sch\u00fctze"}]}, {"paperId": "e316ada2fd1717d49d4c3c84681325ae8665785d", "url": "https://www.semanticscholar.org/paper/e316ada2fd1717d49d4c3c84681325ae8665785d", "title": "Learning with genetic algorithms: An overview", "abstract": null, "year": 1988, "referenceCount": 42, "citationCount": 223, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2069839824", "name": "K. A. Jong"}]}, {"paperId": "34ebde68c7bbc5c95eacd212e7277e529536e922", "url": "https://www.semanticscholar.org/paper/34ebde68c7bbc5c95eacd212e7277e529536e922", "title": "Machine Learning Approaches to Manufacturing", "abstract": null, "year": 1996, "referenceCount": 20, "citationCount": 189, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "152236629", "name": "L. Monostori"}, {"authorId": "144272272", "name": "A. M\u00e1rkus"}, {"authorId": "9280443", "name": "H. V. Brussel"}, {"authorId": "97641597", "name": "E. Westk\u00e4mpfer"}]}, {"paperId": "eef183687fab4d762a381f2e80e357e08e923f0a", "url": "https://www.semanticscholar.org/paper/eef183687fab4d762a381f2e80e357e08e923f0a", "title": "Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning", "abstract": "The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-one-out cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F-test 5x2 cross-validation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.", "year": 2018, "referenceCount": 35, "citationCount": 400, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2562040", "name": "S. Raschka"}]}, {"paperId": "f39178f2a59fa0f62fcfc29e5fcbb60eaae91ceb", "url": "https://www.semanticscholar.org/paper/f39178f2a59fa0f62fcfc29e5fcbb60eaae91ceb", "title": "Argument based machine learning", "abstract": null, "year": 2006, "referenceCount": 139, "citationCount": 145, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145703442", "name": "M. Mozina"}, {"authorId": "3057356", "name": "J. Zabkar"}, {"authorId": "1725040", "name": "I. Bratko"}]}, {"paperId": "b911e5e9c190b8338f5fab2fd5cf23b577e6fbf9", "url": "https://www.semanticscholar.org/paper/b911e5e9c190b8338f5fab2fd5cf23b577e6fbf9", "title": "Preventing Student Dropout in Distance Learning Using Machine Learning Techniques", "abstract": null, "year": 2003, "referenceCount": 19, "citationCount": 192, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697867", "name": "S. Kotsiantis"}, {"authorId": "1744804", "name": "C. Pierrakeas"}, {"authorId": "1725504", "name": "P. Pintelas"}]}, {"paperId": "d3f788ee95e16dac7a2e4d65aa095199bbc3439f", "url": "https://www.semanticscholar.org/paper/d3f788ee95e16dac7a2e4d65aa095199bbc3439f", "title": "An Empirical Comparison of Machine Learning Models for Time Series Forecasting", "abstract": "In this work we present a large scale comparison study for the major machine learning models for time series forecasting. Specifically, we apply the models on the monthly M3 time series competition data (around a thousand time series). There have been very few, if any, large scale comparison studies for machine learning models for the regression or the time series forecasting problems, so we hope this study would fill this gap. The models considered are multilayer perceptron, Bayesian neural networks, radial basis functions, generalized regression neural networks (also called kernel regression), K-nearest neighbor regression, CART regression trees, support vector regression, and Gaussian processes. The study reveals significant differences between the different methods. The best two methods turned out to be the multilayer perceptron and the Gaussian process regression. In addition to model comparisons, we have tested different preprocessing methods and have shown that they have different impacts on the performance.", "year": 2010, "referenceCount": 88, "citationCount": 515, "influentialCitationCount": 37, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47699955", "name": "Nesreen Ahmed"}, {"authorId": "1686506", "name": "A. Atiya"}, {"authorId": "9317897", "name": "N. E. Gayar"}, {"authorId": "1398534738", "name": "H. El-Shishiny"}]}, {"paperId": "7333e127b62eb545d81830df2a66b98c0693a32b", "url": "https://www.semanticscholar.org/paper/7333e127b62eb545d81830df2a66b98c0693a32b", "title": "Quantile Regression Forests", "abstract": "Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power.", "year": 2006, "referenceCount": 20, "citationCount": 1199, "influentialCitationCount": 176, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1941834", "name": "N. Meinshausen"}]}, {"paperId": "4ce2f55585f3156e332721b8ab4f449389dc2a3c", "url": "https://www.semanticscholar.org/paper/4ce2f55585f3156e332721b8ab4f449389dc2a3c", "title": "Hidden stratification causes clinically meaningful failures in machine learning for medical imaging", "abstract": "Machine learning models for medical image analysis often suffer from poor performance on important subsets of a population that are not identified during training or testing. For example, overall performance of a cancer detection model may be high, but the model may still consistently miss a rare but aggressive cancer subtype. We refer to this problem as hidden stratification, and observe that it results from incompletely describing the meaningful variation in a dataset. While hidden stratification can substantially reduce the clinical efficacy of machine learning models, its effects remain difficult to measure. In this work, we assess the utility of several possible techniques for measuring hidden stratification effects, and characterize these effects both via synthetic experiments on the CIFAR-100 benchmark dataset and on multiple real-world medical imaging datasets. Using these measurement techniques, we find evidence that hidden stratification can occur in unidentified imaging subsets with low prevalence, low label quality, subtle distinguishing features, or spurious correlates, and that it can result in relative performance differences of over 20% on clinically important subsets. Finally, we discuss the clinical implications of our findings, and suggest that evaluation of hidden stratification should be a critical component of any machine learning deployment in medical imaging.", "year": 2019, "referenceCount": 60, "citationCount": 188, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "1404268029", "name": "L. Oakden-Rayner"}, {"authorId": "12322385", "name": "J. Dunnmon"}, {"authorId": "145575177", "name": "G. Carneiro"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}]}, {"paperId": "f1d73520d51a0f19c686d9f4bf7af1db33763a2a", "url": "https://www.semanticscholar.org/paper/f1d73520d51a0f19c686d9f4bf7af1db33763a2a", "title": "A support vector machine approach for detection of microcalcifications", "abstract": "We investigate an approach based on support vector machines (SVMs) for detection of microcalcification (MC) clusters in digital mammograms, and propose a successive enhancement learning scheme for improved performance. SVM is a machine-learning method, based on the principle of structural risk minimization, which performs well when applied to data outside the training set. We formulate MC detection as a supervised-learning problem and apply SVM to develop the detection algorithm. We use the SVM to detect at each location in the image whether an MC is present or not. We tested the proposed method using a database of 76 clinical mammograms containing 1120 MCs. We use free-response receiver operating characteristic curves to evaluate detection performance, and compare the proposed algorithm with several existing methods. In our experiments, the proposed SVM framework outperformed all the other methods tested. In particular, a sensitivity as high as 94% was achieved by the SVM method at an error rate of one false-positive cluster per image. The ability of SVM to outperform several well-known methods developed for the widely studied problem of MC detection suggests that SVM is a promising technique for object detection in a medical imaging application.", "year": 2002, "referenceCount": 37, "citationCount": 559, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "11895669", "name": "I. El-Naqa"}, {"authorId": "46286398", "name": "Yongyi Yang"}, {"authorId": "3067670", "name": "M. Wernick"}, {"authorId": "143949487", "name": "N. Galatsanos"}, {"authorId": "1781483", "name": "R. Nishikawa"}]}, {"paperId": "7fcb90f68529cbfab49f471b54719ded7528d0ef", "url": "https://www.semanticscholar.org/paper/7fcb90f68529cbfab49f471b54719ded7528d0ef", "title": "Federated Learning: Strategies for Improving Communication Efficiency", "abstract": "Federated Learning is a machine learning setting where the goal is to train a high-quality centralized model while training data remains distributed over a large number of clients each with unreliable and relatively slow network connections. We consider learning algorithms for this setting where on each round, each client independently computes an update to the current model based on its local data, and communicates this update to a central server, where the client-side updates are aggregated to compute a new global model. The typical clients in this setting are mobile phones, and communication efficiency is of the utmost importance. In this paper, we propose two ways to reduce the uplink communication costs: structured updates, where we directly learn an update from a restricted space parametrized using a smaller number of variables, e.g. either low-rank or a random mask; and sketched updates, where we learn a full model update and then compress it using a combination of quantization, random rotations, and subsampling before sending it to the server. Experiments on both convolutional and recurrent networks show that the proposed methods can reduce the communication cost by two orders of magnitude.", "year": 2016, "referenceCount": 26, "citationCount": 2515, "influentialCitationCount": 222, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32139366", "name": "Jakub Konecn\u00fd"}, {"authorId": "145057514", "name": "H. B. McMahan"}, {"authorId": "1815972", "name": "Felix X. Yu"}, {"authorId": "2662221", "name": "Peter Richt\u00e1rik"}, {"authorId": "9486035", "name": "A. Suresh"}, {"authorId": "36577444", "name": "D. Bacon"}]}, {"paperId": "2a1724427b9c473e73f6abff648e7416269d3a68", "url": "https://www.semanticscholar.org/paper/2a1724427b9c473e73f6abff648e7416269d3a68", "title": "Cooperative Multi-Agent Learning: The State of the Art", "abstract": null, "year": 2005, "referenceCount": 420, "citationCount": 1252, "influentialCitationCount": 69, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1703826", "name": "Liviu Panait"}, {"authorId": "1706276", "name": "S. Luke"}]}, {"paperId": "ce2ff569d69942d81b7975a3a8104dd276428ce2", "url": "https://www.semanticscholar.org/paper/ce2ff569d69942d81b7975a3a8104dd276428ce2", "title": "Comprehensive modeling of microRNA targets predicts functional non-conserved and non-canonical sites", "abstract": null, "year": 2010, "referenceCount": 42, "citationCount": 1462, "influentialCitationCount": 77, "isOpenAccess": true, "fieldsOfStudy": ["Biology", "Medicine"], "authors": [{"authorId": "2612553", "name": "Doron Betel"}, {"authorId": "1897441", "name": "Anjali J. Koppal"}, {"authorId": "3281341", "name": "P. Agius"}, {"authorId": "144882390", "name": "C. Sander"}, {"authorId": "3209133", "name": "C. Leslie"}]}, {"paperId": "bab1753d993c0c028a0fa729569b8c1528af4fe8", "url": "https://www.semanticscholar.org/paper/bab1753d993c0c028a0fa729569b8c1528af4fe8", "title": "TensorFlow.js: Machine Learning for the Web and Beyond", "abstract": "TensorFlow.js is a library for building and executing machine learning algorithms in JavaScript. TensorFlow.js models run in a web browser and in the Node.js environment. The library is part of the TensorFlow ecosystem, providing a set of APIs that are compatible with those in Python, allowing models to be ported between the Python and JavaScript ecosystems. TensorFlow.js has empowered a new set of developers from the extensive JavaScript community to build and deploy machine learning models and enabled new classes of on-device computation. This paper describes the design, API, and implementation of TensorFlow.js, and highlights some of the impactful use cases.", "year": 2019, "referenceCount": 57, "citationCount": 107, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2842148", "name": "D. Smilkov"}, {"authorId": "144203200", "name": "Nikhil Thorat"}, {"authorId": "2867282", "name": "Yannick Assogba"}, {"authorId": "2061016887", "name": "Ann Yuan"}, {"authorId": "66420904", "name": "Nick Kreeger"}, {"authorId": "144909203", "name": "Ping Yu"}, {"authorId": "2145459759", "name": "Kangyi Zhang"}, {"authorId": "49406880", "name": "Shanqing Cai"}, {"authorId": "2055012116", "name": "Eric Nielsen"}, {"authorId": "46773550", "name": "David Soergel"}, {"authorId": "1747918", "name": "S. Bileschi"}, {"authorId": "49427600", "name": "Michael Terry"}, {"authorId": "2053019791", "name": "Charles Nicholson"}, {"authorId": "2118973495", "name": "Sandeep N. Gupta"}, {"authorId": "39876485", "name": "S. Sirajuddin"}, {"authorId": "1733143", "name": "D. Sculley"}, {"authorId": "3089272", "name": "R. Monga"}, {"authorId": "32131713", "name": "G. Corrado"}, {"authorId": "1765169", "name": "F. Vi\u00e9gas"}, {"authorId": "145233583", "name": "M. Wattenberg"}]}, {"paperId": "9008cdacbdcff8a218a6928e94fe7c6dfc237b24", "url": "https://www.semanticscholar.org/paper/9008cdacbdcff8a218a6928e94fe7c6dfc237b24", "title": "Training support vector machines: an application to face detection", "abstract": "We investigate the application of Support Vector Machines (SVMs) in computer vision. SVM is a learning technique developed by V. Vapnik and his team (AT&T Bell Labs., 1985) that can be seen as a new method for training polynomial, neural network, or Radial Basis Functions classifiers. The decision surfaces are found by solving a linearly constrained quadratic programming problem. This optimization problem is challenging because the quadratic form is completely dense and the memory requirements grow with the square of the number of data points. We present a decomposition algorithm that guarantees global optimality, and can be used to train SVM's over very large data sets. The main idea behind the decomposition is the iterative solution of sub-problems and the evaluation of optimality conditions which are used both to generate improved iterative values, and also establish the stopping criteria for the algorithm. We present experimental results of our implementation of SVM, and demonstrate the feasibility of our approach on a face detection problem that involves a data set of 50,000 data points.", "year": 1997, "referenceCount": 19, "citationCount": 2891, "influentialCitationCount": 110, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1781874", "name": "E. Osuna"}, {"authorId": "1771659", "name": "R. Freund"}, {"authorId": "1804489", "name": "F. Girosi"}]}, {"paperId": "60e28c7da56eb61dd8ddb710a6f079ef02668014", "url": "https://www.semanticscholar.org/paper/60e28c7da56eb61dd8ddb710a6f079ef02668014", "title": "Selecting Features for Intrusion Detection: A Feature Relevance Analysis on KDD 99", "abstract": "KDD 99 intrusion detection datasets, which are based on DARPA 98 dataset, provides labeled data for researchers working in the field of intrusion detection and is the only labeled dataset publicly available. Numerous researchers employed the datasets in KDD 99 intrusion detection competition to study the utilization of machine learning for intrusion detection and reported detection rates up to 91% with false positive rates less than 1%. To substantiate the performance of machine learning based detectors that are trained on KDD 99 training data; we investigate the relevance of each feature in KDD 99 intrusion detection datasets. To this end, information gain is employed to determine the most discriminating features for each class.", "year": 2005, "referenceCount": 12, "citationCount": 413, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2595532", "name": "H. G. Kayacik"}, {"authorId": "1398261759", "name": "A. N. Zincir-Heywood"}, {"authorId": "1796941", "name": "M. Heywood"}]}, {"paperId": "d310118dcbd5c72eb8e009ace9449083dcd7f340", "url": "https://www.semanticscholar.org/paper/d310118dcbd5c72eb8e009ace9449083dcd7f340", "title": "Machine learning modeling of superconducting critical temperature", "abstract": null, "year": 2017, "referenceCount": 77, "citationCount": 215, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Mathematics", "Computer Science", "Materials Science"], "authors": [{"authorId": "7825795", "name": "V. Stanev"}, {"authorId": "10438178", "name": "C. Oses"}, {"authorId": "4833974", "name": "A. Kusne"}, {"authorId": "121443103", "name": "Efrain Rodriguez"}, {"authorId": "6262446", "name": "J. Paglione"}, {"authorId": "3445901", "name": "S. Curtarolo"}, {"authorId": "3046458", "name": "I. Takeuchi"}]}, {"paperId": "c7ce37d51ba01e17c9291f1dbf2ca8223b948b7f", "url": "https://www.semanticscholar.org/paper/c7ce37d51ba01e17c9291f1dbf2ca8223b948b7f", "title": "Machine learning for Internet of Things data analysis: A survey", "abstract": null, "year": 2017, "referenceCount": 158, "citationCount": 638, "influentialCitationCount": 24, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35838436", "name": "M. Mahdavinejad"}, {"authorId": "143976214", "name": "M. Rezvan"}, {"authorId": "39393520", "name": "M. Barekatain"}, {"authorId": "49277935", "name": "Peyman Adibi"}, {"authorId": "1740880", "name": "P. Barnaghi"}, {"authorId": "144463965", "name": "A. Sheth"}]}, {"paperId": "bb35ef89addbbc28d960bc0cab70d8a29fdf6eee", "url": "https://www.semanticscholar.org/paper/bb35ef89addbbc28d960bc0cab70d8a29fdf6eee", "title": "A Survey on Multi-Task Learning", "abstract": "Multi-Task Learning (MTL) is a learning paradigm in machine learning and its aim is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. In this paper, we give a survey for MTL from the perspective of algorithmic modeling, applications and theoretical analyses. For algorithmic modeling, we give a definition of MTL and then classify different MTL algorithms into five categories, including feature learning approach, low-rank approach, task clustering approach, task relation learning approach and decomposition approach as well as discussing the characteristics of each approach. In order to improve the performance of learning tasks further, MTL can be combined with other learning paradigms including semi-supervised learning, active learning, unsupervised learning, reinforcement learning, multi-view learning and graphical models. When the number of tasks is large or the data dimensionality is high, we review online, parallel and distributed MTL models as well as dimensionality reduction and feature hashing to reveal their computational and storage advantages. Many real-world applications use MTL to boost their performance and we review representative works in this paper. Finally, we present theoretical analyses and discuss several future directions for MTL.", "year": 2017, "referenceCount": 277, "citationCount": 993, "influentialCitationCount": 62, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "46867608", "name": "Yu Zhang"}, {"authorId": "152290618", "name": "Qiang Yang"}]}, {"paperId": "c9887b9e16380884f2ba6568669883884163551a", "url": "https://www.semanticscholar.org/paper/c9887b9e16380884f2ba6568669883884163551a", "title": "Machine learning for image based species identification", "abstract": "Accurate species identification is the basis for all aspects of taxonomic research and is an essential component of workflows in biological research. Biologists are asking for more efficient methods to meet the identification demand. Smart mobile devices, digital cameras as well as the mass digitisation of natural history collections led to an explosion of openly available image data depicting living organisms. This rapid increase in biological image data in combination with modern machine learning methods, such as deep learning, offers tremendous opportunities for automated species identification. In this paper, we focus on deep learning neural networks as a technology that enabled breakthroughs in automated species identification in the last 2 years. In order to stimulate more work in this direction, we provide a brief overview of machine learning frameworks applicable to the species identification problem. We review selected deep learning approaches for image based species identification and introduce publicly available applications. Eventually, this article aims to provide insights into the current state\u2010of\u2010the\u2010art in automated identification and to serve as a starting point for researchers willing to apply novel machine learning techniques in their biological studies. While modern machine learning approaches only slowly pave their way into the field of species identification, we argue that we are going to see a proliferation of these techniques being applied to the problem in the future. Artificial intelligence systems will provide alternative tools for taxonomic identification in the near future.", "year": 2018, "referenceCount": 47, "citationCount": 166, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50602098", "name": "Jana W\u00e4ldchen"}, {"authorId": "35215848", "name": "Patrick M\u00e4der"}]}, {"paperId": "8dd4a8eefa366b1b7d2471c1b8580df5bea23924", "url": "https://www.semanticscholar.org/paper/8dd4a8eefa366b1b7d2471c1b8580df5bea23924", "title": "Machine learning based phishing detection from URLs", "abstract": null, "year": 2019, "referenceCount": 35, "citationCount": 294, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2594473", "name": "O. K. Sahingoz"}, {"authorId": "2657614", "name": "Ebubekir Buber"}, {"authorId": "39094163", "name": "\u00d6nder Demir"}, {"authorId": "3278786", "name": "B. Diri"}]}, {"paperId": "910e3118b50f426e5e840561e1f9f1fdd67e5724", "url": "https://www.semanticscholar.org/paper/910e3118b50f426e5e840561e1f9f1fdd67e5724", "title": "MATHEMATICS FOR MACHINE LEARNING", "abstract": "Machine learning is a way to study the algorithm and statistical model that is used by computer to perform a specific task through pattern and deduction [1]. It builds a mathematical model from a sample data which may come under either supervised or unsupervised learning. It is closely\n related to computational statistics which is an interface between statistics and computer science. Also, linear algebra and probability theory are two tools of mathematics which form the basis of machine learning. In general, statistics is a science concerned with collecting, analysing, interpreting\n the data. Data are the facts and figure that can be classified as either quantitative or qualitative. From the given set of data, we can predict the expected observation, difference between the outcome of two observations and how data look like which can help in better decision making process\n [2]. Descriptive and inferential statistics are the two methods of data analysis. Descriptive statistics summarize the raw data into information through which common expectation and variation of data can be taken. It also provides graphical methods that can be used to visualize the sample\n of data and qualitative understanding of observation whereas inferential statistics refers to drawing conclusions from data. Inferences are made under the framework of probability theory. So, understanding of data and interpretation of result are two important aspects of machine learning.\n In this paper, we have reviewed the different methods of ML, mathematics behind ML, its application in day to day life and future aspects.", "year": 2020, "referenceCount": 8, "citationCount": 110, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "48700773", "name": "Gaurav Kumar"}, {"authorId": "2128150752", "name": "Rishav Banerjee"}, {"authorId": "2127420955", "name": "Deepak Kr Singh"}, {"authorId": "2127639930", "name": "Nitesh Choubey"}, {"authorId": "2127607412", "name": "Arnaw"}]}, {"paperId": "08dc94471605308669c8d3d8284ba94fcc93e345", "url": "https://www.semanticscholar.org/paper/08dc94471605308669c8d3d8284ba94fcc93e345", "title": "Deep Learning in Microscopy Image Analysis: A Survey", "abstract": "Computerized microscopy image analysis plays an important role in computer aided diagnosis and prognosis. Machine learning techniques have powered many aspects of medical investigation and clinical practice. Recently, deep learning is emerging as a leading machine learning tool in computer vision and has attracted considerable attention in biomedical image analysis. In this paper, we provide a snapshot of this fast-growing field, specifically for microscopy image analysis. We briefly introduce the popular deep neural networks and summarize current deep learning achievements in various tasks, such as detection, segmentation, and classification in microscopy image analysis. In particular, we explain the architectures and the principles of convolutional neural networks, fully convolutional networks, recurrent neural networks, stacked autoencoders, and deep belief networks, and interpret their formulations or modelings for specific tasks on various microscopy images. In addition, we discuss the open challenges and the potential trends of future research in microscopy image analysis using deep learning.", "year": 2018, "referenceCount": 204, "citationCount": 149, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2082604", "name": "F. Xing"}, {"authorId": "1877955", "name": "Yuanpu Xie"}, {"authorId": "143729223", "name": "H. Su"}, {"authorId": "12217443", "name": "Fujun Liu"}, {"authorId": "29163569", "name": "Lin Yang"}]}, {"paperId": "8ace1951c95f9bbbcf83571ff6c0579521507e2e", "url": "https://www.semanticscholar.org/paper/8ace1951c95f9bbbcf83571ff6c0579521507e2e", "title": "The adverse effects of code duplication in machine learning models of code", "abstract": "The field of big code relies on mining large corpora of code to perform some learning task towards creating better tools for software engineers. A significant threat to this approach was recently identified by Lopes et al. (2017) who found a large amount of near-duplicate code on GitHub. However, the impact of code duplication has not been noticed by researchers devising machine learning models for source code. In this work, we explore the effects of code duplication on machine learning models showing that reported performance metrics are sometimes inflated by up to 100% when testing on duplicated code corpora compared to the performance on de-duplicated corpora which more accurately represent how machine learning models of code are used by software engineers. We present a duplication index for widely used datasets, list best practices for collecting code corpora and evaluating machine learning models on them. Finally, we release tools to help the community avoid this problem in future research.", "year": 2018, "referenceCount": 28, "citationCount": 159, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3216345", "name": "Miltiadis Allamanis"}]}, {"paperId": "c21ce2fd906ef6b5c669d55458bd956155eda14f", "url": "https://www.semanticscholar.org/paper/c21ce2fd906ef6b5c669d55458bd956155eda14f", "title": "Machine Learning and Data Mining; Methods and Applications", "abstract": "From the Publisher: \nMaster the new computational tools to get the most out of your information system. \nThis practical guide, the first to clearly outline the situation for the benefit of engineers and scientists, provides a straightforward introduction to basic machine learning and data mining methods, covering the analysis of numerical, text, and sound data.", "year": 1998, "referenceCount": 5, "citationCount": 395, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2421006", "name": "R. Michalski"}, {"authorId": "1725040", "name": "I. Bratko"}, {"authorId": "66327168", "name": "Avan Bratko"}]}, {"paperId": "f70b2f20be241f445a61f33c4b8e76e554760340", "url": "https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340", "title": "Software Engineering for Machine Learning: A Case Study", "abstract": "Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.", "year": 2019, "referenceCount": 37, "citationCount": 403, "influentialCitationCount": 64, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1719124", "name": "Saleema Amershi"}, {"authorId": "1776779", "name": "A. Begel"}, {"authorId": "145193818", "name": "C. Bird"}, {"authorId": "1710751", "name": "R. DeLine"}, {"authorId": "50355692", "name": "H. Gall"}, {"authorId": "1783184", "name": "Ece Kamar"}, {"authorId": "1693689", "name": "Nachiappan Nagappan"}, {"authorId": "2571049", "name": "Besmira Nushi"}, {"authorId": "143609903", "name": "T. Zimmermann"}]}, {"paperId": "42a7f3e90aa4929d7f01b6124b87f82f7735aa88", "url": "https://www.semanticscholar.org/paper/42a7f3e90aa4929d7f01b6124b87f82f7735aa88", "title": "Learn to Grow: A Continual Structure Learning Framework for Overcoming Catastrophic Forgetting", "abstract": "Addressing catastrophic forgetting is one of the key challenges in continual learning where machine learning systems are trained with sequential or streaming tasks. Despite recent remarkable progress in state-of-the-art deep learning, deep neural networks (DNNs) are still plagued with the catastrophic forgetting problem. This paper presents a conceptually simple yet general and effective framework for handling catastrophic forgetting in continual learning with DNNs. The proposed method consists of two components: a neural structure optimization component and a parameter learning and/or fine-tuning component. By separating the explicit neural structure learning and the parameter estimation, not only is the proposed method capable of evolving neural structures in an intuitively meaningful way, but also shows strong capabilities of alleviating catastrophic forgetting in experiments. Furthermore, the proposed method outperforms all other baselines on the permuted MNIST dataset, the split CIFAR100 dataset and the Visual Domain Decathlon dataset in continual learning setting.", "year": 2019, "referenceCount": 30, "citationCount": 200, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2145429743", "name": "Xilai Li"}, {"authorId": "34872128", "name": "Yingbo Zhou"}, {"authorId": "47353858", "name": "Tianfu Wu"}, {"authorId": "2166511", "name": "R. Socher"}, {"authorId": "2228109", "name": "Caiming Xiong"}]}, {"paperId": "0a8149fb5aa8a5684e7d530c264451a5cb9250f5", "url": "https://www.semanticscholar.org/paper/0a8149fb5aa8a5684e7d530c264451a5cb9250f5", "title": "Recent Advances in Hierarchical Reinforcement Learning", "abstract": null, "year": 2003, "referenceCount": 109, "citationCount": 1015, "influentialCitationCount": 81, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1730590", "name": "A. Barto"}, {"authorId": "1850503", "name": "S. Mahadevan"}]}, {"paperId": "038a7a72731a456deb9d03572f1bffab3522b5f5", "url": "https://www.semanticscholar.org/paper/038a7a72731a456deb9d03572f1bffab3522b5f5", "title": "UCI Machine Learning Repository, University of California, Irvine, School of Information and Computer Sciences", "abstract": null, "year": 2007, "referenceCount": 0, "citationCount": 2216, "influentialCitationCount": 226, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2767462", "name": "A. Asuncion"}]}, {"paperId": "4f71ab367eb37cfd145d41327f7bb14077e5e7c5", "url": "https://www.semanticscholar.org/paper/4f71ab367eb37cfd145d41327f7bb14077e5e7c5", "title": "Deep Learning for Hyperspectral Image Classification: An Overview", "abstract": "Hyperspectral image (HSI) classification has become a hot topic in the field of remote sensing. In general, the complex characteristics of hyperspectral data make the accurate classification of such data challenging for traditional machine learning methods. In addition, hyperspectral imaging often deals with an inherently nonlinear relation between the captured spectral information and the corresponding materials. In recent years, deep learning has been recognized as a powerful feature-extraction tool to effectively address nonlinear problems and widely used in a number of image processing tasks. Motivated by those successful applications, deep learning has also been introduced to classify HSIs and demonstrated good performance. This survey paper presents a systematic review of deep learning-based HSI classification literatures and compares several strategies for this topic. Specifically, we first summarize the main challenges of HSI classification which cannot be effectively overcome by traditional machine learning methods, and also introduce the advantages of deep learning to handle these problems. Then, we build a framework that divides the corresponding works into spectral-feature networks, spatial-feature networks, and spectral\u2013spatial-feature networks to systematically review the recent achievements in deep learning-based HSI classification. In addition, considering the fact that available training samples in the remote sensing field are usually very limited and training deep networks require a large number of samples, we include some strategies to improve classification performance, which can provide some guidelines for future studies on this topic. Finally, several representative deep learning-based classification methods are conducted on real HSIs in our experiments.", "year": 2019, "referenceCount": 118, "citationCount": 484, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "2116066317", "name": "Shutao Li"}, {"authorId": "145273596", "name": "Weiwei Song"}, {"authorId": "38140728", "name": "Leyuan Fang"}, {"authorId": "2597809", "name": "Yushi Chen"}, {"authorId": "2370080", "name": "Pedram Ghamisi"}, {"authorId": "1682001", "name": "J. Benediktsson"}]}, {"paperId": "33d26e0b78b2a5d4dfb84fdad1604468a3476933", "url": "https://www.semanticscholar.org/paper/33d26e0b78b2a5d4dfb84fdad1604468a3476933", "title": "Optimal Deep Learning LSTM Model for Electric Load Forecasting using Feature Selection and Genetic Algorithm: Comparison with Machine Learning Approaches \u2020", "abstract": "Background: With the development of smart grids, accurate electric load forecasting has become increasingly important as it can help power companies in better load scheduling and reduce excessive electricity production. However, developing and selecting accurate time series models is a challenging task as this requires training several different models for selecting the best amongst them along with substantial feature engineering to derive informative features and finding optimal time lags, a commonly used input features for time series models. Methods: Our approach uses machine learning and a long short-term memory (LSTM)-based neural network with various configurations to construct forecasting models for short to medium term aggregate load forecasting. The research solves above mentioned problems by training several linear and non-linear machine learning algorithms and picking the best as baseline, choosing best features using wrapper and embedded feature selection methods and finally using genetic algorithm (GA) to find optimal time lags and number of layers for LSTM model predictive performance optimization. Results: Using France metropolitan\u2019s electricity consumption data as a case study, obtained results show that LSTM based model has shown high accuracy then machine learning model that is optimized with hyperparameter tuning. Using the best features, optimal lags, layers and training various LSTM configurations further improved forecasting accuracy. Conclusions: A LSTM model using only optimally selected time lagged features captured all the characteristics of complex time series and showed decreased Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for medium to long range forecasting for a wider metropolitan area.", "year": 2018, "referenceCount": 51, "citationCount": 384, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2840057", "name": "S. Bouktif"}, {"authorId": "144239525", "name": "A. Fiaz"}, {"authorId": "1994554", "name": "Ali Ouni"}, {"authorId": "2394689", "name": "M. A. Serhani"}]}, {"paperId": "76025a0d9f355d9ca9f2562b87ca0ef54fbc113d", "url": "https://www.semanticscholar.org/paper/76025a0d9f355d9ca9f2562b87ca0ef54fbc113d", "title": "Online Sequential Extreme Learning Machine With Kernels", "abstract": "The extreme learning machine (ELM) was recently proposed as a unifying framework for different families of learning algorithms. The classical ELM model consists of a linear combination of a fixed number of nonlinear expansions of the input vector. Learning in ELM is hence equivalent to finding the optimal weights that minimize the error on a dataset. The update works in batch mode, either with explicit feature mappings or with implicit mappings defined by kernels. Although an online version has been proposed for the former, no work has been done up to this point for the latter, and whether an efficient learning algorithm for online kernel-based ELM exists remains an open problem. By explicating some connections between nonlinear adaptive filtering and ELM theory, in this brief, we present an algorithm for this task. In particular, we propose a straightforward extension of the well-known kernel recursive least-squares, belonging to the kernel adaptive filtering (KAF) family, to the ELM framework. We call the resulting algorithm the kernel online sequential ELM (KOS-ELM). Moreover, we consider two different criteria used in the KAF field to obtain sparse filters and extend them to our context. We show that KOS-ELM, with their integration, can result in a highly efficient algorithm, both in terms of obtained generalization error and training time. Empirical evaluations demonstrate interesting results on some benchmarking datasets.", "year": 2015, "referenceCount": 32, "citationCount": 122, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "authors": [{"authorId": "1752983", "name": "Simone Scardapane"}, {"authorId": "1735581", "name": "D. Comminiello"}, {"authorId": "1722048", "name": "M. Scarpiniti"}, {"authorId": "1737292", "name": "A. Uncini"}]}, {"paperId": "10ee4bfd16c2784adf54703daa24c37520fda1f3", "url": "https://www.semanticscholar.org/paper/10ee4bfd16c2784adf54703daa24c37520fda1f3", "title": "Pattern for Python", "abstract": "Pattern is a package for Python 2.4+ with functionality for web mining (Google + Twitter + Wikipedia, web spider, HTML DOM parser), natural language processing (tagger/chunker, n-gram search, sentiment analysis, WordNet), machine learning (vector space model, k-means clustering, Naive Bayes + k-NN + SVM classifiers) and network analysis (graph centrality and visualization). It is well documented and bundled with 30+ examples and 350+ unit tests. The source code is licensed under BSD and available from http://www.clips.ua.ac.be/pages/pattern.", "year": 2012, "referenceCount": 22, "citationCount": 345, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "8231390", "name": "T. D. Smedt"}, {"authorId": "1735272", "name": "Walter Daelemans"}]}, {"paperId": "094ce8379a16030a7a993b2f70af46916f7ea99b", "url": "https://www.semanticscholar.org/paper/094ce8379a16030a7a993b2f70af46916f7ea99b", "title": "A support vector method for optimizing average precision", "abstract": "Machine learning is commonly used to improve ranked retrieval systems. Due to computational difficulties, few learning techniques have been developed to directly optimize for mean average precision (MAP), despite its widespread use in evaluating such systems. Existing approaches optimizing MAP either do not find a globally optimal solution, or are computationally expensive. In contrast, we present a general SVM learning algorithm that efficiently finds a globally optimal solution to a straightforward relaxation of MAP. We evaluate our approach using the TREC 9 and TREC 10 Web Track corpora (WT10g), comparing against SVMs optimized for accuracy and ROCArea. In most cases we show our method to produce statistically significant improvements in MAP scores.", "year": 2007, "referenceCount": 23, "citationCount": 718, "influentialCitationCount": 114, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1740159", "name": "Yisong Yue"}, {"authorId": "50256971", "name": "Thomas Finley"}, {"authorId": "1803571", "name": "Filip Radlinski"}, {"authorId": "1680188", "name": "T. Joachims"}]}, {"paperId": "33108287fbc8d94160787d7b2c7ef249d3ad6437", "url": "https://www.semanticscholar.org/paper/33108287fbc8d94160787d7b2c7ef249d3ad6437", "title": "Modeling Coverage for Neural Machine Translation", "abstract": "Attention mechanism has enhanced state-of-the-art Neural Machine Translation (NMT) by jointly learning to align and translate. It tends to ignore past alignment information, however, which often leads to over-translation and under-translation. To address this problem, we propose coverage-based NMT in this paper. We maintain a coverage vector to keep track of the attention history. The coverage vector is fed to the attention model to help adjust future attention, which lets NMT system to consider more about untranslated source words. Experiments show that the proposed approach significantly improves both translation quality and alignment quality over standard attention-based NMT.", "year": 2016, "referenceCount": 35, "citationCount": 655, "influentialCitationCount": 66, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2909321", "name": "Zhaopeng Tu"}, {"authorId": "11955007", "name": "Zhengdong Lu"}, {"authorId": "2152798100", "name": "Yang Liu"}, {"authorId": "2110998009", "name": "Xiaohua Liu"}, {"authorId": "49404233", "name": "Hang Li"}]}, {"paperId": "18cb13754640a0bbdef07cf8efcfc762a27afa6a", "url": "https://www.semanticscholar.org/paper/18cb13754640a0bbdef07cf8efcfc762a27afa6a", "title": "Intelligent Hybrid Vehicle Power Control\u2014Part I: Machine Learning of Optimal Vehicle Power", "abstract": "In this series of two papers, we present our research on intelligent energy management for hybrid electric vehicles (HEVs). These two papers cover the modeling of power flow in HEVs, the mathematical background of optimization in energy management in HEVs, a machine learning framework that combines dynamic programming (DP) with machine learning to learn about roadway-type- and traffic-congestion-level-specific energy optimization, machine learning algorithms, and real-time quasi-optimal control of energy flow in an HEV. This first paper presents our research on machine learning for optimal energy management in HEVs. We will present a machine learning framework ML_EMO_HEV developed for the optimization of energy management in an HEV, machine learning algorithms for predicting driving environments, and the generation of an optimal power split for a given driving environment. Experiments are conducted based on a simulated Ford Escape Hybrid vehicle model provided by Argonne National Laboratory's Powertrain Systems Analysis Toolkit (PSAT). Based on the experimental results on the test data, we can conclude that the neural networks trained under the ML_EMO_HEV framework are effective in predicting roadway type and traffic congestion levels, predicting driving trends, and learning optimal engine speed and optimal battery power from DP.", "year": 2012, "referenceCount": 26, "citationCount": 152, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "9491649", "name": "Y. Murphey"}, {"authorId": "2087950", "name": "Jungme Park"}, {"authorId": "2109331148", "name": "ZhiHang Chen"}, {"authorId": "2157249", "name": "M. Kuang"}, {"authorId": "144347027", "name": "M. Masrur"}, {"authorId": "31639858", "name": "A. Phillips"}]}, {"paperId": "767dcaf77f73f958cfee0f54cfcde0882e8ec50e", "url": "https://www.semanticscholar.org/paper/767dcaf77f73f958cfee0f54cfcde0882e8ec50e", "title": "Deep Learning: A Practitioner's Approach", "abstract": "Although interest in machine learning has reached a high point, lofty expectations often scuttle projects before they get very far. How can machine learningespecially deep neural networksmake a real difference in your organization? This hands-on guide not only provides the most practical information available on the subject, but also helps you get started building efficient deep learning networks. Authors Adam Gibson and Josh Patterson provide theory on deep learning before introducing their open-source Deeplearning4j (DL4J) library for developing production-class workflows. Through real-world examples, youll learn methods and strategies for training deep network architectures and running deep learning workflows on Spark and Hadoop with DL4J. Dive into machine learning concepts in general, as well as deep learning in particular Understand how deep networks evolved from neural network fundamentals Explore the major deep network architectures, including Convolutional and Recurrent Learn how to map specific deep networks to the right problem Walk through the fundamentals of tuning general neural networks and specific deep network architectures Use vectorization techniques for different data types with DataVec, DL4Js workflow toolLearn how to use DL4J natively on Spark and Hadoop", "year": 2017, "referenceCount": 0, "citationCount": 362, "influentialCitationCount": 50, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2055466552", "name": "Joshua Patterson"}, {"authorId": "2052126530", "name": "Adam Gibson"}]}, {"paperId": "63be611dc8cf09ed081b4ad2b1756420b782b0be", "url": "https://www.semanticscholar.org/paper/63be611dc8cf09ed081b4ad2b1756420b782b0be", "title": "A Perspective on Deep Imaging", "abstract": "The combination of tomographic imaging and deep learning, or machine learning in general, promises to empower not only image analysis but also image reconstruction. The latter aspect is considered in this perspective article with an emphasis on medical imaging to develop a new generation of image reconstruction theories and techniques. This direction might lead to intelligent utilization of domain knowledge from big data, innovative approaches for image reconstruction, and superior performance in clinical and preclinical applications. To realize the full impact of machine learning for tomographic imaging, major theoretical, technical and translational efforts are immediately needed.", "year": 2016, "referenceCount": 59, "citationCount": 324, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Biology"], "authors": [{"authorId": "2108295460", "name": "Ge Wang"}]}, {"paperId": "36d6e7ccf43166107153700d99a87af58684ebc0", "url": "https://www.semanticscholar.org/paper/36d6e7ccf43166107153700d99a87af58684ebc0", "title": "Machine learning and complex biological data", "abstract": null, "year": 2019, "referenceCount": 11, "citationCount": 124, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Biology"], "authors": [{"authorId": "7521190", "name": "Chunming Xu"}, {"authorId": "2495697", "name": "S. Jackson"}]}, {"paperId": "ccb0896acace6ada6a3d2d77b863e0c7b1737bcf", "url": "https://www.semanticscholar.org/paper/ccb0896acace6ada6a3d2d77b863e0c7b1737bcf", "title": "Machine Learning-Based Antenna Selection in Wireless Communications", "abstract": "This letter is the first attempt to conflate a machine learning technique with wireless communications. Through interpreting the antenna selection (AS) in wireless communications (i.e., an optimization-driven decision) to multiclass-classification learning (i.e., data-driven prediction), and through comparing the learning-based AS using k -nearest neighbors and support vector machine algorithms with conventional optimization-driven AS methods in terms of communications performance, computational complexity, and feedback overhead, we provide insight into the potential of fusion of machine learning and wireless communications.", "year": 2016, "referenceCount": 6, "citationCount": 128, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145965302", "name": "J. Joung"}]}, {"paperId": "160a4786dd643d9f758b9cc0758bdd2581524941", "url": "https://www.semanticscholar.org/paper/160a4786dd643d9f758b9cc0758bdd2581524941", "title": "Machine learning for detection and diagnosis of disease.", "abstract": "Machine learning offers a principled approach for developing sophisticated, automatic, and objective algorithms for analysis of high-dimensional and multimodal biomedical data. This review focuses on several advances in the state of the art that have shown promise in improving detection, diagnosis, and therapeutic monitoring of disease. Key in the advancement has been the development of a more in-depth understanding and theoretical analysis of critical issues related to algorithmic construction and learning theory. These include trade-offs for maximizing generalization performance, use of physically realistic constraints, and incorporation of prior knowledge and uncertainty. The review describes recent developments in machine learning, focusing on supervised and unsupervised linear methods and Bayesian inference, which have made significant impacts in the detection and diagnosis of disease in biomedicine. We describe the different methodologies and, for each, provide examples of their application to specific domains in biomedical diagnostics.", "year": 2006, "referenceCount": 152, "citationCount": 293, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1812793", "name": "P. Sajda"}]}, {"paperId": "7c7ab73469c25437332f5c1c1c5cb67c7b2f0855", "url": "https://www.semanticscholar.org/paper/7c7ab73469c25437332f5c1c1c5cb67c7b2f0855", "title": "Low-Shot Visual Recognition by Shrinking and Hallucinating Features", "abstract": "Low-shot visual learning\u2013the ability to recognize novel object categories from very few examples\u2013is a hallmark of human visual intelligence. Existing machine learning approaches fail to generalize in the same way. To make progress on this foundational problem, we present a low-shot learning benchmark on complex images that mimics challenges faced by recognition systems in the wild. We then propose (1) representation regularization techniques, and (2) techniques to hallucinate additional training examples for data-starved classes. Together, our methods improve the effectiveness of convolutional networks in low-shot learning, improving the one-shot accuracy on novel classes by 2.3\u00d7 on the challenging ImageNet dataset.", "year": 2016, "referenceCount": 51, "citationCount": 569, "influentialCitationCount": 60, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1790580", "name": "Bharath Hariharan"}, {"authorId": "2983898", "name": "Ross B. Girshick"}]}, {"paperId": "2550d4f922f9652c4755695c822e492161524bce", "url": "https://www.semanticscholar.org/paper/2550d4f922f9652c4755695c822e492161524bce", "title": "DeepDefense: Identifying DDoS Attack via Deep Learning", "abstract": "Distributed Denial of Service (DDoS) attacks grow rapidly and become one of the fatal threats to the Internet. Automatically detecting DDoS attack packets is one of the main defense mechanisms. Conventional solutions monitor network traffic and identify attack activities from legitimate network traffic based on statistical divergence. Machine learning is another method to improve identifying performance based on statistical features. However, conventional machine learning techniques are limited by the shallow representation models. In this paper, we propose a deep learning based DDoS attack detection approach (DeepDefense). Deep learning approach can automatically extract high-level features from low-level ones and gain powerful representation and inference. We design a recurrent deep neural network to learn patterns from sequences of network traffic and trace network attack activities. The experimental results demonstrate a better performance of our model compared with conventional machine learning models. We reduce the error rate from 7.517% to 2.103% compared with conventional machine learning method in the larger data set.", "year": 2017, "referenceCount": 37, "citationCount": 178, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2115844647", "name": "Xiaoyong Yuan"}, {"authorId": "1838103", "name": "Chuanhuang Li"}, {"authorId": "2108672978", "name": "Xiaolin Li"}]}, {"paperId": "89b22325e7d72d11c5bad8f3893d45d0e184fa9b", "url": "https://www.semanticscholar.org/paper/89b22325e7d72d11c5bad8f3893d45d0e184fa9b", "title": "Neural Networks and Statistical Learning", "abstract": null, "year": 2013, "referenceCount": 0, "citationCount": 268, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144623096", "name": "Ke-Lin Du"}, {"authorId": "143949858", "name": "M. Swamy"}]}, {"paperId": "ee8085bd00f437318cce5b919b47233de8c5f6ff", "url": "https://www.semanticscholar.org/paper/ee8085bd00f437318cce5b919b47233de8c5f6ff", "title": "Pharmacovigilance from social media: mining adverse drug reaction mentions using sequence labeling with word embedding cluster features", "abstract": "Abstract Objective Social media is becoming increasingly popular as a platform for sharing personal health-related information. This information can be utilized for public health monitoring tasks, particularly for pharmacovigilance, via the use of natural language processing (NLP) techniques. However, the language in social media is highly informal, and user-expressed medical concepts are often nontechnical, descriptive, and challenging to extract. There has been limited progress in addressing these challenges, and thus far, advanced machine learning-based NLP techniques have been underutilized. Our objective is to design a machine learning-based approach to extract mentions of adverse drug reactions (ADRs) from highly informal text in social media. Methods We introduce ADRMine, a machine learning-based concept extraction system that uses conditional random fields (CRFs). ADRMine utilizes a variety of features, including a novel feature for modeling words\u2019 semantic similarities. The similarities are modeled by clustering words based on unsupervised, pretrained word representation vectors (embeddings) generated from unlabeled user posts in social media using a deep learning technique. Results ADRMine outperforms several strong baseline systems in the ADR extraction task by achieving an F-measure of 0.82. Feature analysis demonstrates that the proposed word cluster features significantly improve extraction performance. Conclusion It is possible to extract complex medical concepts, with relatively high performance, from informal, user-generated content. Our approach is particularly scalable, suitable for social media mining, as it relies on large volumes of unlabeled data, thus diminishing the need for large, annotated training data sets.", "year": 2015, "referenceCount": 79, "citationCount": 488, "influentialCitationCount": 42, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "1902199", "name": "Azadeh Nikfarjam"}, {"authorId": "48673524", "name": "A. Sarker"}, {"authorId": "1400829836", "name": "K. O'Connor"}, {"authorId": "46645804", "name": "Rachel E. Ginn"}, {"authorId": "1402341088", "name": "G. Gonzalez-Hernandez"}]}, {"paperId": "3f13a5148f7caa51ea946193d261d4f8ed32d81a", "url": "https://www.semanticscholar.org/paper/3f13a5148f7caa51ea946193d261d4f8ed32d81a", "title": "Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon", "abstract": null, "year": 2018, "referenceCount": 85, "citationCount": 573, "influentialCitationCount": 36, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "144390922", "name": "Andrea Lodi"}, {"authorId": "51902590", "name": "Antoine Prouvost"}]}, {"paperId": "efd68f3724942c9de5dc804d3c7cb3f70f42234b", "url": "https://www.semanticscholar.org/paper/efd68f3724942c9de5dc804d3c7cb3f70f42234b", "title": "Deep learning for computational biology", "abstract": "Technological advances in genomics and imaging have led to an explosion of molecular and cellular profiling data from large numbers of samples. This rapid increase in biological data dimension and acquisition rate is challenging conventional analysis strategies. Modern machine learning methods, such as deep learning, promise to leverage very large data sets for finding hidden structure within them, and for making accurate predictions. In this review, we discuss applications of this new breed of analysis approaches in regulatory genomics and cellular imaging. We provide background of what deep learning is, and the settings in which it can be successfully applied to derive biological insights. In addition to presenting specific applications and providing tips for practical use, we also highlight possible pitfalls and limitations to guide computational biologists when and how to make the most use of this new technology.", "year": 2016, "referenceCount": 136, "citationCount": 937, "influentialCitationCount": 29, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Biology"], "authors": [{"authorId": "2414348", "name": "Christof Angermueller"}, {"authorId": "3451978", "name": "Tanel P\u00e4rnamaa"}, {"authorId": "2827805", "name": "L. Parts"}, {"authorId": "152423211", "name": "O. Stegle"}]}, {"paperId": "74b2828dfdf71f2462d88b90a9fc82b90a0f83ca", "url": "https://www.semanticscholar.org/paper/74b2828dfdf71f2462d88b90a9fc82b90a0f83ca", "title": "Ensemble machine learning on gene expression data for cancer classification.", "abstract": "Whole genome RNA expression studies permit systematic approaches to understanding the correlation between gene expression profiles to disease states or different developmental stages of a cell. Microarray analysis provides quantitative information about the complete transcription profile of cells that facilitate drug and therapeutics development, disease diagnosis, and understanding in the basic cell biology. One of the challenges in microarray analysis, especially in cancerous gene expression profiles, is to identify genes or groups of genes that are highly expressed in tumour cells but not in normal cells and vice versa. Previously, we have shown that ensemble machine learning consistently performs well in classifying biological data. In this paper, we focus on three different supervised machine learning techniques in cancer classification, namely C4.5 decision tree, and bagged and boosted decision trees. We have performed classification tasks on seven publicly available cancerous microarray data and compared the classification/prediction performance of these methods. We have observed that ensemble learning (bagged and boosted decision trees) often performs better than single decision trees in this classification task.", "year": 2003, "referenceCount": 29, "citationCount": 404, "influentialCitationCount": 26, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "144665015", "name": "A. Tan"}, {"authorId": "2502593", "name": "D. Gilbert"}]}, {"paperId": "a16e484824b2580e092c985aa659e8680aeda5ee", "url": "https://www.semanticscholar.org/paper/a16e484824b2580e092c985aa659e8680aeda5ee", "title": "Shallow Semantic Parsing using Support Vector Machines", "abstract": "In this paper, we propose a machine learning algorithm for shallow semantic parsing, extending the work of Gildea and Jurafsky (2002), Surdeanu et al. (2003) and others. Our algorithm is based on Support Vector Machines which we show give an improvement in performance over earlier classifiers. We show performance improvements through a number of new features and measure their ability to generalize to a new test set drawn from the AQUAINT corpus.", "year": 2004, "referenceCount": 22, "citationCount": 447, "influentialCitationCount": 52, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1735131", "name": "Sameer Pradhan"}, {"authorId": "1866226", "name": "Wayne H. Ward"}, {"authorId": "2483422", "name": "K. Hacioglu"}, {"authorId": "10796472", "name": "James H. Martin"}, {"authorId": "1746807", "name": "Dan Jurafsky"}]}, {"paperId": "f058d8221ac6eb439cf1b911e2c0065376735752", "url": "https://www.semanticscholar.org/paper/f058d8221ac6eb439cf1b911e2c0065376735752", "title": "Support Vector Machines with Example Dependent Costs", "abstract": null, "year": 2003, "referenceCount": 13, "citationCount": 75, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1765114", "name": "Ulf Brefeld"}, {"authorId": "1764433", "name": "Peter Geibel"}, {"authorId": "1681964", "name": "F. Wysotzki"}]}, {"paperId": "3b12ddce41b574c27fce7fe0da51c653564f7d45", "url": "https://www.semanticscholar.org/paper/3b12ddce41b574c27fce7fe0da51c653564f7d45", "title": "Human-Centred Machine Learning", "abstract": "Machine learning is one of the most important and successful techniques in contemporary computer science. It involves the statistical inference of models (such as classifiers) from data. It is often conceived in a very impersonal way, with algorithms working autonomously on passively collected data. However, this viewpoint hides considerable human work of tuning the algorithms, gathering the data, and even deciding what should be modeled in the first place. Examining machine learning from a human-centered perspective includes explicitly recognising this human work, as well as reframing machine learning workflows based on situated human working practices, and exploring the co-adaptation of humans and systems. A human-centered understanding of machine learning in human context can lead not only to more usable machine learning tools, but to new ways of framing learning computationally. This workshop will bring together researchers to discuss these issues and suggest future research questions aimed at creating a human-centered approach to machine learning.", "year": 2016, "referenceCount": 9, "citationCount": 96, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2589934", "name": "M. Gillies"}, {"authorId": "1745615", "name": "R. Fiebrink"}, {"authorId": "39647503", "name": "A. Tanaka"}, {"authorId": "2110333953", "name": "J\u00e9r\u00e9mie Garcia"}, {"authorId": "1750247", "name": "Fr\u00e9d\u00e9ric Bevilacqua"}, {"authorId": "2812935", "name": "A. H\u00e9loir"}, {"authorId": "2282075", "name": "Fabrizio Nunnari"}, {"authorId": "1732917", "name": "W. Mackay"}, {"authorId": "1719124", "name": "Saleema Amershi"}, {"authorId": "49132427", "name": "Bongshin Lee"}, {"authorId": "1401792700", "name": "N. D'Alessandro"}, {"authorId": "3102364", "name": "J. Tilmanne"}, {"authorId": "1847827", "name": "T. Kulesza"}, {"authorId": "2046048", "name": "Baptiste Caramiaux"}]}, {"paperId": "d76c07211479e233f7c6a6f32d5346c983c5598f", "url": "https://www.semanticscholar.org/paper/d76c07211479e233f7c6a6f32d5346c983c5598f", "title": "Multi-task Sequence to Sequence Learning", "abstract": "Sequence to sequence learning has recently emerged as a new paradigm in supervised learning. To date, most of its applications focused on only one task and not much work explored this framework for multiple tasks. This paper examines three multi-task learning (MTL) settings for sequence to sequence models: (a) the oneto-many setting - where the encoder is shared between several tasks such as machine translation and syntactic parsing, (b) the many-to-one setting - useful when only the decoder can be shared, as in the case of translation and image caption generation, and (c) the many-to-many setting - where multiple encoders and decoders are shared, which is the case with unsupervised objectives and translation. Our results show that training on a small amount of parsing and image caption data can improve the translation quality between English and German by up to 1.5 BLEU points over strong single-task baselines on the WMT benchmarks. Furthermore, we have established a new state-of-the-art result in constituent parsing with 93.0 F1. Lastly, we reveal interesting properties of the two unsupervised learning objectives, autoencoder and skip-thought, in the MTL context: autoencoder helps less in terms of perplexities but more on BLEU scores compared to skip-thought.", "year": 2015, "referenceCount": 34, "citationCount": 714, "influentialCitationCount": 71, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1707242", "name": "Minh-Thang Luong"}, {"authorId": "2827616", "name": "Quoc V. Le"}, {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "1689108", "name": "Oriol Vinyals"}, {"authorId": "40527594", "name": "Lukasz Kaiser"}]}, {"paperId": "ab1258b286c729e2c0cd2546ec0f79ad21bd3883", "url": "https://www.semanticscholar.org/paper/ab1258b286c729e2c0cd2546ec0f79ad21bd3883", "title": "Quantum Boltzmann Machine", "abstract": "Inspired by the success of Boltzmann Machines based on classical Boltzmann distribution, we propose a new machine learning approach based on quantum Boltzmann distribution of a transverse-field Ising Hamiltonian. Due to the non-commutative nature of quantum mechanics, the training process of the Quantum Boltzmann Machine (QBM) can become nontrivial. We circumvent the problem by introducing bounds on the quantum probabilities. This allows us to train the QBM efficiently by sampling. We show examples of QBM training with and without the bound, using exact diagonalization, and compare the results with classical Boltzmann training. We also discuss the possibility of using quantum annealing processors like D-Wave for QBM training and application.", "year": 2016, "referenceCount": 41, "citationCount": 331, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Computer Science"], "authors": [{"authorId": "143707843", "name": "M. Amin"}, {"authorId": "8381113", "name": "E. Andriyash"}, {"authorId": "34965728", "name": "J. Rolfe"}, {"authorId": "103330749", "name": "B. Kulchytskyy"}, {"authorId": "3422513", "name": "R. Melko"}]}]}