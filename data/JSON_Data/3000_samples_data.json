{"total": 5120627, "offset": 2900, "next": 3000, "data": [{"paperId": "89c2a023e0eb3d46bfa9e849e77afb83d6d870f4", "url": "https://www.semanticscholar.org/paper/89c2a023e0eb3d46bfa9e849e77afb83d6d870f4", "title": "Machine learning and excited-state molecular dynamics", "abstract": "Machine learning is employed at an increasing rate in the research field of quantum chemistry. While the majority of approaches target the investigation of chemical systems in their electronic ground state, the inclusion of light into the processes leads to electronically excited states and gives rise to several new challenges. Here, we survey recent advances for excited-state dynamics based on machine learning. In doing so, we highlight successes, pitfalls, challenges and future avenues for machine learning approaches for light-induced molecular processes.", "year": 2020, "referenceCount": 306, "citationCount": 28, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "authors": [{"authorId": "90050189", "name": "J. Westermayr"}, {"authorId": "4850899", "name": "P. Marquetand"}]}, {"paperId": "508571db5d2f77c17d2829878bb1dc645010dda8", "url": "https://www.semanticscholar.org/paper/508571db5d2f77c17d2829878bb1dc645010dda8", "title": "A Comparative Analysis of Methods for Pruning Decision Trees", "abstract": "In this paper, we address the problem of retrospectively pruning decision trees induced from data, according to a top-down approach. This problem has received considerable attention in the areas of pattern recognition and machine learning, and many distinct methods have been proposed in literature. We make a comparative study of six well-known pruning methods with the aim of understanding their theoretical foundations, their computational complexity, and the strengths and weaknesses of their formulation. Comments on the characteristics of each method are empirically supported. In particular, a wide experimentation performed on several data sets leads us to opposite conclusions on the predictive accuracy of simplified trees from some drawn in the literature. We attribute this divergence to differences in experimental designs. Finally, we prove and make use of a property of the reduced error pruning method to obtain an objective evaluation of the tendency to overprune/underprune observed in each method.", "year": 1997, "referenceCount": 27, "citationCount": 554, "influentialCitationCount": 35, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1700821", "name": "F. Esposito"}, {"authorId": "1738657", "name": "D. Malerba"}, {"authorId": "145467353", "name": "G. Semeraro"}]}, {"paperId": "0ebca477733c79e13900d1b4e00b67a9b7bf8abd", "url": "https://www.semanticscholar.org/paper/0ebca477733c79e13900d1b4e00b67a9b7bf8abd", "title": "Towards spike-based machine intelligence with neuromorphic computing", "abstract": null, "year": 2019, "referenceCount": 165, "citationCount": 521, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "143884690", "name": "K. Roy"}, {"authorId": "2468479", "name": "A. Jaiswal"}, {"authorId": "9352814", "name": "P. Panda"}]}, {"paperId": "0dd8bf3c15c19355846719d8eaf300d6954dac95", "url": "https://www.semanticscholar.org/paper/0dd8bf3c15c19355846719d8eaf300d6954dac95", "title": "A Survey of Machine Learning Techniques Applied to Self-Organizing Cellular Networks", "abstract": "In this paper, a survey of the literature of the past 15 years involving machine learning (ML) algorithms applied to self-organizing cellular networks is performed. In order for future networks to overcome the current limitations and address the issues of current cellular systems, it is clear that more intelligence needs to be deployed so that a fully autonomous and flexible network can be enabled. This paper focuses on the learning perspective of self-organizing networks (SON) solutions and provides, not only an overview of the most common ML techniques encountered in cellular networks but also manages to classify each paper in terms of its learning solution, while also giving some examples. The authors also classify each paper in terms of its self-organizing use-case and discuss how each proposed solution performed. In addition, a comparison between the most commonly found ML algorithms in terms of certain SON metrics is performed and general guidelines on when to choose each ML algorithm for each SON function are proposed. Lastly, this paper also provides future research directions and new paradigms that the use of more robust and intelligent algorithms, together with data gathered by operators, can bring to the cellular networks domain and fully enable the concept of SON in the near future.", "year": 2017, "referenceCount": 273, "citationCount": 301, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "30927268", "name": "P. V. Klaine"}, {"authorId": "143948770", "name": "M. Imran"}, {"authorId": "2929470", "name": "Oluwakayode Onireti"}, {"authorId": "1705996", "name": "R. Souza"}]}, {"paperId": "fbe6553641ce3c6a1914ef5d9f8797fa762c8bad", "url": "https://www.semanticscholar.org/paper/fbe6553641ce3c6a1914ef5d9f8797fa762c8bad", "title": "Chained Anomaly Detection Models for Federated Learning: An Intrusion Detection Case Study", "abstract": "The adoption of machine learning and deep learning is on the rise in the cybersecurity domain where these AI methods help strengthen traditional system monitoring and threat detection solutions. However, adversaries too are becoming more effective in concealing malicious behavior amongst large amounts of benign behavior data. To address the increasing time-to-detection of these stealthy attacks, interconnected and federated learning systems can improve the detection of malicious behavior by joining forces and pooling together monitoring data. The major challenge that we address in this work is that in a federated learning setup, an adversary has many more opportunities to poison one of the local machine learning models with malicious training samples, thereby influencing the outcome of the federated learning and evading detection. We present a solution where contributing parties in federated learning can be held accountable and have their model updates audited. We describe a permissioned blockchain-based federated learning method where incremental updates to an anomaly detection machine learning model are chained together on the distributed ledger. By integrating federated learning with blockchain technology, our solution supports the auditing of machine learning models without the necessity to centralize the training data. Experiments with a realistic intrusion detection use case and an autoencoder for anomaly detection illustrate that the increased complexity caused by blockchain technology has a limited performance impact on the federated learning, varying between 5 and 15%, while providing full transparency over the distributed training process of the neural network. Furthermore, our blockchain-based federated learning solution can be generalized and applied to more sophisticated neural network architectures and other use cases.", "year": 2018, "referenceCount": 42, "citationCount": 109, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1722184", "name": "D. Preuveneers"}, {"authorId": "23974422", "name": "Vera Rimmer"}, {"authorId": "103060139", "name": "Ilias Tsingenopoulos"}, {"authorId": "2967638", "name": "Jan Spooren"}, {"authorId": "1752104", "name": "W. Joosen"}, {"authorId": "1410421035", "name": "E. Ilie-Zudor"}]}, {"paperId": "29e6b12d3c6cd55e04bdfb9c22201f99578f4080", "url": "https://www.semanticscholar.org/paper/29e6b12d3c6cd55e04bdfb9c22201f99578f4080", "title": "Federated Learning for Internet of Things: Recent Advances, Taxonomy, and Open Challenges", "abstract": "The Internet of Things (IoT) will be ripe for the deployment of novel machine learning algorithm for both network and application management. However, given the presence of massively distributed and private datasets, it is challenging to use classical centralized learning algorithms in the IoT. To overcome this challenge, federated learning can be a promising solution that enables on-device machine learning without the need to migrate the private end-user data to a central cloud. In federated learning, only learning model updates are transferred between end-devices and the aggregation server. Although federated learning can offer better privacy preservation than centralized machine learning, it has still privacy concerns. In this paper, first, we present the recent advances of federated learning towards enabling federated learning-powered IoT applications. A set of metrics such as sparsification, robustness, quantization, scalability, security, and privacy, is delineated in order to rigorously evaluate the recent advances. Second, we devise a taxonomy for federated learning over IoT networks. Finally, we present several open research challenges with their possible solutions.", "year": 2020, "referenceCount": 182, "citationCount": 117, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2280048", "name": "L. U. Khan"}, {"authorId": "145412074", "name": "W. Saad"}, {"authorId": "145169163", "name": "Zhu Han"}, {"authorId": "144158811", "name": "E. Hossain"}, {"authorId": "143849708", "name": "C. Hong"}]}, {"paperId": "1f4a4769e4d2fb846e59c2f185e0377190739f18", "url": "https://www.semanticscholar.org/paper/1f4a4769e4d2fb846e59c2f185e0377190739f18", "title": "Learning Structured Embeddings of Knowledge Bases", "abstract": "\n \n Many Knowledge Bases (KBs) are now readily available and encompass colossal quantities of information thanks to either a long-term funding effort (e.g. WordNet, OpenCyc) or a collaborative process (e.g. Freebase, DBpedia). However, each of them is based on a different rigorous symbolic framework which makes it hard to use their data in other systems. It is unfortunate because such rich structured knowledge might lead to a huge leap forward in many other areas of AI like nat- ural language processing (word-sense disambiguation, natural language understanding, ...), vision (scene classification, image semantic annotation, ...) or collaborative filtering. In this paper, we present a learning process based on an innovative neural network architecture designed to embed any of these symbolic representations into a more flexible continuous vector space in which the original knowledge is kept and enhanced. These learnt embeddings would allow data from any KB to be easily used in recent machine learning meth- ods for prediction and information retrieval. We illustrate our method on WordNet and Freebase and also present a way to adapt it to knowledge extraction from raw text.\n \n", "year": 2011, "referenceCount": 23, "citationCount": 798, "influentialCitationCount": 89, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1713934", "name": "Antoine Bordes"}, {"authorId": "145183709", "name": "J. Weston"}, {"authorId": "2939803", "name": "Ronan Collobert"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}, {"paperId": "ddacf4b9dea711d2ac50f0d6a29e62f3cd96bccb", "url": "https://www.semanticscholar.org/paper/ddacf4b9dea711d2ac50f0d6a29e62f3cd96bccb", "title": "Applications of support vector machines to speech recognition", "abstract": "Recent work in machine learning has focused on models, such as the support vector machine (SVM), that automatically control generalization and parameterization as part of the overall optimization process. In this paper, we show that SVMs provide a significant improvement in performance on a static pattern classification task based on the Deterding vowel data. We also describe an application of SVMs to large vocabulary speech recognition and demonstrate an improvement in error rate on a continuous alphadigit task (OGI Alphadigits) and a large vocabulary conversational speech task (Switchboard). Issues related to the development and optimization of an SVM/HMM hybrid system are discussed.", "year": 2004, "referenceCount": 66, "citationCount": 292, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1782807", "name": "A. Ganapathiraju"}, {"authorId": "144170973", "name": "J. Hamaker"}, {"authorId": "1744408", "name": "J. Picone"}]}, {"paperId": "779e4492f1cf93fe9d92f58e70ee56003e3c5970", "url": "https://www.semanticscholar.org/paper/779e4492f1cf93fe9d92f58e70ee56003e3c5970", "title": "Big data and black-box medical algorithms", "abstract": "New machine-learning techniques entering medicine present challenges in validation, regulation, and integration into practice. New machine-learning techniques entering medicine present challenges in validation, regulation, and integration into practice.", "year": 2018, "referenceCount": 15, "citationCount": 89, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "40503754", "name": "W. Price"}]}, {"paperId": "274c938a70de1afb9ef1489cab2186c1d699725e", "url": "https://www.semanticscholar.org/paper/274c938a70de1afb9ef1489cab2186c1d699725e", "title": "Electricity Price Forecasting With Extreme Learning Machine and Bootstrapping", "abstract": "Artificial neural networks (ANNs) have been widely applied in electricity price forecasts due to their nonlinear modeling capabilities. However, it is well known that in general, traditional training methods for ANNs such as back-propagation (BP) approach are normally slow and it could be trapped into local optima. In this paper, a fast electricity market price forecast method is proposed based on a recently emerged learning method for single hidden layer feed-forward neural networks, the extreme learning machine (ELM), to overcome these drawbacks. The new approach also has improved price intervals forecast accuracy by incorporating bootstrapping method for uncertainty estimations. Case studies based on chaos time series and Australian National Electricity Market price series show that the proposed method can effectively capture the nonlinearity from the highly volatile price data series with much less computation time compared with other methods. The results show the great potential of this proposed approach for online accurate price forecasting for the spot market prices analysis.", "year": 2012, "referenceCount": 41, "citationCount": 214, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "2109481241", "name": "Xia Chen"}, {"authorId": "144402926", "name": "Z. Dong"}, {"authorId": "145734483", "name": "K. Meng"}, {"authorId": "2118040272", "name": "Yan Xu"}, {"authorId": "144568755", "name": "K. Wong"}, {"authorId": "144948800", "name": "H. Ngan"}]}, {"paperId": "e0f05b84b81a0416b849e604b14f92d573c51635", "url": "https://www.semanticscholar.org/paper/e0f05b84b81a0416b849e604b14f92d573c51635", "title": "Building Domain-Specific Search Engines with Machine Learning Techniques", "abstract": "Domain-specific search engines are growing in popularity because they offer increased accuracy and extra functionality not possible with the general, Web-wide search engines. For example, www.campsearch.com allows complex queries by age-group, size, location and cost over .summer camps. Unfortunately these domain-specific search engines are difficult and timeconsuming to maintain. This paper proposes the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific search engines. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, identifying informative text segments, and populating topic hierarchies. Using these techniques, we have built a demonstration system: a search engine for computer science research papers. It already contaius over 50,000 papers and is publicly available at ~w. cora.justres earch, com.", "year": 1999, "referenceCount": 25, "citationCount": 171, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143753639", "name": "A. McCallum"}, {"authorId": "145172877", "name": "K. Nigam"}, {"authorId": "35211659", "name": "Jason D. M. Rennie"}, {"authorId": "2544946", "name": "K. Seymore"}]}, {"paperId": "36f49b05d764bf5c10428b082c2d96c13c4203b9", "url": "https://www.semanticscholar.org/paper/36f49b05d764bf5c10428b082c2d96c13c4203b9", "title": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent", "abstract": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.", "year": 2011, "referenceCount": 30, "citationCount": 2086, "influentialCitationCount": 295, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "9229182", "name": "B. Recht"}, {"authorId": "1803218", "name": "Christopher R\u00e9"}, {"authorId": "144731788", "name": "Stephen J. Wright"}, {"authorId": "47657030", "name": "Feng Niu"}]}, {"paperId": "e4c2cdca27fbbe2858eeda04bc93ca30a2c24133", "url": "https://www.semanticscholar.org/paper/e4c2cdca27fbbe2858eeda04bc93ca30a2c24133", "title": "Classification of Australian Native Forest Species Using Hyperspectral Remote Sensing and Machine-Learning Classification Algorithms", "abstract": "Mapping forest species is highly relevant for many ecological and forestry applications. In Australia, the classification of native forest species using remote sensing data remains a particular challenge since there are many eucalyptus species that belong to the same genus and, thus, exhibit similar biophysical characteristics. This study assessed the potential of using hyperspectral remote sensing data and state-of-the-art machine-learning classification algorithms to classify Australian forest species at the leaf, canopy and community levels in Beecroft Peninsula, NSW, Australia. Spectral reflectance was acquired from an ASD spectrometer and airborne Hymap imagery for seven native forest species over an Australian eucalyptus forest. Three machine-learning classification algorithms: Support Vector Machine (SVM), AdaBoost and Random Forest (RF) were applied to classify the species. A comparative study was carried out between machine-learning classification algorithms and Linear Discriminant Analysis (LDA). The classification results show that all machine-leaning classification algorithms significantly improve the results produced by LDA. At the leaf level, RF achieved the best classification accuracy (94.7%), and SVM outperformed the other algorithms at both the canopy (84.5%) and community levels (75.5%). This study demonstrates that hyperspectral remote sensing and machine-learning classification has substantial potential for the classification of Australian native forest species.", "year": 2014, "referenceCount": 62, "citationCount": 93, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "14066140", "name": "Xiao-ning Shang"}, {"authorId": "40149354", "name": "L. Chisholm"}]}, {"paperId": "67edb4b2387bfde18cb226536405f4668ff3e07e", "url": "https://www.semanticscholar.org/paper/67edb4b2387bfde18cb226536405f4668ff3e07e", "title": "Learning to Predict Chemical Reactions", "abstract": "Being able to predict the course of arbitrary chemical reactions is essential to the theory and applications of organic chemistry. Approaches to the reaction prediction problems can be organized around three poles corresponding to: (1) physical laws; (2) rule-based expert systems; and (3) inductive machine learning. Previous approaches at these poles, respectively, are not high throughput, are not generalizable or scalable, and lack sufficient data and structure to be implemented. We propose a new approach to reaction prediction utilizing elements from each pole. Using a physically inspired conceptualization, we describe single mechanistic reactions as interactions between coarse approximations of molecular orbitals (MOs) and use topological and physicochemical attributes as descriptors. Using an existing rule-based system (Reaction Explorer), we derive a restricted chemistry data set consisting of 1630 full multistep reactions with 2358 distinct starting materials and intermediates, associated with 2989 productive mechanistic steps and 6.14 million unproductive mechanistic steps. And from machine learning, we pose identifying productive mechanistic steps as a statistical ranking, information retrieval problem: given a set of reactants and a description of conditions, learn a ranking model over potential filled-to-unfilled MO interactions such that the top-ranked mechanistic steps yield the major products. The machine learning implementation follows a two-stage approach, in which we first train atom level reactivity filters to prune 94.00% of nonproductive reactions with a 0.01% error rate. Then, we train an ensemble of ranking models on pairs of interacting MOs to learn a relative productivity function over mechanistic steps in a given system. Without the use of explicit transformation patterns, the ensemble perfectly ranks the productive mechanism at the top 89.05% of the time, rising to 99.86% of the time when the top four are considered. Furthermore, the system is generalizable, making reasonable predictions over reactants and conditions which the rule-based expert does not handle. A web interface to the machine learning based mechanistic reaction predictor is accessible through our chemoinformatics portal ( http://cdb.ics.uci.edu) under the Toolkits section.", "year": 2011, "referenceCount": 54, "citationCount": 137, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2639751", "name": "Matthew A. Kayala"}, {"authorId": "4318923", "name": "Chlo\u00e9-Agathe Azencott"}, {"authorId": "2108263576", "name": "Jonathan H. Chen"}, {"authorId": "144902513", "name": "P. Baldi"}]}, {"paperId": "c7e4075c462b46f0a504bae14cedbd196a31558a", "url": "https://www.semanticscholar.org/paper/c7e4075c462b46f0a504bae14cedbd196a31558a", "title": "Detecting Fake News with Machine Learning Method", "abstract": "Fake news has immense impact in our modern society. Detecting Fake news is an important step. This work proposes the use of machine learning techniques to detect Fake news. Three popular methods are used in the experiments: Naive Bayes, Neural Network and Support Vector Machine. The normalization method is important step for cleaning data before using the machine learning method to classify data. The result show that Naive Bayes to detect Fake news has accuracy 96.08%. Two other more advance methods which are Neural Network and Support Vector Machine achieve the accuracy of 99.90%.", "year": 2018, "referenceCount": 18, "citationCount": 73, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "66581914", "name": "Supanya Aphiwongsophon"}, {"authorId": "1788027", "name": "P. Chongstitvatana"}]}, {"paperId": "1c693530e8f188772a2de1859c00083e5715a5b2", "url": "https://www.semanticscholar.org/paper/1c693530e8f188772a2de1859c00083e5715a5b2", "title": "Machine Discoveries: A Few Simple, Robust Local Expression Principles", "abstract": "The paper presents a new approach to discovering general rules of expressive music performance from real performance data via inductive machine learning. A new learning algorithm is briefly presented, and then an experiment with a very large data set (performances of 13 Mozart piano sonatas) is described. The new learning algorithm succeeds in discovering some extremely simple and general principles of musical performance (at the level of individual notes), in the form of categorical prediction rules. These rules turn out to be very robust and general: when tested on performances by a different pianist and even on music of a different style (Chopin), they exhibit a surprisingly high degree of predictive accuracy.", "year": 2002, "referenceCount": 35, "citationCount": 139, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145964711", "name": "G. Widmer"}]}, {"paperId": "8acf6797593352a4c9ce718665c1868cc2a67c96", "url": "https://www.semanticscholar.org/paper/8acf6797593352a4c9ce718665c1868cc2a67c96", "title": "The Minimum Description Length Principle", "abstract": "The minimum description length (MDL) principle is a powerful method of inductive inference, the basis of statistical modeling, pattern recognition, and machine learning. It holds that the best explanation, given a limited set of observed data, is the one that permits the greatest compression of the data. MDL methods are particularly well-suited for dealing with model selection, prediction, and estimation problems in situations where the models under consideration can be arbitrarily complex, and overfitting the data is a serious concern. This extensive, step-by-step introduction to the MDL Principle provides a comprehensive reference (with an emphasis on conceptual issues) that is accessible to graduate students and researchers in statistics, pattern classification, machine learning, and data mining, to philosophers interested in the foundations of statistics, and to researchers in other applied sciences that involve model selection, including biology, econometrics, and experimental psychology. Part I provides a basic introduction to MDL and an overview of the concepts in statistics and information theory needed to understand MDL. Part II treats universal coding, the information-theoretic notion on which MDL is built, and part III gives a formal treatment of MDL theory as a theory of inductive inference based on universal coding. Part IV provides a comprehensive overview of the statistical theory of exponential families with an emphasis on their information-theoretic properties. The text includes a number of summaries, paragraphs offering the reader a \"fast track\" through the material, and boxes highlighting the most important concepts.", "year": 2007, "referenceCount": 6, "citationCount": 630, "influentialCitationCount": 55, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "144729640", "name": "P. Gr\u00fcnwald"}]}, {"paperId": "2d045aec8585cae8e5ccba56db1d64820cf73773", "url": "https://www.semanticscholar.org/paper/2d045aec8585cae8e5ccba56db1d64820cf73773", "title": "Comparing discriminating transformations and SVM for learning during multimedia retrieval", "abstract": "On-line learning or \"relevance feedback\" techniques for multimedia information retrieval have been explored from many different points of view: from early heuristic-based feature weighting schemes to recently proposed optimal learning algorithms, probabilistic/Bayesian learning algorithms, boosting techniques, discriminant-EM algorithm, support vector machine, and other kernel-based learning machines. Based on a careful examination of the problem and a detailed analysis of the existing solutions, we propose several discriminating transforms as the learning machine during the user interaction. We argue that relevance feedback problem is best represented as a biased classification problem, or a (1+x)-class classification problem. Biased Discriminant Transform (BDT) is shown to outperform all the others. A kernel form is proposed to capture non-linearity in the class distributions.", "year": 2001, "referenceCount": 38, "citationCount": 121, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40403107", "name": "X. Zhou"}, {"authorId": "153652752", "name": "Thomas S. Huang"}]}, {"paperId": "65e536448b7f3763f399841e65a6973cdb170239", "url": "https://www.semanticscholar.org/paper/65e536448b7f3763f399841e65a6973cdb170239", "title": "OpenML-Python: an extensible Python API for OpenML", "abstract": "OpenML is an online platform for open science collaboration in machine learning, used to share datasets and results of machine learning experiments. In this paper we introduce \\emph{OpenML-Python}, a client API for Python, opening up the OpenML platform for a wide range of Python-based tools. It provides easy access to all datasets, tasks and experiments on OpenML from within Python. It also provides functionality to conduct machine learning experiments, upload the results to OpenML, and reproduce results which are stored on OpenML. Furthermore, it comes with a scikit-learn plugin and a plugin mechanism to easily integrate other machine learning libraries written in Python into the OpenML ecosystem. Source code and documentation is available at this https URL.", "year": 2019, "referenceCount": 18, "citationCount": 38, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2868444", "name": "Matthias Feurer"}, {"authorId": "1764155", "name": "J. N. Rijn"}, {"authorId": "1403237566", "name": "Arlind Kadra"}, {"authorId": "41155633", "name": "P. Gijsbers"}, {"authorId": "3359845", "name": "Neeratyoy Mallik"}, {"authorId": "152650432", "name": "Sahithya Ravi"}, {"authorId": "2113786044", "name": "Andreas M\u00fcller"}, {"authorId": "1717534", "name": "J. Vanschoren"}, {"authorId": "144661829", "name": "F. Hutter"}]}, {"paperId": "54dd013c04a3f588cb366d683a3defd947db3e07", "url": "https://www.semanticscholar.org/paper/54dd013c04a3f588cb366d683a3defd947db3e07", "title": "Modelling ecological niches with support vector machines", "abstract": "1. The ecological niche is a fundamental biological concept. Modelling species' niches is central to numerous ecological applications, including predicting species invasions, identifying reservoirs for disease, nature reserve design and forecasting the effects of anthropogenic and natural climate change on species' ranges. 2. A computational analogue of Hutchinson's ecological niche concept (the multidimensional hyperspace of species' environmental requirements) is the support of the distribution of environments in which the species persist. Recently developed machine-learning algorithms can estimate the support of such high-dimensional distributions. We show how support vector machines can be used to map ecological niches using only observations of species presence to train distribution models for 106 species of woody plants and trees in a montane environment using up to nine environmental covariates. 3. We compared the accuracy of three methods that differ in their approaches to reducing model complexity. We tested models with independent observations of both species presence and species absence. We found that the simplest procedure, which uses all available variables and no pre-processing to reduce correlation, was best overall. Ecological niche models based on support vector machines are theoretically superior to models that rely on simulating pseudo-absence data and are comparable in empirical tests. 4. Synthesis and applications. Accurate species distribution models are crucial for effective environmental planning, management and conservation, and for unravelling the role of the environment in human health and welfare. Models based on distribution estimation rather than classification overcome theoretical and practical obstacles that pervade species distribution modelling. In particular, ecological niche models based on machine-learning algorithms for estimating the support of a statistical distribution provide a promising new approach to identifying species' potential distributions and to project changes in these distributions as a result of climate change, land use and landscape alteration.", "year": 2006, "referenceCount": 46, "citationCount": 259, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Biology"], "authors": [{"authorId": "34814486", "name": "J. Drake"}, {"authorId": "4116594", "name": "C. Randin"}, {"authorId": "1830349", "name": "A. Guisan"}]}, {"paperId": "3091689115f308d4c3f7f4421a284af1d5119cb7", "url": "https://www.semanticscholar.org/paper/3091689115f308d4c3f7f4421a284af1d5119cb7", "title": "Large Scale Online Kernel Learning", "abstract": "In this paper, we present a new framework for large scale online kernel learning, making kernel methods efficient and scalable for large-scale online learning applications. Unlike the regular budget online kernel learning scheme that usually uses some budget maintenance strategies to bound the number of support vectors, our framework explores a completely different approach of kernel functional approximation techniques to make the subsequent online learning task efficient and scalable. Specifically, we present two different online kernel machine learning algorithms: (i) Fourier Online Gradient Descent (FOGD) algorithm that applies the random Fourier features for approximating kernel functions; and (ii) Nystrom Online Gradient Descent (NOGD) algorithm that applies the Nystrom method to approximate large kernel matrices. We explore these two approaches to tackle three online learning tasks: binary classification, multi-class classification, and regression. The encouraging results of our experiments on large-scale datasets validate the effectiveness and efficiency of the proposed algorithms, making them potentially more practical than the family of existing budget online kernel learning approaches.", "year": 2016, "referenceCount": 47, "citationCount": 130, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2115404177", "name": "Jing Lu"}, {"authorId": "1741126", "name": "S. Hoi"}, {"authorId": "1809306", "name": "Jialei Wang"}, {"authorId": "144259957", "name": "P. Zhao"}, {"authorId": "145778340", "name": "Zhiyong Liu"}]}, {"paperId": "a85c7f137c3f3a6af802543e4784f6919734d4f4", "url": "https://www.semanticscholar.org/paper/a85c7f137c3f3a6af802543e4784f6919734d4f4", "title": "ifile: An Application of Machine Learning to E-Mail Filtering", "abstract": "The rise of the World Wide Web and the ever-increasing amounts of machine-readable text has caused text classification to become a important aspect of machine learning. One specific application that has the potential to affect almost every user of the Internet is e-mail filtering. The WorldTalk Corporation estimates that over 60 million business people use e-mail [6]. Many more use e-mail purely on a personal basis and the pool of e-mail users is growing daily. And yet, automated techniques for learning to filter e-mail have yet to significantly affect the e-mail market. Here, I attack problems that plague practical e-mail filtering and suggest solutions that will bring us closer to the acceptance of using automated classification techniques to filter personal e-mail. I also present a filtering system, ifile, that is both effective and efficient, and which has been adapted to a popular e-mail client. Results are presented from a number of experiments and show that a system such as ifile could become a useful and valuable part of any e-mail client.", "year": 2000, "referenceCount": 23, "citationCount": 173, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35211659", "name": "Jason D. M. Rennie"}]}, {"paperId": "01fbe65823f0cea82a001a5a4504a66ab73e68f8", "url": "https://www.semanticscholar.org/paper/01fbe65823f0cea82a001a5a4504a66ab73e68f8", "title": "Higher-Order Boltzmann Machines", "abstract": "T h e Boltzmann machine is a nonlinear network of stochastic binary processing units t ha t interact pairwise through symmetric connection strengths. In a third-order Boltzmann machine, triples of units interact through symmetric conjunctive interactions. The Boltzmann learning algorithm is generalized t o higher-order interactions. The rate of learning for internal representations in a higher-order Boltzmann machine should be much faster t han for a second-order Boltzmann machine based on pairwise interactions. I N T R O D U C T I O N Thousands of hours of practice are required by humans t o become experts in domains such as chess, mathematics and physics1. Learning in these domains requires the mastery of a large number of highly interrelated ideas, and a deep understanding requires generalization as well as memorization. There are two traditions in the literature on learning in neural network models. One class of models is based on the problem of content-addressable memory and emphasizes a fast, one-shot form of learning. The second class of models uses slow, incremental learning, which requires many repetitions of examples. I t is difficult in humans t o s tudy fast and slow learning in isolation. In some amnesics, however, the long-term retention of facts is severely impaired, but the slow acquisition of skills, including cognitive skills, is spared2. Thus, it is possible tha t separate memory mechanisms are used t o implement fast learning and slow learning. Long practice is required t o become a n expert, bu t expert performance is swift and difficult to analyze; with more practice there is faster performance1. Why is slow learning so slow? One possibility is t h a t the expert develops internal representations tha t allow fast parallel searches for solutions t o problems in the task domain, in contrast t o a novice who must apply knowledge piecemeal. An internal representation is a mental model of the task domain; t h a t is, internal degrees of freedom between the sensory inputs and motor outputs t h a t efficiently encode the variables relevant t o the solution of the problem. This approach can be made more precise by specifying neural network models and showing how they incorporate internal representations. L E A R N I N G IN NETWORK M O D E L S Network models of fast learning include linear correlation-matrix m o d e l ~ ~ ~ ~ ~ ~ l ~ and the more recent nonlinear autoassociative m o d e ~ s ~ ~ ~ ~ ~ ~ ' ~ . These models use the Hebb learning rule t o store information t h a t can be retrieved by the completion of partially specified input patterns. New patterns are stored by imposing the pattern on the network and altering the connection strengths between the pairs of units that are above threshold. The information that is stored therefore concerns the correlations, .or second-order relationships between the components of the pattern. The internal model is built from correlations. Network models of slow learning include the perceptronll and adaline12. These networks can classify input patterns given only examples of inputs and desired outputs. The connection strengths are changed incrementally during the training and the network gradually converges to a set of weights tha t solves the problem if such as set of weights exists. Unfortunately, there are many difficult problems that cannot be solved with these networks, such as the prediction of parity'3. The perceptron and adaline are limited because they have only one layer of modifiable connection strengths and can only implement linear discriminant functions. Higher-order problems like parity cannot be solved by storing the desired patterns using the class of contentaddressable algorithms based on the Hebb learning rule. These models are limited because the metric of similarity is based on Hamming distance and only correlations can be used t o access patterns. The first network model to demonstrably learn t o solve higher-order problems was the Boltzmann machine, which overcame the limitations of previous network models by introducing hidden units14*15116. Hidden units are added t o the network t o mediate between the input and output units; they provide the extra internal degrees of freedom needed t o form internal representations. The Boltzmann learning algorithm incrementally modifies internal connections in the network to build higher-order pattern detectors. The hidden units can be recruited t o form internal representations for any problem; however, the learning may require an extremely large number of training examples and can be excessively slow. One way t o speed up the learning is to use hidden units that have higher-order interactions with other units. THIRD-ORDER BOLTZMANN MACHINES Consider a Boltzmann machine with a cubic global energy function: where si is the state of the i t h binary unit and w;p is a weight between triples of units. This type of interaction generalizes the pairwise interactions in Hopfield networkslo and Boltzmann machines, which contribute a quadratic term t o the energy. Fig. 1 shows an interpretation of the cubic term as conjunctive synapses. Each unit in the network updates its binary state asynchronously with probability where T is a parameter the i t h unit is given by analagous t o the temperature and the total input to If wijk is symmetric on all pairs of indices then the energy of the network is nonincreasing. It can be shown that in equilibrium the probabilities of global states P, follow a Boltzmann distribution Fig. 1. Third-order interactions between three units. In the diagram the lines between units represent reciprocal interactions that are activated only when the third unit is in the on state. The third unit acts presynaptically to conjunctively control the painvise interactions. There are two forms of the Boltzmann learning algorithm, one for networks with inputs and outputs treated identically, and a second for networks where the input units are always clamped15. The former learning algorithm will be generalized for third-order interactions. The learning metric on weight space remains the same: where P , is the probability of a global state with both the inputs and outputs clamped, and Pd, is the probability of a global state when the network is allowed to run freely. I t can be shown that the gradient of G is given by where p i jk is the ensemble average probability of three units all being in the on state when the input and output units are clamped, and pi;k is the corresponding probability when the network is running freely. T o minimize G , it is sufficient to measure the time averaged triple co-occurence probabilities when the network is in equilibrium under the two conditions and t o change each weight according to where c scales the size of each weight change. HIGHER-ORDER BOLTZMANN MACHINES Define the energy of a k -th order Boltzmann machine as where w 7172 . . . 7t is a k -dimensional weight indices. The G matrix can be minimized by matrix symmetric on all pairs of gradient descent: where P ~ , ~ . . . is the probability of the k-tuple co-occurence of the I (s 71 ,S 72 , . . s 7r ) when the inputs and outputs are clamped, and p 7172. . . 7L is the corresponding probability when the network is freely running. In general, the energy for a Boltzmann machine is the sum over all orders of interaction and the learning algorithm is a linear combination of terms from each order. This is a Markov random field with polynomial interactions17.", "year": 1986, "referenceCount": 13, "citationCount": 84, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2144529123", "name": "N. J. Cohen"}, {"authorId": "38234690", "name": "N. Weinberger"}]}, {"paperId": "08320e328296495d973df25974e66727e63c1c3b", "url": "https://www.semanticscholar.org/paper/08320e328296495d973df25974e66727e63c1c3b", "title": "Use of machine learning approaches for novel drug discovery", "abstract": "abstract Introduction: The use of computational tools in the early stages of drug development has increased in recent decades. Machine learning (ML) approaches have been of special interest, since they can be applied in several steps of the drug discovery methodology, such as prediction of target structure, prediction of biological activity of new ligands through model construction, discovery or optimization of hits, and construction of models that predict the pharmacokinetic and toxicological (ADMET) profile of compounds. Areas covered: This article presents an overview on some applications of ML techniques in drug design. These techniques can be employed in ligand-based drug design (LBDD) and structure-based drug design (SBDD) studies, such as similarity searches, construction of classification and/or prediction models of biological activity, prediction of secondary structures and binding sites docking and virtual screening. Expert opinion: Successful cases have been reported in the literature, demonstrating the efficiency of ML techniques combined with traditional approaches to study medicinal chemistry problems. Some ML techniques used in drug design are: support vector machine, random forest, decision trees and artificial neural networks. Currently, an important application of ML techniques is related to the calculation of scoring functions used in docking and virtual screening assays from a consensus, combining traditional and ML techniques in order to improve the prediction of binding sites and docking solutions.", "year": 2016, "referenceCount": 184, "citationCount": 160, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "46480421", "name": "Ang\u00e9lica Nakagawa Lima"}, {"authorId": "2335673", "name": "E. A. Philot"}, {"authorId": "4982244", "name": "G. Trossini"}, {"authorId": "143865051", "name": "L. Scott"}, {"authorId": "6019396", "name": "V. Maltarollo"}, {"authorId": "2082452", "name": "K. M. Honorio"}]}, {"paperId": "e8d330f11df9c69f38b78a7cc4b1333ebecf7c55", "url": "https://www.semanticscholar.org/paper/e8d330f11df9c69f38b78a7cc4b1333ebecf7c55", "title": "Ethical Machine Learning in Health Care", "abstract": "The use of machine learning (ML) in healthcare raises numerous ethical concerns, especially as models can amplify existing health inequities. Here, we outline ethical considerations for equitable ML in the advancement of healthcare. Specifically, we frame ethics of ML in healthcare through the lens of social justice. We describe ongoing efforts and outline challenges in a proposed pipeline of ethical ML in health, ranging from problem selection to postdeployment considerations. We close by summarizing recommendations to address these challenges.", "year": 2020, "referenceCount": 164, "citationCount": 94, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Psychology", "Computer Science", "Sociology"], "authors": [{"authorId": "34574044", "name": "I. Chen"}, {"authorId": "145192191", "name": "E. Pierson"}, {"authorId": "48345067", "name": "Sherri Rose"}, {"authorId": "34287745", "name": "Shalmali Joshi"}, {"authorId": "6745873", "name": "Kadija Ferryman"}, {"authorId": "2804918", "name": "M. Ghassemi"}]}, {"paperId": "3b718eebc6f4469053dadb5737b67ee18b16722d", "url": "https://www.semanticscholar.org/paper/3b718eebc6f4469053dadb5737b67ee18b16722d", "title": "Beyond prediction: Using big data for policy problems", "abstract": "Machine-learning prediction methods have been extremely productive in applications ranging from medicine to allocating fire and health inspectors in cities. However, there are a number of gaps between making a prediction and making a decision, and underlying assumptions need to be understood in order to optimize data-driven decision-making.", "year": 2017, "referenceCount": 27, "citationCount": 344, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2631417", "name": "S. Athey"}]}, {"paperId": "cb9f47075d37affa2ddc734eceaac10ee27ea86c", "url": "https://www.semanticscholar.org/paper/cb9f47075d37affa2ddc734eceaac10ee27ea86c", "title": "Learning Control of Robot Manipulators", "abstract": "Learning control encompasses a class of control algorithms for programmable machines such as robots which attain, through an iterative process, the motor dexterity that enables the machine to execute complex tasks. In this paper we discuss the use of function identification and adaptive control algorithms in learning controllers for robot manipulators. In particular, we discuss the similarities and differences between betterment learning schemes, repetitive controllers and adaptive learning schemes based on integral transforms. The stability and convergence properties of adaptive learning algorithms based on integral transforms are highlighted and experimental results illustrating some of these properties are presented", "year": 1993, "referenceCount": 0, "citationCount": 121, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1740258", "name": "R. Horowitz"}]}, {"paperId": "59c9da928fbf10da0c46eef255f14aba2a8ed9a9", "url": "https://www.semanticscholar.org/paper/59c9da928fbf10da0c46eef255f14aba2a8ed9a9", "title": "Evaluation of machine-learning protocols for technology-assisted review in electronic discovery", "abstract": "Abstract Using a novel evaluation toolkit that simulates a human reviewer in the loop, we compare the effectiveness of three machine-learning protocols for technology-assisted review as used in document review for discovery in legal proceedings. Our comparison addresses a central question in the deployment of technology-assisted review: Should training documents be selected at random, or should they be selected using one or more non-random methods, such as keyword search or active learning? On eight review tasks -- four derived from the TREC 2009 Legal Track and four derived from actual legal matters -- recall was measured as a function of human review effort. The results show that entirely non-random training methods, in which the initial training documents are selected using a simple keyword search, and subsequent training documents are selected by active learning, require substantially and significantly less human review effort (P<0.01) to achieve any given level of recall, than passive learning, in which the machine-learning algorithm plays no role in the selection of training documents. Among passive-learning methods, significantly less human review effort (P<0.01) is required when keywords are used instead of random sampling to select the initial training documents. Among active-learning methods, continuous active learning with relevance feedback yields generally superior results to simple active learning with uncertainty sampling, while avoiding the vexing issue of \"stabilization\" -- determining when training is adequate, and therefore may stop.", "year": 2014, "referenceCount": 32, "citationCount": 142, "influentialCitationCount": 31, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3114123", "name": "G. Cormack"}, {"authorId": "2637161", "name": "Maura R. Grossman"}]}, {"paperId": "81be22bd4cf4e7ebb6ff5aa2a415e6b9aaf712fc", "url": "https://www.semanticscholar.org/paper/81be22bd4cf4e7ebb6ff5aa2a415e6b9aaf712fc", "title": "Predicting stock and stock price index movement using Trend Deterministic Data Preparation and machine learning techniques", "abstract": null, "year": 2015, "referenceCount": 27, "citationCount": 656, "influentialCitationCount": 38, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2070426849", "name": "Jigar Patel"}, {"authorId": "144038908", "name": "Sahil R. Shah"}, {"authorId": "1977290", "name": "Priyank Thakkar"}, {"authorId": "1794896", "name": "K. Kotecha"}]}, {"paperId": "b9aa5d948aaa882dcd5ed349daa6dca2acfd4a70", "url": "https://www.semanticscholar.org/paper/b9aa5d948aaa882dcd5ed349daa6dca2acfd4a70", "title": "Sentiment Classification Using Machine Learning Techniques with Syntax Features", "abstract": "Sentiment classification has adopted machine learning techniques to improve its precision and efficiency. However, the features are always produced by basic words-bag methods without much consideration for words' syntactic properties, which could play an important role in the judgment of sentiment meanings. To remedy this, we firstly generate syntax trees of the sentences, with the analysis of syntactic features of the sentences. Then we introduce multiple sentiment features into the basic words-bag features. Such features were trained on movie reviews as data, with machine learning methods (Naive Bayes and support vector machines). The features and factors introduced by syntax tree were examined to generate a more accurate solution for sentiment classification.", "year": 2015, "referenceCount": 18, "citationCount": 98, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2105524079", "name": "Huang Zou"}, {"authorId": "30886367", "name": "Xinhuai Tang"}, {"authorId": "2064542123", "name": "Bin Xie"}, {"authorId": "47655430", "name": "Bing Liu"}]}, {"paperId": "8b20f103c1f20074fa35bd8fc41983964283acac", "url": "https://www.semanticscholar.org/paper/8b20f103c1f20074fa35bd8fc41983964283acac", "title": "Fictitious Self-Play in Extensive-Form Games", "abstract": "Fictitious play is a popular game-theoretic model of learning in games. However, it has received little attention in practical applications to large problems. This paper introduces two variants of fictitious play that are implemented in behavioural strategies of an extensive-form game. The first variant is a full-width process that is realization equivalent to its normal-form counterpart and therefore inherits its convergence guarantees. However, its computational requirements are linear in time and space rather than exponential. The second variant, Fictitious Self-Play, is a machine learning framework that implements fictitious play in a sample-based fashion. Experiments in imperfect-information poker games compare our approaches and demonstrate their convergence to approximate Nash equilibria.", "year": 2015, "referenceCount": 36, "citationCount": 170, "influentialCitationCount": 23, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2052354576", "name": "Johannes Heinrich"}, {"authorId": "1975889", "name": "Marc Lanctot"}, {"authorId": "145824029", "name": "David Silver"}]}, {"paperId": "1890c124749d00cce965e0b9495eafe127e16a26", "url": "https://www.semanticscholar.org/paper/1890c124749d00cce965e0b9495eafe127e16a26", "title": "Chapter 11 Transfer Learning", "abstract": "Transfer learning is the improvement of learning in a new task through the transfer of knowledge from a related task that has already been learned. While most machine learning algorithms are designed to address single tasks, the development of algorithms that facilitate transfer learning is a topic of ongoing interest in the machine-learning community. This chapter provides an introduction to the goals, formulations, and challenges of transfer learning. It surveys current research in this area, giving an overview of the state of the art and outlining the open problems. The survey covers transfer in both inductive learning and reinforcement learning, and discusses the issues of negative transfer and task mapping in depth.", "year": 2009, "referenceCount": 63, "citationCount": 93, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1801209", "name": "Lisa A. Torrey"}, {"authorId": "1734317", "name": "J. Shavlik"}]}, {"paperId": "71207632839723b51368fa5221c66dc75325bf58", "url": "https://www.semanticscholar.org/paper/71207632839723b51368fa5221c66dc75325bf58", "title": "Increasing generality in machine learning through procedural content generation", "abstract": null, "year": 2019, "referenceCount": 100, "citationCount": 64, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1745664", "name": "S. Risi"}, {"authorId": "1810053", "name": "J. Togelius"}]}, {"paperId": "b5c0ce947c1d0f3f455deda4d369567d42bfe4a5", "url": "https://www.semanticscholar.org/paper/b5c0ce947c1d0f3f455deda4d369567d42bfe4a5", "title": "Recent advances on artificial intelligence and learning techniques in cognitive radio networks", "abstract": null, "year": 2015, "referenceCount": 150, "citationCount": 116, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3177753", "name": "Nadine Abbas"}, {"authorId": "143957899", "name": "Y. Nasser"}, {"authorId": "2056149922", "name": "Karim Ahmad"}]}, {"paperId": "0b822de3ed689344862eecb5ad86c36eb2ff9de5", "url": "https://www.semanticscholar.org/paper/0b822de3ed689344862eecb5ad86c36eb2ff9de5", "title": "One-Class Classification with Extreme Learning Machine", "abstract": "One-class classification problem has been investigated thoroughly for past decades. Among one of the most effective neural network approaches for one-class classification, autoencoder has been successfully applied for many applications. However, this classifier relies on traditional learning algorithms such as backpropagation to train the network, which is quite time-consuming. To tackle the slow learning speed in autoencoder neural network, we propose a simple and efficient one-class classifier based on extreme learning machine (ELM). The essence of ELM is that the hidden layer need not be tuned and the output weights can be analytically determined, which leads to much faster learning speed. The experimental evaluation conducted on several real-world benchmarks shows that the ELM based one-class classifier can learn hundreds of times faster than autoencoder and it is competitive over a variety of one-class classification methods.", "year": 2015, "referenceCount": 66, "citationCount": 106, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2105528449", "name": "Qian Leng"}, {"authorId": "144097734", "name": "H. Qi"}, {"authorId": "145235303", "name": "Jun Miao"}, {"authorId": "2111465410", "name": "Wentao Zhu"}, {"authorId": "1807065", "name": "Guiping Su"}]}, {"paperId": "d258b2339240529291c512bab214f7304d706234", "url": "https://www.semanticscholar.org/paper/d258b2339240529291c512bab214f7304d706234", "title": "Dynamic Power Management Using Machine Learning", "abstract": "Dynamic power management (DPM) work proposed to date places inactive components into low power states using a single DPM policy. In contrast, we instead dynamically select among a set of DPM policies with a machine learning algorithm. We leverage the fact that different policies outperform each other under different workloads and devices. Our algorithm adapts to changes in workloads and guarantees quick convergence to the best performing policy for each workload. We performed experiments with a policy set representing state of the art DPM policies on a hard disk drive and a WLAN card. Our results show that our algorithm adapts really well with changing device and workload characteristics and achieves an overall performance comparable to the best performing policy at any point of time", "year": 2006, "referenceCount": 25, "citationCount": 97, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2172982065", "name": "Gaurav Dhiman"}, {"authorId": "3560620", "name": "T. Simunic"}]}, {"paperId": "10d7e74cfaaac6f86ddb084a441fe76e65fc9626", "url": "https://www.semanticscholar.org/paper/10d7e74cfaaac6f86ddb084a441fe76e65fc9626", "title": "Orbital-free bond breaking via machine learning.", "abstract": "Using a one-dimensional model, we explore the ability of machine learning to approximate the non-interacting kinetic energy density functional of diatomics. This nonlinear interpolation between Kohn-Sham reference calculations can (i) accurately dissociate a diatomic, (ii) be systematically improved with increased reference data and (iii) generate accurate self-consistent densities via a projection method that avoids directions with no data. With relatively few densities, the error due to the interpolation is smaller than typical errors in standard exchange-correlation functionals.", "year": 2013, "referenceCount": 50, "citationCount": 91, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Mathematics", "Medicine", "Chemistry"], "authors": [{"authorId": "27796226", "name": "John C. Snyder"}, {"authorId": "48041657", "name": "M. Rupp"}, {"authorId": "39960184", "name": "K. Hansen"}, {"authorId": "118171535", "name": "Leo Blooston"}, {"authorId": "145034054", "name": "K. M\u00fcller"}, {"authorId": "2544144", "name": "K. Burke"}]}, {"paperId": "f0eef41b5f8c375812ea8f62ef908d81a53515d4", "url": "https://www.semanticscholar.org/paper/f0eef41b5f8c375812ea8f62ef908d81a53515d4", "title": "MOOC dropout prediction using machine learning techniques: Review and research challenges", "abstract": "MOOC represents an ultimate way to deliver educational content in higher education settings by providing high-quality educational material to the students throughout the world. Considering the differences between traditional learning paradigm and MOOCs, a new research agenda focusing on predicting and explaining dropout of students and low completion rates in MOOCs has emerged. However, due to different problem specifications and evaluation metrics, performing a comparative analysis of state-of-the-art machine learning architectures is a challenging task. In this paper, we provide an overview of the MOOC student dropout prediction phenomenon where machine learning techniques have been utilized. Furthermore, we highlight some solutions being used to tackle with dropout problem, provide an analysis about the challenges of prediction models, and propose some valuable insights and recommendations that might lead to developing useful and effective machine learning solutions to solve the MOOC dropout problem.", "year": 2018, "referenceCount": 41, "citationCount": 93, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2035596", "name": "F. Dalipi"}, {"authorId": null, "name": "Ali Shariq Imran"}, {"authorId": "2429723", "name": "Zenun Kastrati"}]}, {"paperId": "8c117255a9fc1641d0f7309601985bb05b29e639", "url": "https://www.semanticscholar.org/paper/8c117255a9fc1641d0f7309601985bb05b29e639", "title": "Machine Learning for Wideband Localization", "abstract": "Wireless localization has a great importance in a variety of areas including commercial, service, and military positioning and tracking systems. In harsh indoor environments, it is hard to localize an agent with high accuracy due to non-line-of-sight (NLOS) radio blockage or insufficient information from anchors. Therefore, NLOS identification and mitigation are highlighted as an effective way to improve the localization accuracy. In this paper, we develop a robust and efficient algorithm to enhance the accuracy for (ultrawide bandwidth) time-of-arrival localization through identifying and mitigating NLOS signals with relevance vector machine (RVM) techniques. We also propose a new localization algorithm, called the two-step iterative (TSI) algorithm, which converges fast with a finite number of iterations. To enhance the localization accuracy as well as expand the coverage of a localizable area, we continue to exploit the benefits of RVM in both classification and regression for cooperative localization by extending the TSI algorithm to a centralized cooperation case. For self-localization setting, we then develop a distributed cooperative algorithm based on variational Bayesian inference to simplify message representations on factor graphs and reduce communication overheads between agents. In particular, we build a refined version of Gaussian variational message passing to reduce the computational complexity while maintaining the localization accuracy. Finally, we introduce the notion of a stochastic localization network to verify proposed cooperative localization algorithms.", "year": 2015, "referenceCount": 68, "citationCount": 82, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3106384", "name": "T. V. Nguyen"}, {"authorId": "3186716", "name": "Youngmin Jeong"}, {"authorId": "1686120", "name": "Hyundong Shin"}, {"authorId": "1748878", "name": "M. Win"}]}, {"paperId": "a9007cf4c4ec7e4fc956bead7008a3605451de49", "url": "https://www.semanticscholar.org/paper/a9007cf4c4ec7e4fc956bead7008a3605451de49", "title": "Interpretability via Model Extraction", "abstract": "The ability to interpret machine learning models has become increasingly important now that machine learning is used to inform consequential decisions. We propose an approach called model extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.", "year": 2017, "referenceCount": 29, "citationCount": 90, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1697444", "name": "O. Bastani"}, {"authorId": "2166777040", "name": "C. Kim"}, {"authorId": "10803100", "name": "Hamsa Bastani"}]}, {"paperId": "e0797523708c1bc57a66fd24d3f93e666b05752d", "url": "https://www.semanticscholar.org/paper/e0797523708c1bc57a66fd24d3f93e666b05752d", "title": "Replicability is not Reproducibility:Nor is it Good Science", "abstract": "At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our field. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientific discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important differences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.", "year": 2009, "referenceCount": 16, "citationCount": 265, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "2506249", "name": "C. Drummond"}]}, {"paperId": "070af49f0ad24bd1e0e305fd206933aa49eb7107", "url": "https://www.semanticscholar.org/paper/070af49f0ad24bd1e0e305fd206933aa49eb7107", "title": "Multicenter Comparison of Machine Learning Methods and Conventional Regression for Predicting Clinical Deterioration on the Wards", "abstract": "Objective:Machine learning methods are flexible prediction algorithms that may be more accurate than conventional regression. We compared the accuracy of different techniques for detecting clinical deterioration on the wards in a large, multicenter database. Design:Observational cohort study. Setting:Five hospitals, from November 2008 until January 2013. Patients:Hospitalized ward patients Interventions:None Measurements And Main Results:Demographic variables, laboratory values, and vital signs were utilized in a discrete-time survival analysis framework to predict the combined outcome of cardiac arrest, intensive care unit transfer, or death. Two logistic regression models (one using linear predictor terms and a second utilizing restricted cubic splines) were compared to several different machine learning methods. The models were derived in the first 60% of the data by date and then validated in the next 40%. For model derivation, each event time window was matched to a non-event window. All models were compared to each other and to the Modified Early Warning score, a commonly cited early warning score, using the area under the receiver operating characteristic curve (AUC). A total of 269,999 patients were admitted, and 424 cardiac arrests, 13,188 intensive care unit transfers, and 2,840 deaths occurred in the study. In the validation dataset, the random forest model was the most accurate model (AUC, 0.80 [95% CI, 0.80-0.80]). The logistic regression model with spline predictors was more accurate than the model utilizing linear predictors (AUC, 0.77 vs 0.74; p < 0.01), and all models were more accurate than the MEWS (AUC, 0.70 [95% CI, 0.70-0.70]). Conclusions:In this multicenter study, we found that several machine learning methods more accurately predicted clinical deterioration than logistic regression. Use of detection algorithms derived from these techniques may result in improved identification of critically ill patients on the wards.", "year": 2016, "referenceCount": 34, "citationCount": 359, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "5095349", "name": "M. Churpek"}, {"authorId": "5984183", "name": "Trevor C. Yuen"}, {"authorId": "49360839", "name": "C. Winslow"}, {"authorId": "1864266", "name": "D. Meltzer"}, {"authorId": "1987434491", "name": "M. Kattan"}, {"authorId": "2598267", "name": "D. Edelson"}]}, {"paperId": "9f5a4f462b73dd56211d15d9baf74e2e1e9b6ab3", "url": "https://www.semanticscholar.org/paper/9f5a4f462b73dd56211d15d9baf74e2e1e9b6ab3", "title": "Online Ensemble Learning: An Empirical Study", "abstract": null, "year": 2000, "referenceCount": 50, "citationCount": 162, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145841336", "name": "Alan Fern"}, {"authorId": "1718146", "name": "R. Givan"}]}, {"paperId": "2ea4a33a468958de14303daaaba2349d0ed07b73", "url": "https://www.semanticscholar.org/paper/2ea4a33a468958de14303daaaba2349d0ed07b73", "title": "Deterministic Boltzmann Learning Performs Steepest Descent in Weight-Space", "abstract": "The Boltzmann machine learning procedure has been successfully applied in deterministic networks of analog units that use a mean field approximation to efficiently simulate a truly stochastic system (Peterson and Anderson 1987). This type of deterministic Boltzmann machine (DBM) learns much faster than the equivalent stochastic Boltzmann machine (SBM), but since the learning procedure for DBM's is only based on an analogy with SBM's, there is no existing proof that it performs gradient descent in any function, and it has only been justified by simulations. By using the appropriate interpretation for the way in which a DBM represents the probability of an output vector given an input vector, it is shown that the DBM performs steepest descent in the same function as the original SBM, except at rare discontinuities. A very simple way of forcing the weights to become symmetrical is also described, and this makes the DBM more biologically plausible than back-propagation (Werbos 1974; Parker 1985; Rumelhart et al. 1986).", "year": 1989, "referenceCount": 5, "citationCount": 181, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1695689", "name": "Geoffrey E. Hinton"}]}, {"paperId": "a4f981c06d398849eb78894e98e7c16ae4372d7e", "url": "https://www.semanticscholar.org/paper/a4f981c06d398849eb78894e98e7c16ae4372d7e", "title": "Learning multidimensional signal processing", "abstract": "This paper presents our general strategy for designing learning machines as well as a number of particular designs. The search for methods allowing a sufficient level of adaptivity are based on two main principles: 1) simple adaptive local models; and 2) adaptive model distribution. Particularly important concepts in our work is mutual information and canonical correlation. Examples are given on learning feature descriptors, modeling disparity, synthesis of a global 3-mode model and a setup for reinforcement learning of online video coder parameter control.", "year": 1998, "referenceCount": 192, "citationCount": 224, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3117916", "name": "H. Knutsson"}, {"authorId": "145290346", "name": "M. Borga"}, {"authorId": "2976013", "name": "T. Landelius"}]}, {"paperId": "afeecd9a41fecd8c19e40e004cbcbf6bb4b4e39d", "url": "https://www.semanticscholar.org/paper/afeecd9a41fecd8c19e40e004cbcbf6bb4b4e39d", "title": "Interactive optimization for steering machine classification", "abstract": "Interest has been growing within HCI on the use of machine learning and reasoning in applications to classify such hidden states as user intentions, based on observations. HCI researchers with these interests typically have little expertise in machine learning and often employ toolkits as relatively fixed \"black boxes\" for generating statistical classifiers. However, attempts to tailor the performance of classifiers to specific application requirements may require a more sophisticated understanding and custom-tailoring of methods. We present ManiMatrix, a system that provides controls and visualizations that enable system builders to refine the behavior of classification systems in an intuitive manner. With ManiMatrix, users directly refine parameters of a confusion matrix via an interactive cycle of re-classification and visualization. We present the core methods and evaluate the effectiveness of the approach in a user study. Results show that users are able to quickly and effectively modify decision boundaries of classifiers to tai-lor the behavior of classifiers to problems at hand.", "year": 2010, "referenceCount": 27, "citationCount": 151, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2189118", "name": "Ashish Kapoor"}, {"authorId": "49132427", "name": "Bongshin Lee"}, {"authorId": "1719056", "name": "Desney S. Tan"}, {"authorId": "145479841", "name": "E. Horvitz"}]}, {"paperId": "ba0a255fc5f3a948d73e69005b6c64f0365b0d17", "url": "https://www.semanticscholar.org/paper/ba0a255fc5f3a948d73e69005b6c64f0365b0d17", "title": "Statistical Procedures for Forecasting Criminal Behavior", "abstract": "There is a substantial and powerful literature in statistics and computer science clearly demonstrating that modern machine learning procedures can forecast more accurately than conventional parametric statistical models such as logistic regression. Yet, several recent studies have claimed that for criminal justice applications, forecasting accuracy is about the same. In this paper, we address the apparent contradiction. Forecasting accuracy will depend on the complexity of the decision boundary. When that boundary is simple, most forecasting tools will have similar accuracy. When that boundary is complex, procedures such as machine learning, that proceed adaptively from the data will improve forecasting accuracy, sometimes dramatically. Machine learning has other benefits as well, and e\u21b5ective software is readily available.", "year": 2013, "referenceCount": 46, "citationCount": 136, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "50496565", "name": "R. Berk"}, {"authorId": "1576051500", "name": "J. Bleich"}]}, {"paperId": "0e54e1d7ca6d795bb5e6bd9ad9291af46fbcaa72", "url": "https://www.semanticscholar.org/paper/0e54e1d7ca6d795bb5e6bd9ad9291af46fbcaa72", "title": "Trident: Efficient 4PC Framework for Privacy Preserving Machine Learning", "abstract": "Machine learning has started to be deployed in fields such as healthcare and finance, which propelled the need for and growth of privacy-preserving machine learning (PPML). We propose an actively secure four-party protocol (4PC), and a framework for PPML, showcasing its applications on four of the most widely-known machine learning algorithms -- Linear Regression, Logistic Regression, Neural Networks, and Convolutional Neural Networks. \nOur 4PC protocol tolerating at most one malicious corruption is practically efficient as compared to the existing works. We use the protocol to build an efficient mixed-world framework (Trident) to switch between the Arithmetic, Boolean, and Garbled worlds. Our framework operates in the offline-online paradigm over rings and is instantiated in an outsourced setting for machine learning. Also, we propose conversions especially relevant to privacy-preserving machine learning. \nThe highlights of our framework include using a minimal number of expensive circuits overall as compared to ABY3. This can be seen in our technique for truncation, which does not affect the online cost of multiplication and removes the need for any circuits in the offline phase. Our B2A conversion has an improvement of $\\mathbf{7} \\times$ in rounds and $\\mathbf{18} \\times$ in the communication complexity. In addition to these, all of the special conversions for machine learning, e.g. Secure Comparison, achieve constant round complexity. \nThe practicality of our framework is argued through improvements in the benchmarking of the aforementioned algorithms when compared with ABY3. All the protocols are implemented over a 64-bit ring in both LAN and WAN settings. Our improvements go up to $\\mathbf{187} \\times$ for the training phase and $\\mathbf{158} \\times$ for the prediction phase when observed over LAN and WAN.", "year": 2019, "referenceCount": 57, "citationCount": 92, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "65831050", "name": "Rahul Rachuri"}, {"authorId": "153456203", "name": "Ajith Suresh"}]}, {"paperId": "a68fccc152d238f62848de1be8522ccd71137ac0", "url": "https://www.semanticscholar.org/paper/a68fccc152d238f62848de1be8522ccd71137ac0", "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models", "abstract": "Machine learning (ML) has become a core component of many real-world applications and training data is a key factor that drives current progress. This huge success has led Internet companies to deploy machine learning as a service (MLaaS). Recently, the first membership inference attack has shown that extraction of information on the training set is possible in such MLaaS settings, which has severe security and privacy implications. \nHowever, the early demonstrations of the feasibility of such attacks have many assumptions on the adversary, such as using multiple so-called shadow models, knowledge of the target model structure, and having a dataset from the same distribution as the target model's training data. We relax all these key assumptions, thereby showing that such attacks are very broadly applicable at low cost and thereby pose a more severe risk than previously thought. We present the most comprehensive study so far on this emerging and developing threat using eight diverse datasets which show the viability of the proposed attacks across domains. \nIn addition, we propose the first effective defense mechanisms against such broader class of membership inference attacks that maintain a high level of utility of the ML model.", "year": 2018, "referenceCount": 58, "citationCount": 433, "influentialCitationCount": 92, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "66697271", "name": "A. Salem"}, {"authorId": "2145954003", "name": "Yang Zhang"}, {"authorId": "144887171", "name": "Mathias Humbert"}, {"authorId": "1739548", "name": "Mario Fritz"}, {"authorId": "144588806", "name": "M. Backes"}]}, {"paperId": "95d129c40631ca1284c931ecb7d1c10d1ad11b55", "url": "https://www.semanticscholar.org/paper/95d129c40631ca1284c931ecb7d1c10d1ad11b55", "title": "Machine Learning for Wireless Networks with Artificial Intelligence: A Tutorial on Neural Networks", "abstract": "Next-generation wireless networks must support ultra-reliable, low-latency communication and intelligently manage a massive number of Internet of Things (IoT) devices in real-time, within a highly dynamic environment. This need for stringent communication quality-of-service (QoS) requirements as well as mobile edge and core intelligence can only be realized by integrating fundamental notions of artificial intelligence (AI) and machine learning across the wireless infrastructure and end-user devices. In this context, this paper provides a comprehensive tutorial that introduces the main concepts of machine learning, in general, and artificial neural networks (ANNs), in particular, and their potential applications in wireless communications. For this purpose, we present a comprehensive overview on a number of key types of neural networks that include feed-forward, recurrent, spiking, and deep neural networks. For each type of neural network, we present the basic architecture and training procedure, as well as the associated challenges and opportunities. Then, we provide an in-depth overview on the variety of wireless communication problems that can be addressed using ANNs, ranging from communication using unmanned aerial vehicles to virtual reality and edge caching.For each individual application, we present the main motivation for using ANNs along with the associated challenges while also providing a detailed example for a use case scenario and outlining future works that can be addressed using ANNs. In a nutshell, this article constitutes one of the first holistic tutorials on the development of machine learning techniques tailored to the needs of future wireless networks.", "year": 2017, "referenceCount": 281, "citationCount": 183, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2888344", "name": "Mingzhe Chen"}, {"authorId": "2318575", "name": "Ursula Challita"}, {"authorId": "145412074", "name": "W. Saad"}, {"authorId": "1768333", "name": "Changchuan Yin"}, {"authorId": "145118318", "name": "M. Debbah"}]}, {"paperId": "751fc1ea2363125183c992633844b5a27d4c89b7", "url": "https://www.semanticscholar.org/paper/751fc1ea2363125183c992633844b5a27d4c89b7", "title": "An efficient flow-based botnet detection using supervised machine learning", "abstract": "Botnet detection represents one of the most crucial prerequisites of successful botnet neutralization. This paper explores how accurate and timely detection can be achieved by using supervised machine learning as the tool of inferring about malicious botnet traffic. In order to do so, the paper introduces a novel flow-based detection system that relies on supervised machine learning for identifying botnet network traffic. For use in the system we consider eight highly regarded machine learning algorithms, indicating the best performing one. Furthermore, the paper evaluates how much traffic needs to be observed per flow in order to capture the patterns of malicious traffic. The proposed system has been tested through the series of experiments using traffic traces originating from two well-known P2P botnets and diverse non-malicious applications. The results of experiments indicate that the system is able to accurately and timely detect botnet traffic using purely flow-based traffic analysis and supervised machine learning. Additionally, the results show that in order to achieve accurate detection traffic flows need to be monitored for only a limited time period and number of packets per flow. This indicates a strong potential of using the proposed approach within a future on-line detection framework.", "year": 2014, "referenceCount": 8, "citationCount": 114, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35428029", "name": "M. Stevanovic"}, {"authorId": "3369847", "name": "J. Pedersen"}]}, {"paperId": "2ccc855b23888cea4afd3e946fe1e7fa38bd75ca", "url": "https://www.semanticscholar.org/paper/2ccc855b23888cea4afd3e946fe1e7fa38bd75ca", "title": "Machine learning for ecosystem services", "abstract": null, "year": 2018, "referenceCount": 86, "citationCount": 86, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50499034", "name": "S. Willcock"}, {"authorId": "1400821516", "name": "J. Mart\u00ednez-L\u00f3pez"}, {"authorId": "144690213", "name": "Danny A. P. Hooftman"}, {"authorId": "2401775", "name": "K. Bagstad"}, {"authorId": "2532027", "name": "S. Balbi"}, {"authorId": "38174154", "name": "A. Marzo"}, {"authorId": "98176867", "name": "C. Prato"}, {"authorId": "4404920", "name": "S. Sciandrello"}, {"authorId": "35672394", "name": "G. Signorello"}, {"authorId": "21823432", "name": "B. Voigt"}, {"authorId": "2360945", "name": "F. Villa"}, {"authorId": "2446334", "name": "J. Bullock"}, {"authorId": "144373046", "name": "I. Athanasiadis"}]}, {"paperId": "93cc470466516346346e685ef8b9f11a1c3af8f8", "url": "https://www.semanticscholar.org/paper/93cc470466516346346e685ef8b9f11a1c3af8f8", "title": "Blind Image Deconvolution Using Machine Learning for Three-Dimensional Microscopy", "abstract": "In this work, we propose a novel method for the regularization of blind deconvolution algorithms. The proposed method employs example-based machine learning techniques for modeling the space of point spread functions. During an iterative blind deconvolution process, a prior term attracts the point spread function estimates to the learned point spread function space. We demonstrate the usage of this regularizer within a Bayesian blind deconvolution framework and also integrate into the latter a method for noise reduction, thus creating a complete blind deconvolution method. The application of the proposed algorithm is demonstrated on synthetic and real-world three-dimensional images acquired by a wide-field fluorescence microscope, where the need for blind deconvolution algorithms is indispensable, yielding excellent results.", "year": 2010, "referenceCount": 75, "citationCount": 72, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "authors": [{"authorId": "3021829", "name": "T. Kenig"}, {"authorId": "2377382", "name": "Z. Kam"}, {"authorId": "2153902", "name": "A. Feuer"}]}, {"paperId": "a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "url": "https://www.semanticscholar.org/paper/a7621b4ec18719b08f3a2a444b6d37a2e20227b7", "title": "Fast Training of Convolutional Networks through FFTs", "abstract": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges.", "year": 2013, "referenceCount": 10, "citationCount": 500, "influentialCitationCount": 60, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143949035", "name": "Micha\u00ebl Mathieu"}, {"authorId": "39713408", "name": "Mikael Henaff"}, {"authorId": "1688882", "name": "Yann LeCun"}]}, {"paperId": "80bf9577411e9f94bcc172200c41b27da542bcb1", "url": "https://www.semanticscholar.org/paper/80bf9577411e9f94bcc172200c41b27da542bcb1", "title": "Modified Logistic Regression: An Approximation to SVM and Its Applications in Large-Scale Text Categorization", "abstract": "Logistic Regression (LR) has been widely used in statistics for many years, and has received extensive study in machine learning community recently due to its close relations to Support Vector Machines (SVM) and AdaBoost. In this paper, we use a modified version of LR to approximate the optimization of SVM by a sequence of unconstrained optimization problems. We prove that our approximation will converge to SVM, and propose an iterative algorithm called \"MLR-CG\" which uses Conjugate Gradient as its inner loop. Multiclass version \"MMLR-CG\" is also obtained after simple modifications. We compare the MLR-CG with SVMlight over different text categorization collections, and show that our algorithm is much more efficient than SVMlight when the number of training examples is very large. Results of the multiclass version MMLR-CG is also reported.", "year": 2003, "referenceCount": 23, "citationCount": 150, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2151812271", "name": "Jian Zhang"}, {"authorId": "2068300013", "name": "Rong Jin"}, {"authorId": "35729970", "name": "Yiming Yang"}, {"authorId": "7661726", "name": "Alexander Hauptmann"}]}, {"paperId": "e562e95865ab7b8e12b6ae9e29dadc112833b252", "url": "https://www.semanticscholar.org/paper/e562e95865ab7b8e12b6ae9e29dadc112833b252", "title": "Feature Selection using PSO-SVM", "abstract": "The feature selection process can be considered a problem of global combinatorial optimization in machine learning, which reduces the number of features, removes irrelevant, noisy and redundant data, and results in an acceptable classification accuracy. Feature selection is of great importance in pattern classification, medical data processing, machine learning, and data mining applications. Therefore, a good feature selection method based on the number of features investigated for sample classification is needed in order to speed up the processing rate, predictive accuracy, and to avoid incomprehensibility. In this paper, particle swarm optimization (PSO) is used to implement a feature selection, and support vector machines (SVMs) with the one-versus-rest method serve as a fitness function of PSO for the classification problem. The proposed method is applied to five classification problems from the literature. Experimental results show that our method simplifies features effectively and obtains a higher classification accuracy compared to the other feature selection methods.", "year": 2007, "referenceCount": 22, "citationCount": 181, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2320900", "name": "Chung-Jui Tu"}, {"authorId": "38234756", "name": "L. Chuang"}, {"authorId": "2185216", "name": "J. Chang"}, {"authorId": "1773605", "name": "Cheng-Hong Yang"}]}, {"paperId": "4d801d7376fce8bd77be20ee8b82b42edfe3e565", "url": "https://www.semanticscholar.org/paper/4d801d7376fce8bd77be20ee8b82b42edfe3e565", "title": "Machine learning enabled autonomous microstructural characterization in 3D samples", "abstract": null, "year": 2020, "referenceCount": 42, "citationCount": 179, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Materials Science"], "authors": [{"authorId": "143777552", "name": "Henry Chan"}, {"authorId": "12480512", "name": "M. Cherukara"}, {"authorId": "4810023", "name": "T. Loeffler"}, {"authorId": "145454890", "name": "B. Narayanan"}, {"authorId": "2929500", "name": "S. Sankaranarayanan"}]}, {"paperId": "f2e79452aef17bc27ceb72aa9382db74c0fe827b", "url": "https://www.semanticscholar.org/paper/f2e79452aef17bc27ceb72aa9382db74c0fe827b", "title": "MLxtend: Providing machine learning and data science utilities and extensions to Python's scientific computing stack", "abstract": "MLxtend is a library that implements a variety of core algorithms and utilities for machine learning and data mining. The primary goal of MLxtend is to make commonly used tools accessible to researchers in academia and data scientists in industries focussing on userfriendly and intuitive APIs and compatibility to existing machine learning libraries, such as scikit-learn, when appropriate. While MLxtend implements a large variety of functions, highlights include sequential feature selection algorithms (Pudil, Novovi\u010dov\u00e1, and Kittler 1994), implementations of stacked generalization (Wolpert 1992) for classification and regression, and algorithms for frequent pattern mining (Agrawal and Ramakrishnan 1994). The sequential feature selection algorithms cover forward, backward, forward floating, and backward floating selection and leverage scikit-learn\u2019s cross-validation API (Pedregosa et al. 2011) to ensure satisfactory generalization performance upon constructing and selecting feature subsets. Besides, visualization functions are provided that allow users to inspect the estimated predictive performance, including performance intervals, for different feature subsets. The ensemble methods in MLxtend cover majority voting, stacking, and stacked generalization, all of which are compatible with scikit-learn estimators and other libraries as XGBoost (Chen and Guestrin 2016). In addition to feature selection, classification, and regression algorithms, MLxtend implements model evaluation techniques for comparing the performance of two different models via McNemar\u2019s test and multiple models via Cochran\u2019s Q test. An implementation of the 5x2 cross-validated paired t-test (Dietterich 1998) allows users to compare the performance of machine learning algorithms to each other. Furthermore, different flavors of the Bootstrap method (Efron and Tibshirani 1994), such as the .632 Bootstrap method (Efron 1983) are implemented to compute confidence intervals of performance estimates. All in all, MLxtend provides a large variety of different utilities that build upon and extend the capabilities of Python\u2019s scientific computing stack.", "year": 2018, "referenceCount": 9, "citationCount": 276, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2562040", "name": "S. Raschka"}]}, {"paperId": "2bdae3b9484e139696c983fbd2e6d3ebae581725", "url": "https://www.semanticscholar.org/paper/2bdae3b9484e139696c983fbd2e6d3ebae581725", "title": "Enhancing understanding and improving prediction of severe weather through spatiotemporal relational learning", "abstract": null, "year": 2013, "referenceCount": 88, "citationCount": 42, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2299685", "name": "A. McGovern"}, {"authorId": "30751341", "name": "D. Gagne"}, {"authorId": "2111929791", "name": "John K. Williams"}, {"authorId": "2150172379", "name": "Rodger A. Brown"}, {"authorId": "2112800", "name": "J. Basara"}]}, {"paperId": "868dbe425f0b1414c84c41941fe8f82c5d3e43ff", "url": "https://www.semanticscholar.org/paper/868dbe425f0b1414c84c41941fe8f82c5d3e43ff", "title": "Feature Selection for Dimensionality Reduction", "abstract": null, "year": 2005, "referenceCount": 55, "citationCount": 99, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1764321", "name": "D. Mladenic"}]}, {"paperId": "ac0a59165ee2ac666b1880316eefe349b87f6ba0", "url": "https://www.semanticscholar.org/paper/ac0a59165ee2ac666b1880316eefe349b87f6ba0", "title": "Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond", "abstract": "With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning such as Deep Learning (DL), LSTMs, and kernel methods are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.", "year": 2020, "referenceCount": 165, "citationCount": 70, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1699054", "name": "W. Samek"}, {"authorId": "144535526", "name": "G. Montavon"}, {"authorId": "3633358", "name": "S. Lapuschkin"}, {"authorId": "51004625", "name": "Christopher J. Anders"}, {"authorId": "145034054", "name": "K. M\u00fcller"}]}, {"paperId": "2871786cb59173a1ea2806f78a789bb1a371b815", "url": "https://www.semanticscholar.org/paper/2871786cb59173a1ea2806f78a789bb1a371b815", "title": "Ordinal extreme learning machine", "abstract": null, "year": 2010, "referenceCount": 33, "citationCount": 77, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2606144", "name": "W. Deng"}, {"authorId": "2817677", "name": "Qinghua Zheng"}, {"authorId": "143763659", "name": "S. Lian"}, {"authorId": "2118537230", "name": "Lin Chen"}, {"authorId": "47119537", "name": "X. Wang"}]}, {"paperId": "bc81b43b30a92256ec8b6aed82533955f748e2cf", "url": "https://www.semanticscholar.org/paper/bc81b43b30a92256ec8b6aed82533955f748e2cf", "title": "On the role of gradients for machine learning of molecular energies and forces", "abstract": "The accuracy of any machine learning potential can only be as good as the data used in the fitting process. The most efficient model therefore selects the training data that will yield the highest accuracy compared to the cost of obtaining the training data. We investigate the convergence of prediction errors of quantum machine learning models for organic molecules trained on energy and force labels, two common data types in molecular simulations. When training models for the potential energy surface of a single molecule, we find that the inclusion of atomic forces in the training data increases the accuracy of the predicted energies and forces 7-fold, compared to models trained on energy only. Surprisingly, for models trained on sets of organic molecules of varying size and composition in non-equilibrium conformations, inclusion of forces in the training does not improve the predicted energies of unseen molecules in new conformations. Predicted forces, however, improve about 7-fold. For the systems studied, we find that force labels and energy labels contribute equally per label to the convergence of the prediction errors. The optimal choice of what type of training data to include depends on several factors: the computational cost of acquiring the force and energy labels for training, the application domain, the property of interest and the complexity of the machine learning model. Based on our observations we describe key considerations for the creation of new datasets for potential energy surfaces of molecules which maximize the efficiency of the resulting machine learning models.", "year": 2020, "referenceCount": 63, "citationCount": 44, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Physics", "Computer Science"], "authors": [{"authorId": "145487354", "name": "Anders S. Christensen"}, {"authorId": "7847508", "name": "O. A. von Lilienfeld"}]}, {"paperId": "638342beccc1922285c81499e3e67fb65b32d6e8", "url": "https://www.semanticscholar.org/paper/638342beccc1922285c81499e3e67fb65b32d6e8", "title": "Random Forest for Bioinformatics", "abstract": null, "year": 2012, "referenceCount": 51, "citationCount": 371, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "1791105", "name": "Yanjun Qi"}]}, {"paperId": "19a935d39239497ca67fc9c560302d905b85c165", "url": "https://www.semanticscholar.org/paper/19a935d39239497ca67fc9c560302d905b85c165", "title": "A Machine-Learning Approach to Automated Knowledge-Base Building for Remote Sensing Image Analysis with GIs Data", "abstract": "A machine learning approach to automated building of knowledge bases for image analysis expert systems incorporating GIS data is presented. The method uses an inductive learning algorithm to generate production rules from training data. With this method, building a knowledge base for a rule-based expert system is easier than using the conventional knowledge acquisition approach. The knowledge base built by this method was used by an expert system to pe$orm a wetland classification of Par Pond on the Savannah River Site, South Carolina using SPOT multispectral imagery and GIs data. To evaluate the peqformance of the resultant knowledge base, the classification result was compared to classifications with two conventional methods. The accuracy assessment and the analysis of the resultant production rules suggest that the knowledge base built by the machine learning method was of good quality for image analysis with GIS data.", "year": 1997, "referenceCount": 49, "citationCount": 153, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Geography"], "authors": [{"authorId": "46422628", "name": "Xueqiao Huang"}, {"authorId": "144724659", "name": "J. R. Jensen"}]}, {"paperId": "df29abf8119450993fe299b983b6e50922192fb7", "url": "https://www.semanticscholar.org/paper/df29abf8119450993fe299b983b6e50922192fb7", "title": "An investigation of machine learning based prediction systems", "abstract": null, "year": 2000, "referenceCount": 40, "citationCount": 229, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1746905", "name": "C. Mair"}, {"authorId": "7563522", "name": "G. Kadoda"}, {"authorId": "2107476", "name": "M. Lefley"}, {"authorId": "1680525", "name": "Keith Phalp"}, {"authorId": "49058659", "name": "C. Schofield"}, {"authorId": "2319775", "name": "M. Shepperd"}, {"authorId": "98553459", "name": "S. Webster"}]}, {"paperId": "40577975766f36b4ebe4e99cb9e8fbd4671278c1", "url": "https://www.semanticscholar.org/paper/40577975766f36b4ebe4e99cb9e8fbd4671278c1", "title": "Data-driven prediction of battery cycle life before capacity degradation", "abstract": null, "year": 2019, "referenceCount": 82, "citationCount": 626, "influentialCitationCount": 35, "isOpenAccess": true, "fieldsOfStudy": ["Environmental Science"], "authors": [{"authorId": "2781167", "name": "K. Severson"}, {"authorId": "51292612", "name": "Peter M. Attia"}, {"authorId": "47243380", "name": "Norman Jin"}, {"authorId": "2056753463", "name": "Nicholas Perkins"}, {"authorId": "2114189448", "name": "Benben Jiang"}, {"authorId": "2111906467", "name": "Zi Yang"}, {"authorId": "2107951564", "name": "Michael H. Chen"}, {"authorId": "7995028", "name": "Muratahan Aykol"}, {"authorId": "144509011", "name": "Patrick K. Herring"}, {"authorId": "11550909", "name": "D. Fraggedakis"}, {"authorId": "2964245", "name": "M. Bazant"}, {"authorId": "2113975130", "name": "Stephen J. Harris"}, {"authorId": "3992087", "name": "W. Chueh"}, {"authorId": "144960825", "name": "R. Braatz"}]}, {"paperId": "27146d93a3552a8da2939e382859ad3b3333c06d", "url": "https://www.semanticscholar.org/paper/27146d93a3552a8da2939e382859ad3b3333c06d", "title": "Machine learning approaches for estimating commercial building energy consumption", "abstract": null, "year": 2017, "referenceCount": 39, "citationCount": 231, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "152830035", "name": "Caleb Robinson"}, {"authorId": "1796375", "name": "B. Dilkina"}, {"authorId": "101708515", "name": "Jeffrey Hubbs"}, {"authorId": "50550171", "name": "Wenwen Zhang"}, {"authorId": "39568110", "name": "S. Guhathakurta"}, {"authorId": "145270591", "name": "Marilyn A Brown"}, {"authorId": "95879567", "name": "R. Pendyala"}]}, {"paperId": "1eb7f46b1a0a7df823194d86543e5554aa21021a", "url": "https://www.semanticscholar.org/paper/1eb7f46b1a0a7df823194d86543e5554aa21021a", "title": "Can You Trust Your Model's Uncertainty? Evaluating Predictive Uncertainty Under Dataset Shift", "abstract": "Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive {\\em uncertainty}. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and non-Bayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous large-scale empirical comparison of these methods under dataset shift. We present a large-scale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.", "year": 2019, "referenceCount": 63, "citationCount": 843, "influentialCitationCount": 97, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2067138676", "name": "Yaniv Ovadia"}, {"authorId": "35105647", "name": "Emily Fertig"}, {"authorId": "143702207", "name": "J. Ren"}, {"authorId": "81408931", "name": "Zachary Nado"}, {"authorId": "1733143", "name": "D. Sculley"}, {"authorId": "2388416", "name": "S. Nowozin"}, {"authorId": "2403637", "name": "Joshua V. Dillon"}, {"authorId": "40627523", "name": "Balaji Lakshminarayanan"}, {"authorId": "144108062", "name": "Jasper Snoek"}]}, {"paperId": "b3f2a11d45757e675be123d55ec0eb192bcca990", "url": "https://www.semanticscholar.org/paper/b3f2a11d45757e675be123d55ec0eb192bcca990", "title": "Chiron: Privacy-preserving Machine Learning as a Service", "abstract": "Major cloud operators offer machine learning (ML) as a service, enabling customers who have the data but not ML expertise or infrastructure to train predictive models on this data. Existing ML-as-a-service platforms require users to reveal all training data to the service operator. We design, implement, and evaluate Chiron, a system for privacy-preserving machine learning as a service. First, Chiron conceals the training data from the service operator. Second, in keeping with how many existing ML-as-a-service platforms work, Chiron reveals neither the training algorithm nor the model structure to the user, providing only black-box access to the trained model. Chiron is implemented using SGX enclaves, but SGX alone does not achieve the dual goals of data privacy and model confidentiality. Chiron runs the standard ML training toolchain (including the popular Theano framework and C compiler) in an enclave, but the untrusted model-creation code from the service operator is further confined in a Ryoan sandbox to prevent it from leaking the training data outside the enclave. To support distributed training, Chiron executes multiple concurrent enclaves that exchange model parameters via a parameter server. We evaluate Chiron on popular deep learning models, focusing on benchmark image classification tasks such as CIFAR and ImageNet, and show that its training performance and accuracy of the resulting models are practical for common uses of ML-as-a-service.", "year": 2018, "referenceCount": 54, "citationCount": 145, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48062937", "name": "T. Hunt"}, {"authorId": "3469125", "name": "Congzheng Song"}, {"authorId": "2520493", "name": "R. Shokri"}, {"authorId": "1723945", "name": "Vitaly Shmatikov"}, {"authorId": "1683338", "name": "E. Witchel"}]}, {"paperId": "982a68e3bb4600331eb49c07a84718c3f839b689", "url": "https://www.semanticscholar.org/paper/982a68e3bb4600331eb49c07a84718c3f839b689", "title": "Machine-learning algorithms for credit-card applications", "abstract": null, "year": 1992, "referenceCount": 0, "citationCount": 127, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2107453430", "name": "R. H. Davis"}, {"authorId": "6587321", "name": "D. Edelman"}, {"authorId": "49028220", "name": "A. Gammerman"}]}, {"paperId": "bd52ed6a7dfefaee7df6e7d925095503d95bcca1", "url": "https://www.semanticscholar.org/paper/bd52ed6a7dfefaee7df6e7d925095503d95bcca1", "title": "The evolution of boosting algorithms. From machine learning to statistical modelling.", "abstract": "BACKGROUND\nThe concept of boosting emerged from the field of machine learning. The basic idea is to boost the accuracy of a weak classifying tool by combining various instances into a more accurate prediction. This general concept was later adapted to the field of statistical modelling. Nowadays, boosting algorithms are often applied to estimate and select predictor effects in statistical regression models.\n\n\nOBJECTIVES\nThis review article attempts to highlight the evolution of boosting algorithms from machine learning to statistical modelling.\n\n\nMETHODS\nWe describe the AdaBoost algorithm for classification as well as the two most prominent statistical boosting approaches, gradient boosting and likelihood-based boosting for statistical modelling. We highlight the methodological background and present the most common software implementations.\n\n\nRESULTS\nAlthough gradient boosting and likelihood-based boosting are typically treated separately in the literature, they share the same methodological roots and follow the same fundamental concepts. Compared to the initial machine learning algorithms, which must be seen as black-box prediction schemes, they result in statistical models with a straight-forward interpretation.\n\n\nCONCLUSIONS\nStatistical boosting algorithms have gained substantial interest during the last decade and offer a variety of options to address important research questions in modern biomedicine.", "year": 2014, "referenceCount": 91, "citationCount": 180, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "144831678", "name": "A. Mayr"}, {"authorId": "38156792", "name": "H. Binder"}, {"authorId": "144270430", "name": "O. Gefeller"}, {"authorId": "144323802", "name": "M. Schmid"}]}, {"paperId": "f742f7e0394b36faf456818a718e11f35db0ae2b", "url": "https://www.semanticscholar.org/paper/f742f7e0394b36faf456818a718e11f35db0ae2b", "title": "Decision Forests for Classification, Regression, Density Estimation, Manifold Learning and Semi-Supervised Learning", "abstract": "This paper presents a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision and medical image analysis tasks. Our model extends existing forest-based techniques as it unifies classification, regression, density estimation, manifold learning, semi-supervised learning and active learning under the same decision forest framework. This means that the core implementation needs be written and optimized only once, and can then be applied to many diverse tasks. The proposed model may be used both in a generative or discriminative way and may be applied to discrete or continuous, labelled or unlabelled data. The main contributions of this paper are: 1) proposing a single, probabilistic and efficient model for a variety of learning tasks; 2) demonstrating margin-maximizing properties of classification forests; 3) introducing density forests for learning accurate probability density functions; 4) proposing efficient algorithms for sampling from the forest generative model; 5) introducing manifold forests for non-linear embedding and dimensionality reduction; 6) proposing new and efficient forest-based algorithms for transductive and active learning. We discuss how alternatives such as random ferns and extremely randomized trees stem from our more general model. This paper is directed at both students who wish to learn the basics of decision forests, as well as researchers interested in our new contributions. It presents both fundamental and novel concepts in a structured way, with many illustrative examples and real-world applications. Thorough comparisons with state of the art algorithms such as support vector machines, boosting and Gaussian processes are presented and relative advantages and disadvantages discussed.The many synthetic examples and existing commercial applications demonstrate the validity of the proposed model and its flexibility. Powerpoint slides (with many examples and animations) are also available from http://research.microsoft.com/groups/vision/decisionforests.aspx", "year": 2011, "referenceCount": 109, "citationCount": 322, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "1716777", "name": "A. Criminisi"}, {"authorId": "1796918", "name": "E. Konukoglu"}, {"authorId": "143774737", "name": "J. Shotton"}]}, {"paperId": "90895f2ca4fcc76bcf1549a0b2106c4d1590f6b5", "url": "https://www.semanticscholar.org/paper/90895f2ca4fcc76bcf1549a0b2106c4d1590f6b5", "title": "Machine Learning of Musical Gestures", "abstract": "We present an overview of machine learning (ML) techniques and their application in interactive music and new digital instrument design. We first provide the non-specialist reader an introduction to two ML tasks, classification and regression, that are particularly relevant for gestural interaction. We then present a review of the literature in current NIME research that uses ML in musical gesture analysis and gestural sound control. We describe the ways in which machine learning is useful for creating expressive musical interaction, and in turn why live music performance presents a pertinent and challenging use case for machine learning.", "year": 2013, "referenceCount": 33, "citationCount": 63, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2046048", "name": "Baptiste Caramiaux"}, {"authorId": "39647503", "name": "A. Tanaka"}]}, {"paperId": "d26b6180c52013f7eabd06c0c576604bb3e60fb4", "url": "https://www.semanticscholar.org/paper/d26b6180c52013f7eabd06c0c576604bb3e60fb4", "title": "Using Reinforcement Learning to Spider the Web Efficiently", "abstract": "Consider the task of exploring the Web in order to find pages of a particular kind or on a particular topic. This task arises in the construction of search engines and Web knowledge bases. This paper argues that the creation of efficient web spiders is best framed and solved by reinforcement learning, a branch of machine learning that concerns itself with optimal sequential decision making. One strength of reinforcement learning is that it provides a formalism for measuring the utility of actions that give benefit only in the future. We present an algorithm for learning a value function that maps hyperlinks to future discounted reward using a naive Bayes text classifier. Experiments on two real-world spidering tasks show a threefold improvement in spidering efficiency over traditional breadth-first search, and up to a two-fold improvement over reinforcement learning with immediate reward only.", "year": 1999, "referenceCount": 19, "citationCount": 298, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35211659", "name": "Jason D. M. Rennie"}, {"authorId": "143753639", "name": "A. McCallum"}]}, {"paperId": "50716b6a67eabb104d1890d210a81e0b825a0e5a", "url": "https://www.semanticscholar.org/paper/50716b6a67eabb104d1890d210a81e0b825a0e5a", "title": "Hybrid MPI/OpenMP Parallel Linear Support Vector Machine Training", "abstract": "Support vector machines are a powerful machine learning technology, but the training process involves a dense quadratic optimization problem and is computationally challenging. A parallel implementation of linear Support Vector Machine training has been developed, using a combination of MPI and OpenMP. Using an interior point method for the optimization and a reformulation that avoids the dense Hessian matrix, the structure of the augmented system matrix is exploited to partition data and computations amongst parallel processors efficiently. The new implementation has been applied to solve problems from the PASCAL Challenge on Large-scale Learning. We show that our approach is competitive, and is able to solve problems in the Challenge many times faster than other parallel approaches. We also demonstrate that the hybrid version performs more efficiently than the version using pure MPI.", "year": 2009, "referenceCount": 38, "citationCount": 58, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2993306", "name": "K. Woodsend"}, {"authorId": "51043392", "name": "J. Gondzio"}]}, {"paperId": "ca914bd3cfced5f10907d642b2d64b2f9260d67d", "url": "https://www.semanticscholar.org/paper/ca914bd3cfced5f10907d642b2d64b2f9260d67d", "title": "Machine Learning for Quantum Mechanical Properties of Atoms in Molecules", "abstract": "We introduce machine learning models of quantum mechanical observables of atoms in molecules. Instant out-of-sample predictions for proton and carbon nuclear chemical shifts, atomic core level excitations, and forces on atoms reach accuracies on par with density functional theory reference. Locality is exploited within nonlinear regression via local atom-centered coordinate systems. The approach is validated on a diverse set of 9 k small organic molecules. Linear scaling of computational cost in system size is demonstrated for saturated polymers with up to submesoscale lengths.", "year": 2015, "referenceCount": 45, "citationCount": 158, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "48041657", "name": "M. Rupp"}, {"authorId": "6781829", "name": "R. Ramakrishnan"}, {"authorId": "11615881", "name": "O. A. V. Lilienfeld"}]}, {"paperId": "7b53a308a41507a2ef2faed78eb48812633e75fb", "url": "https://www.semanticscholar.org/paper/7b53a308a41507a2ef2faed78eb48812633e75fb", "title": "Self-Supervised Video Representation Learning with Odd-One-Out Networks", "abstract": "We propose a new self-supervised CNN pre-training technique based on a novel auxiliary task called odd-one-out learning. In this task, the machine is asked to identify the unrelated or odd element from a set of otherwise related elements. We apply this technique to self-supervised video representation learning where we sample subsequences from videos and ask the network to learn to predict the odd video subsequence. The odd video subsequence is sampled such that it has wrong temporal order of frames while the even ones have the correct temporal order. Therefore, to generate a odd-one-out question no manual annotation is required. Our learning machine is implemented as multi-stream convolutional neural network, which is learned end-to-end. Using odd-one-out networks, we learn temporal representations for videos that generalizes to other related tasks such as action recognition. On action classification, our method obtains 60.3% on the UCF101 dataset using only UCF101 data for training which is approximately 10% better than current state-of-the-art self-supervised learning methods. Similarly, on HMDB51 dataset we outperform self-supervised state-of-the art methods by 12.7% on action classification task.", "year": 2016, "referenceCount": 50, "citationCount": 346, "influentialCitationCount": 24, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1688071", "name": "Basura Fernando"}, {"authorId": "2518212", "name": "Hakan Bilen"}, {"authorId": "2304222", "name": "E. Gavves"}, {"authorId": "145273587", "name": "Stephen Gould"}]}, {"paperId": "46038cb1606041001be2f42b4e09f630d4b4147f", "url": "https://www.semanticscholar.org/paper/46038cb1606041001be2f42b4e09f630d4b4147f", "title": "On Challenges in Machine Learning Model Management", "abstract": "The training, maintenance, deployment, monitoring, organization and documentation of machine learning (ML) models \u2013 in short model management \u2013 is a critical task in virtually all production ML use cases. Wrong model management decisions can lead to poor performance of a ML system and result in high maintenance cost. As both research on infrastructure as well as on algorithms is quickly evolving, there is a lack of understanding of challenges and best practices for ML model management. Therefore, this field is receiving increased attention in recent years, both from the data management as well as from the ML community. In this paper, we discuss a selection of ML use cases, develop an overview over conceptual, engineering, and data-related challenges arising in the management of the corresponding ML models, and point out future research directions.", "year": 2018, "referenceCount": 42, "citationCount": 99, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2180399", "name": "Sebastian Schelter"}, {"authorId": "2170760", "name": "F. Biessmann"}, {"authorId": "2166235", "name": "Tim Januschowski"}, {"authorId": "144607961", "name": "David Salinas"}, {"authorId": "1792476", "name": "Stephan Seufert"}, {"authorId": "72270481", "name": "Gyuri Szarvas"}]}, {"paperId": "eea4ca46542125e02cd7b6de60f28c3710b3f7a3", "url": "https://www.semanticscholar.org/paper/eea4ca46542125e02cd7b6de60f28c3710b3f7a3", "title": "Enhancing one-class support vector machines for unsupervised anomaly detection", "abstract": "Support Vector Machines (SVMs) have been one of the most successful machine learning techniques for the past decade. For anomaly detection, also a semi-supervised variant, the one-class SVM, exists. Here, only normal data is required for training before anomalies can be detected. In theory, the one-class SVM could also be used in an unsupervised anomaly detection setup, where no prior training is conducted. Unfortunately, it turns out that a one-class SVM is sensitive to outliers in the data. In this work, we apply two modifications in order to make one-class SVMs more suitable for unsupervised anomaly detection: Robust one-class SVMs and eta one-class SVMs. The key idea of both modifications is, that outliers should contribute less to the decision boundary as normal instances. Experiments performed on datasets from UCI machine learning repository show that our modifications are very promising: Comparing with other standard unsupervised anomaly detection algorithms, the enhanced one-class SVMs are superior on two out of four datasets. In particular, the proposed eta one-class SVM has shown the most promising results.", "year": 2013, "referenceCount": 76, "citationCount": 293, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "11137394", "name": "Mennatallah Amer"}, {"authorId": "2060959597", "name": "Markus Goldstein"}, {"authorId": "2890092", "name": "S. Abdennadher"}]}, {"paperId": "e4be4ba1d428bc89f63662ae7b0db097fa83d651", "url": "https://www.semanticscholar.org/paper/e4be4ba1d428bc89f63662ae7b0db097fa83d651", "title": "Deep Learning for Just-in-Time Defect Prediction", "abstract": "Defect prediction is a very meaningful topic, particularly at change-level. Change-level defect prediction, which is also referred as just-in-time defect prediction, could not only ensure software quality in the development process, but also make the developers check and fix the defects in time. Nowadays, deep learning is a hot topic in the machine learning literature. Whether deep learning can be used to improve the performance of just-in-time defect prediction is still uninvestigated. In this paper, to bridge this research gap, we propose an approach Deeper which leverages deep learning techniques to predict defect-prone changes. We first build a set of expressive features from a set of initial change features by leveraging a deep belief network algorithm. Next, a machine learning classifier is built on the selected features. To evaluate the performance of our approach, we use datasets from six large open source projects, i.e., Bugzilla, Columba, JDT, Platform, Mozilla, and PostgreSQL, containing a total of 137,417 changes. We compare our approach with the approach proposed by Kamei et al. The experimental results show that on average across the 6 projects, Deeper could discover 32.22% more bugs than Kamei et al's approach (51.04% versus 18.82% on average). In addition, Deeper can achieve F1-scores of 0.22-0.63, which are statistically significantly higher than those of Kamei et al.'s approach on 4 out of the 6 projects.", "year": 2015, "referenceCount": 42, "citationCount": 254, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "3312173", "name": "Xinli Yang"}, {"authorId": "143960553", "name": "D. Lo"}, {"authorId": "144558659", "name": "Xin Xia"}, {"authorId": "2049104435", "name": "Yun Zhang"}, {"authorId": "4346236", "name": "Jianling Sun"}]}, {"paperId": "56b63f646939db212b8a1da008e3ee5d7fba5df5", "url": "https://www.semanticscholar.org/paper/56b63f646939db212b8a1da008e3ee5d7fba5df5", "title": "A Topology Layer for Machine Learning", "abstract": "Topology applied to real world data using persistent homology has started to find applications within machine learning, including deep learning. We present a differentiable topology layer that computes persistent homology based on level set filtrations and distance-bases filtrations. We present three novel applications: the topological layer can (i) serve as a regularizer directly on data or the weights of machine learning models, (ii) construct a loss on the output of a deep generative network to incorporate topological priors, and (iii) perform topological adversarial attacks on deep networks trained with persistence features. The code is publicly available and we hope its availability will facilitate the use of persistent homology in deep learning and other gradient based applications.", "year": 2019, "referenceCount": 43, "citationCount": 83, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "51910464", "name": "Rickard Br\u00fcel Gabrielsson"}, {"authorId": "2053022936", "name": "Bradley J. Nelson"}, {"authorId": "5856108", "name": "Anjan Dwaraknath"}, {"authorId": "1701872", "name": "P. Skraba"}, {"authorId": "1744254", "name": "L. Guibas"}, {"authorId": "40119133", "name": "G. Carlsson"}]}, {"paperId": "5974642c9db54349fceade67b53d1020ed1d3c4e", "url": "https://www.semanticscholar.org/paper/5974642c9db54349fceade67b53d1020ed1d3c4e", "title": "Computational methods in authorship attribution", "abstract": "Statistical authorship attribution has a long history, culminating in the use of modern machine learning classification methods. Nevertheless, most of this work suffers from the limitation of assuming a small closed set of candidate authors and essentially unlimited training text for each. Real-life authorship attribution problems, however, typically fall short of this ideal. Thus, following detailed discussion of previous work, three scenarios are considered here for which solutions to the basic attribution problem are inadequate. In the first variant, the profiling problem, there is no candidate set at all; in this case, the challenge is to provide as much demographic or psychological information as possible about the author. In the second variant, the needle-in-a-haystack problem, there are many thousands of candidates for each of whom we might have a very limited writing sample. In the third variant, the verification problem, there is no closed candidate set but there is one suspect; in this case, the challenge is to determine if the suspect is or is not the author. For each variant, it is shown how machine learning methods can be adapted to handle the special challenges of that variant. \u00a9 2009 Wiley Periodicals, Inc.", "year": 2009, "referenceCount": 129, "citationCount": 626, "influentialCitationCount": 38, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145200175", "name": "Moshe Koppel"}, {"authorId": "2012715", "name": "Jonathan Schler"}, {"authorId": "144628595", "name": "S. Argamon"}]}, {"paperId": "3bd8c180acd9363c5d177c04b2973f2a3ffef068", "url": "https://www.semanticscholar.org/paper/3bd8c180acd9363c5d177c04b2973f2a3ffef068", "title": "Limits of End-to-End Learning", "abstract": "End-to-end learning refers to training a possibly complex learning system by applying gradient-based learning to the system as a whole. End-to-end learning system is specifically designed so that all modules are differentiable. In effect, not only a central learning machine, but also all \"peripheral\" modules like representation learning and memory formation are covered by a holistic learning process. The power of end-to-end learning has been demonstrated on many tasks, like playing a whole array of Atari video games with a single architecture. While pushing for solutions to more challenging tasks, network architectures keep growing more and more complex. \nIn this paper we ask the question whether and to what extent end-to-end learning is a future-proof technique in the sense of scaling to complex and diverse data processing architectures. We point out potential inefficiencies, and we argue in particular that end-to-end learning does not make optimal use of the modular design of present neural networks. Our surprisingly simple experiments demonstrate these inefficiencies, up to the complete breakdown of learning.", "year": 2017, "referenceCount": 36, "citationCount": 108, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1756710", "name": "T. Glasmachers"}]}, {"paperId": "43288cf3876d73575b7397ee9ab2853cd4ff810a", "url": "https://www.semanticscholar.org/paper/43288cf3876d73575b7397ee9ab2853cd4ff810a", "title": "Reliability and validity in comparative studies of software prediction models", "abstract": "Empirical studies on software prediction models do not converge with respect to the question \"which prediction model is best?\" The reason for this lack of convergence is poorly understood. In this simulation study, we have examined a frequently used research procedure comprising three main ingredients: a single data sample, an accuracy indicator, and cross validation. Typically, these empirical studies compare a machine learning model with a regression model. In our study, we use simulation and compare a machine learning and a regression model. The results suggest that it is the research procedure itself that is unreliable. This lack of reliability may strongly contribute to the lack of convergence. Our findings thus cast some doubt on the conclusions of any study of competing software prediction models that used this research procedure as a basis of model comparison. Thus, we need to develop more reliable research procedures before we can have confidence in the conclusions of comparative studies of software prediction models.", "year": 2005, "referenceCount": 43, "citationCount": 272, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1701866", "name": "I. Myrtveit"}, {"authorId": "1715823", "name": "E. Stensrud"}, {"authorId": "2319775", "name": "M. Shepperd"}]}, {"paperId": "772c3f272882c75b6ec847b9502b625501ba3866", "url": "https://www.semanticscholar.org/paper/772c3f272882c75b6ec847b9502b625501ba3866", "title": "Supervised Machine Learning for Population Genetics: A New Paradigm", "abstract": null, "year": 2018, "referenceCount": 95, "citationCount": 231, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Biology", "Medicine"], "authors": [{"authorId": "3060593", "name": "Daniel R. Schrider"}, {"authorId": "32093740", "name": "A. Kern"}]}, {"paperId": "9eb47e857cdf1f97413de0e3993f8429d7c33503", "url": "https://www.semanticscholar.org/paper/9eb47e857cdf1f97413de0e3993f8429d7c33503", "title": "Deep convolutional neural network based medical image classification for disease diagnosis", "abstract": null, "year": 2019, "referenceCount": 44, "citationCount": 308, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "121055148", "name": "Samir S. Yadav"}, {"authorId": "35925236", "name": "S. Jadhav"}]}, {"paperId": "9adb484a477e77e9defa9cd022d142c7b1b578e4", "url": "https://www.semanticscholar.org/paper/9adb484a477e77e9defa9cd022d142c7b1b578e4", "title": "A survey of learning-based techniques of email spam filtering", "abstract": null, "year": 2008, "referenceCount": 112, "citationCount": 424, "influentialCitationCount": 28, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2930903", "name": "E. Blanzieri"}, {"authorId": "2549958", "name": "Anton Bryl"}]}, {"paperId": "cf5a39d7ba0483683af2b010a13850d466f43229", "url": "https://www.semanticscholar.org/paper/cf5a39d7ba0483683af2b010a13850d466f43229", "title": "Recurrent least squares support vector machines", "abstract": "The method of support vector machines (SVM's) has been developed for solving classification and static function approximation problems. In this paper we introduce SVM's within the context of recurrent neural networks. Instead of Vapnik's epsilon insensitive loss function, we consider a least squares version related to a cost function with equality constraints for a recurrent network. Essential features of SVM's remain, such as Mercer's condition and the fact that the output weights are a Lagrange multiplier weighted sum of the data points. The solution to recurrent least squares (LS-SVM's) is characterized by a set of nonlinear equations. Due to its high computational complexity, we focus on a limited case of assigning the squared error an infinitely large penalty factor with early stopping as a form of regularization. The effectiveness of the approach is demonstrated on trajectory learning of the double scroll attractor in Chua's circuit.", "year": 2000, "referenceCount": 34, "citationCount": 328, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "1744439", "name": "J. Suykens"}, {"authorId": "1704135", "name": "J. Vandewalle"}]}, {"paperId": "77ce2e36c56fb470fe93b379f3a5c29494809670", "url": "https://www.semanticscholar.org/paper/77ce2e36c56fb470fe93b379f3a5c29494809670", "title": "Search Personalization Using Machine Learning", "abstract": "Query-based search is commonly used by many businesses to help consumers find information/products on their websites. Examples include search engines (Google, Bing), online retailers (Amazon, Macy's), and entertainment sites (Hulu, YouTube). Nevertheless, a significant portion of search sessions are unsuccessful, i.e., do not provide information that the user was looking for. We present a machine learning framework that improves the quality of search results through automated personalization based on a user's search history. Our framework consists of three modules -- (a) Feature generation, (b) NDCG-based LambdaMART algorithm, and (c) Feature selection wrapper. We estimate our framework on large-scale data from a leading search engine using Amazon EC2 servers. We show that our framework offers a significant improvement in search quality compared to non-personalized results. We also show that the returns to personalization are monotonically, but concavely increasing with the length of user history. Next, we find that personalization based on short-term history or \"within-session\" behavior is less valuable than long-term or \"across-session\" personalization. We also derive the value of different feature sets -- user-specific features contribute over 50% of the improvement and click-specific over 28%. Finally, we demonstrate scalability to big data and derive the set of optimal features that maximize accuracy while minimizing computing speed.", "year": 2017, "referenceCount": 125, "citationCount": 71, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2863824", "name": "Hema Yoganarasimhan"}]}, {"paperId": "2798df47acbeb89a9e044e43e747782ec05968af", "url": "https://www.semanticscholar.org/paper/2798df47acbeb89a9e044e43e747782ec05968af", "title": "Rapid Adaptation with Conditionally Shifted Neurons", "abstract": "We describe a mechanism by which artificial neural networks can learn rapid adaptation - the ability to adapt on the fly, with little data, to new tasks - that we call conditionally shifted neurons. We apply this mechanism in the framework of metalearning, where the aim is to replicate some of the flexibility of human learning in machines. Conditionally shifted neurons modify their activation values with task-specific shifts retrieved from a memory module, which is populated rapidly based on limited task experience. On metalearning benchmarks from the vision and language domains, models augmented with conditionally shifted neurons achieve state-of-the-art results.", "year": 2017, "referenceCount": 48, "citationCount": 208, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2227827", "name": "Tsendsuren Munkhdalai"}, {"authorId": "2854297", "name": "Xingdi Yuan"}, {"authorId": "34719201", "name": "Soroush Mehri"}, {"authorId": "3382568", "name": "Adam Trischler"}]}, {"paperId": "55d9ae6dd0ce6f27e2b50d66d355bf6986f03c70", "url": "https://www.semanticscholar.org/paper/55d9ae6dd0ce6f27e2b50d66d355bf6986f03c70", "title": "Inductive transfer with context-sensitive neural networks", "abstract": null, "year": 2008, "referenceCount": 36, "citationCount": 50, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "49601276", "name": "D. Silver"}, {"authorId": "2621572", "name": "R. Poirier"}, {"authorId": "40341071", "name": "D. Currie"}]}, {"paperId": "01a22723901061bfa7728f7fdfced2c1f54a3f57", "url": "https://www.semanticscholar.org/paper/01a22723901061bfa7728f7fdfced2c1f54a3f57", "title": "UP-Fall Detection Dataset: A Multimodal Approach", "abstract": "Falls, especially in elderly persons, are an important health problem worldwide. Reliable fall detection systems can mitigate negative consequences of falls. Among the important challenges and issues reported in literature is the difficulty of fair comparison between fall detection systems and machine learning techniques for detection. In this paper, we present UP-Fall Detection Dataset. The dataset comprises raw and feature sets retrieved from 17 healthy young individuals without any impairment that performed 11 activities and falls, with three attempts each. The dataset also summarizes more than 850 GB of information from wearable sensors, ambient sensors and vision devices. Two experimental use cases were shown. The aim of our dataset is to help human activity recognition and machine learning research communities to fairly compare their fall detection solutions. It also provides many experimental possibilities for the signal recognition, vision, and machine learning community.", "year": 2019, "referenceCount": 50, "citationCount": 133, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "1390004875", "name": "Mar\u00eda de Lourdes Mart\u00ednez-Villase\u00f1or"}, {"authorId": "1963352", "name": "Hiram Ponce"}, {"authorId": "145273511", "name": "J. Brieva"}, {"authorId": "1409244306", "name": "E. Moya-Albor"}, {"authorId": "1399280617", "name": "Jos\u00e9 N\u00fa\u00f1ez-Mart\u00ednez"}, {"authorId": "1404743114", "name": "Carlos Pe\u00f1afort-Asturiano"}]}, {"paperId": "66a932c0273a9d21c24e38161fed0a8f3bde3322", "url": "https://www.semanticscholar.org/paper/66a932c0273a9d21c24e38161fed0a8f3bde3322", "title": "Survey of machine learning techniques for malware analysis", "abstract": null, "year": 2017, "referenceCount": 124, "citationCount": 229, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40271658", "name": "Daniele Ucci"}, {"authorId": "2047634", "name": "Leonardo Aniello"}, {"authorId": "1696934", "name": "R. Baldoni"}]}, {"paperId": "012a69efefbea1802d51eae2c2a3d782cad09981", "url": "https://www.semanticscholar.org/paper/012a69efefbea1802d51eae2c2a3d782cad09981", "title": "International journal of machine learning and cybernetics", "abstract": null, "year": 2010, "referenceCount": 0, "citationCount": 55, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40162103", "name": "Xizhao Wang"}]}, {"paperId": "48335cea4f925a427cf04723b17236e5a5cb3c74", "url": "https://www.semanticscholar.org/paper/48335cea4f925a427cf04723b17236e5a5cb3c74", "title": "Frog classification using machine learning techniques", "abstract": null, "year": 2009, "referenceCount": 14, "citationCount": 125, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1774845", "name": "Chenn-Jung Huang"}, {"authorId": "2046956", "name": "Yi-Ju Yang"}, {"authorId": "3035235", "name": "Dian-Xiu Yang"}, {"authorId": "2111274", "name": "You-Jia Chen"}]}, {"paperId": "443fddff5e76c79c58f224535feedf280b41d32f", "url": "https://www.semanticscholar.org/paper/443fddff5e76c79c58f224535feedf280b41d32f", "title": "Intelligent Machine Homicide - Breaking Cryptographic Devices Using Support Vector Machines", "abstract": null, "year": 2012, "referenceCount": 24, "citationCount": 152, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "2563182", "name": "Annelie Heuser"}, {"authorId": "1744880", "name": "Michael Zohner"}]}, {"paperId": "30bc3984a3702f7947bf3bf18efc8908afeb4837", "url": "https://www.semanticscholar.org/paper/30bc3984a3702f7947bf3bf18efc8908afeb4837", "title": "Learning to Classify Ordinal Data: The Data Replication Method", "abstract": "Classification of ordinal data is one of the most important tasks of relation learning. This paper introduces a new machine learning paradigm specifically intended for classification problems where the classes have a natural order. The technique reduces the problem of classifying ordered classes to the standard two-class problem. The introduced method is then mapped into support vector machines and neural networks. Generalization bounds of the proposed ordinal classifier are also provided. An experimental study with artificial and real data sets, including an application to gene expression analysis, verifies the usefulness of the proposed approach.", "year": 2007, "referenceCount": 24, "citationCount": 172, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3698192", "name": "Jaime S. Cardoso"}, {"authorId": "144395232", "name": "J. Costa"}]}, {"paperId": "ce1889c543f2a85e7a98c020efa265cdad8a7647", "url": "https://www.semanticscholar.org/paper/ce1889c543f2a85e7a98c020efa265cdad8a7647", "title": "A Machine Learning Approach to Anomaly Detection", "abstract": "Much of the intrusion detection research focuses on signature (misuse) detection, where models are built to recognize known attacks. However, signature detection, by its nature, cannot detect novel attacks. Anomaly detection focuses on modeling the normal behavior and identifying significant deviations, which could be novel attacks. In this paper we explore two machine learning methods that can construct anomaly detection models from past behavior. The first method is a rule learning algorithm that characterizes normal behavior in the absence of labeled attack data. The second method uses a clustering algorithm to identify outliers.", "year": 2003, "referenceCount": 38, "citationCount": 90, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144316325", "name": "P. Chan"}, {"authorId": "35256019", "name": "M. Mahoney"}, {"authorId": "2057786595", "name": "Muhammad H. Arshad"}]}, {"paperId": "f9a45caaab1b6d3d777b53053d94181a3bf20cca", "url": "https://www.semanticscholar.org/paper/f9a45caaab1b6d3d777b53053d94181a3bf20cca", "title": "Bridging Machine Learning and Logical Reasoning by Abductive Learning", "abstract": "Perception and reasoning are two representative abilities of intelligence that are integrated seamlessly during human problem-solving processes. In the area of artificial intelligence (AI), the two abilities are usually realised by machine learning and logic programming, respectively. However, the two categories of techniques were developed separately throughout most of the history of AI. In this paper, we present the abductive learning targeted at unifying the two AI paradigms in a mutually beneficial way, where the machine learning model learns to perceive primitive logic facts from data, while logical reasoning can exploit symbolic domain knowledge and correct the wrongly perceived facts for improving the machine learning models. Furthermore, we propose a novel approach to optimise the machine learning model and the logical reasoning model jointly. We demonstrate that by using abductive learning, machines can learn to recognise numbers and resolve unknown mathematical operations simultaneously from images of simple hand-written equations. Moreover, the learned models can be generalised to longer equations and adapted to different tasks, which is beyond the capability of state-of-the-art deep learning models.", "year": 2019, "referenceCount": 43, "citationCount": 63, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2204237", "name": "Wang-Zhou Dai"}, {"authorId": "2110411571", "name": "Qiu-Ling Xu"}, {"authorId": "2152846045", "name": "Yang Yu"}, {"authorId": "2149133792", "name": "Zhi-Hua Zhou"}]}]}