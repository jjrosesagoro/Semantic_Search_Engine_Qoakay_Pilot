{"total": 5120627, "offset": 4700, "next": 4800, "data": [{"paperId": "3ab11a96f6c475afa7e5977886da301be4ed2f7f", "url": "https://www.semanticscholar.org/paper/3ab11a96f6c475afa7e5977886da301be4ed2f7f", "title": "Tux2: Distributed Graph Computation for Machine Learning", "abstract": "TUX2 is a new distributed graph engine that bridges graph computation and distributed machine learning. TUX2 inherits the benefits of an elegant graph computation model, efficient graph layout, and balanced parallelism to scale to billion-edge graphs; we extend and optimize it for distributed machine learning to support heterogeneity, a Stale Synchronous Parallel model, and a new MEGA (Mini-batch, Exchange, GlobalSync, and Apply) model. \n \nWe have developed a set of representative distributed machine learning algorithms in TUX2, covering both supervised and unsupervised learning. Compared to implementations on distributedmachine learning platforms, writing these algorithms in TUX2 takes only about 25% of the code: Our graph computation model hides the detailed management of data layout, partitioning, and parallelism from developers. Our extensive evaluation of TUX2, using large data sets with up to 64 billion edges, shows that TUX2 outperforms state-of-the-art distributed graph engines PowerGraph and PowerLyra by an order of magnitude, while beating two state-of-the-art distributed machine learning systems by at least 48%.", "year": 2017, "referenceCount": 43, "citationCount": 53, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9754946", "name": "Wencong Xiao"}, {"authorId": "2870618", "name": "Jilong Xue"}, {"authorId": "11009920", "name": "Youshan Miao"}, {"authorId": "1700892", "name": "Z. Li"}, {"authorId": "2145774559", "name": "Cheng Chen"}, {"authorId": "145217421", "name": "Ming Wu"}, {"authorId": null, "name": "Wei Li"}, {"authorId": "93317102", "name": "Lidong Zhou"}]}, {"paperId": "db2b5d12152025f65d4e313c321a95be3aecbb29", "url": "https://www.semanticscholar.org/paper/db2b5d12152025f65d4e313c321a95be3aecbb29", "title": "Shop Class as Soulcraft: An Inquiry into the Value of Work", "abstract": "Author Crawford argues for the revival of teaching the skills of tool, machine, and material use in the schools. He points to the decline in our knowledge of the artifacts of our culture and the resulting loss of self-determination. The teaching of manual competence has also provided a method of learning that is well suited to many students who are otherwise disenchanted with school.", "year": 2009, "referenceCount": 1, "citationCount": 323, "influentialCitationCount": 32, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "153856987", "name": "Matthew B. Crawford"}]}, {"paperId": "7c839a0cdbff5061c088be9f62ed3f72456ce4dd", "url": "https://www.semanticscholar.org/paper/7c839a0cdbff5061c088be9f62ed3f72456ce4dd", "title": "Machine Translation: Mining Text for Social Theory", "abstract": "More of the social world lives within electronic text than ever before, from collective activity on the web, social media, and instant messaging to online transactions, government intelligence, and digitized libraries. This supply of text has elicited demand for natural language processing and machine learning tools to filter, search, and translate text into valuable data. We survey some of the most exciting computational approaches to text analysis, highlighting both supervised methods that extend old theories to new data and unsupervised techniques that discover hidden regularities worth theorizing. We then review recent research that uses these tools to develop social insight by exploring (a) collective attention and reasoning through the content of communication; (b) social relationships through the process of communication; and (c) social states, roles, and moves identified through heterogeneous signals within communication. We highlight social questions for which these advances could offer powerful ...", "year": 2016, "referenceCount": 120, "citationCount": 178, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Sociology"], "authors": [{"authorId": "144002439", "name": "James A. Evans"}, {"authorId": "144231556", "name": "Pedro Aceves"}]}, {"paperId": "672b052adc3996349d5f5d09b942154fa86f55ff", "url": "https://www.semanticscholar.org/paper/672b052adc3996349d5f5d09b942154fa86f55ff", "title": "Progressive sampling-based Bayesian optimization for efficient and automatic machine learning model selection", "abstract": null, "year": 2017, "referenceCount": 51, "citationCount": 55, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "2111165132", "name": "Xueqiang Zeng"}, {"authorId": "144050855", "name": "Gang Luo"}]}, {"paperId": "08fa8d94f7d004547204f0743a24976c52cff796", "url": "https://www.semanticscholar.org/paper/08fa8d94f7d004547204f0743a24976c52cff796", "title": "High-level Decision Making for Safe and Reasonable Autonomous Lane Changing using Reinforcement Learning", "abstract": "Machine learning techniques have been shown to outperform many rule-based systems for the decision-making of autonomous vehicles. However, applying machine learning is challenging due to the possibility of executing unsafe actions and slow learning rates. We address these issues by presenting a reinforcement learning-based approach, which is combined with formal safety verification to ensure that only safe actions are chosen at any time. We let a deep reinforcement learning (RL) agent learn to drive as close as possible to a desired velocity by executing reasonable lane changes on simulated highways with an arbitrary number of lanes. By making use of a minimal state representation, consisting of only 13 continuous features, and a Deep Q-Network (DQN), we are able to achieve fast learning rates. Our RL agent is able to learn the desired task without causing collisions and outperforms a complex, rule-based agent that we use for benchmarking.", "year": 2018, "referenceCount": 27, "citationCount": 85, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "52170802", "name": "Branka Mirchevska"}, {"authorId": "5142479", "name": "Christian Pek"}, {"authorId": "50337113", "name": "M. Werling"}, {"authorId": "3280431", "name": "M. Althoff"}, {"authorId": "145581493", "name": "J. Boedecker"}]}, {"paperId": "2f31cfb36a8d0c9b2039eee1b5848f618d1a50dc", "url": "https://www.semanticscholar.org/paper/2f31cfb36a8d0c9b2039eee1b5848f618d1a50dc", "title": "Predicting sample size required for classification performance", "abstract": null, "year": 2012, "referenceCount": 59, "citationCount": 312, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2604229", "name": "Rosa L. Figueroa"}, {"authorId": "1398401588", "name": "Q. Zeng-Treitler"}, {"authorId": "2794000", "name": "S. Kandula"}, {"authorId": "144121161", "name": "Long H. Ngo"}]}, {"paperId": "622b68ab823b64e19430b7c41663c5aa0e835561", "url": "https://www.semanticscholar.org/paper/622b68ab823b64e19430b7c41663c5aa0e835561", "title": "Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation", "abstract": "We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the \u21132,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.", "year": 2016, "referenceCount": 61, "citationCount": 137, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "36685537", "name": "Lei Zhang"}, {"authorId": "66396175", "name": "D. Zhang"}]}, {"paperId": "68e3fca8f6f60ca1c70854b9d09228ece37f02b2", "url": "https://www.semanticscholar.org/paper/68e3fca8f6f60ca1c70854b9d09228ece37f02b2", "title": "Deep Boltzmann Machines and the Centering Trick", "abstract": null, "year": 2012, "referenceCount": 26, "citationCount": 80, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "144535526", "name": "G. Montavon"}, {"authorId": "145034054", "name": "K. M\u00fcller"}]}, {"paperId": "cbb044f113fa5b824cb776b370c08b1038b5b2eb", "url": "https://www.semanticscholar.org/paper/cbb044f113fa5b824cb776b370c08b1038b5b2eb", "title": "Machine Learning for Public Administration Research, with Application to Organizational Reputation", "abstract": "Machine learning methods have gained a great deal of popularity in recent years among public administration scholars and practitioners. These techniques open the door to the analysis of text, image and other types of data that allow us to test foundational theories of public administration and to develop new theories. Despite the excitement surrounding machine learning methods, clarity regarding their proper use and potential pitfalls is lacking. This paper attempts to fill this gap in the literature through providing a machine learning \"guide to practice\" for public administration scholars and practitioners. Here, we take a foundational view of machine learning and describe how these methods can enrich public administration research and practice through their ability develop new measures, tap into new sources of data and conduct statistical inference and causal inference in a principled manner. We then turn our attention to the pitfalls of using these methods such as unvalidated measures and lack of interpretability. Finally, we demonstrate how machine learning techniques can help us learn about organizational reputation in federal agencies through an illustrated example using tweets from 13 executive federal agencies.", "year": 2018, "referenceCount": 37, "citationCount": 59, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "41096358", "name": "L. Anastasopoulos"}, {"authorId": "49153953", "name": "Andrew B. Whitford"}]}, {"paperId": "4532a6dc17973a44d068ef64d8a40b8070209fbd", "url": "https://www.semanticscholar.org/paper/4532a6dc17973a44d068ef64d8a40b8070209fbd", "title": "Two-Layer Multiple Kernel Learning", "abstract": "Multiple Kernel Learning (MKL) aims to learn kernel machines for solving a real machine learning problem (e.g. classification) by exploring the combinations of multiple kernels. The traditional MKL approach is in general \u201cshallow\u201d in the sense that the target kernel is simply a linear (or convex) combination of some base kernels. In this paper, we investigate a framework of Multi-Layer Multiple Kernel Learning (MLMKL) that aims to learn \u201cdeep\u201d kernel machines by exploring the combinations of multiple kernels in a multi-layer structure, which goes beyond the conventional MKL approach. Through a multiple layer mapping, the proposed MLMKL framework offers higher flexibility than the regular MKL for finding the optimal kernel for applications. As the first attempt to this new MKL framework, we present a Two-Layer Multiple Kernel Learning (2LMKL) method together with two efficient algorithms for classification tasks. We analyze their generalization performances and have conducted an extensive set of experiments over 16 benchmark datasets, in which encouraging results showed that our method performed better than the conventional MKL methods.", "year": 2011, "referenceCount": 32, "citationCount": 99, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2146277", "name": "Jinfeng Zhuang"}, {"authorId": "1807998", "name": "I. Tsang"}, {"authorId": "1741126", "name": "S. Hoi"}]}, {"paperId": "3d60a8c4e5a726380c47f99f6515de8276a8aafa", "url": "https://www.semanticscholar.org/paper/3d60a8c4e5a726380c47f99f6515de8276a8aafa", "title": "Machine Learning for Crack Detection: Review and Model Performance Comparison", "abstract": "AbstractWith the advancement of machine learning (ML) and deep learning (DL), there is a great opportunity to enhance the development of automatic crack detection algorithms. In this paper, the aut...", "year": 2020, "referenceCount": 45, "citationCount": 67, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2000576666", "name": "Yung-An Hsieh"}, {"authorId": "145073451", "name": "Y. Tsai"}]}, {"paperId": "627c14fe9097d459b8fd47e8a901694198be9d5d", "url": "https://www.semanticscholar.org/paper/627c14fe9097d459b8fd47e8a901694198be9d5d", "title": "Compressed Learning : Universal Sparse Dimensionality Reduction and Learning in the Measurement Domain", "abstract": "In this paper, we provide theoretical results to show that compressed learning , learning directly in the compressed domain, is possible. In Particular, we provide tight bounds demonstrating that the linear kernel SVM\u2019s classifier in the measurement domain, with high probability, has true accuracy close to the accuracy of the best linear threshold classifier in the data domain. We show that this is beneficial both from the compressed sensing and the machine learning points of view. Furthermore, we indicate that for a family of well-known compressed sensing matrices, compressed learning is universal, in the sense that learning and classification in the measurement domain works provided that the data are sparse in some, even unknown, basis. Moreover, we show that our results are also applicable to a family of smooth manifold-learning tasks. Finally, we support our claims with experimental results.", "year": 2009, "referenceCount": 36, "citationCount": 168, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "120874496", "name": "R. Calderbank"}]}, {"paperId": "cdc90e091e64b796f5069bda7bbfdc5a04bd6365", "url": "https://www.semanticscholar.org/paper/cdc90e091e64b796f5069bda7bbfdc5a04bd6365", "title": "Extrapolation and learning equations", "abstract": "In classical machine learning, regression is treated as a black box process of identifying a suitable function from a hypothesis set without attempting to gain insight into the mechanism connecting inputs and outputs. In the natural sciences, however, finding an interpretable function for a phenomenon is the prime goal as it allows to understand and generalize results. This paper proposes a novel type of function learning network, called equation learner (EQL), that can learn analytical expressions and is able to extrapolate to unseen domains. It is implemented as an end-to-end differentiable feed-forward network and allows for efficient gradient based training. Due to sparsity regularization concise interpretable expressions can be obtained. Often the true underlying source expression is identified.", "year": 2016, "referenceCount": 28, "citationCount": 63, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144247521", "name": "G. Martius"}, {"authorId": "1787591", "name": "Christoph H. Lampert"}]}, {"paperId": "6a116fbf23a698e5547079a80ee1497b18b5a04c", "url": "https://www.semanticscholar.org/paper/6a116fbf23a698e5547079a80ee1497b18b5a04c", "title": "VizML: A Machine Learning Approach to Visualization Recommendation", "abstract": "Visualization recommender systems aim to lower the barrier to exploring basic visualizations by automatically generating results for analysts to search and select, rather than manually specify. Here, we demonstrate a novel machine learning-based approach to visualization recommendation that learns visualization design choices from a large corpus of datasets and associated visualizations. First, we identify five key design choices made by analysts while creating visualizations, such as selecting a visualization type and choosing to encode a column along the X- or Y-axis. We train models to predict these design choices using one million dataset-visualization pairs collected from a popular online visualization platform. Neural networks predict these design choices with high accuracy compared to baseline models. We report and interpret feature importances from one of these baseline models. To evaluate the generalizability and uncertainty of our approach, we benchmark with a crowdsourced test set, and show that the performance of our model is comparable to human performance when predicting consensus visualization type, and exceeds that of other visualization recommender systems.", "year": 2018, "referenceCount": 79, "citationCount": 111, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "7018573", "name": "K. Hu"}, {"authorId": "50985185", "name": "Michiel A. Bakker"}, {"authorId": "2109073168", "name": "Stephen Li"}, {"authorId": "1746961", "name": "Tim Kraska"}, {"authorId": "2132924", "name": "C\u00e9sar A. Hidalgo"}]}, {"paperId": "93e348038aebdc6b4c728527635a06d72d261f4f", "url": "https://www.semanticscholar.org/paper/93e348038aebdc6b4c728527635a06d72d261f4f", "title": "ASD-DiagNet: A Hybrid Learning Approach for Detection of Autism Spectrum Disorder Using fMRI Data", "abstract": "Heterogeneous mental disorders such as Autism Spectrum Disorder (ASD) are notoriously difficult to diagnose, especially in children. The current psychiatric diagnostic process is based purely on the behavioral observation of symptomology (DSM-5/ICD-10) and may be prone to misdiagnosis. In order to move the field toward more quantitative diagnosis, we need advanced and scalable machine learning infrastructure that will allow us to identify reliable biomarkers of mental health disorders. In this paper, we propose a framework called ASD-DiagNet for classifying subjects with ASD from healthy subjects by using only fMRI data. We designed and implemented a joint learning procedure using an autoencoder and a single layer perceptron (SLP) which results in improved quality of extracted features and optimized parameters for the model. Further, we designed and implemented a data augmentation strategy, based on linear interpolation on available feature vectors, that allows us to produce synthetic datasets needed for training of machine learning models. The proposed approach is evaluated on a public dataset provided by Autism Brain Imaging Data Exchange including 1, 035 subjects coming from 17 different brain imaging centers. Our machine learning model outperforms other state of the art methods from 10 imaging centers with increase in classification accuracy up to 28% with maximum accuracy of 82%. The machine learning technique presented in this paper, in addition to yielding better quality, gives enormous advantages in terms of execution time (40 min vs. 7 h on other methods). The implemented code is available as GPL license on GitHub portal of our lab (https://github.com/pcdslab/ASD-DiagNet).", "year": 2019, "referenceCount": 62, "citationCount": 111, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering", "Mathematics", "Medicine"], "authors": [{"authorId": "23131613", "name": "Taban Eslami"}, {"authorId": "2067221484", "name": "Vahid Mirjalili"}, {"authorId": "32250369", "name": "A. Fong"}, {"authorId": "8075165", "name": "A. Laird"}, {"authorId": "145052952", "name": "F. Saeed"}]}, {"paperId": "e215b1b54f5a1682466a0e4034984dda61d3c6f8", "url": "https://www.semanticscholar.org/paper/e215b1b54f5a1682466a0e4034984dda61d3c6f8", "title": "Learning Theory for Distribution Regression", "abstract": "We focus on the distribution regression problem: regressing to vector-valued outputs from probability measures. Many important machine learning and statistical tasks fit into this framework, including multi-instance learning and point estimation problems without analytical solution (such as hyperparameter or entropy estimation). Despite the large number of available heuristics in the literature, the inherent two-stage sampled nature of the problem makes the theoretical analysis quite challenging, since in practice only samples from sampled distributions are observable, and the estimates have to rely on similarities computed between sets of points. To the best of our knowledge, the only existing technique with consistency guarantees for distribution regression requires kernel density estimation as an intermediate step (which often performs poorly in practice), and the domain of the distributions to be compact Euclidean. In this paper, we study a simple, analytically computable, ridge regression-based alternative to distribution regression, where we embed the distributions to a reproducing kernel Hilbert space, and learn the regressor from the embeddings to the outputs. Our main contribution is to prove that this scheme is consistent in the two-stage sampled setup under mild conditions (on separable topological domains enriched with kernels): we present an exact computational-statistical efficiency trade-off analysis showing that our estimator is able to match the one-stage sampled minimax optimal rate [Caponnetto and De Vito, 2007; Steinwart et al., 2009]. This result answers a 17-year-old open question, establishing the consistency of the classical set kernel [Haussler, 1999; Gaertner et. al, 2002] in regression. We also cover consistency for more recent kernels on distributions, including those due to [Christmann and Steinwart, 2010].", "year": 2014, "referenceCount": 118, "citationCount": 103, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "145675507", "name": "Z. Szab\u00f3"}, {"authorId": "3313411", "name": "Bharath K. Sriperumbudur"}, {"authorId": "1719347", "name": "B. P\u00f3czos"}, {"authorId": "1708497", "name": "A. Gretton"}]}, {"paperId": "2ec9681a021ffed564ac364def44e9fbcedacd48", "url": "https://www.semanticscholar.org/paper/2ec9681a021ffed564ac364def44e9fbcedacd48", "title": "Deep Learning-based Job Placement in Distributed Machine Learning Clusters", "abstract": "Production machine learning (ML) clusters commonly host a variety of distributed ML workloads, e.g., speech recognition, machine translation. While server sharing among jobs improves resource utilization, interference among co-located ML jobs can lead to significant performance downgrade. Existing cluster schedulers (e.g., Mesos) are interference-oblivious in their job placement, causing suboptimal resource efficiency. Interference-aware job placement has been studied in the literature, but was treated using detailed workload profiling and interference modeling, which is not a general solution. This paper presents Harmony, a deep learning-driven ML cluster scheduler that places training jobs in a manner that minimizes interference and maximizes performance (i.e., training completion time). Harmony is based on a carefully designed deep reinforcement learning (DRL) framework augmented with reward modeling. The DRL employs state-of-the-art techniques to stabilize training and improve convergence, including actor-critic algorithm, job-aware action space exploration and experience replay. In view of a common lack of reward samples corresponding to different placement decisions, we build an auxiliary reward prediction model, which is trained using historical samples and used for producing reward for unseen placement. Experiments using real ML workloads in a Kubernetes cluster of 6 GPU servers show that Harmony outperforms representative schedulers by 25% in terms of average job completion time.", "year": 2019, "referenceCount": 35, "citationCount": 58, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3393850", "name": "Yixin Bao"}, {"authorId": "9561490", "name": "Yanghua Peng"}, {"authorId": "1726963", "name": "Chuan Wu"}]}, {"paperId": "f4cd6e606ad27eb91e3758d769029cb97c2285db", "url": "https://www.semanticscholar.org/paper/f4cd6e606ad27eb91e3758d769029cb97c2285db", "title": "A performance comparison of supervised machine learning models for Covid-19 tweets sentiment analysis", "abstract": "The spread of Covid-19 has resulted in worldwide health concerns. Social media is increasingly used to share news and opinions about it. A realistic assessment of the situation is necessary to utilize resources optimally and appropriately. In this research, we perform Covid-19 tweets sentiment analysis using a supervised machine learning approach. Identification of Covid-19 sentiments from tweets would allow informed decisions for better handling the current pandemic situation. The used dataset is extracted from Twitter using IDs as provided by the IEEE data port. Tweets are extracted by an in-house built crawler that uses the Tweepy library. The dataset is cleaned using the preprocessing techniques and sentiments are extracted using the TextBlob library. The contribution of this work is the performance evaluation of various machine learning classifiers using our proposed feature set. This set is formed by concatenating the bag-of-words and the term frequency-inverse document frequency. Tweets are classified as positive, neutral, or negative. Performance of classifiers is evaluated on the accuracy, precision, recall, and F1 score. For completeness, further investigation is made on the dataset using the Long Short-Term Memory (LSTM) architecture of the deep learning model. The results show that Extra Trees Classifiers outperform all other models by achieving a 0.93 accuracy score using our proposed concatenated features set. The LSTM achieves low accuracy as compared to machine learning classifiers. To demonstrate the effectiveness of our proposed feature set, the results are compared with the Vader sentiment analysis technique based on the GloVe feature extraction approach.", "year": 2021, "referenceCount": 60, "citationCount": 92, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "1430656665", "name": "Furqan Rustam"}, {"authorId": "47554565", "name": "M. Khalid"}, {"authorId": "48001308", "name": "W. Aslam"}, {"authorId": "1820791603", "name": "Vaibhav Rupapara"}, {"authorId": "1403306506", "name": "A. Mehmood"}, {"authorId": "32016133", "name": "G. Choi"}]}, {"paperId": "38b6540ddd5beebffd05047c78183f7575559fb2", "url": "https://www.semanticscholar.org/paper/38b6540ddd5beebffd05047c78183f7575559fb2", "title": "Selective Search for Object Recognition", "abstract": null, "year": 2013, "referenceCount": 46, "citationCount": 5113, "influentialCitationCount": 487, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1823362", "name": "J. Uijlings"}, {"authorId": "1756979", "name": "K. V. D. Sande"}, {"authorId": "1695527", "name": "T. Gevers"}, {"authorId": "144638781", "name": "A. Smeulders"}]}, {"paperId": "9a333461262d159023e8809e25f744101311bc8b", "url": "https://www.semanticscholar.org/paper/9a333461262d159023e8809e25f744101311bc8b", "title": "Automated Directed Fairness Testing", "abstract": "Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aequitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aequitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aequitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aequitas and we have evaluated it on six state-of-the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aequitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aequitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.", "year": 2018, "referenceCount": 19, "citationCount": 94, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "9451906", "name": "Sakshi Udeshi"}, {"authorId": "51126393", "name": "Pryanshu Arora"}, {"authorId": "145213689", "name": "Sudipta Chattopadhyay"}]}, {"paperId": "17e4ab48349fe0bd7ce119dbd63554f7244e92e8", "url": "https://www.semanticscholar.org/paper/17e4ab48349fe0bd7ce119dbd63554f7244e92e8", "title": "Associative Engines: Connectionism, Concepts, and Representational Change", "abstract": "Part 1 Melting the inner code: computational models, syntax, and the folk solids connectionism, code, and context what networks know what networks don't know concept, category and prototype. Part 2 From code to process: the presence of a symbol the role of representational trajectories the cascade of significant virtual machines associative learning in a hostile world the fate of the folk associative engines - the next generation.", "year": 1993, "referenceCount": 123, "citationCount": 322, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "37700983", "name": "A. Clark"}]}, {"paperId": "92e0cd6877ae7dc4997023f34476cd68fee9ef69", "url": "https://www.semanticscholar.org/paper/92e0cd6877ae7dc4997023f34476cd68fee9ef69", "title": "Scaling up Machine Learning", "abstract": "This book comprises a collection of representative approaches for scaling up machine learning and data mining methods on parallel and distributed computing platforms. Demand for parallelizing learning algorithms is highly task-specific: in some settings it is driven by the enormous dataset sizes, in others by model complexity or by real-time performance requirements. Making task-appropriate algorithm and platform choices for large-scale machine learning requires understanding the benefits, trade-offs, and constraints of the available options. Solutions presented in the book cover a range of parallelization platforms from FPGAs and GPUs to multi-core systems and commodity clusters; concurrent programming frameworks that include CUDA, MPI, MapReduce, and DryadLINQ; and various learning settings: supervised, unsupervised, semi-supervised, and online learning. Extensive coverage of parallelization of boosted trees, support vector machines, spectral clustering, belief propagation, and other popular learning algorithms accompanied by deep dives into several applications make the book equally useful for researchers, students, and practitioners.", "year": 2011, "referenceCount": 0, "citationCount": 35, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1988453", "name": "R. Bekkerman"}, {"authorId": "47695762", "name": "M. Bilenko"}, {"authorId": "144162125", "name": "J. Langford"}]}, {"paperId": "2ed7baf3dc26de1f934c938e2a806c28eea445a7", "url": "https://www.semanticscholar.org/paper/2ed7baf3dc26de1f934c938e2a806c28eea445a7", "title": "Synaptic Plasticity Dynamics for Deep Continuous Local Learning (DECOLLE)", "abstract": "A growing body of work underlines striking similarities between biological neural networks and recurrent, binary neural networks. A relatively smaller body of work, however, addresses the similarities between learning dynamics employed in deep artificial neural networks and synaptic plasticity in spiking neural networks. The challenge preventing this is largely caused by the discrepancy between the dynamical properties of synaptic plasticity and the requirements for gradient backpropagation. Learning algorithms that approximate gradient backpropagation using local error functions can overcome this challenge. Here, we introduce Deep Continuous Local Learning (DECOLLE), a spiking neural network equipped with local error functions for online learning with no memory overhead for computing gradients. DECOLLE is capable of learning deep spatio temporal representations from spikes relying solely on local information, making it compatible with neurobiology and neuromorphic hardware. Synaptic plasticity rules are derived systematically from user-defined cost functions and neural dynamics by leveraging existing autodifferentiation methods of machine learning frameworks. We benchmark our approach on the event-based neuromorphic dataset N-MNIST and DvsGesture, on which DECOLLE performs comparably to the state-of-the-art. DECOLLE networks provide continuously learning machines that are relevant to biology and supportive of event-based, low-power computer vision architectures matching the accuracies of conventional computers on tasks where temporal precision and speed are essential.", "year": 2018, "referenceCount": 76, "citationCount": 132, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "40647726", "name": "Jacques Kaiser"}, {"authorId": "34516897", "name": "H. Mostafa"}, {"authorId": "1734355", "name": "E. Neftci"}]}, {"paperId": "f86afc5f0b76d2b7a6147ac2b6c367eaa0d24c3b", "url": "https://www.semanticscholar.org/paper/f86afc5f0b76d2b7a6147ac2b6c367eaa0d24c3b", "title": "Machine learning for long-distance quantum communication", "abstract": "Machine learning can help us in solving problems in the context big data analysis and classification, as well as in playing complex games such as Go. But can it also be used to find novel protocols and algorithms for applications such as large-scale quantum communication? Here we show that machine learning can be used to identify central quantum protocols, including teleportation, entanglement purification and the quantum repeater. These schemes are of importance in long-distance quantum communication, and their discovery has shaped the field of quantum information processing. However, the usefulness of learning agents goes beyond the mere re-production of known protocols; the same approach allows one to find improved solutions to long-distance communication problems, in particular when dealing with asymmetric situations where channel noise and segment distance are non-uniform. Our findings are based on the use of projective simulation, a model of a learning agent that combines reinforcement learning and decision making in a physically motivated framework. The learning agent is provided with a universal gate set, and the desired task is specified via a reward scheme. From a technical perspective, the learning agent has to deal with stochastic environments and reactions. We utilize an idea reminiscent of hierarchical skill acquisition, where solutions to sub-problems are learned and re-used in the overall scheme. This is of particular importance in the development of long-distance communication schemes, and opens the way for using machine learning in the design and implementation of quantum networks.", "year": 2019, "referenceCount": 69, "citationCount": 48, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "authors": [{"authorId": "67091356", "name": "J. Walln\u00f6fer"}, {"authorId": "47610941", "name": "A. Melnikov"}, {"authorId": "2863386", "name": "W. D\u00fcr"}, {"authorId": "32534184", "name": "H. Briegel"}]}, {"paperId": "8229fc91f497e40a9092d4544248491fe40203e7", "url": "https://www.semanticscholar.org/paper/8229fc91f497e40a9092d4544248491fe40203e7", "title": "Sparse Bayesian nonparametric regression", "abstract": "One of the most common problems in machine learning and statistics consists of estimating the mean response <i>X\u03b2</i> from a vector of observations <i>y</i> assuming <i>y</i> = <i>X\u03b2</i> + <i>\u03b5</i> where <i>X</i> is known, \u03b2 is a vector of parameters of interest and <i>\u03b5</i> a vector of stochastic errors. We are particularly interested here in the case where the dimension <i>K</i> of \u03b2 is much higher than the dimension of <i>y</i>. We propose some flexible Bayesian models which can yield sparse estimates of \u03b2. We show that as <i>K</i> \u2192 \u221e these models are closely related to a class of L\u00e9vy processes. Simulations demonstrate that our models outperform significantly a range of popular alternatives.", "year": 2008, "referenceCount": 23, "citationCount": 105, "influentialCitationCount": 14, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1774837", "name": "F. Caron"}, {"authorId": "1701800", "name": "A. Doucet"}]}, {"paperId": "7347d2128f18dc48e98953ba029c28f097c94f39", "url": "https://www.semanticscholar.org/paper/7347d2128f18dc48e98953ba029c28f097c94f39", "title": "A Method for Classification Using Machine Learning Technique for Diabetes", "abstract": "Machine learning has been one of the standard and improving techniques with strong methods for classification and reorganization based on recursive learning. Machine learning allows to train and test classification system, with Artificial Intelligence. Machine learning has provided greatest support for predicting disease with correct case of training and testing. Diabetes needs greatest support of machine learning to detect diabetes disease in early stage, since it cannot be cured and also brings great complication to our health system. One of the promising techniques in machine learning is Support Vector Machine (SVM). SVM is used for classification of system. Upshot of SVM has provided with classification of system.", "year": 2013, "referenceCount": 17, "citationCount": 33, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3078075", "name": "N. Jaisankar"}]}, {"paperId": "99d8b368273ae4184bfa4ce0ce283ec5c365478e", "url": "https://www.semanticscholar.org/paper/99d8b368273ae4184bfa4ce0ce283ec5c365478e", "title": "The Number of Confirmed Cases of Covid-19 by using Machine Learning: Methods and Challenges", "abstract": null, "year": 2020, "referenceCount": 89, "citationCount": 53, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "39997148", "name": "Amir Ahmad"}, {"authorId": "48343663", "name": "S. Garhwal"}, {"authorId": "3138802", "name": "S. Ray"}, {"authorId": "2090135949", "name": "G. Kumar"}, {"authorId": "8774764", "name": "S. Malebary"}, {"authorId": "97819265", "name": "O. Barukab"}]}, {"paperId": "89dbfb9b75d3902748d73bfb5965e7d11e83c10e", "url": "https://www.semanticscholar.org/paper/89dbfb9b75d3902748d73bfb5965e7d11e83c10e", "title": "Learning Discriminative Appearance-Based Models Using Partial Least Squares", "abstract": "Appearance information is essential for applications such as tracking and people recognition. One of the main problems of using appearance-based discriminative models is the ambiguities among classes when the number of persons being considered increases. To reduce the amount of ambiguity, we propose the use of a rich set of feature descriptors based on color, textures and edges. Another issue regarding appearance modeling is the limited number of training samples available for each appearance. The discriminative models are created using a powerful statistical tool called Partial Least Squares (PLS), responsible for weighting the features according to their discriminative power for each different appearance. The experimental results, based on appearance-based person recognition, demonstrate that the use of an enriched feature set analyzed by PLS reduces the ambiguity among different appearances and provides higher recognition rates when compared to other machine learning techniques.", "year": 2009, "referenceCount": 22, "citationCount": 374, "influentialCitationCount": 49, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9385043", "name": "W. R. Schwartz"}, {"authorId": "1693428", "name": "L. Davis"}]}, {"paperId": "0858fb6efb0e7ef549db94813b9d6f896073d60a", "url": "https://www.semanticscholar.org/paper/0858fb6efb0e7ef549db94813b9d6f896073d60a", "title": "Bayesian Reinforcement Learning: A Survey", "abstract": "Bayesian methods for machine learning have been widely investigated, yielding principled methods for incorporating prior information into inference algorithms. In this survey, we provide an in-depth review of the role of Bayesian methods for the reinforcement learning (RL) paradigm. The major incentives for incorporating Bayesian reasoning in RL are: 1) it provides an elegant approach to action-selection (exploration/exploitation) as a function of the uncertainty in learning; and 2) it provides a machinery to incorporate prior knowledge into the algorithms. We first discuss models and methods for Bayesian inference in the simple single-step Bandit model. We then review the extensive recent literature on Bayesian methods for model-based RL, where prior information can be expressed on the parameters of the Markov model. We also present Bayesian methods for model-free RL, where priors are expressed over the value function or policy class. The objective of the paper is to provide a comprehensive survey on Bayesian RL algorithms and their theoretical and empirical properties.", "year": 2015, "referenceCount": 184, "citationCount": 311, "influentialCitationCount": 26, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1678622", "name": "M. Ghavamzadeh"}, {"authorId": "1712535", "name": "Shie Mannor"}, {"authorId": "145134886", "name": "Joelle Pineau"}, {"authorId": "3025260", "name": "Aviv Tamar"}]}, {"paperId": "3983793116c441dc541866ce98da433113d0dca7", "url": "https://www.semanticscholar.org/paper/3983793116c441dc541866ce98da433113d0dca7", "title": "KNOWLEDGE BASED ANALYSIS OF VARIOUS STATISTICAL TOOLS IN DETECTING BREAST CANCER", "abstract": "In this paper, we study the performance criterion of machine learning tools in classifying breast cancer. We compare the data mining tools such as Na\u00efve Bayes, Support vector machines, Radial basis neural networks, Decision trees J48 and simple CART. We used both binary and multi class data sets namely WBC, WDBC and Breast tissue from UCI machine learning depositary. The experiments are conducted in WEKA. The aim of this research is to find out the best classifier with respect to accuracy, precision, sensitivity and specificity in detecting breast cancer.", "year": 2011, "referenceCount": 29, "citationCount": 80, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2054537621", "name": "S. Rajagopalan"}, {"authorId": "2681542", "name": "L. V. Nandakishore"}]}, {"paperId": "c48e2b6da0383e5602d883ca6b42354aea06e882", "url": "https://www.semanticscholar.org/paper/c48e2b6da0383e5602d883ca6b42354aea06e882", "title": "First-order and Stochastic Optimization Methods for Machine Learning", "abstract": null, "year": 2020, "referenceCount": 0, "citationCount": 143, "influentialCitationCount": 19, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2070945", "name": "Guanghui Lan"}]}, {"paperId": "c4f294221008c4ff36ff7bfaac97fb648a5ff1ef", "url": "https://www.semanticscholar.org/paper/c4f294221008c4ff36ff7bfaac97fb648a5ff1ef", "title": "Mind Bugs: The Origins of Procedural Misconceptions", "abstract": "From the Publisher: \nAs children acquire arithmetic skills, they often develop \"bugs\" - small, local misconceptions that cause systematic errors. Mind Bugs combines a novel cognitive simulation process with careful hypothesis testing to explore how mathematics students acquire procedural skills in instructional settings, focusing in particular on these procedural misconceptions and what they reveal about the learning process. \nVanLehn develops a theory of learning that explains how students develop procedural misconceptions that cause systematic errors. He describes a computer program, \"Sierra,\" that simulates learning processes and predicts exactly what types of procedural errors should occur. These predictions are tested with error data from several thousand subjects from schools all over the world. Moreover, each hypothesis of the theory is tested individually by determining how the predictions would change if it were removed from the theory. \nIntegrating ideas from research in machine learning, artificial intelligence, cognitive psychology, and linguistics, Mind Bugs specifically addresses error patterns on subtraction tests, showing, for example, why some students have an imperfect understanding of the rules for borrowing. Alternative explanatory hypotheses are explored by incorporating them in Sierra in place of the primary hypotheses, and seeing if the program still explains all the subtraction bugs that it explained before. \nKurt VanLehn is Assistant Professor in the Department of Psychology at Carnegie Mellon University. Mind Bugs is included in the series Learning, Development, and Conceptual Change, edited by Lila Gleitman, Susan Carey, ElissaNewport, and Elizabeth Spelke. A Bradford Book", "year": 1990, "referenceCount": 1, "citationCount": 364, "influentialCitationCount": 37, "isOpenAccess": false, "fieldsOfStudy": ["Psychology"], "authors": [{"authorId": "1797292", "name": "K. VanLehn"}]}, {"paperId": "c50b13842e77a7a26e52fd39fc78d8ae0be65414", "url": "https://www.semanticscholar.org/paper/c50b13842e77a7a26e52fd39fc78d8ae0be65414", "title": "Fuzzy Rule-Based Expert Systems and Genetic Machine Learning", "abstract": "This work presents fuzzy rule-languages as links between quantitative and qualitative models. In the first part, the semantic of fuzzy rule-languages is extended with a type system and an object-oriented system. In the second part, fuzzy rule-languages are integrated with genetic algorithms and with classifier systems. For this purpose, the class of genetic algorithms over context-free languages has been developed.", "year": 1996, "referenceCount": 0, "citationCount": 134, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1401067057", "name": "A. Geyer-Schulz"}]}, {"paperId": "647412df4f7e8ac2deb3daa9d0d943c140c54094", "url": "https://www.semanticscholar.org/paper/647412df4f7e8ac2deb3daa9d0d943c140c54094", "title": "Apples-to-apples in cross-validation studies: pitfalls in classifier performance measurement", "abstract": "Cross-validation is a mainstay for measuring performance and progress in machine learning. There are subtle differences in how exactly to compute accuracy, F-measure and Area Under the ROC Curve (AUC) in cross-validation studies. However, these details are not discussed in the literature, and incompatible methods are used by various papers and software packages. This leads to inconsistency across the research literature. Anomalies in performance calculations for particular folds and situations go undiscovered when they are buried in aggregated results over many folds and datasets, without ever a person looking at the intermediate performance measurements. This research note clarifies and illustrates the differences, and it provides guidance for how best to measure classification performance under cross-validation. In particular, there are several divergent methods used for computing F-measure, which is often recommended as a performance measure under class imbalance, e.g., for text classification domains and in one-vs.-all reductions of datasets having many classes. We show by experiment that all but one of these computation methods leads to biased measurements, especially under high class imbalance. This paper is of particular interest to those designing machine learning software libraries and researchers focused on high class imbalance.", "year": 2010, "referenceCount": 9, "citationCount": 350, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3332330", "name": "George Forman"}, {"authorId": "144362899", "name": "Martin Scholz"}]}, {"paperId": "ac38bb21d18f070fcc4c815e24fae56cbfff519c", "url": "https://www.semanticscholar.org/paper/ac38bb21d18f070fcc4c815e24fae56cbfff519c", "title": "Cross-platform normalization of microarray and RNA-seq data for machine learning applications", "abstract": "Large, publicly available gene expression datasets are often analyzed with the aid of machine learning algorithms. Although RNA-seq is increasingly the technology of choice, a wealth of expression data already exist in the form of microarray data. If machine learning models built from legacy data can be applied to RNA-seq data, larger, more diverse training datasets can be created and validation can be performed on newly generated data. We developed Training Distribution Matching (TDM), which transforms RNA-seq data for use with models constructed from legacy platforms. We evaluated TDM, as well as quantile normalization, nonparanormal transformation, and a simple log2 transformation, on both simulated and biological datasets of gene expression. Our evaluation included both supervised and unsupervised machine learning approaches. We found that TDM exhibited consistently strong performance across settings and that quantile normalization also performed well in many circumstances. We also provide a TDM package for the R programming language.", "year": 2016, "referenceCount": 52, "citationCount": 71, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2109870436", "name": "Jeffrey A. Thompson"}, {"authorId": "145745139", "name": "J. Tan"}, {"authorId": "2104940", "name": "C. Greene"}]}, {"paperId": "884bf194b09b4409fc29cf08bc5dce623578826b", "url": "https://www.semanticscholar.org/paper/884bf194b09b4409fc29cf08bc5dce623578826b", "title": "A Brief Survey and an Application of Semantic Image Segmentation for Autonomous Driving", "abstract": null, "year": 2018, "referenceCount": 60, "citationCount": 38, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "121042267", "name": "\u00c7agri Kaymak"}, {"authorId": "3154444", "name": "A. U\u00e7ar"}]}, {"paperId": "d8c1840c5f155a9f2a6caf26fd999e9bf068c73a", "url": "https://www.semanticscholar.org/paper/d8c1840c5f155a9f2a6caf26fd999e9bf068c73a", "title": "Multi-task Learning", "abstract": "A fundamental limitation of standard machine learning methods is the cost incurred by the preparation of the large training samples required for good generalization. A potential remedy is offered by multi-task learning: in many cases, while individual sample sizes are rather small, there are samples to represent a large number of learning tasks (linear regression problems), which share some constraining or generative property. If this property is sufficiently simple it should allow for better learning of the individual tasks despite their small individual sample sizes. In this talk I will review a wide class of multi-task learning methods which encourage low-dimensional representations of the regression vectors. I will describe techniques to solve the underlying optimization problems and present an analysis of the generalization performance of these learning methods which provides a proof of the superiority of multi-task learning under specific conditions. ROKS 2013", "year": 2020, "referenceCount": 276, "citationCount": 55, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1704699", "name": "M. Pontil"}]}, {"paperId": "5e7a7fb69ef7447c2cb8966589e49a5acbee416e", "url": "https://www.semanticscholar.org/paper/5e7a7fb69ef7447c2cb8966589e49a5acbee416e", "title": "Photonics for artificial intelligence and neuromorphic computing", "abstract": null, "year": 2020, "referenceCount": 136, "citationCount": 349, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Computer Science"], "authors": [{"authorId": "2694890", "name": "B. Shastri"}, {"authorId": "2304580", "name": "A. Tait"}, {"authorId": "3400685", "name": "T. F. D. Lima"}, {"authorId": "144372362", "name": "W. Pernice"}, {"authorId": "1771881", "name": "H. Bhaskaran"}, {"authorId": "51032756", "name": "C. Wright"}, {"authorId": "144504492", "name": "P. Prucnal"}]}, {"paperId": "c3870164e7513f805d04edfd4fe17f0100a933ec", "url": "https://www.semanticscholar.org/paper/c3870164e7513f805d04edfd4fe17f0100a933ec", "title": "Machine Learning Algorithms", "abstract": null, "year": 2017, "referenceCount": 24, "citationCount": 47, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2084044072", "name": "Giuseppe Bonaccorso"}]}, {"paperId": "3ed98c18307fece4403c67b3f0888268fb5cbd26", "url": "https://www.semanticscholar.org/paper/3ed98c18307fece4403c67b3f0888268fb5cbd26", "title": "Algorithms, Machine Learning, and Collusion", "abstract": "This paper discusses whether self-learning price-setting algorithms can coordinate their pricing behavior to achieve a collusive outcome that maximizes the joint profits of the firms using them. Although legal scholars have generally assumed that algorithmic collusion is not only possible but also exceptionally easy, computer scientists examining cooperation between algorithms as well as economists investigating collusion in experimental oligopolies have countered that coordinated, tacitly collusive behavior is not as rapid, easy, or even inevitable as often suggested. Research in experimental economics has shown that the exchange of information is vital to collusion when more than two firms operate within a given market. Communication between algorithms is also a topic in research on artificial intelligence, in which some scholars have recently indicated that algorithms can learn to communicate, albeit in somewhat limited ways. Taken together, algorithmic collusion currently seems far more difficult to achieve than legal scholars have often assumed and is thus not a particularly relevant competitive concern at present. Moreover, there are several legal problems associated with algorithmic collusion, including questions of liability, of auditing and monitoring algorithms, and of enforcing competition law.", "year": 2018, "referenceCount": 111, "citationCount": 55, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "40482477", "name": "Ulrich Schwalbe"}]}, {"paperId": "be7f2245d56206b41d5dcc979b861bf156e5e0cb", "url": "https://www.semanticscholar.org/paper/be7f2245d56206b41d5dcc979b861bf156e5e0cb", "title": "A quantum machine learning algorithm based on generative models", "abstract": "We propose a quantum learning algorithm for a quantum generative model and prove its advantages compared with classical models. Quantum computing and artificial intelligence, combined together, may revolutionize future technologies. A significant school of thought regarding artificial intelligence is based on generative models. Here, we propose a general quantum algorithm for machine learning based on a quantum generative model. We prove that our proposed model is more capable of representing probability distributions compared with classical generative models and has exponential speedup in learning and inference at least for some instances if a quantum computer cannot be efficiently simulated classically. Our result opens a new direction for quantum machine learning and offers a remarkable example where a quantum algorithm shows exponential improvement over classical algorithms in an important application field.", "year": 2018, "referenceCount": 41, "citationCount": 71, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2157928630", "name": "X. Gao"}, {"authorId": "2108995385", "name": "Z. Zhang"}, {"authorId": "144794202", "name": "L. Duan"}]}, {"paperId": "e2a240cc67c2909128f2154db2facba5810a94b0", "url": "https://www.semanticscholar.org/paper/e2a240cc67c2909128f2154db2facba5810a94b0", "title": "Artificial Intelligence, Economics, and Industrial Organization", "abstract": "Machine learning (ML) and artificial intelligence (AI) have been around for many years. However, in the last 5 years, remarkable progress has been made using multilayered neural networks in diverse areas such as image recognition, speech recognition, and machine translation. AI is a general purpose technology that is likely to impact many industries. In this chapter I consider how machine learning availability might affect the industrial organization of both firms that provide AI services and industries that adopt AI technology. My intent is not to provide an extensive overview of this rapidly-evolving area, but instead to provide a short summary of some of the forces at work and to describe some possible areas for future research.", "year": 2018, "referenceCount": 46, "citationCount": 116, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2070970", "name": "H. Varian"}]}, {"paperId": "bb86e6d01eb795d2ca1079986348b9aa8721164b", "url": "https://www.semanticscholar.org/paper/bb86e6d01eb795d2ca1079986348b9aa8721164b", "title": "A machine learning approach to geochemical mapping", "abstract": null, "year": 2016, "referenceCount": 75, "citationCount": 81, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Geology"], "authors": [{"authorId": "102538074", "name": "Charlie Kirkwood"}, {"authorId": "1909448", "name": "M. Cave"}, {"authorId": "35102483", "name": "D. Beamish"}, {"authorId": "3251075", "name": "S. Grebby"}, {"authorId": "152731330", "name": "Ant\u00f3nio Ferreira"}]}, {"paperId": "39cbb3a0c09ff8bb30437dd8503935ca2aa07d91", "url": "https://www.semanticscholar.org/paper/39cbb3a0c09ff8bb30437dd8503935ca2aa07d91", "title": "Single trajectory characterization via machine learning", "abstract": "In order to study transport in complex environments, it is extremely important to determine the physical mechanism underlying diffusion and precisely characterize its nature and parameters. Often, this task is strongly impacted by data consisting of trajectories with short length (either due to brief recordings or previous trajectory segmentation) and limited localization precision. In this paper, we propose a machine learning method based on a random forest architecture, which is able to associate single trajectories to the underlying diffusion mechanism with high accuracy. In addition, the algorithm is able to determine the anomalous exponent with a small error, thus inherently providing a classification of the motion as normal or anomalous (sub- or super-diffusion). The method provides highly accurate outputs even when working with very short trajectories and in the presence of experimental noise. We further demonstrate the application of transfer learning to experimental and simulated data not included in the training/test dataset. This allows for a full, high-accuracy characterization of experimental trajectories without the need of any prior information.", "year": 2020, "referenceCount": 41, "citationCount": 63, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "1412620650", "name": "G. Mu\u00f1oz-Gil"}, {"authorId": "1398728149", "name": "M. Garcia-March"}, {"authorId": "40025272", "name": "C. Manzo"}, {"authorId": "84306443", "name": "J. Mart\u00edn-Guerrero"}, {"authorId": "145453642", "name": "M. Lewenstein"}]}, {"paperId": "95d27c42dd28ade9cd3b192d1e6dc34a0b87ca02", "url": "https://www.semanticscholar.org/paper/95d27c42dd28ade9cd3b192d1e6dc34a0b87ca02", "title": "Nonlinear modelling and support vector machines", "abstract": "Neural networks such as multilayer perceptrons and radial basis function networks have been very successful in a wide range of problems. In this paper we give a short introduction to some new developments related to support vector machines (SVM), a new class of kernel based techniques introduced within statistical learning theory and structural risk minimization. This new approach lends to solving convex optimization problems and also the model complexity follows from this solution. We especially focus on a least squares support vector machine formulation (LS-SVM) which enables to solve highly nonlinear and noisy black-box modelling problems, even in very high dimensional input spaces. While standard SVMs have been basically only applied to static problems like classification and function estimation, LS-SVM models have been extended to recurrent models and use in optimal control problems. Moreover, using weighted least squares and special pruning techniques, LS-SVMs can be employed for robust nonlinear estimation and sparse approximation. Applications of (LS)-SVMs to a large variety of artificial and real-life data sets indicate the huge potential of these methods.", "year": 2001, "referenceCount": 46, "citationCount": 268, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "71431726", "name": "J. Suykens"}]}, {"paperId": "0c0d709976bb5593e8dc9dafae1f8d54cd585faa", "url": "https://www.semanticscholar.org/paper/0c0d709976bb5593e8dc9dafae1f8d54cd585faa", "title": "Machine learning under the spotlight", "abstract": null, "year": 2017, "referenceCount": 0, "citationCount": 44, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2735803", "name": "D. Zibar"}, {"authorId": "1697959", "name": "H. Wymeersch"}, {"authorId": "3072554", "name": "I. Lyubomirsky"}]}, {"paperId": "de513fec33889e46c674efb6a3e520824d48bcf8", "url": "https://www.semanticscholar.org/paper/de513fec33889e46c674efb6a3e520824d48bcf8", "title": "Drowsy Driver Detection Through Facial Movement Analysis", "abstract": null, "year": 2007, "referenceCount": 16, "citationCount": 225, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1484645524", "name": "E. Vural"}, {"authorId": "145724968", "name": "M. \u00c7etin"}, {"authorId": "1694874", "name": "A. Er\u00e7il"}, {"authorId": "2724380", "name": "G. Littlewort"}, {"authorId": "2218905", "name": "M. Bartlett"}, {"authorId": "1741200", "name": "J. Movellan"}]}, {"paperId": "d0aa1ae9d8df097524fda2b7ff0d4ee81ee8be73", "url": "https://www.semanticscholar.org/paper/d0aa1ae9d8df097524fda2b7ff0d4ee81ee8be73", "title": "Learning to Filter Spam E-Mail: A Comparison of a Naive Bayesian and a Memory-Based Approach", "abstract": "We investigate the performance of two machine learning algorithms in the context of antispam filtering. The increasing volume of unsolicited bulk e-mail (spam) has generated a need for reliable anti-spam filters. Filters of this type have so far been based mostly on keyword patterns that are constructed by hand and perform poorly. The Naive Bayesian classifier has recently been suggested as an effective method to construct automatically anti-spam filters with superior performance. We investigate thoroughly the performance of the Naive Bayesian filter on a publicly available corpus, contributing towards standard benchmarks. At the same time, we compare the performance of the Naive Bayesian filter to an alternative memorybased learning approach, after introducing suitable cost-sensitive evaluation measures. Both methods achieve very accurate spam filtering, outperforming clearly the keyword-based filter of a widely used e-mail reader.", "year": 2000, "referenceCount": 21, "citationCount": 403, "influentialCitationCount": 45, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1752430", "name": "Ion Androutsopoulos"}, {"authorId": "4738873", "name": "G. Paliouras"}, {"authorId": "1745631", "name": "V. Karkaletsis"}, {"authorId": "2589004", "name": "Georgios Sakkis"}, {"authorId": "2124138", "name": "C. Spyropoulos"}, {"authorId": "1800781", "name": "Panagiotis Stamatopoulos"}]}, {"paperId": "c0ecd56d11eb063ad0a153897aed575c82f72ede", "url": "https://www.semanticscholar.org/paper/c0ecd56d11eb063ad0a153897aed575c82f72ede", "title": "Brain age prediction using deep learning uncovers associated sequence variants", "abstract": null, "year": 2019, "referenceCount": 105, "citationCount": 154, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Mathematics"], "authors": [{"authorId": "88148517", "name": "B. Jonsson"}, {"authorId": "3151908", "name": "G. Bjornsdottir"}, {"authorId": "2157308", "name": "T. Thorgeirsson"}, {"authorId": "1601974918", "name": "L. M. Ellingsen"}, {"authorId": "143915625", "name": "G. Walters"}, {"authorId": "2068997", "name": "D. Gudbjartsson"}, {"authorId": "1922055", "name": "H. Stef\u00e1nsson"}, {"authorId": "145074995", "name": "K. Stef\u00e1nsson"}, {"authorId": "2189837", "name": "M. Ulfarsson"}]}, {"paperId": "29588cfa8150b0c1d0ddf7635eac6d658be3dff0", "url": "https://www.semanticscholar.org/paper/29588cfa8150b0c1d0ddf7635eac6d658be3dff0", "title": "Cryptographically private support vector machines", "abstract": "We propose private protocols implementing the Kernel Adatron and Kernel Perceptron learning algorithms, give private classification protocols and private polynomial kernel computation protocols. The new protocols return their outputs - either the kernel value, the classifier or the classifications - in encrypted form so that they can be decrypted only by a common agreement by the protocol participants. We show how to use the encrypted classifications to privately estimate many properties of the data and the classifier. The new SVM classifiers are the first to be proven private according to the standard cryptographic definitions.", "year": 2006, "referenceCount": 45, "citationCount": 139, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1801340", "name": "S. Laur"}, {"authorId": "1704861", "name": "H. Lipmaa"}, {"authorId": "1962611", "name": "Taneli Mielik\u00e4inen"}]}, {"paperId": "6976e5762d5cb285cbea8644a2406600248ea19a", "url": "https://www.semanticscholar.org/paper/6976e5762d5cb285cbea8644a2406600248ea19a", "title": "Predictive Power of Machine Learning for Optimizing Solar Water Heater Performance: The Potential Application of High-Throughput Screening", "abstract": "Predicting the performance of solar water heater (SWH) is challenging due to the complexity of the system. Fortunately, knowledge-based machine learning can provide a fast and precise prediction method for SWH performance. With the predictive power of machine learning models, we can further solve a more challenging question: how to cost-effectively design a high-performance SWH? Here, we summarize our recent studies and propose a general framework of SWH design using a machine learning-based high-throughput screening (HTS) method. Design of water-in-glass evacuated tube solar water heater (WGET-SWH) is selected as a case study to show the potential application of machine learning-based HTS to the design and optimization of solar energy systems.", "year": 2017, "referenceCount": 53, "citationCount": 54, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Chemistry"], "authors": [{"authorId": "2152377478", "name": "Hao Li"}, {"authorId": "46271321", "name": "Zhijian Liu"}, {"authorId": "2152352792", "name": "Kejun Liu"}, {"authorId": "10389367", "name": "Zhien Zhang"}]}, {"paperId": "f54b36edae733ab9cd7a748595947710bd28a2e3", "url": "https://www.semanticscholar.org/paper/f54b36edae733ab9cd7a748595947710bd28a2e3", "title": "A Study of Reinforcement Learning for Neural Machine Translation", "abstract": "Recent studies have shown that reinforcement learning (RL) is an effective approach for improving the performance of neural machine translation (NMT) system. However, due to its instability, successfully RL training is challenging, especially in real-world systems where deep models and large datasets are leveraged. In this paper, taking several large-scale translation tasks as testbeds, we conduct a systematic study on how to train better NMT models using reinforcement learning. We provide a comprehensive comparison of several important factors (e.g., baseline reward, reward shaping) in RL training. Furthermore, to fill in the gap that it remains unclear whether RL is still beneficial when monolingual data is used, we propose a new method to leverage RL to further boost the performance of NMT systems trained with source/target monolingual data. By integrating all our findings, we obtain competitive results on WMT14 English-German, WMT17 English-Chinese, and WMT17 Chinese-English translation tasks, especially setting a state-of-the-art performance on WMT17 Chinese-English translation task.", "year": 2018, "referenceCount": 44, "citationCount": 126, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "47767550", "name": "Lijun Wu"}, {"authorId": "143853336", "name": "Fei Tian"}, {"authorId": "143826491", "name": "Tao Qin"}, {"authorId": "66117656", "name": "J. Lai"}, {"authorId": "2110264337", "name": "Tie-Yan Liu"}]}, {"paperId": "c46d80f83813fba0e8363a0ab36a19fba062540e", "url": "https://www.semanticscholar.org/paper/c46d80f83813fba0e8363a0ab36a19fba062540e", "title": "Learning Actionable Representations with Goal-Conditioned Policies", "abstract": "Representation learning is a central challenge across a range of machine learning areas. In reinforcement learning, effective and functional representations have the potential to tremendously accelerate learning progress and solve more challenging problems. Most prior work on representation learning has focused on generative approaches, learning representations that capture all underlying factors of variation in the observation space in a more disentangled or well-ordered manner. In this paper, we instead aim to learn functionally salient representations: representations that are not necessarily complete in terms of capturing all factors of variation in the observation space, but rather aim to capture those factors of variation that are important for decision making -- that are \"actionable.\" These representations are aware of the dynamics of the environment, and capture only the elements of the observation that are necessary for decision making rather than all factors of variation, without explicit reconstruction of the observation. We show how these representations can be useful to improve exploration for sparse reward problems, to enable long horizon hierarchical reinforcement learning, and as a state representation for learning policies for downstream tasks. We evaluate our method on a number of simulated environments, and compare it to prior methods for representation learning, exploration, and hierarchical reinforcement learning.", "year": 2018, "referenceCount": 44, "citationCount": 79, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "8021910", "name": "Dibya Ghosh"}, {"authorId": "2129458064", "name": "Abhishek Gupta"}, {"authorId": "1736651", "name": "S. Levine"}]}, {"paperId": "17738e6664307e83a35fe72e6bbb5b405a1f7aeb", "url": "https://www.semanticscholar.org/paper/17738e6664307e83a35fe72e6bbb5b405a1f7aeb", "title": "Review on Machine Learning Algorithm Based Fault Detection in Induction Motors", "abstract": null, "year": 2020, "referenceCount": 123, "citationCount": 43, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2118921137", "name": "Prashant Kumar"}, {"authorId": "50840931", "name": "Ananda Shankar Hati"}]}, {"paperId": "0f5954b91a2f7bdd343caecab123cf077f566fb4", "url": "https://www.semanticscholar.org/paper/0f5954b91a2f7bdd343caecab123cf077f566fb4", "title": "Going Deep in Medical Image Analysis: Concepts, Methods, Challenges, and Future Directions", "abstract": "Medical image analysis is currently experiencing a paradigm shift due to deep learning. This technology has recently attracted so much interest of the Medical Imaging Community that it led to a specialized conference in \u201cMedical Imaging with Deep Learning\u201d in the year 2018. This paper surveys the recent developments in this direction and provides a critical review of the related major aspects. We organize the reviewed literature according to the underlying pattern recognition tasks and further sub-categorize it following a taxonomy based on human anatomy. This paper does not assume prior knowledge of deep learning and makes a significant contribution in explaining the core deep learning concepts to the non-experts in the Medical Community. This paper provides a unique computer vision/machine learning perspective taken on the advances of deep learning in medical imaging. This enables us to single out \u201clack of appropriately annotated large-scale data sets\u201d as the core challenge (among other challenges) in this research direction. We draw on the insights from the sister research fields of computer vision, pattern recognition, and machine learning, where the techniques of dealing with such challenges have already matured, to provide promising directions for the Medical Imaging Community to fully harness deep learning in the future.", "year": 2019, "referenceCount": 322, "citationCount": 116, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48438050", "name": "F. Altaf"}, {"authorId": "2112527803", "name": "S. M. Islam"}, {"authorId": "47398812", "name": "Naveed Akhtar"}, {"authorId": "1979066", "name": "N. Janjua"}]}, {"paperId": "77332906ee0dabba93ddf6a088d80d8b13329574", "url": "https://www.semanticscholar.org/paper/77332906ee0dabba93ddf6a088d80d8b13329574", "title": "Low-Rank Tensor Networks for Dimensionality Reduction and Large-Scale Optimization Problems: Perspectives and Challenges PART 1", "abstract": "Machine learning and data mining algorithms are becoming increasingly important in analyzing large volume, multi-relational and \u2217Copyright A.Cichocki et al. Please make reference to: A. Cichocki, N. Lee, I. Oseledets, A.-H. Phan, Q. Zhao and D.P. Mandic (2016), \u201cTensor Networks for Dimensionality Reduction and Large-scale Optimization: Part 1 Low-Rank Tensor Decompositions\u201d, Foundations and Trends in Machine Learning: Vol. 9: No. 4-5, pp 249-429.", "year": 2016, "referenceCount": 327, "citationCount": 81, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145683892", "name": "A. Cichocki"}, {"authorId": "1777061", "name": "Namgil Lee"}, {"authorId": "1738205", "name": "I. Oseledets"}, {"authorId": "9377326", "name": "A. Phan"}, {"authorId": "144251712", "name": "Qibin Zhao"}, {"authorId": "9368519", "name": "D. Mandic"}]}, {"paperId": "8cfd6e2a1e9fe38f0a09b7e2448995c9398722b1", "url": "https://www.semanticscholar.org/paper/8cfd6e2a1e9fe38f0a09b7e2448995c9398722b1", "title": "Information Geometry of U-Boost and Bregman Divergence", "abstract": "We aim at an extension of AdaBoost to U-Boost, in the paradigm to build a stronger classification machine from a set of weak learning machines. A geometric understanding of the Bregman divergence defined by a generic convex function U leads to the U-Boost method in the framework of information geometry extended to the space of the finite measures over a label set. We propose two versions of U-Boost learning algorithms by taking account of whether the domain is restricted to the space of probability functions. In the sequential step, we observe that the two adjacent and the initial classifiers are associated with a right triangle in the scale via the Bregman divergence, called the Pythagorean relation. This leads to a mild convergence property of the U-Boost algorithm as seen in the expectation-maximization algorithm. Statistical discussions for consistency and robustness elucidate the properties of the U-Boost methods based on a stochastic assumption for training data.", "year": 2004, "referenceCount": 38, "citationCount": 190, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "authors": [{"authorId": "2653061", "name": "N. Murata"}, {"authorId": "2071010", "name": "T. Takenouchi"}, {"authorId": "1897617", "name": "T. Kanamori"}, {"authorId": "1760376", "name": "S. Eguchi"}]}, {"paperId": "08cbed1690fb778b06343450ebe4f33057d5742a", "url": "https://www.semanticscholar.org/paper/08cbed1690fb778b06343450ebe4f33057d5742a", "title": "Convex Learning with Invariances", "abstract": "Incorporating invariances into a learning algorithm is a common problem in machine learning. We provide a convex formulation which can deal with arbitrary loss functions and arbitrary losses. In addition, it is a drop-in replacement for most optimization algorithms for kernels, including solvers of the SVMStruct family. The advantage of our setting is that it relies on column generation instead of modifying the underlying optimization problem directly.", "year": 2007, "referenceCount": 16, "citationCount": 104, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "37077406", "name": "C. Teo"}, {"authorId": "1786843", "name": "A. Globerson"}, {"authorId": "9330607", "name": "S. Roweis"}, {"authorId": "46234526", "name": "Alex Smola"}]}, {"paperId": "b47dffe8e582aff39c5ee2bd5b6f4628a53641b0", "url": "https://www.semanticscholar.org/paper/b47dffe8e582aff39c5ee2bd5b6f4628a53641b0", "title": "Machine learning for the prediction of sepsis: a systematic review and meta-analysis of diagnostic test accuracy", "abstract": null, "year": 2020, "referenceCount": 83, "citationCount": 172, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "26430703", "name": "L. Fleuren"}, {"authorId": "9838704", "name": "T. Klausch"}, {"authorId": "1421683819", "name": "Charlotte L. Zwager"}, {"authorId": "8696327", "name": "L. Schoonmade"}, {"authorId": "81098042", "name": "Tingjie Guo"}, {"authorId": "79586220", "name": "Luca F Roggeveen"}, {"authorId": "37547303", "name": "E. Swart"}, {"authorId": "152345684", "name": "A. Girbes"}, {"authorId": "80032381", "name": "P. Thoral"}, {"authorId": "144710784", "name": "A. Ercole"}, {"authorId": "144074133", "name": "M. Hoogendoorn"}, {"authorId": "2035038", "name": "P. Elbers"}]}, {"paperId": "101aad03db53cac05e89e5d474fe68948afae09d", "url": "https://www.semanticscholar.org/paper/101aad03db53cac05e89e5d474fe68948afae09d", "title": "Multimodal Neuroimaging Feature Learning for Multiclass Diagnosis of Alzheimer's Disease", "abstract": "The accurate diagnosis of Alzheimer's disease (AD) is essential for patient care and will be increasingly important as disease modifying agents become available, early in the course of the disease. Although studies have applied machine learning methods for the computer-aided diagnosis of AD, a bottleneck in the diagnostic performance was shown in previous methods, due to the lacking of efficient strategies for representing neuroimaging biomarkers. In this study, we designed a novel diagnostic framework with deep learning architecture to aid the diagnosis of AD. This framework uses a zero-masking strategy for data fusion to extract complementary information from multiple data modalities. Compared to the previous state-of-the-art workflows, our method is capable of fusing multimodal neuroimaging features in one setting and has the potential to require less labeled data. A performance gain was achieved in both binary classification and multiclass classification of AD. The advantages and limitations of the proposed framework are discussed.", "year": 2015, "referenceCount": 66, "citationCount": 359, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Psychology", "Computer Science", "Medicine"], "authors": [{"authorId": "47130333", "name": "Siqi Liu"}, {"authorId": "1847158", "name": "Sidong Liu"}, {"authorId": "122905659", "name": "Weidong (Tom) Cai"}, {"authorId": "2520245", "name": "Hangyu Che"}, {"authorId": "2455284", "name": "Sonia Pujol"}, {"authorId": "48307303", "name": "R. Kikinis"}, {"authorId": "145855523", "name": "D. Feng"}, {"authorId": "2110883", "name": "M. Fulham"}]}, {"paperId": "f938a29f108fe901ba4bc9162bbece909e8940f1", "url": "https://www.semanticscholar.org/paper/f938a29f108fe901ba4bc9162bbece909e8940f1", "title": "Future Intelligent and Secure Vehicular Network Toward 6G: Machine-Learning Approaches", "abstract": "As a powerful tool, the vehicular network has been built to connect human communication and transportation around the world for many years to come. However, with the rapid growth of vehicles, the vehicular network becomes heterogeneous, dynamic, and large scaled, which makes it difficult to meet the strict requirements, such as ultralow latency, high reliability, high security, and massive connections of the next-generation (6G) network. Recently, machine learning (ML) has emerged as a powerful artificial intelligence (AI) technique to make both the vehicle and wireless communication highly efficient and adaptable. Naturally, employing ML into vehicular communication and network becomes a hot topic and is being widely studied in both academia and industry, paving the way for the future intelligentization in 6G vehicular networks. In this article, we provide a survey on various ML techniques applied to communication, networking, and security parts in vehicular networks and envision the ways of enabling AI toward a future 6G vehicular network, including the evolution of intelligent radio (IR), network intelligentization, and self-learning with proactive exploration.", "year": 2020, "referenceCount": 134, "citationCount": 257, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "19282835", "name": "Fengxiao Tang"}, {"authorId": "3246156", "name": "Y. Kawamoto"}, {"authorId": "145842185", "name": "N. Kato"}, {"authorId": "48211895", "name": "Jiajia Liu"}]}, {"paperId": "e64579d8593140396b518682bb3a47ba246684eb", "url": "https://www.semanticscholar.org/paper/e64579d8593140396b518682bb3a47ba246684eb", "title": "Machine learning can predict survival of patients with heart failure from serum creatinine and ejection fraction alone", "abstract": null, "year": 2020, "referenceCount": 112, "citationCount": 164, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1702328", "name": "D. Chicco"}, {"authorId": "1680819", "name": "Giuseppe Jurman"}]}, {"paperId": "f774a7b3a843736cfa477df718243b90a5d83792", "url": "https://www.semanticscholar.org/paper/f774a7b3a843736cfa477df718243b90a5d83792", "title": "Autoregressive Energy Machines", "abstract": "Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.", "year": 2019, "referenceCount": 51, "citationCount": 38, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "36942233", "name": "Charlie Nash"}, {"authorId": "49089615", "name": "Conor Durkan"}]}, {"paperId": "c990dcda41076956426872b7d72ec4121f354102", "url": "https://www.semanticscholar.org/paper/c990dcda41076956426872b7d72ec4121f354102", "title": "Computational Modeling of \u03b2-Secretase 1 (BACE-1) Inhibitors Using Ligand Based Approaches", "abstract": "The binding affinities (IC50) reported for diverse structural and chemical classes of human \u03b2-secretase 1 (BACE-1) inhibitors in literature were modeled using multiple in silico ligand based modeling approaches and statistical techniques. The descriptor space encompasses simple binary molecular fingerprint, one- and two-dimensional constitutional, physicochemical, and topological descriptors, and sophisticated three-dimensional molecular fields that require appropriate structural alignments of varied chemical scaffolds in one universal chemical space. The affinities were modeled using qualitative classification or quantitative regression schemes involving linear, nonlinear, and deep neural network (DNN) machine-learning methods used in the scientific literature for quantitative-structure activity relationships (QSAR). In a departure from tradition, \u223c20% of the chemically diverse data set (205 compounds) was used to train the model with the remaining \u223c80% of the structural and chemical analogs used as part of an external validation (1273 compounds) and prospective test (69 compounds) sets respectively to ascertain the model performance. The machine-learning methods investigated herein performed well in both the qualitative classification (\u223c70% accuracy) and quantitative IC50 predictions (RMSE \u223c 1 log). The success of the 2D descriptor based machine learning approach when compared against the 3D field based technique pursued for hBACE-1 inhibitors provides a strong impetus for systematically applying such methods during the lead identification and optimization efforts for other protein families as well.", "year": 2016, "referenceCount": 74, "citationCount": 143, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine", "Chemistry"], "authors": [{"authorId": "144865677", "name": "G. Subramanian"}, {"authorId": "2378027", "name": "Bharath Ramsundar"}, {"authorId": "1806271", "name": "V. Pande"}, {"authorId": "144030239", "name": "R. Denny"}]}, {"paperId": "5d07b34998465bd2907d5d05424ed69b068b3e3f", "url": "https://www.semanticscholar.org/paper/5d07b34998465bd2907d5d05424ed69b068b3e3f", "title": "Machine learning in laboratory medicine: waiting for the flood?", "abstract": "Abstract This review focuses on machine learning and on how methods and models combining data analytics and artificial intelligence have been applied to laboratory medicine so far. Although still in its infancy, the potential for applying machine learning to laboratory data for both diagnostic and prognostic purposes deserves more attention by the readership of this journal, as well as by physician-scientists who will want to take advantage of this new computer-based support in pathology and laboratory medicine.", "year": 2017, "referenceCount": 45, "citationCount": 63, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3037324", "name": "F. Cabitza"}, {"authorId": "145244821", "name": "G. Banfi"}]}, {"paperId": "a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "url": "https://www.semanticscholar.org/paper/a6e92f6fa9e91b7e869562a63b30a9a56cf14582", "title": "Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations", "abstract": "Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.", "year": 2019, "referenceCount": 54, "citationCount": 46, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science", "Mathematics"], "authors": [{"authorId": "24593911", "name": "Tri Dao"}, {"authorId": "39499001", "name": "Albert Gu"}, {"authorId": "41022841", "name": "Matthew Eichhorn"}, {"authorId": "1755572", "name": "A. Rudra"}, {"authorId": "2114485554", "name": "C. R\u00e9"}]}, {"paperId": "5577b57291c69135c5ce8a2fca493b9f655fb075", "url": "https://www.semanticscholar.org/paper/5577b57291c69135c5ce8a2fca493b9f655fb075", "title": "Machine learning prediction of mechanical properties of concrete: Critical review", "abstract": null, "year": 2020, "referenceCount": 123, "citationCount": 144, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "82573478", "name": "W. Chaabene"}, {"authorId": "2003244210", "name": "M. Flah"}, {"authorId": "37823335", "name": "M. Nehdi"}]}, {"paperId": "5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "url": "https://www.semanticscholar.org/paper/5e4eb58d5b47ac1c73f4cf189497170e75ae6237", "title": "Neural GPUs Learn Algorithms", "abstract": "Learning an algorithm from examples is a fundamental problem that has been widely studied. Recently it has been addressed using neural networks, in particular by Neural Turing Machines (NTMs). These are fully differentiable computers that use backpropagation to learn their own programming. Despite their appeal NTMs have a weakness that is caused by their sequential nature: they are not parallel and are are hard to train due to their large depth when unfolded. \nWe present a neural network architecture to address this problem: the Neural GPU. It is based on a type of convolutional gated recurrent unit and, like the NTM, is computationally universal. Unlike the NTM, the Neural GPU is highly parallel which makes it easier to train and efficient to run. \nAn essential property of algorithms is their ability to handle inputs of arbitrary size. We show that the Neural GPU can be trained on short instances of an algorithmic task and successfully generalize to long instances. We verified it on a number of tasks including long addition and long multiplication of numbers represented in binary. We train the Neural GPU on numbers with upto 20 bits and observe no errors whatsoever while testing it, even on much longer numbers. \nTo achieve these results we introduce a technique for training deep recurrent networks: parameter sharing relaxation. We also found a small amount of dropout and gradient noise to have a large positive effect on learning and generalization.", "year": 2015, "referenceCount": 48, "citationCount": 311, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId": "1701686", "name": "Ilya Sutskever"}]}, {"paperId": "3f8c2a71dd1f399079cc2fa87e15e065b59ebfbe", "url": "https://www.semanticscholar.org/paper/3f8c2a71dd1f399079cc2fa87e15e065b59ebfbe", "title": "Feature space interpretation of SVMs with indefinite kernels", "abstract": "Kernel methods are becoming increasingly popular for various kinds of machine learning tasks, the most famous being the support vector machine (SVM) for classification. The SVM is well understood when using conditionally positive definite (cpd) kernel functions. However, in practice, non-cpd kernels arise and demand application in SVM. The procedure of \"plugging\" these indefinite kernels in SVM often yields good empirical classification results. However, they are hard to interpret due to missing geometrical and theoretical understanding. In this paper, we provide a step toward the comprehension of SVM classifiers in these situations. We give a geometric interpretation of SVM with indefinite kernel functions. We show that such SVM are optimal hyperplane classifiers not by margin maximization, but by minimization of distances between convex hulls in pseudo-Euclidean spaces. By this, we obtain a sound framework and motivation for indefinite SVM. This interpretation is the basis for further theoretical analysis, e.g., investigating uniqueness, and for the derivation of practical guidelines like characterizing the suitability of indefinite SVM.", "year": 2005, "referenceCount": 32, "citationCount": 255, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "authors": [{"authorId": "2800929", "name": "B. Haasdonk"}]}, {"paperId": "dd30a98f5274541f018a762fba46a0730519606a", "url": "https://www.semanticscholar.org/paper/dd30a98f5274541f018a762fba46a0730519606a", "title": "Federated Learning on Non-IID Data Silos: An Experimental Study", "abstract": "Due to the increasing privacy concerns and data regulations, training data have been increasingly fragmented, forming distributed databases of multiple \u201cdata silos\u201d (e.g., within different organizations and countries). To develop effective machine learning services, there is a must to exploit data from such distributed databases without exchanging the raw data. Recently, federated learning (FL) has been a solution with growing interests, which enables multiple parties to collaboratively train a machine learning model without exchanging their local data. A key and common challenge on distributed databases is the heterogeneity of the data distribution among the parties. The data of different parties are usually non-independently and identically distributed (i.e., non-IID). There have been many FL algorithms to address the learning effectiveness under non-IID data settings. However, there lacks an experimental study on systematically understanding their advantages and disadvantages, as previous studies have very rigid data partitioning strategies among parties, which are hardly representative and thorough. In this paper, to help researchers better understand and study the non-IID data setting in federated learning, we propose comprehensive data partitioning strategies to cover the typical non-IID data cases. Moreover, we conduct extensive experiments to evaluate state-of-the-art FL algorithms. We find that non-IID does bring significant challenges in learning accuracy of FL algorithms, and none of the existing state-of-the-art FL algorithms outperforms others in all cases. Our experiments provide insights for future studies of addressing the challenges in \u201cdata silos\u201d.", "year": 2021, "referenceCount": 94, "citationCount": 137, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "92621060", "name": "Q. Li"}, {"authorId": "1871617938", "name": "Yiqun Diao"}, {"authorId": "2109396907", "name": "Quan Chen"}, {"authorId": "143824511", "name": "Bingsheng He"}]}, {"paperId": "c2f792c74ae86f41f74e7a5babdcbe8ec347fa6e", "url": "https://www.semanticscholar.org/paper/c2f792c74ae86f41f74e7a5babdcbe8ec347fa6e", "title": "Automated de novo molecular design by hybrid machine intelligence and rule-driven chemical synthesis", "abstract": null, "year": 2019, "referenceCount": 27, "citationCount": 41, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "47000850", "name": "Alexander L. Button"}, {"authorId": "2077451014", "name": "D. Merk"}, {"authorId": "40008184", "name": "J. A. Hiss"}, {"authorId": "144522872", "name": "G. Schneider"}]}, {"paperId": "ea8dede934a3f4124d1a8995b8289f64a6c3d8b3", "url": "https://www.semanticscholar.org/paper/ea8dede934a3f4124d1a8995b8289f64a6c3d8b3", "title": "A Backtesting Protocol in the Era of Machine Learning", "abstract": "Machine learning offers a set of powerful tools that holds considerable promise for investment management. As with most quantitative applications in finance, the danger of misapplying these techniques can lead to disappointment. One crucial limitation involves data availability. Many of machine learning\u2019s early successes originated in the physical and biological sciences, in which truly vast amounts of data are available. Machine learning applications often require far more data than are available in finance, which is of particular concern in longer-horizon investing. Hence, choosing the right applications before applying the tools is important. In addition, capital markets reflect the actions of people, who may be influenced by the actions of others and by the findings of past research. In many ways, the challenges that affect machine learning are merely a continuation of the long-standing issues researchers have always faced in quantitative finance. Although investors need to be cautious\u2014indeed, more cautious than in past applications of quantitative methods\u2014these new tools offer many potential applications in finance. In this article, the authors develop a research protocol that pertains both to the application of machine learning techniques and to quantitative finance in general.", "year": 2018, "referenceCount": 27, "citationCount": 56, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46371615", "name": "R. Arnott"}, {"authorId": "34913143", "name": "Campbell R. Harvey"}, {"authorId": "50018942", "name": "H. Markowitz"}]}, {"paperId": "796aa29837824d307ef43270f8f7f91bb20812ec", "url": "https://www.semanticscholar.org/paper/796aa29837824d307ef43270f8f7f91bb20812ec", "title": "mlr3: A modern object-oriented machine learning framework in R", "abstract": "The R (R Core Team, 2019) package mlr3 and its associated ecosystem of extension packages implements a powerful, object-oriented and extensible framework for machine learning (ML) in R. It provides a unified interface to many learning algorithms available on CRAN, augmenting them with model-agnostic general-purpose functionality that is needed in every ML project, for example train-test-evaluation, resampling, preprocessing, hyperparameter tuning, nested resampling, and visualization of results from ML experiments. The package is a complete reimplementation of the mlr (Bischl et al., 2016) package that leverages many years of experience and learned best practices to provide a state-of-the-art system that is powerful, flexible, extensible, and maintainable. We target both practitioners who want to quickly apply ML algorithms to their problems and researchers who want to implement, benchmark, and compare their new methods in a structured environment. mlr3 is suitable for short scripts that test an idea, for complex multi-stage experiments with advanced functionality that use a broad range of ML functionality, as a foundation to implement new ML (meta-)algorithms (for example AutoML systems), and everything in between. Functional correctness is ensured through extensive unit and integration tests.", "year": 2019, "referenceCount": 8, "citationCount": 109, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143962282", "name": "Michel Lang"}, {"authorId": "143780936", "name": "Martin Binder"}, {"authorId": "145444740", "name": "Jakob Richter"}, {"authorId": "40896579", "name": "P. Schratz"}, {"authorId": "3468393", "name": "F. Pfisterer"}, {"authorId": "51123419", "name": "Stefan Coors"}, {"authorId": "51306278", "name": "Quay Au"}, {"authorId": "8662947", "name": "Giuseppe Casalicchio"}, {"authorId": "1722782", "name": "Lars Kotthoff"}, {"authorId": "1686924", "name": "B. Bischl"}]}, {"paperId": "efa8b1c728a06a4f01a60f586ea684592cb746a2", "url": "https://www.semanticscholar.org/paper/efa8b1c728a06a4f01a60f586ea684592cb746a2", "title": "An Empirical Study of the Behavior of Active Learning for Word Sense Disambiguation", "abstract": "This paper shows that two uncertainty-based active learning methods, combined with a maximum entropy model, work well on learning English verb senses. Data analysis on the learning process, based on both instance and feature levels, suggests that a careful treatment of feature extraction is important for the active learning to be useful for WSD. The overfitting phenomena that occurred during the active learning process are identified as classic overfitting in machine learning based on the data analysis.", "year": 2006, "referenceCount": 16, "citationCount": 89, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2108216213", "name": "Jinying Chen"}, {"authorId": "34568108", "name": "A. Schein"}, {"authorId": "1412391493", "name": "L. Ungar"}, {"authorId": "145755155", "name": "Martha Palmer"}]}, {"paperId": "7d878fe31b9b57f75071586d83cdec2e8b81e039", "url": "https://www.semanticscholar.org/paper/7d878fe31b9b57f75071586d83cdec2e8b81e039", "title": "Fr\u00e9chet ChemNet Distance: A Metric for Generative Models for Molecules in Drug Discovery", "abstract": "The new wave of successful generative models in machine learning has increased the interest in deep learning driven de novo drug design. However, method comparison is difficult because of various flaws of the currently employed evaluation metrics. We propose an evaluation metric for generative models called Fr\u00e9chet ChemNet distance (FCD). The advantage of the FCD over previous metrics is that it can detect whether generated molecules are diverse and have similar chemical and biological properties as real molecules.", "year": 2018, "referenceCount": 29, "citationCount": 143, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Biology", "Mathematics", "Medicine"], "authors": [{"authorId": "32764807", "name": "Kristina Preuer"}, {"authorId": "30578364", "name": "Philipp Renz"}, {"authorId": "2465270", "name": "Thomas Unterthiner"}, {"authorId": "3308557", "name": "S. Hochreiter"}, {"authorId": "1994964", "name": "G. Klambauer"}]}, {"paperId": "0ebcf6037c0aae64a65e3fc7b058b806defbe22c", "url": "https://www.semanticscholar.org/paper/0ebcf6037c0aae64a65e3fc7b058b806defbe22c", "title": "VILLAGE - Virtual Immersive Language Learning and Gaming Environment: Immersion and presence", "abstract": "3 D virtual worlds are promising for immersive learning in English as a Foreign Language ( EFL). Unlike English as a Second Language ( ESL), EFL typically takes place in the learners' home countries, and the potential of the language is limited by geography. Although learning contexts where English is spoken is important, in most EFL courses at the college level, EFL is taught by acquiring vocabularies, grammar and pragmatic features without contextual immersion. In this study, an immersive English learning environment in a 3 D virtual world, Open Simulator, was developed with two key learning artifacts, chatbot and time machine. A single-factor, independent measures design was used to examines learners' presence under four learning conditions: virtual learning environment without digital learning artifacts ( VE), virtual learning environment with chatbot ( VEC), virtual learning environment with time machine ( VETM) and virtual learning environment with chatbot and time machine ( VECTM). Three research questions emerging from the four learning conditions form the backbone of this study: (1) Does chatbot increase language learners' presence in the immersive virtual English learning environment? (2) Does time machine increase language learners' presence in the immersive virtual English learning environment? (3) Does the combined use of chatbot and time machine increase presence more than either learning artifact alone? The experimental results indicate that the chatbot and time machine increase the learners' sense of immersion and presence. Best design practices should address how immersion and presence can be integrated into affordances of virtual worlds. [ABSTRACT FROM AUTHOR]", "year": 2017, "referenceCount": 24, "citationCount": 51, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2115568428", "name": "Yifei Wang"}, {"authorId": "9675772", "name": "S. Petrina"}, {"authorId": "46782202", "name": "Franc Feng"}]}, {"paperId": "c31b35fbac605b6c47e29eae1c68ba0c267ebd09", "url": "https://www.semanticscholar.org/paper/c31b35fbac605b6c47e29eae1c68ba0c267ebd09", "title": "Software Defect Identification Using Machine Learning Techniques", "abstract": "Software engineering is a tedious job that includes people, tight deadlines and limited budgets. Delivering what customer wants involves minimizing the defects in the programs. Hence, it is important to establish quality measures early on in the project life cycle. The main objective of this research is to analyze problems in software code and propose a model that will help catching those problems earlier in the project life cycle. Our proposed model uses machine learning methods. Principal component analysis is used for dimensionality reduction, and decision tree, multi layer perceptron and radial basis functions are used for defect prediction. The experiments in this research are carried out with different software metric datasets that are obtained from real-life projects of three big software companies in Turkey. We can say that, the improved method that we proposed brings out satisfactory results in terms of defect prediction", "year": 2006, "referenceCount": 18, "citationCount": 61, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "15371442", "name": "Evren Ceylan"}, {"authorId": "2530616", "name": "F. O. Kutlubay"}, {"authorId": "1730000", "name": "A. Bener"}]}, {"paperId": "9ce42ca0666a52107a08b15affd496f2c781df72", "url": "https://www.semanticscholar.org/paper/9ce42ca0666a52107a08b15affd496f2c781df72", "title": "The power of comparative reasoning", "abstract": "Rank correlation measures are known for their resilience to perturbations in numeric values and are widely used in many evaluation metrics. Such ordinal measures have rarely been applied in treatment of numeric features as a representational transformation. We emphasize the benefits of ordinal representations of input features both theoretically and empirically. We present a family of algorithms for computing ordinal embeddings based on partial order statistics. Apart from having the stability benefits of ordinal measures, these embeddings are highly nonlinear, giving rise to sparse feature spaces highly favored by several machine learning methods. These embeddings are deterministic, data independent and by virtue of being based on partial order statistics, add another degree of resilience to noise. These machine-learning-free methods when applied to the task of fast similarity search outperform state-of-the-art machine learning methods with complex optimization setups. For solving classification problems, the embeddings provide a nonlinear transformation resulting in sparse binary codes that are well-suited for a large class of machine learning algorithms. These methods show significant improvement on VOC 2010 using simple linear classifiers which can be trained quickly. Our method can be extended to the case of polynomial kernels, while permitting very efficient computation. Further, since the popular Min Hash algorithm is a special case of our method, we demonstrate an efficient scheme for computing Min Hash on conjunctions of binary features. The actual method can be implemented in about 10 lines of code in most languages (2 lines in MAT-LAB), and does not require any data-driven optimization.", "year": 2011, "referenceCount": 33, "citationCount": 128, "influentialCitationCount": 31, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1842163", "name": "J. Yagnik"}, {"authorId": "1713003", "name": "Dennis W. Strelow"}, {"authorId": "144711958", "name": "David A. Ross"}, {"authorId": "2900733", "name": "Ruei-Sung Lin"}]}, {"paperId": "206bf82d2b9a4a31e8cf749ea027485d9f6ffa1c", "url": "https://www.semanticscholar.org/paper/206bf82d2b9a4a31e8cf749ea027485d9f6ffa1c", "title": "Classification using Hierarchical Na\u00efve Bayes models", "abstract": null, "year": 2006, "referenceCount": 64, "citationCount": 91, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3310967", "name": "H. Langseth"}, {"authorId": "145830646", "name": "Thomas D. Nielsen"}]}, {"paperId": "44148499c23789781873f1ca56449c33422045f4", "url": "https://www.semanticscholar.org/paper/44148499c23789781873f1ca56449c33422045f4", "title": "A Web Survey on the Use of Active Learning to Support Annotation of Text Data", "abstract": "As supervised machine learning methods for addressing tasks in natural language processing (NLP) prove increasingly viable, the focus of attention is naturally shifted towards the creation of training data. The manual annotation of corpora is a tedious and time consuming process. To obtain high-quality annotated data constitutes a bottleneck in machine learning for NLP today. Active learning is one way of easing the burden of annotation. This paper presents a first probe into the NLP research community concerning the nature of the annotation projects undertaken in general, and the use of active learning as annotation support in particular.", "year": 2009, "referenceCount": 8, "citationCount": 75, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3357473", "name": "Katrin Tomanek"}, {"authorId": "2653947", "name": "Fredrik Olsson"}]}, {"paperId": "48231ac69e8d17ce08a2868b27d1a9b08f99be83", "url": "https://www.semanticscholar.org/paper/48231ac69e8d17ce08a2868b27d1a9b08f99be83", "title": "FlexPS: Flexible Parallelism Control in Parameter Server Architecture", "abstract": "As a general abstraction for coordinating the distributed storage and access of model parameters, the parameter server (PS) architecture enables distributed machine learning to handle large datasets and high dimensional models. Many systems, such as Parameter Server and Petuum, have been developed based on the PS architecture and widely used in practice. However, none of these systems supports changing parallelism during runtime, which is crucial for the efficient execution of machine learning tasks with dynamic workloads. We propose a new system, called FlexPS, which introduces a novel multi-stage abstraction to support flexible parallelism control. With the multi-stage abstraction, a machine learning task can be mapped to a series of stages and the parallelism for a stage can be set according to its workload. Optimizations such as stage scheduler, stage-aware consistency controller, and direct model transfer are proposed for the efficiency of multi-stage machine learning in FlexPS. As a general and complete PS systems, FlexPS also incorporates many optimizations that are not limited to multi-stage machine learning. We conduct extensive experiments using a variety of machine learning workloads, showing that FlexPS achieves significant speedups and resource saving compared with the state-of-the-art PS systems such as Petuum and Multiverso.", "year": 2018, "referenceCount": 53, "citationCount": 59, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2143454301", "name": "Yuzhen Huang"}, {"authorId": "35295263", "name": "Tatiana Jin"}, {"authorId": "47096554", "name": "Yidi Wu"}, {"authorId": "35349851", "name": "Zhenkun Cai"}, {"authorId": "145837716", "name": "Xiao Yan"}, {"authorId": "47829900", "name": "Fan Yang"}, {"authorId": null, "name": "Jinfeng Li"}, {"authorId": "2124920668", "name": "Yuying Guo"}, {"authorId": "1717691", "name": "James Cheng"}]}, {"paperId": "805d950d6df9bdabd6b87d06de213909192341db", "url": "https://www.semanticscholar.org/paper/805d950d6df9bdabd6b87d06de213909192341db", "title": "The carbon impact of artificial intelligence", "abstract": null, "year": 2020, "referenceCount": 0, "citationCount": 66, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "40564935", "name": "Payal Dhar"}]}, {"paperId": "c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716", "url": "https://www.semanticscholar.org/paper/c9a1e8e1ba2913ef0bdf1c5eaaa1ac0a79be3716", "title": "YouTube-8M: A Large-Scale Video Classification Benchmark", "abstract": "Many recent advancements in Computer Vision are attributed to large datasets. Open-source software packages for Machine Learning and inexpensive commodity hardware have reduced the barrier of entry for exploring novel approaches at scale. It is possible to train models over millions of examples within a few days. Although large-scale datasets exist for image understanding, such as ImageNet, there are no comparable size video classification datasets. \nIn this paper, we introduce YouTube-8M, the largest multi-label video classification dataset, composed of ~8 million videos (500K hours of video), annotated with a vocabulary of 4800 visual entities. To get the videos and their labels, we used a YouTube video annotation system, which labels videos with their main topics. While the labels are machine-generated, they have high-precision and are derived from a variety of human-based signals including metadata and query click signals. We filtered the video labels (Knowledge Graph entities) using both automated and manual curation strategies, including asking human raters if the labels are visually recognizable. Then, we decoded each video at one-frame-per-second, and used a Deep CNN pre-trained on ImageNet to extract the hidden representation immediately prior to the classification layer. Finally, we compressed the frame features and make both the features and video-level labels available for download. \nWe trained various (modest) classification models on the dataset, evaluated them using popular evaluation metrics, and report them as baselines. Despite the size of the dataset, some of our models train to convergence in less than a day on a single machine using TensorFlow. We plan to release code for training a TensorFlow model and for computing metrics.", "year": 2016, "referenceCount": 42, "citationCount": 956, "influentialCitationCount": 127, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1389570466", "name": "Sami Abu-El-Haija"}, {"authorId": "144317839", "name": "Nisarg Kothari"}, {"authorId": "2119006", "name": "Joonseok Lee"}, {"authorId": "1820908", "name": "A. Natsev"}, {"authorId": "1805076", "name": "G. Toderici"}, {"authorId": "2758088", "name": "Balakrishnan Varadarajan"}, {"authorId": "2259154", "name": "Sudheendra Vijayanarasimhan"}]}, {"paperId": "4fd355ee3a17ee1cc3e3eb7ebe5064200611adbb", "url": "https://www.semanticscholar.org/paper/4fd355ee3a17ee1cc3e3eb7ebe5064200611adbb", "title": "Machine Learning in Acute Ischemic Stroke Neuroimaging", "abstract": "Machine Learning (ML) through pattern recognition algorithms is currently becoming an essential aid for the diagnosis, treatment, and prediction of complications and patient outcomes in a number of neurological diseases. The evaluation and treatment of Acute Ischemic Stroke (AIS) have experienced a significant advancement over the past few years, increasingly requiring the use of neuroimaging for decision-making. In this review, we offer an insight into the recent developments and applications of ML in neuroimaging focusing on acute ischemic stroke.", "year": 2018, "referenceCount": 72, "citationCount": 61, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "47150922", "name": "H. Kamal"}, {"authorId": "2061878276", "name": "V. Lopez"}, {"authorId": "31965568", "name": "S. Sheth"}]}, {"paperId": "6241702ad6c469b1e347a018ec191b0c31eef4a5", "url": "https://www.semanticscholar.org/paper/6241702ad6c469b1e347a018ec191b0c31eef4a5", "title": "A case for machine learning to optimize multicore performance", "abstract": "Multicore architectures have become so complex and diverse that there is no obvious path to achieving good performance. Hundreds of code transformations, compiler flags, architectural features and optimization parameters result in a search space that can take many machinemonths to explore exhaustively. Inspired by successes in the systems community, we apply state-of-the-art machine learning techniques to explore this space more intelligently. On 7-point and 27-point stencil code, our technique takes about two hours to discover a configuration whose performance is within 1% of and up to 18% better than that achieved by a human expert. This factor of 2000 speedup over manual exploration of the auto-tuning parameter space enables us to explore optimizations that were previously off-limits. We believe the opportunity for using machine learning in multicore autotuning is even more promising than the successes to date in the systems literature.", "year": 2009, "referenceCount": 18, "citationCount": 99, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1701816", "name": "A. Ganapathi"}, {"authorId": "49589461", "name": "K. Datta"}, {"authorId": "2057888081", "name": "Armando Fox"}, {"authorId": "2052997025", "name": "David A. Patterson"}]}, {"paperId": "817f5e02d30195305e8e7f9f7e036152b48faf53", "url": "https://www.semanticscholar.org/paper/817f5e02d30195305e8e7f9f7e036152b48faf53", "title": "Evaluating Late Blight Severity in Potato Crops Using Unmanned Aerial Vehicles and Machine Learning Algorithms", "abstract": "This work presents quantitative prediction of severity of the disease caused by Phytophthora infestans in potato crops using machine learning algorithms such as multilayer perceptron, deep learning convolutional neural networks, support vector regression, and random forests. The machine learning algorithms are trained using datasets extracted from multispectral data captured at the canopy level with an unmanned aerial vehicle, carrying an inexpensive digital camera. The results indicate that deep learning convolutional neural networks, random forests and multilayer perceptron using band differences can predict the level of Phytophthora infestans affectation on potato crops with acceptable accuracy.", "year": 2018, "referenceCount": 39, "citationCount": 60, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Geology"], "authors": [{"authorId": "1397244014", "name": "J. Duarte-Carvajalino"}, {"authorId": "2064606609", "name": "Diego F. Alzate"}, {"authorId": "144909163", "name": "Andr\u00e9s A. Ramirez"}, {"authorId": "1423439241", "name": "Juan D. Santa-Sepulveda"}, {"authorId": "1423438724", "name": "Alexandra E. Fajardo-Rojas"}, {"authorId": "1490706353", "name": "Mauricio Soto-Su\u00e1rez"}]}, {"paperId": "3dac0f361aa02435c39ba5a0b7d33543d4195145", "url": "https://www.semanticscholar.org/paper/3dac0f361aa02435c39ba5a0b7d33543d4195145", "title": "Designing Anticancer Peptides by Constructive Machine Learning", "abstract": "Constructive (generative) machine learning enables the automated generation of novel chemical structures without the need for explicit molecular design rules. This study presents the experimental application of such a deep machine learning model to design membranolytic anticancer peptides (ACPs) de novo. A recurrent neural network with long short\u2010term memory cells was trained on \u03b1\u2010helical cationic amphipathic peptide sequences and then fine\u2010tuned with 26 known ACPs by transfer learning. This optimized model was used to generate unique and novel amino acid sequences. Twelve of the peptides were synthesized and tested for their activity on MCF7 human breast adenocarcinoma cells and selectivity against human erythrocytes. Ten of these peptides were active against cancer cells. Six of the active peptides killed MCF7 cancer cells without affecting human erythrocytes with at least threefold selectivity. These results advocate constructive machine learning for the automated design of peptides with desired biological activities.", "year": 2018, "referenceCount": 18, "citationCount": 53, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "3674456", "name": "F. Grisoni"}, {"authorId": "46703885", "name": "Claudia S Neuhaus"}, {"authorId": "8760576", "name": "G. Gabernet"}, {"authorId": "35276897", "name": "A. T. M\u00fcller"}, {"authorId": "40008184", "name": "J. A. Hiss"}, {"authorId": "144522872", "name": "G. Schneider"}]}, {"paperId": "334c32286f3af8cfbce4dc4bed8e110dd91826cb", "url": "https://www.semanticscholar.org/paper/334c32286f3af8cfbce4dc4bed8e110dd91826cb", "title": "Bond Risk Premia with Machine Learning", "abstract": "We show that machine learning methods, in particular extreme trees and neural networks (NNs), provide strong statistical evidence in favor of bond return predictability. NN forecasts based on macroeconomic and yield information translate into economic gains that are larger than those obtained using yields alone. Interestingly, the nature of unspanned factors changes along the yield curve: stock and labor market related variables are more relevant for short-term maturities, whereas output and income variables matter more for longer maturities. Finally, NN forecasts correlate with proxies for time-varying risk aversion and uncertainty, lending support to models featuring both of these channels.", "year": 2020, "referenceCount": 155, "citationCount": 61, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "146467911", "name": "Daniele Bianchi"}, {"authorId": "48556577", "name": "M. B\u00fcchner"}, {"authorId": "2468965", "name": "A. Tamoni"}]}, {"paperId": "ee71c6bc5c5102e68eee86795b3cea35a83a1e22", "url": "https://www.semanticscholar.org/paper/ee71c6bc5c5102e68eee86795b3cea35a83a1e22", "title": "Search Engines that Learn from Implicit Feedback", "abstract": "Search-engine logs provide a wealth of information that machine-learning techniques can harness to improve search quality. With proper interpretations that avoid inherent biases, a search engine can use training data extracted from the logs to automatically tailor ranking functions to a particular user group or collection.", "year": 2007, "referenceCount": 13, "citationCount": 195, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1680188", "name": "T. Joachims"}, {"authorId": "1803571", "name": "Filip Radlinski"}]}, {"paperId": "5507dc32b368c8afd3b9507e9b5888da7bd7d7cd", "url": "https://www.semanticscholar.org/paper/5507dc32b368c8afd3b9507e9b5888da7bd7d7cd", "title": "Sequence-to-Sequence Learning as Beam-Search Optimization", "abstract": "Sequence-to-Sequence (seq2seq) modeling has rapidly become an important general-purpose NLP tool that has proven effective for many text-generation and sequence-labeling tasks. Seq2seq builds on deep neural language modeling and inherits its remarkable accuracy in estimating local, next-word distributions. In this work, we introduce a model and beam-search training scheme, based on the work of Daume III and Marcu (2005), that extends seq2seq to learn global sequence scores. This structured approach avoids classical biases associated with local training and unifies the training loss with the test-time usage, while preserving the proven model architecture of seq2seq and its efficient training approach. We show that our system outperforms a highly-optimized attention-based seq2seq system and other baselines on three different sequence to sequence tasks: word ordering, parsing, and machine translation.", "year": 2016, "referenceCount": 46, "citationCount": 493, "influentialCitationCount": 36, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2844243", "name": "Sam Wiseman"}, {"authorId": "2531268", "name": "Alexander M. Rush"}]}, {"paperId": "fd5f4d69d60372ea8c16ee185a7bd66abeb5d9f0", "url": "https://www.semanticscholar.org/paper/fd5f4d69d60372ea8c16ee185a7bd66abeb5d9f0", "title": "Disjunctions of Conjunctions, Cognitive Simplicity, and Consideration Sets", "abstract": "The authors test methods, based on cognitively simple decision rules, that predict which products consumers select for their consideration sets. Drawing on qualitative research, the authors propose disjunctions-of-conjunctions (DOC) decision rules that generalize well-studied decision models, such as disjunctive, conjunctive, lexicographic, and subset conjunctive rules. They propose two machine-learning methods to estimate cognitively simple DOC rules. They observe consumers' consideration sets for global positioning systems for both calibration and validation data. They compare the proposed methods with both machine-learning and hierarchical Bayes methods, each based on five extant compensatory and noncompensatory rules. For the validation data, the cognitively simple DOC-based methods predict better than the ten benchmark methods on an information theoretic measure and on hit rates. The results are robust with respect to format by which consideration is measured, sample, and presentation of profiles. The article closes with an illustration of how DOC-based rules can affect managerial decisions.", "year": 2010, "referenceCount": 89, "citationCount": 160, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "2344635", "name": "J. Hauser"}, {"authorId": "3089062", "name": "Olivier Toubia"}, {"authorId": "1801089", "name": "T. Evgeniou"}, {"authorId": "103227759", "name": "R. Befurt"}, {"authorId": "2530415", "name": "Daria Dzyabura"}]}, {"paperId": "8fd58293e18a43fa97746bf3cd1273641054084b", "url": "https://www.semanticscholar.org/paper/8fd58293e18a43fa97746bf3cd1273641054084b", "title": "A Machine Learning Framework to Identify Students at Risk of Adverse Academic Outcomes", "abstract": "Many school districts have developed successful intervention programs to help students graduate high school on time. However, identifying and prioritizing students who need those interventions the most remains challenging. This paper describes a machine learning framework to identify such students, discusses features that are useful for this task, applies several classification algorithms, and evaluates them using metrics important to school administrators. To help test this framework and make it practically useful, we partnered with two U.S. school districts with a combined enrollment of approximately 200,000 students. We together designed several evaluation metrics to assess the goodness of machine learning algorithms from an educator's perspective. This paper focuses on students at risk of not finishing high school on time, but our framework lays a strong foundation for future work on other adverse academic outcomes.", "year": 2015, "referenceCount": 36, "citationCount": 123, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1892673", "name": "Himabindu Lakkaraju"}, {"authorId": "49112220", "name": "E. Aguiar"}, {"authorId": "1393011928", "name": "Carl Shan"}, {"authorId": "152724738", "name": "David I Miller"}, {"authorId": "2067076804", "name": "Nasir Bhanpuri"}, {"authorId": "1791498", "name": "R. Ghani"}, {"authorId": "36990335", "name": "Kecia L. Addison"}]}, {"paperId": "3ee6334ccf788cab8469295a8b143390e90d247c", "url": "https://www.semanticscholar.org/paper/3ee6334ccf788cab8469295a8b143390e90d247c", "title": "Pattern Classification Using Ensemble Methods", "abstract": "Researchers from various disciplines such as pattern recognition, statistics, and machine learning have explored the use of ensemble methodology since the late seventies. Thus, they are faced with a wide variety of methods, given the growing interest in the field. This book aims to impose a degree of order upon this diversity by presenting a coherent and unified repository of ensemble methods, theories, trends, challenges and applications. The book describes in detail the classical methods, as well as the extensions and novel approaches developed recently. Along with algorithmic descriptions of each method, it also explains the circumstances in which this method is applicable and the consequences and the trade-offs incurred by using the method.", "year": 2009, "referenceCount": 0, "citationCount": 258, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1732091", "name": "L. Rokach"}]}, {"paperId": "33656048f1eb1ffcc08739368102ee46f69a86d7", "url": "https://www.semanticscholar.org/paper/33656048f1eb1ffcc08739368102ee46f69a86d7", "title": "Predicting Software Anomalies Using Machine Learning Techniques", "abstract": "In this paper, we present a detailed evaluation of a set of well-known Machine Learning classifiers in front of dynamic and non-deterministic software anomalies. The system state prediction is based on monitoring system metrics. This allows software proactive rejuvenation to be triggered automatically. Random Forest approach achieves validation errors less than 1% in comparison to the well-known ML algorithms under a valuation. In order to reduce automatically the number of monitored parameters, needed to predict software anomalies, we analyze Lasso Regularization technique jointly with the Machine Learning classifiers to evaluate how the prediction accuracy could be guaranteed within an acceptable threshold. This allows to reduce drastically (around 60% in the best case) the number of monitoring parameters. The framework, based on ML and Lasso regularization techniques, has been validated using an ecommerce environment with Apache Tomcat server, and MySql database server.", "year": 2011, "referenceCount": 27, "citationCount": 59, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2072641308", "name": "Javier Alonso"}, {"authorId": "1692333", "name": "L. B. Mu\u00f1oz"}, {"authorId": "1795485", "name": "D. Avresky"}]}, {"paperId": "22499d33e81a9bc63bc45ac682c1f26498048425", "url": "https://www.semanticscholar.org/paper/22499d33e81a9bc63bc45ac682c1f26498048425", "title": "Template Attacks vs. Machine Learning Revisited (and the Curse of Dimensionality in Side-Channel Analysis)", "abstract": null, "year": 2015, "referenceCount": 36, "citationCount": 120, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39319437", "name": "Liran Lerman"}, {"authorId": "2759945", "name": "R. Poussier"}, {"authorId": "1772497", "name": "Gianluca Bontempi"}, {"authorId": "1802747", "name": "O. Markowitch"}, {"authorId": "1706533", "name": "Fran\u00e7ois-Xavier Standaert"}]}, {"paperId": "d997919c30fa6711bc5c25cf8c8aea34fac27b91", "url": "https://www.semanticscholar.org/paper/d997919c30fa6711bc5c25cf8c8aea34fac27b91", "title": "OpenFace: An open source facial behavior analysis toolkit", "abstract": "Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.", "year": 2016, "referenceCount": 80, "citationCount": 1025, "influentialCitationCount": 157, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1756344", "name": "T. Baltru\u0161aitis"}, {"authorId": "2149814967", "name": "P. Robinson"}, {"authorId": "49933077", "name": "Louis-Philippe Morency"}]}, {"paperId": "442230be8f0b239d9f158b9bbfeb444c2974286b", "url": "https://www.semanticscholar.org/paper/442230be8f0b239d9f158b9bbfeb444c2974286b", "title": "Machine Learning for Cultural Heritage: A Survey", "abstract": null, "year": 2020, "referenceCount": 59, "citationCount": 81, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2461401", "name": "M. Fiorucci"}, {"authorId": "10437540", "name": "M. Khoroshiltseva"}, {"authorId": "1704699", "name": "M. Pontil"}, {"authorId": "3256158", "name": "A. Traviglia"}, {"authorId": "8955013", "name": "A. D. Bue"}, {"authorId": "49280934", "name": "Stuart James"}]}, {"paperId": "1d8465c3f5aee1b7a790f6eeb44637343861ba47", "url": "https://www.semanticscholar.org/paper/1d8465c3f5aee1b7a790f6eeb44637343861ba47", "title": "Towards Machine Learning-Based Auto-tuning of MapReduce", "abstract": "MapReduce, which is the de facto programming model for large-scale distributed data processing, and its most popular implementation Hadoop have enjoyed widespread adoption in industry during the past few years. Unfortunately, from a performance point of view getting the most out of Hadoop is still a big challenge due to the large number of configuration parameters. Currently these parameters are tuned manually by trial and error, which is ineffective due to the large parameter space and the complex interactions among the parameters. Even worse, the parameters have to be re-tuned for different MapReduce applications and clusters. To make the parameter tuning process more effective, in this paper we explore machine learning-based performance models that we use to auto-tune the configuration parameters. To this end, we first evaluate several machine learning models with diverse MapReduce applications and cluster configurations, and we show that support vector regression model (SVR) has good accuracy and is also computationally efficient. We further assess our auto-tuning approach, which uses the SVR performance model, against the Starfish auto tuner, which uses a cost-based performance model. Our findings reveal that our auto-tuning approach can provide comparable or in some cases better performance improvements than Starfish with a smaller number of parameters. Finally, we propose and discuss a complete and practical end-to-end auto-tuning flow that combines our machine learning-based performance models with smart search algorithms for the effective training of the models and the effective exploration of the parameter space.", "year": 2013, "referenceCount": 20, "citationCount": 94, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3160525", "name": "N. Yigitbasi"}, {"authorId": "2999876", "name": "Theodore L. Willke"}, {"authorId": "2750402", "name": "Guangdeng Liao"}, {"authorId": "1776848", "name": "D. Epema"}]}, {"paperId": "0bd88eb08295a8aff219fa986112478883921452", "url": "https://www.semanticscholar.org/paper/0bd88eb08295a8aff219fa986112478883921452", "title": "Quantifying rooftop photovoltaic solar energy potential: A machine learning approach", "abstract": null, "year": 2017, "referenceCount": 65, "citationCount": 141, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Environmental Science"], "authors": [{"authorId": "51933823", "name": "D. Assouline"}, {"authorId": "1753406", "name": "Nahid Mohajeri"}, {"authorId": "72646123", "name": "J. Scartezzini"}]}, {"paperId": "600be3dde18d1059c6b56170bd04ee65ce79a848", "url": "https://www.semanticscholar.org/paper/600be3dde18d1059c6b56170bd04ee65ce79a848", "title": "Lookahead Optimizer: k steps forward, 1 step back", "abstract": "The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.", "year": 2019, "referenceCount": 51, "citationCount": 450, "influentialCitationCount": 69, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "50495487", "name": "Michael Ruogu Zhang"}, {"authorId": "145202377", "name": "James Lucas"}, {"authorId": "1695689", "name": "Geoffrey E. Hinton"}, {"authorId": "2503659", "name": "Jimmy Ba"}]}]}