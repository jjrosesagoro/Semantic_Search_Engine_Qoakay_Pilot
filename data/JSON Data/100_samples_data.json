{"total": 5797238, "offset": 0, "next": 100, "data": [{"paperId": "46200b99c40e8586c8a0f588488ab6414119fb28", "url": "https://www.semanticscholar.org/paper/46200b99c40e8586c8a0f588488ab6414119fb28", "title": "TensorFlow: A system for large-scale machine learning", "abstract": "TensorFlow is a machine learning system that operates at large scale and in heterogeneous environments. Tensor-Flow uses dataflow graphs to represent computation, shared state, and the operations that mutate that state. It maps the nodes of a dataflow graph across many machines in a cluster, and within a machine across multiple computational devices, including multicore CPUs, general-purpose GPUs, and custom-designed ASICs known as Tensor Processing Units (TPUs). This architecture gives flexibility to the application developer: whereas in previous \"parameter server\" designs the management of shared state is built into the system, TensorFlow enables developers to experiment with novel optimizations and training algorithms. TensorFlow supports a variety of applications, with a focus on training and inference on deep neural networks. Several Google services use TensorFlow in production, we have released it as an open-source project, and it has become widely used for machine learning research. In this paper, we describe the TensorFlow dataflow model and demonstrate the compelling performance that TensorFlow achieves for several real-world applications.", "year": 2016, "referenceCount": 94, "citationCount": 13969, "influentialCitationCount": 1679, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2057642721", "name": "Mart\u00edn Abadi"}, {"authorId": "144758007", "name": "P. Barham"}, {"authorId": "2108406634", "name": "Jianmin Chen"}, {"authorId": "2545358", "name": "Z. Chen"}, {"authorId": "36347083", "name": "Andy Davis"}, {"authorId": "49959210", "name": "J. Dean"}, {"authorId": "145139947", "name": "Matthieu Devin"}, {"authorId": "1780892", "name": "S. Ghemawat"}, {"authorId": "2060655766", "name": "Geoffrey Irving"}, {"authorId": "2090818", "name": "M. Isard"}, {"authorId": "1942300", "name": "M. Kudlur"}, {"authorId": "3369421", "name": "J. Levenberg"}, {"authorId": "3089272", "name": "R. Monga"}, {"authorId": "144375552", "name": "Sherry Moore"}, {"authorId": "20154699", "name": "D. Murray"}, {"authorId": "32163737", "name": "Benoit Steiner"}, {"authorId": "2080690", "name": "P. Tucker"}, {"authorId": "2053781980", "name": "Vijay Vasudevan"}, {"authorId": "47941411", "name": "P. Warden"}, {"authorId": "35078078", "name": "M. Wicke"}, {"authorId": "2117163698", "name": "Yuan Yu"}, {"authorId": "2108113547", "name": "Xiaoqiang Zhang"}]}, {"paperId": "f9c602cc436a9ea2f9e7db48c77d924e09ce3c32", "url": "https://www.semanticscholar.org/paper/f9c602cc436a9ea2f9e7db48c77d924e09ce3c32", "title": "Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms", "abstract": "We present Fashion-MNIST, a new dataset comprising of 28x28 grayscale images of 70,000 fashion products from 10 categories, with 7,000 images per category. The training set has 60,000 images and the test set has 10,000 images. Fashion-MNIST is intended to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms, as it shares the same image size, data format and the structure of training and testing splits. The dataset is freely available at this https URL", "year": 2017, "referenceCount": 6, "citationCount": 4588, "influentialCitationCount": 1340, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145642373", "name": "Han Xiao"}, {"authorId": "4565995", "name": "Kashif Rasul"}, {"authorId": "2742129", "name": "Roland Vollgraf"}]}, {"paperId": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "url": "https://www.semanticscholar.org/paper/9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d", "title": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems", "abstract": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org.", "year": 2016, "referenceCount": 55, "citationCount": 9429, "influentialCitationCount": 1008, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2057642721", "name": "Mart\u00edn Abadi"}, {"authorId": "2078528337", "name": "Ashish Agarwal"}, {"authorId": "144758007", "name": "P. Barham"}, {"authorId": "2445241", "name": "E. Brevdo"}, {"authorId": "2545358", "name": "Z. Chen"}, {"authorId": "48738717", "name": "C. Citro"}, {"authorId": "32131713", "name": "G. Corrado"}, {"authorId": "36347083", "name": "Andy Davis"}, {"authorId": "49959210", "name": "J. Dean"}, {"authorId": "145139947", "name": "Matthieu Devin"}, {"authorId": "1780892", "name": "S. Ghemawat"}, {"authorId": "153440022", "name": "Ian J. Goodfellow"}, {"authorId": "2064102917", "name": "A. Harp"}, {"authorId": "2060655766", "name": "Geoffrey Irving"}, {"authorId": "2090818", "name": "M. Isard"}, {"authorId": "39978391", "name": "Yangqing Jia"}, {"authorId": "1944541", "name": "R. J\u00f3zefowicz"}, {"authorId": "40527594", "name": "Lukasz Kaiser"}, {"authorId": "1942300", "name": "M. Kudlur"}, {"authorId": "3369421", "name": "J. Levenberg"}, {"authorId": "30415265", "name": "Dandelion Man\u00e9"}, {"authorId": "3089272", "name": "R. Monga"}, {"authorId": "144375552", "name": "Sherry Moore"}, {"authorId": "20154699", "name": "D. Murray"}, {"authorId": "37232298", "name": "C. Olah"}, {"authorId": "144927151", "name": "M. Schuster"}, {"authorId": "1789737", "name": "Jonathon Shlens"}, {"authorId": "32163737", "name": "Benoit Steiner"}, {"authorId": "1701686", "name": "Ilya Sutskever"}, {"authorId": "35210462", "name": "Kunal Talwar"}, {"authorId": "2080690", "name": "P. Tucker"}, {"authorId": "2657155", "name": "Vincent Vanhoucke"}, {"authorId": "2053781980", "name": "Vijay Vasudevan"}, {"authorId": "1765169", "name": "F. Vi\u00e9gas"}, {"authorId": "1689108", "name": "Oriol Vinyals"}, {"authorId": "47941411", "name": "P. Warden"}, {"authorId": "145233583", "name": "M. Wattenberg"}, {"authorId": "35078078", "name": "M. Wicke"}, {"authorId": "2117163698", "name": "Yuan Yu"}, {"authorId": "2152198093", "name": "Xiaoqiang Zheng"}]}, {"paperId": "f9c990b1b5724e50e5632b94fdb7484ece8a6ce7", "url": "https://www.semanticscholar.org/paper/f9c990b1b5724e50e5632b94fdb7484ece8a6ce7", "title": "Convolutional LSTM Network: A Machine Learning Approach for Precipitation Nowcasting", "abstract": "The goal of precipitation nowcasting is to predict the future rainfall intensity in a local region over a relatively short period of time. Very few previous studies have examined this crucial and challenging weather forecasting problem from the machine learning perspective. In this paper, we formulate precipitation nowcasting as a spatiotemporal sequence forecasting problem in which both the input and the prediction target are spatiotemporal sequences. By extending the fully connected LSTM (FC-LSTM) to have convolutional structures in both the input-to-state and state-to-state transitions, we propose the convolutional LSTM (ConvLSTM) and use it to build an end-to-end trainable model for the precipitation nowcasting problem. Experiments show that our ConvLSTM network captures spatiotemporal correlations better and consistently outperforms FC-LSTM and the state-of-the-art operational ROVER algorithm for precipitation nowcasting.", "year": 2015, "referenceCount": 26, "citationCount": 4697, "influentialCitationCount": 738, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3008587", "name": "Xingjian Shi"}, {"authorId": "2192200", "name": "Zhourong Chen"}, {"authorId": "49528584", "name": "Hao Wang"}, {"authorId": "1739816", "name": "D. Yeung"}, {"authorId": "145771919", "name": "W. Wong"}, {"authorId": "2183294", "name": "W. Woo"}]}, {"paperId": "0090023afc66cd2741568599057f4e82b566137c", "url": "https://www.semanticscholar.org/paper/0090023afc66cd2741568599057f4e82b566137c", "title": "A Survey on Bias and Fairness in Machine Learning", "abstract": "With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.", "year": 2019, "referenceCount": 176, "citationCount": 1360, "influentialCitationCount": 80, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "51997673", "name": "Ninareh Mehrabi"}, {"authorId": "2775559", "name": "Fred Morstatter"}, {"authorId": "51884035", "name": "N. Saxena"}, {"authorId": "1782658", "name": "Kristina Lerman"}, {"authorId": "143728483", "name": "A. Galstyan"}]}, {"paperId": "25badc676197a70aaf9911865eb03469e402ba57", "url": "https://www.semanticscholar.org/paper/25badc676197a70aaf9911865eb03469e402ba57", "title": "Machine learning - a probabilistic perspective", "abstract": "Today's Web-enabled deluge of electronic data calls for automated methods of data analysis. Machine learning provides these, developing methods that can automatically detect patterns in data and then use the uncovered patterns to predict future data. This textbook offers a comprehensive and self-contained introduction to the field of machine learning, based on a unified, probabilistic approach. The coverage combines breadth and depth, offering necessary background material on such topics as probability, optimization, and linear algebra as well as discussion of recent developments in the field, including conditional random fields, L1 regularization, and deep learning. The book is written in an informal, accessible style, complete with pseudo-code for the most important algorithms. All topics are copiously illustrated with color images and worked examples drawn from such application domains as biology, text processing, computer vision, and robotics. Rather than providing a cookbook of different heuristic methods, the book stresses a principled model-based approach, often using the language of graphical models to specify models in a concise and intuitive way. Almost all the models described have been implemented in a MATLAB software package--PMTK (probabilistic modeling toolkit)--that is freely available online. The book is suitable for upper-level undergraduates with an introductory-level college math background and beginning graduate students.", "year": 2012, "referenceCount": 1032, "citationCount": 7662, "influentialCitationCount": 976, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2056417995", "name": "K. Murphy"}]}, {"paperId": "b42b1bfdc262bf99e9484e2e9df94df216b96374", "url": "https://www.semanticscholar.org/paper/b42b1bfdc262bf99e9484e2e9df94df216b96374", "title": "Data Mining Practical Machine Learning Tools and Techniques", "abstract": null, "year": 2014, "referenceCount": 0, "citationCount": 11044, "influentialCitationCount": 1535, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "71619391", "name": "\u0e2d\u0e19\u0e34\u0e23\u0e38\u0e18 \u0e2a\u0e37\u0e1a\u0e2a\u0e34\u0e07\u0e2b\u0e4c"}]}, {"paperId": "168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "url": "https://www.semanticscholar.org/paper/168f28ac3c8c7ea63bf7ed25f2288e8b67e2fe74", "title": "Scikit-learn: Machine Learning in Python", "abstract": "Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.", "year": 2011, "referenceCount": 18, "citationCount": 48681, "influentialCitationCount": 3060, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2570016", "name": "Fabian Pedregosa"}, {"authorId": "3025780", "name": "G. Varoquaux"}, {"authorId": "1797840", "name": "Alexandre Gramfort"}, {"authorId": "52200573", "name": "V. Michel"}, {"authorId": "8493461", "name": "B. Thirion"}, {"authorId": "2958756", "name": "O. Grisel"}, {"authorId": "27257992", "name": "Mathieu Blondel"}, {"authorId": "1881041", "name": "Gilles Louppe"}, {"authorId": "2780213", "name": "P. Prettenhofer"}, {"authorId": "2067827437", "name": "Ron Weiss"}, {"authorId": "39571582", "name": "Ron J. Weiss"}, {"authorId": "2081469", "name": "J. Vanderplas"}, {"authorId": "144720379", "name": "Alexandre Passos"}, {"authorId": "3084321", "name": "D. Cournapeau"}, {"authorId": "2423884", "name": "M. Brucher"}, {"authorId": "35243423", "name": "M. Perrot"}, {"authorId": "1710398", "name": "E. Duchesnay"}]}, {"paperId": "bc00ff34ec7772080c7039b17f7069a2f7df0889", "url": "https://www.semanticscholar.org/paper/bc00ff34ec7772080c7039b17f7069a2f7df0889", "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "abstract": null, "year": 2018, "referenceCount": 71, "citationCount": 2458, "influentialCitationCount": 167, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "48395540", "name": "C. Rudin"}]}, {"paperId": "2e2089ae76fe914706e6fa90081a79c8fe01611e", "url": "https://www.semanticscholar.org/paper/2e2089ae76fe914706e6fa90081a79c8fe01611e", "title": "Practical Bayesian Optimization of Machine Learning Algorithms", "abstract": "The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a \"black art\" requiring expert experience, rules of thumb, or sometimes brute-force search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.", "year": 2012, "referenceCount": 30, "citationCount": 5532, "influentialCitationCount": 496, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "144108062", "name": "Jasper Snoek"}, {"authorId": "1777528", "name": "H. Larochelle"}, {"authorId": "1722180", "name": "Ryan P. Adams"}]}, {"paperId": "73f15adaeb9d59cf83e6f556500ab9e54d6487c6", "url": "https://www.semanticscholar.org/paper/73f15adaeb9d59cf83e6f556500ab9e54d6487c6", "title": "An Introduction to Machine Learning", "abstract": null, "year": 2017, "referenceCount": 99, "citationCount": 2982, "influentialCitationCount": 235, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1713687", "name": "M. Kub\u00e1t"}]}, {"paperId": "53b047e503f4c24602f376a774d653f7ed56c024", "url": "https://www.semanticscholar.org/paper/53b047e503f4c24602f376a774d653f7ed56c024", "title": "Practical Black-Box Attacks against Machine Learning", "abstract": "Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19% and 88.94%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.", "year": 2016, "referenceCount": 39, "citationCount": 2572, "influentialCitationCount": 211, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1967156", "name": "Nicolas Papernot"}, {"authorId": "144061974", "name": "P. Mcdaniel"}, {"authorId": "153440022", "name": "Ian J. Goodfellow"}, {"authorId": "1680133", "name": "S. Jha"}, {"authorId": "144643812", "name": "Z. B. Celik"}, {"authorId": "144231976", "name": "A. Swami"}]}, {"paperId": "5c39e37022661f81f79e481240ed9b175dec6513", "url": "https://www.semanticscholar.org/paper/5c39e37022661f81f79e481240ed9b175dec6513", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "abstract": "As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.", "year": 2017, "referenceCount": 63, "citationCount": 2204, "influentialCitationCount": 203, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1388372395", "name": "Finale Doshi-Velez"}, {"authorId": "3351164", "name": "Been Kim"}]}, {"paperId": "e2a85a6766b982ff7c8980e57ca6342d22493827", "url": "https://www.semanticscholar.org/paper/e2a85a6766b982ff7c8980e57ca6342d22493827", "title": "Adversarial Machine Learning at Scale", "abstract": "Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a \"label leaking\" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.", "year": 2016, "referenceCount": 19, "citationCount": 2142, "influentialCitationCount": 426, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145714153", "name": "A. Kurakin"}, {"authorId": "153440022", "name": "Ian J. Goodfellow"}, {"authorId": "1751569", "name": "Samy Bengio"}]}, {"paperId": "ce615ae61d67db8537e981a0a08da7f0f2ff1cee", "url": "https://www.semanticscholar.org/paper/ce615ae61d67db8537e981a0a08da7f0f2ff1cee", "title": "Understanding Machine Learning - From Theory to Algorithms", "abstract": "Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.", "year": 2014, "referenceCount": 202, "citationCount": 3460, "influentialCitationCount": 474, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1389955537", "name": "S. Shalev-Shwartz"}, {"authorId": "1401829700", "name": "Shai Ben-David"}]}, {"paperId": "d21703674ae562bae4a849a75847cdd9ead417df", "url": "https://www.semanticscholar.org/paper/d21703674ae562bae4a849a75847cdd9ead417df", "title": "Optimization Methods for Large-Scale Machine Learning", "abstract": "This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.", "year": 2016, "referenceCount": 186, "citationCount": 2022, "influentialCitationCount": 272, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "52184096", "name": "L. Bottou"}, {"authorId": "2198436", "name": "Frank E. Curtis"}, {"authorId": "2784955", "name": "J. Nocedal"}]}, {"paperId": "82266f6103bade9005ec555ed06ba20b5210ff22", "url": "https://www.semanticscholar.org/paper/82266f6103bade9005ec555ed06ba20b5210ff22", "title": "Gaussian Processes for Machine Learning", "abstract": null, "year": 2003, "referenceCount": 221, "citationCount": 19657, "influentialCitationCount": 3295, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3472959", "name": "C. Rasmussen"}, {"authorId": "145715698", "name": "Christopher K. I. Williams"}]}, {"paperId": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "url": "https://www.semanticscholar.org/paper/fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5", "title": "Neural Machine Translation by Jointly Learning to Align and Translate", "abstract": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.", "year": 2014, "referenceCount": 32, "citationCount": 21135, "influentialCitationCount": 2671, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3335364", "name": "Dzmitry Bahdanau"}, {"authorId": "1979489", "name": "Kyunghyun Cho"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}, {"paperId": "c38e83a9d0fe7005183f3c5bedf32a23df1b579b", "url": "https://www.semanticscholar.org/paper/c38e83a9d0fe7005183f3c5bedf32a23df1b579b", "title": "Federated Machine Learning: Concept and Applications", "abstract": "Today's AI still faces two major challenges. One is that in most industries, data exists in the form of isolated islands. The other is the strengthening of data privacy and security. We propose a possible solution to these challenges: secure federated learning. Beyond the federated learning framework first proposed by Google in 2016, we introduce a comprehensive secure federated learning framework, which includes horizontal federated learning, vertical federated learning and federated transfer learning. We provide definitions, architectures and applications for the federated learning framework, and provide a comprehensive survey of existing works on this subject. In addition, we propose building data networks among organizations based on federated mechanisms as an effective solution to allow knowledge to be shared without compromising user privacy.", "year": 2019, "referenceCount": 74, "citationCount": 1581, "influentialCitationCount": 146, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "153096457", "name": "Qiang Yang"}, {"authorId": "1614034792", "name": "Yang Liu"}, {"authorId": "11573257", "name": "Tianjian Chen"}, {"authorId": "8230559", "name": "Yongxin Tong"}]}, {"paperId": "aab43c9c33af00b718cf2ae374b861d49862a563", "url": "https://www.semanticscholar.org/paper/aab43c9c33af00b718cf2ae374b861d49862a563", "title": "Machine learning", "abstract": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.).", "year": 1996, "referenceCount": 8, "citationCount": 16293, "influentialCitationCount": 1389, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144299726", "name": "Thomas G. Dietterich"}]}, {"paperId": "807c1f19047f96083e13614f7ce20f2ac98c239a", "url": "https://www.semanticscholar.org/paper/807c1f19047f96083e13614f7ce20f2ac98c239a", "title": "C4.5: Programs for Machine Learning", "abstract": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses.", "year": 1992, "referenceCount": 0, "citationCount": 22149, "influentialCitationCount": 1279, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145341779", "name": "J. R. Quinlan"}]}, {"paperId": "597bd2e45427563cdf025e53a3239006aa364cfc", "url": "https://www.semanticscholar.org/paper/597bd2e45427563cdf025e53a3239006aa364cfc", "title": "Open Graph Benchmark: Datasets for Machine Learning on Graphs", "abstract": "We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .", "year": 2020, "referenceCount": 108, "citationCount": 895, "influentialCitationCount": 228, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "48594758", "name": "Weihua Hu"}, {"authorId": "3410500", "name": "Matthias Fey"}, {"authorId": "2095762", "name": "M. Zitnik"}, {"authorId": "2047998", "name": "Yuxiao Dong"}, {"authorId": "40046694", "name": "Hongyu Ren"}, {"authorId": "2156641189", "name": "Bowen Liu"}, {"authorId": "1754926", "name": "Michele Catasta"}, {"authorId": "1702139", "name": "J. Leskovec"}]}, {"paperId": "3f13a5148f7caa51ea946193d261d4f8ed32d81a", "url": "https://www.semanticscholar.org/paper/3f13a5148f7caa51ea946193d261d4f8ed32d81a", "title": "Machine Learning for Combinatorial Optimization: a Methodological Tour d'Horizon", "abstract": null, "year": 2018, "referenceCount": 85, "citationCount": 564, "influentialCitationCount": 35, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1751762", "name": "Yoshua Bengio"}, {"authorId": "144390922", "name": "Andrea Lodi"}, {"authorId": "51902590", "name": "Antoine Prouvost"}]}, {"paperId": "d422df8bff4e677a3077635db116679d25142bfc", "url": "https://www.semanticscholar.org/paper/d422df8bff4e677a3077635db116679d25142bfc", "title": "Machine learning: Trends, perspectives, and prospects", "abstract": "Machine learning addresses the question of how to build computers that improve automatically through experience. It is one of today\u2019s most rapidly growing technical fields, lying at the intersection of computer science and statistics, and at the core of artificial intelligence and data science. Recent progress in machine learning has been driven both by the development of new learning algorithms and theory and by the ongoing explosion in the availability of online data and low-cost computation. The adoption of data-intensive machine-learning methods can be found throughout science, technology and commerce, leading to more evidence-based decision-making across many walks of life, including health care, manufacturing, education, financial modeling, policing, and marketing.", "year": 2015, "referenceCount": 54, "citationCount": 3383, "influentialCitationCount": 68, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "1694621", "name": "Michael I. Jordan"}, {"authorId": "2066277988", "name": "T. Mitchell"}]}, {"paperId": "f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d", "url": "https://www.semanticscholar.org/paper/f0dcc9aa31dc9b31b836bcac1b140c8c94a2982d", "title": "Membership Inference Attacks Against Machine Learning Models", "abstract": "We quantitatively investigate how machine learning models leak information about the individual data records on which they were trained. We focus on the basic membership inference attack: given a data record and black-box access to a model, determine if the record was in the model's training dataset. To perform membership inference against a target model, we make adversarial use of machine learning and train our own inference model to recognize differences in the target model's predictions on the inputs that it trained on versus the inputs that it did not train on. We empirically evaluate our inference techniques on classification models trained by commercial \"machine learning as a service\" providers such as Google and Amazon. Using realistic datasets and classification tasks, including a hospital discharge dataset whose membership is sensitive from the privacy perspective, we show that these models can be vulnerable to membership inference attacks. We then investigate the factors that influence this leakage and evaluate mitigation strategies.", "year": 2016, "referenceCount": 40, "citationCount": 1977, "influentialCitationCount": 320, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2520493", "name": "R. Shokri"}, {"authorId": "34828439", "name": "Marco Stronati"}, {"authorId": "3469125", "name": "Congzheng Song"}, {"authorId": "1723945", "name": "Vitaly Shmatikov"}]}, {"paperId": "3bb5a439a0d610a7eac68f73068cdd278b8c9775", "url": "https://www.semanticscholar.org/paper/3bb5a439a0d610a7eac68f73068cdd278b8c9775", "title": "Pattern Recognition and Machine Learning", "abstract": "the selection of symmetric factorial designs, that is, a design where all factors have the same number of levels. Chapter 3 focuses on selection of two-level factorial designs and discusses complementary design theory and related topics in the selection of designs. Chapter 4 covers the selection of three level designs followed by the general case of s-levels. Chapter 5 discusses estimation capacity, presenting the connections with complementary designs followed by the estimation capacity for two-level and s-level designs. Chapter 6 discusses and presents results for the construction of mixed-level designs. Giving many examples of the use of mixed two and four-level designs. The final unit of the book discusses designs where there are two-different groups of factors. Chapters 7 and 8 discuss factorial designs with restricted randomization. Focusing first on blocked designs for full factorials as well as blocked fractional factorial designs. Chapter 8 focuses on split-plot designs. The booked is concluded with a chapter on robust parameter designs. This book covers a broad range of topics for regular factorial designs and presents all of the material in very mathematical fashion. However, the authors do a wonderful job of keeping the statistical methodology at the forefront of the book and the mathematical detail is presented as the necessary tool to study these designs. The book will serve as a great text for an advanced graduate level course in design theory for students with the necessary mathematical background. The book will surely become an invaluable resource for researchers and graduate students doing research in the design of factorial experiments. In addition, practitioners will also find the book useful for the comprehensive collection of optimal designs presented at the end of many chapters. Overall, this is a very well written book and a necessary addition to the existing literature on the design of factorial experiments.", "year": 2007, "referenceCount": 0, "citationCount": 22404, "influentialCitationCount": 3378, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1764325", "name": "Radford M. Neal"}]}, {"paperId": "fbc6562814e08e416e28a268ce7beeaa3d0708c8", "url": "https://www.semanticscholar.org/paper/fbc6562814e08e416e28a268ce7beeaa3d0708c8", "title": "Large-Scale Machine Learning with Stochastic Gradient Descent", "abstract": null, "year": 2010, "referenceCount": 25, "citationCount": 4810, "influentialCitationCount": 636, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "52184096", "name": "L. Bottou"}]}, {"paperId": "6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "url": "https://www.semanticscholar.org/paper/6bc4b1376ec2812b6d752c4f6bc8d8fd0512db91", "title": "Multimodal Machine Learning: A Survey and Taxonomy", "abstract": "Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.", "year": 2017, "referenceCount": 267, "citationCount": 1332, "influentialCitationCount": 97, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1756344", "name": "T. Baltru\u0161aitis"}, {"authorId": "118242121", "name": "Chaitanya Ahuja"}, {"authorId": "49933077", "name": "Louis-Philippe Morency"}]}, {"paperId": "2e62d1345b340d5fda3b092c460264b9543bc4b5", "url": "https://www.semanticscholar.org/paper/2e62d1345b340d5fda3b092c460264b9543bc4b5", "title": "Genetic Algorithms in Search Optimization and Machine Learning", "abstract": "From the Publisher: \nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n \nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required.", "year": 1988, "referenceCount": 1, "citationCount": 59058, "influentialCitationCount": 3517, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1715339", "name": "D. Goldberg"}]}, {"paperId": "f156ecbbb9243522275490d698c6825f4d2e01af", "url": "https://www.semanticscholar.org/paper/f156ecbbb9243522275490d698c6825f4d2e01af", "title": "Explainable AI: A Review of Machine Learning Interpretability Methods", "abstract": "Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into \u201cblack box\u201d approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.", "year": 2020, "referenceCount": 165, "citationCount": 405, "influentialCitationCount": 14, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1582428254", "name": "Pantelis Linardatos"}, {"authorId": "1753333150", "name": "Vasilis Papastefanopoulos"}, {"authorId": "1697867", "name": "S. Kotsiantis"}]}, {"paperId": "3a83d8595e6727269c876fcebd23ee9ddd524b76", "url": "https://www.semanticscholar.org/paper/3a83d8595e6727269c876fcebd23ee9ddd524b76", "title": "A Survey on Data Collection for Machine Learning: A Big Data - AI Integration Perspective", "abstract": "Data collection is a major bottleneck in machine learning and an active research topic in multiple communities. There are largely two reasons data collection has recently become a critical issue. First, as machine learning is becoming more widely-used, we are seeing new applications that do not necessarily have enough labeled data. Second, unlike traditional machine learning, deep learning techniques automatically generate features, which saves feature engineering costs, but in return may require larger amounts of labeled data. Interestingly, recent research in data collection comes not only from the machine learning, natural language, and computer vision communities, but also from the data management community due to the importance of handling large amounts of data. In this survey, we perform a comprehensive study of data collection from a data management point of view. Data collection largely consists of data acquisition, data labeling, and improvement of existing data or models. We provide a research landscape of these operations, provide guidelines on which technique to use when, and identify interesting research challenges. The integration of machine learning and data management for data collection is part of a larger trend of Big data and Artificial Intelligence (AI) integration and opens many opportunities for new research.", "year": 2018, "referenceCount": 186, "citationCount": 309, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "30840932", "name": "Yuji Roh"}, {"authorId": "2539412", "name": "Geon Heo"}, {"authorId": "3288247", "name": "Steven Euijong Whang"}]}, {"paperId": "9c1b9940e301b51c31775cca86230844679384c8", "url": "https://www.semanticscholar.org/paper/9c1b9940e301b51c31775cca86230844679384c8", "title": "Physics-informed machine learning", "abstract": null, "year": 2021, "referenceCount": 193, "citationCount": 307, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1720124", "name": "G. Karniadakis"}, {"authorId": "3439407", "name": "I. Kevrekidis"}, {"authorId": "2149373363", "name": "Lu Lu"}, {"authorId": "3410970", "name": "P. Perdikaris"}, {"authorId": "118188801", "name": "Sifan Wang"}, {"authorId": "2145494483", "name": "Liu Yang"}]}, {"paperId": "53c9f3c34d8481adaf24df3b25581ccf1bc53f5c", "url": "https://www.semanticscholar.org/paper/53c9f3c34d8481adaf24df3b25581ccf1bc53f5c", "title": "Physics-Informed Machine Learning", "abstract": "Traditional lithium-ion (Li-ion) battery state of health (SOH) estimation methodologies that focused on estimating present cell capacity do not provide sufficient information to determine the cell\u2019s lifecycle stage or value in second-life use. Quantifying the underlying degradation modes that cause capacity fade can give further insight into the electrochemical state of the cell and provide more detailed health information such as the remaining active materials and lithium inventory. However, current physics-based methods for degradation diagnostics require long-term cycling data and are computationally expensive to deploy locally on a device. To improve upon current methods, we propose and extensively test two light-weight physics-informed machine learning methods for online estimating the capacity of a battery cell and diagnosing its primary degradation modes using only limited early-life experimental degradation data. To enable late-life prediction (e.g. > 1.5 years) without the use of late-life experimental data, each of the methods is trained using simulation data from a physics-based half-cell model and early-life (e.g. < 3 months) degradation data obtained from cycling tests. The proposed methods are comprehensively evaluated using data from a long-term (3.5 years) cycling experiment of 16 implantable-grade Li-ion cells cycled under two temperatures and C-rates. Results from a four-fold cross-validation study show that the proposed physics-informed machine learning models are capable of improving the estimation accuracy of cell capacity and the state of three primary degradation modes by over 50% compared to a purely data-driven approach. Additionally, this work provides insights into the role of temperature and C-rate in cell degradation. loss in lithiated active material in one of the electrodes (LAM PE/NE,li ) and 10% loss of lithium due to the solid electrolyte interface (SEI) growth (pure LLI), the total loss of lithium inventory quantified by the LII parameter is 20%, and the total LAM PE/NE is 10%. The LAM parameter used throughout this study is used to quantify delithiated LAM, whereas lithiated LAM can be quantified by a linear combination of both LAM and LII. Practically, the increase of LLI results from the accumulation of parasitic reactions in the cell that contribute to lithium inventory loss (e.g., SEI growth, electrolyte decomposition, and delamination of lithiated electrode materials).", "year": 2021, "referenceCount": 206, "citationCount": 348, "influentialCitationCount": 23, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "1403016350", "name": "Niklas Wahlstr\u00f6m"}, {"authorId": "47846099", "name": "A. Wills"}, {"authorId": "37792275", "name": "J. Hendriks"}, {"authorId": "153651688", "name": "Alexander Gregg"}, {"authorId": "14112150", "name": "C. Wensrich"}, {"authorId": "145410662", "name": "A. Solin"}, {"authorId": "30443320", "name": "S. S\u00e4rkk\u00e4"}]}, {"paperId": "b631ba962b4403a9c0fd9cce446ef3b1e21ea059", "url": "https://www.semanticscholar.org/paper/b631ba962b4403a9c0fd9cce446ef3b1e21ea059", "title": "Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods", "abstract": null, "year": 2019, "referenceCount": 161, "citationCount": 318, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1691955", "name": "E. H\u00fcllermeier"}, {"authorId": "3249834", "name": "W. Waegeman"}]}, {"paperId": "7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea", "url": "https://www.semanticscholar.org/paper/7872f34e2a164c5cf3c34a7a7433dc3342b6c7ea", "title": "Machine Learning: Algorithms, Real-World Applications and Research Directions", "abstract": null, "year": 2021, "referenceCount": 147, "citationCount": 316, "influentialCitationCount": 15, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "3456687", "name": "I. Sarker"}]}, {"paperId": "2b49156cf855dbb39768ae0ba7d7cb9263d17e5c", "url": "https://www.semanticscholar.org/paper/2b49156cf855dbb39768ae0ba7d7cb9263d17e5c", "title": "Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems", "abstract": "Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process, which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. First, we provide a definition and propose a concept for informed machine learning, which illustrates its building blocks and distinguishes it from conventional machine learning. Second, we introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Third, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.", "year": 2019, "referenceCount": 196, "citationCount": 172, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "7440202", "name": "Laura von Rueden"}, {"authorId": "145001529", "name": "S. Mayer"}, {"authorId": "1583348396", "name": "Katharina Beckh"}, {"authorId": "88714212", "name": "B. Georgiev"}, {"authorId": "51065069", "name": "Sven Giesselbach"}, {"authorId": "72453217", "name": "R. Heese"}, {"authorId": "36891596", "name": "Birgit Kirsch"}, {"authorId": "2046830943", "name": "Julius Pfrommer"}, {"authorId": "1380036932", "name": "Annika Pick"}, {"authorId": "21780262", "name": "R. Ramamurthy"}, {"authorId": "51879558", "name": "M. Walczak"}, {"authorId": "2279864", "name": "J. Garcke"}, {"authorId": "1692283", "name": "C. Bauckhage"}, {"authorId": "4488139", "name": "Jannis Schuecker"}]}, {"paperId": "330b5844d170b6b77f5f9fa4c2024150cef2af18", "url": "https://www.semanticscholar.org/paper/330b5844d170b6b77f5f9fa4c2024150cef2af18", "title": "Benchmark and Survey of Automated Machine Learning Frameworks", "abstract": "Machine learning (ML) has become a vital part in many aspects of our daily life. However, building well performing machine learning applications requires highly specialized data scientists and domain experts. Automated machine learning (AutoML) aims to reduce the demand for data scientists by enabling domain experts to automatically build machine learning applications without extensive knowledge of statistics and machine learning. This paper is a combination of a survey on current AutoML methods and a benchmark of popular AutoML frameworks on real data sets. Driven by the selected frameworks for evaluation, we summarize and review important AutoML techniques and methods concerning every step in building an ML pipeline. The selected AutoML frameworks are evaluated on 137 different data sets.", "year": 2019, "referenceCount": 214, "citationCount": 153, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "114984566", "name": "Marc-Andr\u00e9 Z\u00f6ller"}, {"authorId": "1959068", "name": "Marco F. Huber"}]}, {"paperId": "57e6cca1479a4642f867e69b4dee93d14259dc3d", "url": "https://www.semanticscholar.org/paper/57e6cca1479a4642f867e69b4dee93d14259dc3d", "title": "Power of data in quantum machine learning", "abstract": null, "year": 2020, "referenceCount": 64, "citationCount": 191, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Computer Science", "Medicine"], "authors": [{"authorId": "2343654", "name": "Hsin-Yuan Huang"}, {"authorId": "96014865", "name": "M. Broughton"}, {"authorId": "145233982", "name": "M. Mohseni"}, {"authorId": "47066334", "name": "R. Babbush"}, {"authorId": "48703443", "name": "S. Boixo"}, {"authorId": "2665814", "name": "H. Neven"}, {"authorId": "1933508", "name": "J. McClean"}]}, {"paperId": "5cae4598b1de79fb5a26d92fcb0435bf2a20691e", "url": "https://www.semanticscholar.org/paper/5cae4598b1de79fb5a26d92fcb0435bf2a20691e", "title": "Machine Learning Force Fields", "abstract": "In recent years, the use of machine learning (ML) in computational chemistry has enabled numerous advances previously out of reach due to the computational complexity of traditional electronic-structure methods. One of the most promising applications is the construction of ML-based force fields (FFs), with the aim to narrow the gap between the accuracy of ab initio methods and the efficiency of classical FFs. The key idea is to learn the statistical relation between chemical structure and potential energy without relying on a preconceived notion of fixed chemical bonds or knowledge about the relevant interactions. Such universal ML approximations are in principle only limited by the quality and quantity of the reference data used to train them. This review gives an overview of applications of ML-FFs and the chemical insights that can be obtained from them. The core concepts underlying ML-FFs are described in detail, and a step-by-step guide for constructing and testing them from scratch is given. The text concludes with a discussion of the challenges that remain to be overcome by the next generation of ML-FFs.", "year": 2020, "referenceCount": 333, "citationCount": 196, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Physics", "Mathematics", "Computer Science"], "authors": [{"authorId": "9914431", "name": "Oliver T. Unke"}, {"authorId": "7631063", "name": "Stefan Chmiela"}, {"authorId": "10667063", "name": "H. E. Sauceda"}, {"authorId": "5742764", "name": "M. Gastegger"}, {"authorId": "5667638", "name": "I. Poltavsky"}, {"authorId": "33075217", "name": "Kristof T. Sch\u00fctt"}, {"authorId": "2462983", "name": "A. Tkatchenko"}, {"authorId": "145034054", "name": "K. M\u00fcller"}]}, {"paperId": "80d9f0eb47b712988d19cbe29a7bfa63f2a175d0", "url": "https://www.semanticscholar.org/paper/80d9f0eb47b712988d19cbe29a7bfa63f2a175d0", "title": "A guide to machine learning for biologists", "abstract": null, "year": 2021, "referenceCount": 169, "citationCount": 127, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "36526409", "name": "Joe G. Greener"}, {"authorId": "2262107", "name": "S. Kandathil"}, {"authorId": "50881145", "name": "Lewis Moffat"}, {"authorId": "2118729790", "name": "David T Jones"}]}, {"paperId": "b09ad6ef6e63ddf2e1d07faa783adecce0cc2a77", "url": "https://www.semanticscholar.org/paper/b09ad6ef6e63ddf2e1d07faa783adecce0cc2a77", "title": "Coronavirus disease (COVID-19) cases analysis using machine-learning applications", "abstract": null, "year": 2021, "referenceCount": 40, "citationCount": 177, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "2100113493", "name": "Ameer Sardar Kwekha-Rashid"}, {"authorId": "66680362", "name": "H. N. Abduljabbar"}, {"authorId": "98174258", "name": "Bilal S. A. Alhayani"}]}, {"paperId": "0b544dfe355a5070b60986319a3f51fb45d1348e", "url": "https://www.semanticscholar.org/paper/0b544dfe355a5070b60986319a3f51fb45d1348e", "title": "Learning Phrase Representations using RNN Encoder\u2013Decoder for Statistical Machine Translation", "abstract": "In this paper, we propose a novel neural network model called RNN Encoder\u2010 Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder\u2010Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.", "year": 2014, "referenceCount": 33, "citationCount": 16721, "influentialCitationCount": 3042, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1979489", "name": "Kyunghyun Cho"}, {"authorId": "3158246", "name": "Bart van Merrienboer"}, {"authorId": "1854385", "name": "\u00c7aglar G\u00fcl\u00e7ehre"}, {"authorId": "3335364", "name": "Dzmitry Bahdanau"}, {"authorId": "2076086", "name": "Fethi Bougares"}, {"authorId": "144518416", "name": "Holger Schwenk"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}, {"paperId": "07ae300bb15df0fc5a2c5140bcae5773a946a296", "url": "https://www.semanticscholar.org/paper/07ae300bb15df0fc5a2c5140bcae5773a946a296", "title": "Machine learning and deep learning", "abstract": null, "year": 2021, "referenceCount": 60, "citationCount": 115, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3012649", "name": "Christian Janiesch"}, {"authorId": "2148733", "name": "Patrick Zschech"}, {"authorId": "30004936", "name": "K. Heinrich"}]}, {"paperId": "6cf98b123feac6504b0dc3a8b46e1462dd69121e", "url": "https://www.semanticscholar.org/paper/6cf98b123feac6504b0dc3a8b46e1462dd69121e", "title": "Data mining: practical machine learning tools and techniques, 3rd Edition", "abstract": "Data Mining: Practical Machine Learning Tools and Techniques offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. *Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projects *Offers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methods *Includes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks-in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization", "year": 1999, "referenceCount": 171, "citationCount": 12272, "influentialCitationCount": 562, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9419406", "name": "I. Witten"}, {"authorId": "143713826", "name": "Eibe Frank"}, {"authorId": "118860642", "name": "M. Hall"}]}, {"paperId": "776b5c7b2966fd700980361aa32ee573edb70bf3", "url": "https://www.semanticscholar.org/paper/776b5c7b2966fd700980361aa32ee573edb70bf3", "title": "Quantum Machine Learning", "abstract": "Machine learning techniques are applied for solving a large variety of practical problems. The tasks attacked by machine learning algorithms include classification, regression, pattern recognition, etc. Traditionally, machine learning algorithms are divided into two groups depending on the nature of training data: supervised and unsupervised. Supervised machine learning algorithms take on the input labeled samples and learn how to predict labels for new data points. Unsupervised learning algorithms like the ones for clustering, pattern recognition, on the other hand, take on the input unlabeled data and try to discover the hidden structure in the data. Algorithms of machine learning manipulate large amounts of data represented in a form of arrays of vectors of high dimension. These algorithms become practically intractable for classical computers as the amount of data grows rapidly [4], whereas quantum computing potentially provides an exponential speed up [11]. So far, a large number of quantum machine learning algorithm has been developed, for example, the algorithm for solving systems of linear equations HHL [10] is used for generic classification problems as perceptron or linear regression training, there were developed quantum algorithms for nearest centroid, k-nearest neighbours and support vector machines classsification [2]. In this review we will focus on the supervised classification quantum algorithm of nearest centroid, presented in [11]. Quantum computational model helps to overcome the main bottleneck of the algorithm: calculation of the distances between the vectors in highly dimensional space. Methods of computing distances similar to the one presented in the context of nearest centroid algorithm can be used in many other practical applications, and the first experimental results on implementation of this algorithm on a small-scale quantum photonic computer can be found in [4].", "year": 2020, "referenceCount": 12, "citationCount": 878, "influentialCitationCount": 38, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "25106284", "name": "Ievgeniia Oshurko"}]}, {"paperId": "6d873542f9722a520a9a6602378d6344907b1183", "url": "https://www.semanticscholar.org/paper/6d873542f9722a520a9a6602378d6344907b1183", "title": "DOME: recommendations for supervised machine learning validation in biology.", "abstract": null, "year": 2020, "referenceCount": 44, "citationCount": 50, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "144218611", "name": "Ian Walsh"}, {"authorId": "39084373", "name": "D. Fishman"}, {"authorId": "1389953200", "name": "D. Garc\u00eda-Gasulla"}, {"authorId": "6544704", "name": "T. Titma"}, {"authorId": "2028880", "name": "G. Pollastri"}, {"authorId": "1988753275", "name": "The Elixir Machine Learning focus group"}, {"authorId": "3119529", "name": "J. Harrow"}, {"authorId": "71018930", "name": "Fotis Psomopoulos"}, {"authorId": "1801645", "name": "S. Tosatto"}]}, {"paperId": "12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "url": "https://www.semanticscholar.org/paper/12d0353ce8b41b7e5409e5a4a611110aee33c7bc", "title": "Thumbs up? Sentiment Classification using Machine Learning Techniques", "abstract": "We consider the problem of classifying documents not by topic, but by overall sentiment, e.g., determining whether a review is positive or negative. Using movie reviews as data, we find that standard machine learning techniques definitively outperform human-produced baselines. However, the three machine learning methods we employed (Naive Bayes, maximum entropy classification, and support vector machines) do not perform as well on sentiment classification as on traditional topic-based categorization. We conclude by examining factors that make the sentiment classification problem more challenging.", "year": 2002, "referenceCount": 39, "citationCount": 8759, "influentialCitationCount": 810, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144865353", "name": "B. Pang"}, {"authorId": "145810617", "name": "Lillian Lee"}, {"authorId": "2066721", "name": "Shivakumar Vaithyanathan"}]}, {"paperId": "6b20af22b0734757d9ead382b201a65f9dd637cc", "url": "https://www.semanticscholar.org/paper/6b20af22b0734757d9ead382b201a65f9dd637cc", "title": "Machine learning in automated text categorization", "abstract": "The automated categorization (or classification) of texts into predefined categories has witnessed a booming interest in the last 10 years, due to the increased availability of documents in digital form and the ensuing need to organize them. In the research community the dominant approach to this problem is based on machine learning techniques: a general inductive process automatically builds a classifier by learning, from a set of preclassified documents, the characteristics of the categories. The advantages of this approach over the knowledge engineering approach (consisting in the manual definition of a classifier by domain experts) are a very good effectiveness, considerable savings in terms of expert labor power, and straightforward portability to different domains. This survey discusses the main approaches to text categorization that fall within the machine learning paradigm. We will discuss in detail issues pertaining to three different problems, namely, document representation, classifier construction, and classifier evaluation.", "year": 2001, "referenceCount": 190, "citationCount": 8624, "influentialCitationCount": 752, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145077269", "name": "F. Sebastiani"}]}, {"paperId": "932a106c21a1db1e1876459c1521d27fd152caac", "url": "https://www.semanticscholar.org/paper/932a106c21a1db1e1876459c1521d27fd152caac", "title": "Pattern Recognition and Machine Learning (Information Science and Statistics)", "abstract": "Looking for competent reading resources? We have pattern recognition and machine learning information science and statistics to read, not only read, but also download them or even check out online. Locate this fantastic book writtern by by now, simply here, yeah just here. Obtain the reports in the kinds of txt, zip, kindle, word, ppt, pdf, as well as rar. Once again, never ever miss to review online and download this book in our site right here. Click the link.", "year": 2006, "referenceCount": 0, "citationCount": 8761, "influentialCitationCount": 894, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2113861474", "name": "C. Bishop"}]}, {"paperId": "668b1277fbece28c4841eeab1c97e4ebd0079700", "url": "https://www.semanticscholar.org/paper/668b1277fbece28c4841eeab1c97e4ebd0079700", "title": "Pattern Recognition and Machine Learning", "abstract": "Probability Distributions.- Linear Models for Regression.- Linear Models for Classification.- Neural Networks.- Kernel Methods.- Sparse Kernel Machines.- Graphical Models.- Mixture Models and EM.- Approximate Inference.- Sampling Methods.- Continuous Latent Variables.- Sequential Data.- Combining Models.", "year": 2006, "referenceCount": 368, "citationCount": 10183, "influentialCitationCount": 562, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2113861474", "name": "C. Bishop"}, {"authorId": "8147588", "name": "N. Nasrabadi"}]}, {"paperId": "e068be31ded63600aea068eacd12931efd2a1029", "url": "https://www.semanticscholar.org/paper/e068be31ded63600aea068eacd12931efd2a1029", "title": "UCI Repository of machine learning databases", "abstract": null, "year": 1998, "referenceCount": 0, "citationCount": 13704, "influentialCitationCount": 828, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145298005", "name": "Catherine Blake"}]}, {"paperId": "4087e84fc695bb6433d0104ee94f9d7e9f4b7da5", "url": "https://www.semanticscholar.org/paper/4087e84fc695bb6433d0104ee94f9d7e9f4b7da5", "title": "Machine Learning for Fluid Mechanics", "abstract": "The field of fluid mechanics is rapidly advancing, driven by unprecedented volumes of data from experiments, field measurements, and large-scale simulations at multiple spatiotemporal scales. Machine learning (ML) offers a wealth of techniques to extract information from data that can be translated into knowledge about the underlying fluid mechanics. Moreover, ML algorithms can augment domain knowledge and automate tasks related to flow control and optimization. This article presents an overview of past history, current developments, and emerging opportunities of ML for fluid mechanics. We outline fundamental ML methodologies and discuss their uses for understanding, modeling, optimizing, and controlling fluid flows. The strengths and limitations of these methods are addressed from the perspective of scientific inquiry that considers data as an inherent part of modeling, experiments, and simulations. ML provides a powerful information-processing framework that can augment, and possibly even transform, current lines of fluid mechanics research and industrial applications.", "year": 2019, "referenceCount": 152, "citationCount": 887, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics", "Mathematics"], "authors": [{"authorId": "3083169", "name": "S. Brunton"}, {"authorId": "144875384", "name": "B. R. Noack"}, {"authorId": "1802604", "name": "P. Koumoutsakos"}]}, {"paperId": "5e934c4d76b3c664149f7970ec11c3c8cabab91e", "url": "https://www.semanticscholar.org/paper/5e934c4d76b3c664149f7970ec11c3c8cabab91e", "title": "Machine Learning with R", "abstract": "Learn how to use R to apply powerful machine learning methods and gain an insight into real-world applications Overview Harness the power of R for statistical computing and data science Use R to apply common machine learning algorithms with real-world applications Prepare, examine, and visualize data for analysis Understand how to choose between machine learning models Packed with clear instructions to explore, forecast, and classify data In Detail Machine learning, at its core, is concerned with transforming data into actionable knowledge. This fact makes machine learning well-suited to the present-day era of \"big data\" and \"data science\". Given the growing prominence of Ra cross-platform, zero-cost statistical programming environmentthere has never been a better time to start applying machine learning. Whether you are new to data science or a veteran, machine learning with R offers a powerful set of methods for quickly and easily gaining insight from your data. \"Machine Learning with R\" is a practical tutorial that uses hands-on examples to step through real-world application of machine learning. Without shying away from the technical details, we will explore Machine Learning with R using clear and practical examples. Well-suited to machine learning beginners or those with experience. Explore R to find the answer to all of your questions. How can we use machine learning to transform data into action? Using practical examples, we will explore how to prepare data for analysis, choose a machine learning method, and measure the success of the process. We will learn how to apply machine learning methods to a variety of common tasks including classification, prediction, forecasting, market basket analysis, and clustering. By applying the most effective machine learning methods to real-world problems, you will gain hands-on experience that will transform the way you think about data. \"Machine Learning with R\" will provide you with the analytical tools you need to quickly gain insight from complex data. What you will learn from this book Understand the basic terminology of machine learning and how to differentiate among various machine learning approaches Use R to prepare data for machine learning Explore and visualize data with R Classify data using nearest neighbor methods Learn about Bayesian methods for classifying data Predict values using decision trees, rules, and support vector machines Forecast numeric values using linear regression Model data using neural networks Find patterns in data using association rules for market basket analysis Group data into clusters for segmentation Evaluate and improve the performance of machine learning models Learn specialized machine learning techniques for text mining, social network data, and big data Approach Written as a tutorial to explore and understand the power of R for machine learning. This practical guide that covers all of the need to know topics in a very systematic way. For each machine learning approach, each step in the process is detailed, from preparing the data for analysis to evaluating the results. These steps will build the knowledge you need to apply them to your own data science tasks.", "year": 2013, "referenceCount": 0, "citationCount": 203, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2074662926", "name": "Brett Lantz"}]}, {"paperId": "2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c", "url": "https://www.semanticscholar.org/paper/2936cbd6a90d7153a9fa34e8e4fd947907fe7f6c", "title": "Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems", "abstract": "Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks-scikit-learn and TensorFlow-author Aurelien Geron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started. Explore the machine learning landscape, particularly neural nets Use scikit-learn to track an example machine-learning project end-to-end Explore several training models, including support vector machines, decision trees, random forests, and ensemble methods Use the TensorFlow library to build and train neural nets Dive into neural net architectures, including convolutional nets, recurrent nets, and deep reinforcement learning Learn techniques for training and scaling deep neural nets Apply practical code examples without acquiring excessive machine learning theory or algorithm details", "year": 2017, "referenceCount": 0, "citationCount": 1766, "influentialCitationCount": 259, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2078716805", "name": "Aur\u00e9lien G\u00e9ron"}]}, {"paperId": "b0c34618ffd1154f35863e2ce7250ac6b6f2c424", "url": "https://www.semanticscholar.org/paper/b0c34618ffd1154f35863e2ce7250ac6b6f2c424", "title": "Interpretable Machine Learning", "abstract": "Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.", "year": 2019, "referenceCount": 167, "citationCount": 1124, "influentialCitationCount": 97, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "144641487", "name": "Tyler Martin"}]}, {"paperId": "5d433da6d0f143f20936379910104d2bb139d4ae", "url": "https://www.semanticscholar.org/paper/5d433da6d0f143f20936379910104d2bb139d4ae", "title": "ilastik: interactive machine learning for (bio)image analysis", "abstract": null, "year": 2019, "referenceCount": 42, "citationCount": 963, "influentialCitationCount": 51, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "48275624", "name": "Stuart E. Berg"}, {"authorId": "145708428", "name": "D. Kutra"}, {"authorId": "2066522012", "name": "Thorben Kroeger"}, {"authorId": "145486652", "name": "C. Straehle"}, {"authorId": "2834950", "name": "Bernhard X. Kausler"}, {"authorId": "2057581", "name": "C. Haubold"}, {"authorId": "1725192", "name": "Martin Schiegg"}, {"authorId": "144560453", "name": "J. Ale\u0161"}, {"authorId": "46478540", "name": "T. Beier"}, {"authorId": "1391661732", "name": "Markus Rudy"}, {"authorId": "47829712", "name": "Kemal Eren"}, {"authorId": "2062532551", "name": "Jaime I Cervantes"}, {"authorId": "1968352", "name": "Buote Xu"}, {"authorId": "1397152727", "name": "Fynn Beuttenmueller"}, {"authorId": "67091521", "name": "A. Wolny"}, {"authorId": "2111387698", "name": "Chong Zhang"}, {"authorId": "1708103", "name": "U. K\u00f6the"}, {"authorId": "1685187", "name": "F. Hamprecht"}, {"authorId": "3190177", "name": "A. Kreshuk"}]}, {"paperId": "f86f1748d1b6d22870f4347fd5d65314ba800583", "url": "https://www.semanticscholar.org/paper/f86f1748d1b6d22870f4347fd5d65314ba800583", "title": "Reconciling modern machine-learning practice and the classical bias\u2013variance trade-off", "abstract": "Significance While breakthroughs in machine learning and artificial intelligence are changing society, our fundamental understanding has lagged behind. It is traditionally believed that fitting models to the training data exactly is to be avoided as it leads to poor performance on unseen data. However, powerful modern classifiers frequently have near-perfect fit in training, a disconnect that spurred recent intensive research and controversy on whether theory provides practical insights. In this work, we show how classical theory and modern practice can be reconciled within a single unified performance curve and propose a mechanism underlying its emergence. We believe this previously unknown pattern connecting the structure and performance of learning architectures will help shape design and understanding of learning algorithms. Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias\u2013variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias\u2013variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This \u201cdouble-descent\u201d curve subsumes the textbook U-shaped bias\u2013variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.", "year": 2018, "referenceCount": 45, "citationCount": 854, "influentialCitationCount": 95, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "authors": [{"authorId": "145520115", "name": "Mikhail Belkin"}, {"authorId": "143724861", "name": "Daniel J. Hsu"}, {"authorId": "143791100", "name": "Siyuan Ma"}, {"authorId": "151213231", "name": "Soumik Mandal"}]}, {"paperId": "3df952d4a724655f7520ff95d4b2cef90fff0cae", "url": "https://www.semanticscholar.org/paper/3df952d4a724655f7520ff95d4b2cef90fff0cae", "title": "Techniques for interpretable machine learning", "abstract": "Uncovering the mysterious ways machine learning models make decisions.", "year": 2018, "referenceCount": 51, "citationCount": 517, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3432460", "name": "Mengnan Du"}, {"authorId": "47717322", "name": "Ninghao Liu"}, {"authorId": "48539382", "name": "Xia Hu"}]}, {"paperId": "1b0f4bd3872bb590d457990ac2b26b29f770fc44", "url": "https://www.semanticscholar.org/paper/1b0f4bd3872bb590d457990ac2b26b29f770fc44", "title": "Explainable machine learning in deployment", "abstract": "Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.", "year": 2019, "referenceCount": 128, "citationCount": 291, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "32326200", "name": "Umang Bhatt"}, {"authorId": "4990825", "name": "Alice Xiang"}, {"authorId": "2034354321", "name": "Shubham Sharma"}, {"authorId": "145689461", "name": "Adrian Weller"}, {"authorId": "40511120", "name": "Ankur Taly"}, {"authorId": "12488214", "name": "Yunhan Jia"}, {"authorId": "34724702", "name": "Joydeep Ghosh"}, {"authorId": "1718933", "name": "R. Puri"}, {"authorId": "51283515", "name": "J. Moura"}, {"authorId": "2654106", "name": "P. Eckersley"}]}, {"paperId": "b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0", "url": "https://www.semanticscholar.org/paper/b674a7aee72e9b9cc5390eca13f9c5c7812f2ba0", "title": "Machine learning for molecular simulation", "abstract": "Machine learning (ML) is transforming all areas of science. The complex and time-consuming calculations in molecular simulations are particularly suitable for an ML revolution and have already been profoundly affected by the application of existing ML methods. Here we review recent ML methods for molecular simulation, with particular focus on (deep) neural networks for the prediction of quantum-mechanical energies and forces, on coarse-grained molecular dynamics, on the extraction of free energy surfaces and kinetics, and on generative network approaches to sample molecular equilibrium structures and compute thermodynamics. To explain these methods and illustrate open methodological problems, we review some important principles of molecular physics and describe how they can be incorporated into ML structures. Finally, we identify and describe a list of open challenges for the interface between ML and molecular simulation. Expected final online publication date for the Annual Review of Physical Chemistry, Volume 71 is April 20, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.", "year": 2019, "referenceCount": 147, "citationCount": 273, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science", "Physics", "Mathematics"], "authors": [{"authorId": "1967674", "name": "F. No\u00e9"}, {"authorId": "2462983", "name": "A. Tkatchenko"}, {"authorId": "145034054", "name": "K. M\u00fcller"}, {"authorId": "2839696", "name": "C. Clementi"}]}, {"paperId": "4eca52f892f288c0b33b74aa4cfed56ed968fb4e", "url": "https://www.semanticscholar.org/paper/4eca52f892f288c0b33b74aa4cfed56ed968fb4e", "title": "Explainable Machine Learning for Scientific Insights and Discoveries", "abstract": "Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.", "year": 2019, "referenceCount": 140, "citationCount": 278, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "46525320", "name": "R. Roscher"}, {"authorId": "34832887", "name": "B. Bohn"}, {"authorId": "1888626", "name": "Marco F. Duarte"}, {"authorId": "2279864", "name": "J. Garcke"}]}, {"paperId": "f9a855ae59579d16dca6a5133cd8daddd3305582", "url": "https://www.semanticscholar.org/paper/f9a855ae59579d16dca6a5133cd8daddd3305582", "title": "A Survey on Distributed Machine Learning", "abstract": "The demand for artificial intelligence has grown significantly over the past decade, and this growth has been fueled by advances in machine learning techniques and the ability to leverage hardware acceleration. However, to increase the quality of predictions and render machine learning solutions feasible for more complex applications, a substantial amount of training data is required. Although small machine learning models can be trained with modest amounts of data, the input for training larger models such as neural networks grows exponentially with the number of parameters. Since the demand for processing training data has outpaced the increase in computation power of computing machinery, there is a need for distributing the machine learning workload across multiple machines, and turning the centralized into a distributed system. These distributed systems present new challenges: first and foremost, the efficient parallelization of the training process and the creation of a coherent model. This article provides an extensive overview of the current state-of-the-art in the field by outlining the challenges and opportunities of distributed machine learning over conventional (centralized) machine learning, discussing the techniques used for distributed machine learning, and providing an overview of the systems that are available.", "year": 2019, "referenceCount": 178, "citationCount": 216, "influentialCitationCount": 15, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1470488636", "name": "Joost Verbraeken"}, {"authorId": "1470488392", "name": "Matthijs Wolting"}, {"authorId": "2065360808", "name": "Jonathan Katzy"}, {"authorId": "1470485471", "name": "Jeroen Kloppenburg"}, {"authorId": "2413244", "name": "Tim Verbelen"}, {"authorId": "2342602", "name": "Jan S. Rellermeyer"}]}, {"paperId": "6068d39e92aef1bb0e1291e9931894c35692a85e", "url": "https://www.semanticscholar.org/paper/6068d39e92aef1bb0e1291e9931894c35692a85e", "title": "Counterfactual Explanations for Machine Learning: A Review", "abstract": "Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.", "year": 2020, "referenceCount": 102, "citationCount": 215, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1780214785", "name": "Sahil Verma"}, {"authorId": "1718974", "name": "John P. Dickerson"}, {"authorId": "4634403", "name": "Keegan E. Hines"}]}, {"paperId": "3119ea9c7ad7a5e044dc7c267329a4bbf00d0158", "url": "https://www.semanticscholar.org/paper/3119ea9c7ad7a5e044dc7c267329a4bbf00d0158", "title": "A Survey of Optimization Methods From a Machine Learning Perspective", "abstract": "Machine learning develops rapidly, which has made many theoretical breakthroughs and is widely applied in various fields. Optimization, as an important part of machine learning, has attracted much attention of researchers. With the exponential growth of data amount and the increase of model complexity, optimization methods in machine learning face more and more challenges. A lot of work on solving optimization problems or improving optimization methods in machine learning has been proposed successively. The systematic retrospect and summary of the optimization methods from the perspective of machine learning are of great significance, which can offer guidance for both developments of optimization and machine learning research. In this article, we first describe the optimization problems in machine learning. Then, we introduce the principles and progresses of commonly used optimization methods. Finally, we explore and give some challenges and open problems for the optimization in machine learning.", "year": 2019, "referenceCount": 227, "citationCount": 194, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"], "authors": [{"authorId": "20632291", "name": "Shiliang Sun"}, {"authorId": "12076996", "name": "Zehui Cao"}, {"authorId": "2122172099", "name": "Han Zhu"}, {"authorId": "46509200", "name": "Jing Zhao"}]}, {"paperId": "09e85ad84c4ed40461340ac1bd5fadbd2a5b2340", "url": "https://www.semanticscholar.org/paper/09e85ad84c4ed40461340ac1bd5fadbd2a5b2340", "title": "Integrating Physics-Based Modeling with Machine Learning: A Survey", "abstract": "In this manuscript, we provide a structured and comprehensive overview of techniques to integrate machine learning with physics-based modeling. First, we provide a summary of application areas for which these approaches have been applied. Then, we describe classes of methodologies used to construct physics-guided machine learning models and hybrid physics-machine learning frameworks from a machine learning standpoint. With this foundation, we then provide a systematic organization of these existing techniques and discuss ideas for future research.", "year": 2020, "referenceCount": 325, "citationCount": 190, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51439518", "name": "J. Willard"}, {"authorId": "38139853", "name": "X. Jia"}, {"authorId": "4632515", "name": "Shaoming Xu"}, {"authorId": "1707756", "name": "M. Steinbach"}, {"authorId": "2107978833", "name": "Vipin Kumar"}]}, {"paperId": "fee8f63972906214b77f16cfeca0b93ee8f36ba2", "url": "https://www.semanticscholar.org/paper/fee8f63972906214b77f16cfeca0b93ee8f36ba2", "title": "Fairness in Machine Learning: A Survey", "abstract": "As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.", "year": 2020, "referenceCount": 263, "citationCount": 169, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2452441", "name": "Simon Caton"}, {"authorId": "152864672", "name": "C. Haas"}]}, {"paperId": "7feb0fc888cd55360949554db032d7d1cba9e947", "url": "https://www.semanticscholar.org/paper/7feb0fc888cd55360949554db032d7d1cba9e947", "title": "Programs for Machine Learning", "abstract": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students.", "year": 1994, "referenceCount": 3, "citationCount": 8002, "influentialCitationCount": 436, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1744109", "name": "S. Salzberg"}, {"authorId": "1800681", "name": "Alberto Maria Segre"}]}, {"paperId": "f70b2f20be241f445a61f33c4b8e76e554760340", "url": "https://www.semanticscholar.org/paper/f70b2f20be241f445a61f33c4b8e76e554760340", "title": "Software Engineering for Machine Learning: A Case Study", "abstract": "Recent advances in machine learning have stimulated widespread interest within the Information Technology sector on integrating AI capabilities into software and services. This goal has forced organizations to evolve their development processes. We report on a study that we conducted on observing software teams at Microsoft as they develop AI-based applications. We consider a nine-stage workflow process informed by prior experiences developing AI applications (e.g., search and NLP) and data science tools (e.g. application diagnostics and bug reporting). We found that various Microsoft teams have united this workflow into preexisting, well-evolved, Agile-like software engineering processes, providing insights about several essential engineering challenges that organizations may face in creating large-scale AI solutions for the marketplace. We collected some best practices from Microsoft teams to address these challenges. In addition, we have identified three aspects of the AI domain that make it fundamentally different from prior software application domains: 1) discovering, managing, and versioning the data needed for machine learning applications is much more complex and difficult than other types of software engineering, 2) model customization and model reuse require very different skills than are typically found in software teams, and 3) AI components are more difficult to handle as distinct modules than traditional software components - models may be \"entangled\" in complex ways and experience non-monotonic error behavior. We believe that the lessons learned by Microsoft teams will be valuable to other organizations.", "year": 2019, "referenceCount": 37, "citationCount": 399, "influentialCitationCount": 63, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1719124", "name": "Saleema Amershi"}, {"authorId": "1776779", "name": "A. Begel"}, {"authorId": "145193818", "name": "C. Bird"}, {"authorId": "1710751", "name": "R. DeLine"}, {"authorId": "50355692", "name": "H. Gall"}, {"authorId": "1783184", "name": "Ece Kamar"}, {"authorId": "1693689", "name": "Nachiappan Nagappan"}, {"authorId": "2571049", "name": "Besmira Nushi"}, {"authorId": "143609903", "name": "T. Zimmermann"}]}, {"paperId": "adade3149b2a6177296de352f003471eefa958b8", "url": "https://www.semanticscholar.org/paper/adade3149b2a6177296de352f003471eefa958b8", "title": "The Impact of Machine Learning on Economics", "abstract": "This paper provides an assessment of the early contributions of machine learning to economics, as well as predictions about its future contributions. It begins by briefly overviewing some themes from the literature on machine learning, and then draws some contrasts with traditional approaches to estimating the impact of counterfactual policies in economics. Next, we review some of the initial \u201coff-the-shelf\u201d applications of machine learning to economics, including applications in analyzing text and images. We then describe new types of questions that have been posed surrounding the application of machine learning to policy problems, including \u201cprediction policy problems,\u201d as well as considerations of fairness and manipulability. We present some highlights from the emerging econometric literature combining machine learning and causal inference. Finally, we overview a set of broader predictions about the future impact of machine learning on economics, including its impacts on the nature of collaboration, funding, research tools, and research questions.", "year": 2018, "referenceCount": 110, "citationCount": 302, "influentialCitationCount": 45, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2631417", "name": "S. Athey"}]}, {"paperId": "739769f4862753fc80057194456d758d2a148ee3", "url": "https://www.semanticscholar.org/paper/739769f4862753fc80057194456d758d2a148ee3", "title": "Extreme Learning Machine for Regression and Multiclass Classification", "abstract": "Due to the simplicity of their implementations, least square support vector machine (LS-SVM) and proximal support vector machine (PSVM) have been widely used in binary classification applications. The conventional LS-SVM and PSVM cannot be used in regression and multiclass classification applications directly, although variants of LS-SVM and PSVM have been proposed to handle such cases. This paper shows that both LS-SVM and PSVM can be simplified further and a unified learning framework of LS-SVM, PSVM, and other regularization algorithms referred to extreme learning machine (ELM) can be built. ELM works for the \u201cgeneralized\u201d single-hidden-layer feedforward networks (SLFNs), but the hidden layer (or called feature mapping) in ELM need not be tuned. Such SLFNs include but are not limited to SVM, polynomial network, and the conventional feedforward neural networks. This paper shows the following: 1) ELM provides a unified learning platform with a widespread type of feature mappings and can be applied in regression and multiclass classification applications directly; 2) from the optimization method point of view, ELM has milder optimization constraints compared to LS-SVM and PSVM; 3) in theory, compared to ELM, LS-SVM and PSVM achieve suboptimal solutions and require higher computational complexity; and 4) in theory, ELM can approximate any target continuous function and classify any disjoint regions. As verified by the simulation results, ELM tends to have better scalability and achieve similar (for regression and binary class cases) or much better (for multiclass cases) generalization performance at much faster learning speed (up to thousands times) than traditional SVM and LS-SVM.", "year": 2012, "referenceCount": 61, "citationCount": 4476, "influentialCitationCount": 490, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "145678691", "name": "G. Huang"}, {"authorId": "2986982", "name": "Hongming Zhou"}, {"authorId": "3210833", "name": "Xiaojian Ding"}, {"authorId": "2118403946", "name": "Rui Zhang"}]}, {"paperId": "b6c4c7477d88fc6fa2cf04f5ece7b34b8e6f0eb6", "url": "https://www.semanticscholar.org/paper/b6c4c7477d88fc6fa2cf04f5ece7b34b8e6f0eb6", "title": "Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent", "abstract": "We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.", "year": 2017, "referenceCount": 26, "citationCount": 312, "influentialCitationCount": 51, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51310474", "name": "Yudong Chen"}, {"authorId": "2255869", "name": "Lili Su"}, {"authorId": "47883294", "name": "Jiaming Xu"}]}, {"paperId": "7ae2783a9196fb4bc2a610ae812d19722daddce5", "url": "https://www.semanticscholar.org/paper/7ae2783a9196fb4bc2a610ae812d19722daddce5", "title": "Applications of machine learning to machine fault diagnosis: A review and roadmap", "abstract": null, "year": 2020, "referenceCount": 451, "citationCount": 674, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1829456", "name": "Y. Lei"}, {"authorId": "2118581438", "name": "Bin Yang"}, {"authorId": "2144793979", "name": "Xinwei Jiang"}, {"authorId": "46691607", "name": "Feng Jia"}, {"authorId": "2157947735", "name": "Naipeng Li"}, {"authorId": "145720325", "name": "A. Nandi"}]}, {"paperId": "9e27190f2d9b2167d4a66b88696def4585072fd5", "url": "https://www.semanticscholar.org/paper/9e27190f2d9b2167d4a66b88696def4585072fd5", "title": "SoilGrids250m: Global gridded soil information based on machine learning", "abstract": "This paper describes the technical development and accuracy assessment of the most recent and improved version of the SoilGrids system at 250m resolution (June 2016 update). SoilGrids provides global predictions for standard numeric soil properties (organic carbon, bulk density, Cation Exchange Capacity (CEC), pH, soil texture fractions and coarse fragments) at seven standard depths (0, 5, 15, 30, 60, 100 and 200 cm), in addition to predictions of depth to bedrock and distribution of soil classes based on the World Reference Base (WRB) and USDA classification systems (ca. 280 raster layers in total). Predictions were based on ca. 150,000 soil profiles used for training and a stack of 158 remote sensing-based soil covariates (primarily derived from MODIS land products, SRTM DEM derivatives, climatic images and global landform and lithology maps), which were used to fit an ensemble of machine learning methods\u2014random forest and gradient boosting and/or multinomial logistic regression\u2014as implemented in the R packages ranger, xgboost, nnet and caret. The results of 10\u2013fold cross-validation show that the ensemble models explain between 56% (coarse fragments) and 83% (pH) of variation with an overall average of 61%. Improvements in the relative accuracy considering the amount of variation explained, in comparison to the previous version of SoilGrids at 1 km spatial resolution, range from 60 to 230%. Improvements can be attributed to: (1) the use of machine learning instead of linear regression, (2) to considerable investments in preparing finer resolution covariate layers and (3) to insertion of additional soil profiles. Further development of SoilGrids could include refinement of methods to incorporate input uncertainties and derivation of posterior probability distributions (per pixel), and further automation of spatial modeling so that soil maps can be generated for potentially hundreds of soil variables. Another area of future research is the development of methods for multiscale merging of SoilGrids predictions with local and/or national gridded soil products (e.g. up to 50 m spatial resolution) so that increasingly more accurate, complete and consistent global soil information can be produced. SoilGrids are available under the Open Data Base License.", "year": 2017, "referenceCount": 108, "citationCount": 1736, "influentialCitationCount": 107, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Environmental Science"], "authors": [{"authorId": "2856207", "name": "T. Hengl"}, {"authorId": "7549589", "name": "Jorge Mendes de Jesus"}, {"authorId": "145253407", "name": "G. Heuvelink"}, {"authorId": "9583743", "name": "Maria Ruiperez Gonz\u00e1lez"}, {"authorId": "30504500", "name": "M. Kilibarda"}, {"authorId": "9030720", "name": "Aleksandar Blagoti\u0107"}, {"authorId": "2479912", "name": "W. Shangguan"}, {"authorId": "3376186", "name": "Marvin N. Wright"}, {"authorId": "3018223", "name": "X. Geng"}, {"authorId": "1402912902", "name": "B. Bauer-Marschallinger"}, {"authorId": "145669099", "name": "M. Guevara"}, {"authorId": "145632581", "name": "R. Vargas"}, {"authorId": "145028966", "name": "R. MacMillan"}, {"authorId": "49399380", "name": "N. Batjes"}, {"authorId": "100653750", "name": "J. Leenaars"}, {"authorId": "32830771", "name": "E. Ribeiro"}, {"authorId": "2924968", "name": "I. Wheeler"}, {"authorId": "2146245", "name": "S. Mantel"}, {"authorId": "4953836", "name": "B. Kempen"}]}, {"paperId": "d1e701665e73faa648cb15473952576f40e8e122", "url": "https://www.semanticscholar.org/paper/d1e701665e73faa648cb15473952576f40e8e122", "title": "The Machine\u2010Learning Approach", "abstract": "For past several years, microarray technology has attracted tremendous interest for both scientific community and industry. Recently, the applications of microarrays include gene discovery, disease diagnosis and prognosis, drug discovery, etc. High dimensional data with small sample size is the main problem that generate the application of dimension reduction in microarray data analysis. It is seen that SVM, ANN and NB have recently gained wide popularity for cancer classification problems. An efficient and reliable method of dimension reduction plays an important role to improve the performance of SVM, ANN and NB, when applied for classification of high dimensional microarray data. In this book, we applied different combinations of feature selection / extraction methods, as a novel hybrid dimension reduction method for SVM, ANN and NB classifiers. The obtained results are compared with other popular published dimension reduction methods for SVM, NB and ANN classifiers.", "year": 2020, "referenceCount": 33, "citationCount": 359, "influentialCitationCount": 21, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50151200", "name": "N. Srivastava"}, {"authorId": "12735080", "name": "C. Verma"}, {"authorId": "89075256", "name": "Rabia Aziz Musheer"}]}, {"paperId": "db0cc2f21b20cbc0ab8946090967399c25709614", "url": "https://www.semanticscholar.org/paper/db0cc2f21b20cbc0ab8946090967399c25709614", "title": "Practical Secure Aggregation for Privacy-Preserving Machine Learning", "abstract": "We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.", "year": 2017, "referenceCount": 58, "citationCount": 1441, "influentialCitationCount": 197, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2039588", "name": "Keith Bonawitz"}, {"authorId": "2072422622", "name": "Vladimir Ivanov"}, {"authorId": "4908509", "name": "Ben Kreuter"}, {"authorId": "2212030", "name": "Antonio Marcedone"}, {"authorId": "145057514", "name": "H. B. McMahan"}, {"authorId": "34521172", "name": "Sarvar Patel"}, {"authorId": "1878835", "name": "D. Ramage"}, {"authorId": "2064203013", "name": "Aaron Segal"}, {"authorId": "34185195", "name": "Karn Seth"}]}, {"paperId": "c292e473b3825eeb9db03c70b2e1c033aea190d5", "url": "https://www.semanticscholar.org/paper/c292e473b3825eeb9db03c70b2e1c033aea190d5", "title": "Machine learning for molecular and materials science", "abstract": null, "year": 2018, "referenceCount": 121, "citationCount": 1587, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Chemistry", "Materials Science", "Medicine"], "authors": [{"authorId": "3300287", "name": "K. Butler"}, {"authorId": "48592354", "name": "Daniel W. Davies"}, {"authorId": "2527511", "name": "H. Cartwright"}, {"authorId": "2385206", "name": "O. Isayev"}, {"authorId": "144485183", "name": "A. Walsh"}]}, {"paperId": "26ae952599aa9ba5815a80356024258247fc2b10", "url": "https://www.semanticscholar.org/paper/26ae952599aa9ba5815a80356024258247fc2b10", "title": "Data mining: practical machine learning tools and techniques with Java implementations", "abstract": "1. What's It All About? 2. Input: Concepts, Instances, Attributes 3. Output: Knowledge Representation 4. Algorithms: The Basic Methods 5. Credibility: Evaluating What's Been Learned 6. Implementations: Real Machine Learning Schemes 7. Moving On: Engineering The Input And Output 8. Nuts And Bolts: Machine Learning Algorithms In Java 9. Looking Forward", "year": 2002, "referenceCount": 7, "citationCount": 5928, "influentialCitationCount": 550, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9419406", "name": "I. Witten"}, {"authorId": "143713826", "name": "Eibe Frank"}]}, {"paperId": "e0408181bccb7e3754dd5e6785ec47d8beb8b6bd", "url": "https://www.semanticscholar.org/paper/e0408181bccb7e3754dd5e6785ec47d8beb8b6bd", "title": "Machine Learning for High-Speed Corner Detection", "abstract": null, "year": 2006, "referenceCount": 44, "citationCount": 4211, "influentialCitationCount": 474, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1721991", "name": "E. Rosten"}, {"authorId": "144418842", "name": "T. Drummond"}]}, {"paperId": "f75b70c9d7078724b592ec3e21de705e7b6ff73f", "url": "https://www.semanticscholar.org/paper/f75b70c9d7078724b592ec3e21de705e7b6ff73f", "title": "Double/Debiased Machine Learning for Treatment and Structural Parameters", "abstract": "We revisit the classic semiparametric problem of inference on a low dimensional parameter \u03b8_0 in the presence of high-dimensional nuisance parameters \u03b7_0. We depart from the classical setting by allowing for \u03b7_0 to be so high-dimensional that the traditional assumptions, such as Donsker properties, that limit complexity of the parameter space for this object break down. To estimate \u03b7_0, we consider the use of statistical or machine learning (ML) methods which are particularly well-suited to estimation in modern, very high-dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating \u03b7_0 cause a heavy bias in estimators of \u03b8_0 that are obtained by naively plugging ML estimators of \u03b7_0 into estimating equations for \u03b8_0. This bias results in the naive estimator failing to be N^(-1/2) consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest \u03b8_0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman-orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate \u03b8_0, and (2) making use of cross-fitting which provides an efficient form of data-splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in a N^(-1/2)-neighborhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of DML applied to learn the main regression parameter in a partially linear regression model, DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model, DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness, and DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.", "year": 2017, "referenceCount": 97, "citationCount": 1058, "influentialCitationCount": 306, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "26331346", "name": "V. Chernozhukov"}, {"authorId": "88741529", "name": "D. Chetverikov"}, {"authorId": "88741890", "name": "Mert Demirer"}, {"authorId": "2259683", "name": "E. Duflo"}, {"authorId": "144613163", "name": "Christian Hansen"}, {"authorId": "3646527", "name": "W. Newey"}, {"authorId": "145607066", "name": "J. Robins"}]}, {"paperId": "da118b8aa99699edd7609fbbd081d5b93bc2e87b", "url": "https://www.semanticscholar.org/paper/da118b8aa99699edd7609fbbd081d5b93bc2e87b", "title": "Automatic differentiation in machine learning: a survey", "abstract": "Derivatives, mostly in the form of gradients and Hessians, are ubiquitous in machine learning. Automatic differentiation (AD), also called algorithmic differentiation or simply \u201cauto-diff\u201d, is a family of techniques similar to but more general than backpropagation for efficiently and accurately evaluating derivatives of numeric functions expressed as computer programs. AD is a small but established field with applications in areas including computational fluid dynamics, atmospheric sciences, and engineering design optimization. Until \nvery recently, the fields of machine learning and AD have largely been unaware of each other and, in some cases, have independently discovered each other\u2019s results. Despite its \nrelevance, general-purpose AD has been missing from the machine learning toolbox, a situation slowly changing with its ongoing adoption under the names \u201cdynamic computational \ngraphs\u201d and \u201cdifferentiable programming\u201d. We survey the intersection of AD and machine learning, cover applications where AD has direct relevance, and address the main imple- \nmentation techniques. By precisely defining the main differentiation techniques and their interrelationships, we aim to bring clarity to the usage of the terms \u201cautodiff\u201d, \u201cautomatic differentiation\u201d, and \u201csymbolic differentiation\u201d as these are encountered more and more in machine learning settings.", "year": 2015, "referenceCount": 248, "citationCount": 1446, "influentialCitationCount": 80, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1739503", "name": "A. G. Baydin"}, {"authorId": "1700974", "name": "Barak A. Pearlmutter"}, {"authorId": "1767709", "name": "Alexey Radul"}, {"authorId": "1737754", "name": "J. Siskind"}]}, {"paperId": "f2e07ccce9a89f7c018bd62d390240e8ee0b0045", "url": "https://www.semanticscholar.org/paper/f2e07ccce9a89f7c018bd62d390240e8ee0b0045", "title": "Machine learning with Python", "abstract": "This presentation is a case study taken from the travel and holiday industry. Paxport/Multicom, based in UK and Sweden, have recently adopted a recommendation system for holiday accommodation bookings. Machine learning techniques such as Collaborative Filtering have been applied using Python (3.5.1), with Jupyter (4.0.6) as the main framework. Data scale and sparsity present significant challenges in the case study, and so the effectiveness of various techniques are described as well as the performance of Python-based libraries such as Python Data Analysis Library (Pandas), and Scikit-learn (built on NumPy, SciPy and matplotlib). The presentation is suitable for all levels of programmers.", "year": 2017, "referenceCount": 0, "citationCount": 1463, "influentialCitationCount": 47, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2054691817", "name": "Pedro Ferreira"}, {"authorId": "2095376", "name": "C. Simons"}]}, {"paperId": "47f2f60995e4393ad689233ee870e1e0707d4d60", "url": "https://www.semanticscholar.org/paper/47f2f60995e4393ad689233ee870e1e0707d4d60", "title": "Automated Machine Learning: Methods, Systems, Challenges", "abstract": null, "year": 2019, "referenceCount": 0, "citationCount": 642, "influentialCitationCount": 33, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144661829", "name": "F. Hutter"}, {"authorId": "1722782", "name": "Lars Kotthoff"}, {"authorId": "1717534", "name": "J. Vanschoren"}]}, {"paperId": "7e7eb0f93c9550d7336f4bbfad5fe89604295705", "url": "https://www.semanticscholar.org/paper/7e7eb0f93c9550d7336f4bbfad5fe89604295705", "title": "Quantum Machine Learning in Feature Hilbert Spaces.", "abstract": "A basic idea of quantum computing is surprisingly similar to that of kernel methods in machine learning, namely, to efficiently perform computations in an intractably large Hilbert space. In this Letter we explore some theoretical foundations of this link and show how it opens up a new avenue for the design of quantum machine learning algorithms. We interpret the process of encoding inputs in a quantum state as a nonlinear feature map that maps data to quantum Hilbert space. A quantum computer can now analyze the input data in this feature space. Based on this link, we discuss two approaches for building a quantum model for classification. In the first approach, the quantum device estimates inner products of quantum states to compute a classically intractable kernel. The kernel can be fed into any classical kernel method such as a support vector machine. In the second approach, we use a variational quantum circuit as a linear model that classifies data explicitly in Hilbert space. We illustrate these ideas with a feature map based on squeezing in a continuous-variable system, and visualize the working principle with two-dimensional minibenchmark datasets.", "year": 2018, "referenceCount": 45, "citationCount": 518, "influentialCitationCount": 35, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics", "Medicine"], "authors": [{"authorId": "3048564", "name": "M. Schuld"}, {"authorId": "3399181", "name": "N. Killoran"}]}, {"paperId": "1b225474e7a5794f98cdfbde8b12ccbc56799409", "url": "https://www.semanticscholar.org/paper/1b225474e7a5794f98cdfbde8b12ccbc56799409", "title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models", "abstract": "Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .", "year": 2017, "referenceCount": 28, "citationCount": 786, "influentialCitationCount": 119, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "40634590", "name": "Wieland Brendel"}, {"authorId": "19237612", "name": "Jonas Rauber"}, {"authorId": "1731199", "name": "M. Bethge"}]}, {"paperId": "c0bc4ef587b4cbebd5839baeed95274fbf26c43a", "url": "https://www.semanticscholar.org/paper/c0bc4ef587b4cbebd5839baeed95274fbf26c43a", "title": "Applications of machine learning to diagnosis and treatment of neurodegenerative diseases", "abstract": null, "year": 2020, "referenceCount": 309, "citationCount": 110, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "5569230", "name": "Monika A. Myszczynska"}, {"authorId": "10753629", "name": "P. Ojamies"}, {"authorId": "2003831275", "name": "Alix M. B. Lacoste"}, {"authorId": "145243593", "name": "Daniel Neil"}, {"authorId": "1741702", "name": "Amir Saffari"}, {"authorId": "2059872991", "name": "R. Mead"}, {"authorId": "4080239", "name": "G. Hautbergue"}, {"authorId": "2602146", "name": "J. Holbrook"}, {"authorId": "6346151", "name": "L. Ferraiuolo"}]}, {"paperId": "0fcd8359aa964a118adf10dd524a90d61e10048b", "url": "https://www.semanticscholar.org/paper/0fcd8359aa964a118adf10dd524a90d61e10048b", "title": "Auto-sklearn: Efficient and Robust Automated Machine Learning", "abstract": null, "year": 2019, "referenceCount": 46, "citationCount": 227, "influentialCitationCount": 34, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2868444", "name": "Matthias Feurer"}, {"authorId": "145227684", "name": "Aaron Klein"}, {"authorId": "2607675", "name": "Katharina Eggensperger"}, {"authorId": "2060551", "name": "J. T. Springenberg"}, {"authorId": "2058090778", "name": "Manuel Blum"}, {"authorId": "144661829", "name": "F. Hutter"}]}, {"paperId": "67df7bf02fe2d618c7c18448c2668a526dc4d423", "url": "https://www.semanticscholar.org/paper/67df7bf02fe2d618c7c18448c2668a526dc4d423", "title": "Machine Learning at Facebook: Understanding Inference at the Edge", "abstract": "At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized datacenter infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a datadriven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms.", "year": 2019, "referenceCount": 74, "citationCount": 275, "influentialCitationCount": 28, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2797270", "name": "Carole-Jean Wu"}, {"authorId": "1896817", "name": "D. Brooks"}, {"authorId": "2152955923", "name": "Kevin Chen"}, {"authorId": "2158194412", "name": "Douglas Chen"}, {"authorId": "89754631", "name": "Sy Choudhury"}, {"authorId": "3193064", "name": "Marat Dukhan"}, {"authorId": "1775500", "name": "Kim M. Hazelwood"}, {"authorId": "89274305", "name": "Eldad Isaac"}, {"authorId": "39978391", "name": "Yangqing Jia"}, {"authorId": "33920592", "name": "Bill Jia"}, {"authorId": "3316156", "name": "Tommer Leyvand"}, {"authorId": "2115608531", "name": "Hao Lu"}, {"authorId": "2146557923", "name": "Yang Lu"}, {"authorId": "2065508882", "name": "Lin Qiao"}, {"authorId": "2732658", "name": "Brandon Reagen"}, {"authorId": "90591458", "name": "Joe Spisak"}, {"authorId": "2075373634", "name": "Fei Sun"}, {"authorId": "3609856", "name": "Andrew Tulloch"}, {"authorId": "48682997", "name": "P\u00e9ter Vajda"}, {"authorId": "2108429236", "name": "Xiaodong Wang"}, {"authorId": "2146020227", "name": "Yanghan Wang"}, {"authorId": "46240090", "name": "Bram Wasti"}, {"authorId": "2115877352", "name": "Yiming Wu"}, {"authorId": "2066571092", "name": "Ran Xian"}, {"authorId": "1808405", "name": "S. Yoo"}, {"authorId": "2918780", "name": "Peizhao Zhang"}]}, {"paperId": "2b7f9117eb6608a58be4c078ca3d69c0e5ccb875", "url": "https://www.semanticscholar.org/paper/2b7f9117eb6608a58be4c078ca3d69c0e5ccb875", "title": "SecureML: A System for Scalable Privacy-Preserving Machine Learning", "abstract": "Machine learning is widely used in practice to produce predictive models for applications such as image processing, speech and text recognition. These models are more accurate when trained on large amount of data collected from different sources. However, the massive data collection raises privacy concerns. In this paper, we present new and efficient protocols for privacy preserving machine learning for linear regression, logistic regression and neural network training using the stochastic gradient descent method. Our protocols fall in the two-server model where data owners distribute their private data among two non-colluding servers who train various models on the joint data using secure two-party computation (2PC). We develop new techniques to support secure arithmetic operations on shared decimal numbers, and propose MPC-friendly alternatives to non-linear functions such as sigmoid and softmax that are superior to prior work. We implement our system in C++. Our experiments validate that our protocols are several orders of magnitude faster than the state of the art implementations for privacy preserving linear and logistic regressions, and scale to millions of data samples with thousands of features. We also implement the first privacy preserving system for training neural networks.", "year": 2017, "referenceCount": 40, "citationCount": 1076, "influentialCitationCount": 204, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1773836", "name": "Payman Mohassel"}, {"authorId": "2108473999", "name": "Yupeng Zhang"}]}, {"paperId": "05c5b732fb92546c7d6eeabfadb5c14610d07373", "url": "https://www.semanticscholar.org/paper/05c5b732fb92546c7d6eeabfadb5c14610d07373", "title": "Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning", "abstract": "Imbalanced-learn is an open-source python toolbox aiming at providing a wide range of methods to cope with the problem of imbalanced dataset frequently encountered in machine learning and pattern recognition. The implemented state-of-the-art methods can be categorized into 4 groups: (i) under-sampling, (ii) over-sampling, (iii) combination of over- and under-sampling, and (iv) ensemble learning methods. The proposed toolbox only depends on numpy, scipy, and scikit-learn and is distributed under MIT license. Furthermore, it is fully compatible with scikit-learn and is part of the scikit-learn-contrib supported project. Documentation, unit tests as well as integration tests are provided to ease usage and contribution. The toolbox is publicly available in GitHub: this https URL.", "year": 2016, "referenceCount": 29, "citationCount": 1217, "influentialCitationCount": 50, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "144272231", "name": "G. Lema\u00eetre"}, {"authorId": "2054680697", "name": "Fernando Nogueira"}, {"authorId": "3451187", "name": "Christos K. Aridas"}]}, {"paperId": "caf9e0fa2c340fb07cef8d547ea8849508e5c358", "url": "https://www.semanticscholar.org/paper/caf9e0fa2c340fb07cef8d547ea8849508e5c358", "title": "Empirical Asset Pricing Via Machine Learning", "abstract": "\n We perform a comparative analysis of machine learning methods for the canonical problem of empirical asset pricing: measuring asset risk premiums. We demonstrate large economic gains to investors using machine learning forecasts, in some cases doubling the performance of leading regression-based strategies from the literature. We identify the best-performing methods (trees and neural networks) and trace their predictive gains to allowing nonlinear predictor interactions missed by other methods. All methods agree on the same set of dominant predictive signals, a set that includes variations on momentum, liquidity, and volatility.\n Authors have furnished an Internet Appendix, which is available on the Oxford University Press Web site next to the link to the final published paper online.", "year": 2018, "referenceCount": 127, "citationCount": 708, "influentialCitationCount": 100, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Economics"], "authors": [{"authorId": "90255997", "name": "Shihao Gu"}, {"authorId": "152467398", "name": "B. Kelly"}, {"authorId": "90286912", "name": "D. Xiu"}]}, {"paperId": "262c0e54370dfc03a7ad53d79930568d18dd448c", "url": "https://www.semanticscholar.org/paper/262c0e54370dfc03a7ad53d79930568d18dd448c", "title": "Speeding Up Distributed Machine Learning Using Codes", "abstract": "Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms\u2014straggler nodes, system failures, or communication bottlenecks\u2014but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=\"LaTeX\">$\\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=\"LaTeX\">$\\left({\\alpha + \\frac {1}{n}}\\right)\\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=\"LaTeX\">$\\gamma (n) \\simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=\"LaTeX\">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.", "year": 2015, "referenceCount": 99, "citationCount": 634, "influentialCitationCount": 108, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "27911143", "name": "Kangwook Lee"}, {"authorId": "2347284", "name": "Maximilian Lam"}, {"authorId": "2686521", "name": "Ramtin Pedarsani"}, {"authorId": "1740595", "name": "Dimitris Papailiopoulos"}, {"authorId": "144161012", "name": "K. Ramchandran"}]}, {"paperId": "0522fdd8efe0e4baa5602d1384047a5705613765", "url": "https://www.semanticscholar.org/paper/0522fdd8efe0e4baa5602d1384047a5705613765", "title": "Supervised Machine Learning: A Review of Classification Techniques", "abstract": "The goal of supervised learning is to build a concise model of the distribution of class labels in terms of predictor features. The resulting classifier is then used to assign class labels to the testing instances where the values of the predictor features are known, but the value of the class label is unknown. This paper describes various supervised machine learning classification techniques. Of course, a single chapter cannot be a complete review of all supervised machine learning classification algorithms (also known induction classification algorithms), yet we hope that the references cited will cover the major theoretical issues, guiding the researcher in interesting research directions and suggesting possible bias combinations that have yet to be explored.", "year": 2007, "referenceCount": 169, "citationCount": 4182, "influentialCitationCount": 261, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697867", "name": "S. Kotsiantis"}]}, {"paperId": "c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7", "url": "https://www.semanticscholar.org/paper/c8f216f663660ff3bc195ecd3a8ad61f0ed1d9d7", "title": "Privacy Risk in Machine Learning: Analyzing the Connection to Overfitting", "abstract": "Machine learning algorithms, when applied to sensitive data, pose a distinct threat to privacy. A growing body of prior work demonstrates that models produced by these algorithms may leak specific private information in the training data to an attacker, either through the models' structure or their observable behavior. However, the underlying cause of this privacy risk is not well understood beyond a handful of anecdotal accounts that suggest overfitting and influence might play a role. This paper examines the effect that overfitting and influence have on the ability of an attacker to learn information about the training data from machine learning models, either through training set membership inference or attribute inference attacks. Using both formal and empirical analyses, we illustrate a clear relationship between these factors and the privacy risk that arises in several popular machine learning algorithms. We find that overfitting is sufficient to allow an attacker to perform membership inference and, when the target attribute meets certain conditions about its influence, attribute inference attacks. Interestingly, our formal analysis also shows that overfitting is not necessary for these attacks and begins to shed light on what other factors may be in play. Finally, we explore the connection between membership inference and attribute inference, showing that there are deep connections between the two that lead to effective new attacks.", "year": 2017, "referenceCount": 57, "citationCount": 484, "influentialCitationCount": 105, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "26378728", "name": "Samuel Yeom"}, {"authorId": "3025831", "name": "Irene Giacomelli"}, {"authorId": "2623167", "name": "Matt Fredrikson"}, {"authorId": "1680133", "name": "S. Jha"}]}, {"paperId": "d75356e2bf674902a06a14bb55d18ee88af5b4bb", "url": "https://www.semanticscholar.org/paper/d75356e2bf674902a06a14bb55d18ee88af5b4bb", "title": "Machine Learning Methods That Economists Should Know About", "abstract": "We discuss the relevance of the recent machine learning (ML) literature for economics and econometrics. First we discuss the differences in goals, methods, and settings between the ML literature and the traditional econometrics and statistics literatures. Then we discuss some specific methods from the ML literature that we view as important for empirical researchers in economics. These include supervised learning methods for regression and classification, unsupervised learning methods, and matrix completion methods. Finally, we highlight newly developed methods at the intersection of ML and econometrics that typically perform better than either off-the-shelf ML or more traditional econometric methods when applied to particular classes of problems, including causal inference for average treatment effects, optimal policy estimation, and estimation of the counterfactual effect of price changes in consumer choice models.", "year": 2019, "referenceCount": 146, "citationCount": 338, "influentialCitationCount": 27, "isOpenAccess": false, "fieldsOfStudy": ["Economics", "Computer Science"], "authors": [{"authorId": "2631417", "name": "S. Athey"}, {"authorId": "47166531", "name": "G. Imbens"}]}, {"paperId": "d7701e78e0bfc92b03a89582e80cfb751ac03f26", "url": "https://www.semanticscholar.org/paper/d7701e78e0bfc92b03a89582e80cfb751ac03f26", "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning", "abstract": "There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.", "year": 2018, "referenceCount": 87, "citationCount": 1062, "influentialCitationCount": 53, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145019478", "name": "Leilani H. Gilpin"}, {"authorId": "144159726", "name": "David Bau"}, {"authorId": "144002190", "name": "Ben Z. Yuan"}, {"authorId": "50397921", "name": "Ayesha Bajwa"}, {"authorId": "144417360", "name": "Michael A. Specter"}, {"authorId": "1735243", "name": "Lalana Kagal"}]}, {"paperId": "218062f45c15f39bc8f4fb2c930ddf20b5809b11", "url": "https://www.semanticscholar.org/paper/218062f45c15f39bc8f4fb2c930ddf20b5809b11", "title": "Machine Learning Testing: Survey, Landscapes and Horizons", "abstract": "This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.", "year": 2019, "referenceCount": 280, "citationCount": 351, "influentialCitationCount": 33, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "51250527", "name": "J Zhang"}, {"authorId": "145836176", "name": "M. Harman"}, {"authorId": "2109704789", "name": "Lei Ma"}, {"authorId": "144440741", "name": "Yang Liu"}]}, {"paperId": "61306b52c2d292928f7cbb2f2ef5711d15a2566c", "url": "https://www.semanticscholar.org/paper/61306b52c2d292928f7cbb2f2ef5711d15a2566c", "title": "ABY3: A Mixed Protocol Framework for Machine Learning", "abstract": "Machine learning is widely used to produce models for a range of applications and is increasingly offered as a service by major technology companies. However, the required massive data collection raises privacy concerns during both training and prediction stages. In this paper, we design and implement a general framework for privacy-preserving machine learning and use it to obtain new solutions for training linear regression, logistic regression and neural network models. Our protocols are in a three-server model wherein data owners secret share their data among three servers who train and evaluate models on the joint data using three-party computation (3PC). Our main contribution is a new and complete framework ($\\textABY ^3$) for efficiently switching back and forth between arithmetic, binary, and Yao 3PC which is of independent interest. Many of the conversions are based on new techniques that are designed and optimized for the first time in this paper. We also propose new techniques for fixed-point multiplication of shared decimal values that extends beyond the three-party case, and customized protocols for evaluating piecewise polynomial functions. We design variants of each building block that is secure against \\em malicious adversaries who deviate arbitrarily. We implement our system in C++. Our protocols are up to \\em four orders of magnitude faster than the best prior work, hence significantly reducing the gap between privacy-preserving and plaintext training.", "year": 2018, "referenceCount": 60, "citationCount": 388, "influentialCitationCount": 68, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1773836", "name": "Payman Mohassel"}, {"authorId": "3433542", "name": "Peter Rindal"}]}, {"paperId": "62df84d6a4d26f95e4714796c2337c9848cc13b5", "url": "https://www.semanticscholar.org/paper/62df84d6a4d26f95e4714796c2337c9848cc13b5", "title": "MXNet: A Flexible and Efficient Machine Learning Library for Heterogeneous Distributed Systems", "abstract": "MXNet is a multi-language machine learning (ML) library to ease the development of ML algorithms, especially for deep neural networks. Embedded in the host language, it blends declarative symbolic expression with imperative tensor computation. It offers auto differentiation to derive gradients. MXNet is computation and memory efficient and runs on various heterogeneous systems, ranging from mobile devices to distributed GPU clusters. \nThis paper describes both the API design and the system implementation of MXNet, and explains how embedding of both symbolic expression and tensor operation is handled in a unified fashion. Our preliminary experiments reveal promising results on large scale deep neural network applications using multiple GPU machines.", "year": 2015, "referenceCount": 13, "citationCount": 1982, "influentialCitationCount": 258, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1913774", "name": "Tianqi Chen"}, {"authorId": "2124778071", "name": "Mu Li"}, {"authorId": "2110420880", "name": "Yutian Li"}, {"authorId": "1491081747", "name": "Min Lin"}, {"authorId": "48246959", "name": "Naiyan Wang"}, {"authorId": "1508337194", "name": "Minjie Wang"}, {"authorId": "39102205", "name": "Tianjun Xiao"}, {"authorId": "2113742783", "name": "Bing Xu"}, {"authorId": "151505981", "name": "Chiyuan Zhang"}, {"authorId": "38448016", "name": "Zheng Zhang"}]}, {"paperId": "b3de1062d8a462dfdc2938558258f8884abe9f4e", "url": "https://www.semanticscholar.org/paper/b3de1062d8a462dfdc2938558258f8884abe9f4e", "title": "Implementation of machine-learning classification in remote sensing: an applied review", "abstract": "ABSTRACT Machine learning offers the potential for effective and efficient classification of remotely sensed imagery. The strengths of machine learning include the capacity to handle data of high dimensionality and to map classes with very complex characteristics. Nevertheless, implementing a machine-learning classification is not straightforward, and the literature provides conflicting advice regarding many key issues. This article therefore provides an overview of machine learning from an applied perspective. We focus on the relatively mature methods of support vector machines, single decision trees (DTs), Random Forests, boosted DTs, artificial neural networks, and k-nearest neighbours (k-NN). Issues considered include the choice of algorithm, training data requirements, user-defined parameter selection and optimization, feature space impacts and reduction, and computational costs. We illustrate these issues through applying machine-learning classification to two publically available remotely sensed data sets.", "year": 2018, "referenceCount": 138, "citationCount": 724, "influentialCitationCount": 40, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "26339328", "name": "Aaron E. Maxwell"}, {"authorId": "5875612", "name": "T. Warner"}, {"authorId": "2014814754", "name": "Fang Fang"}]}, {"paperId": "0273507eb05f1135f3a05f9c7adc9a56f12c7c5c", "url": "https://www.semanticscholar.org/paper/0273507eb05f1135f3a05f9c7adc9a56f12c7c5c", "title": "Recent advances and applications of machine learning in solid-state materials science", "abstract": null, "year": 2019, "referenceCount": 501, "citationCount": 871, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "2149390974", "name": "Jonathan Schmidt"}, {"authorId": "143837407", "name": "M\u00e1rio R. G. Marques"}, {"authorId": "5517714", "name": "S. Botti"}, {"authorId": "145096746", "name": "M. Marques"}]}]}