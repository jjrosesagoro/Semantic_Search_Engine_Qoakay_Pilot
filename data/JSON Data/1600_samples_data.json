{"total": 5120627, "offset": 1500, "next": 1600, "data": [{"paperId": "0cb7ebc80fadaa775c3ca64da8930384d910f00e", "url": "https://www.semanticscholar.org/paper/0cb7ebc80fadaa775c3ca64da8930384d910f00e", "title": "Predicting the Price of Bitcoin Using Machine Learning", "abstract": "The goal of this paper is to ascertain with what accuracy the direction of Bitcoin price in USD can be predicted. The price data is sourced from the Bitcoin Price Index. The task is achieved with varying degrees of success through the implementation of a Bayesian optimised recurrent neural network (RNN) and a Long Short Term Memory (LSTM) network. The LSTM achieves the highest classification accuracy of 52% and a RMSE of 8%. The popular ARIMA model for time series forecasting is implemented as a comparison to the deep learning models. As expected, the non-linear deep learning methods outperform the ARIMA forecast which performs poorly. Finally, both deep learning models are benchmarked on both a GPU and a CPU with the training time on the GPU outperforming the CPU implementation by 67.7%.", "year": 2018, "referenceCount": 56, "citationCount": 335, "influentialCitationCount": 26, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "46391717", "name": "S. Mcnally"}, {"authorId": "2059704800", "name": "Jason Roche"}, {"authorId": "2452441", "name": "Simon Caton"}]}, {"paperId": "b34fc78de28be598e21118d7cb9d84d63374addc", "url": "https://www.semanticscholar.org/paper/b34fc78de28be598e21118d7cb9d84d63374addc", "title": "Analysis of Dimensionality Reduction Techniques on Big Data", "abstract": "Due to digitization, a huge volume of data is being generated across several sectors such as healthcare, production, sales, IoT devices, Web, organizations. Machine learning algorithms are used to uncover patterns among the attributes of this data. Hence, they can be used to make predictions that can be used by medical practitioners and people at managerial level to make executive decisions. Not all the attributes in the datasets generated are important for training the machine learning algorithms. Some attributes might be irrelevant and some might not affect the outcome of the prediction. Ignoring or removing these irrelevant or less important attributes reduces the burden on machine learning algorithms. In this work two of the prominent dimensionality reduction techniques, Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are investigated on four popular Machine Learning (ML) algorithms, Decision Tree Induction, Support Vector Machine (SVM), Naive Bayes Classifier and Random Forest Classifier using publicly available Cardiotocography (CTG) dataset from University of California and Irvine Machine Learning Repository. The experimentation results prove that PCA outperforms LDA in all the measures. Also, the performance of the classifiers, Decision Tree, Random Forest examined is not affected much by using PCA and LDA.To further analyze the performance of PCA and LDA the eperimentation is carried out on Diabetic Retinopathy (DR) and Intrusion Detection System (IDS) datasets. Experimentation results prove that ML algorithms with PCA produce better results when dimensionality of the datasets is high. When dimensionality of datasets is low it is observed that the ML algorithms without dimensionality reduction yields better results.", "year": 2020, "referenceCount": 44, "citationCount": 245, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "38608914", "name": "G. T. Reddy"}, {"authorId": "49247239", "name": "M. S. K. Reddy"}, {"authorId": "51017707", "name": "K. Lakshmanna"}, {"authorId": "46234199", "name": "Rajesh Kaluri"}, {"authorId": "32336300", "name": "D. Rajput"}, {"authorId": "144369609", "name": "Gautam Srivastava"}, {"authorId": "37022396", "name": "T. Baker"}]}, {"paperId": "9da1310a7158854a8070d9207d7042c9fff4f6ba", "url": "https://www.semanticscholar.org/paper/9da1310a7158854a8070d9207d7042c9fff4f6ba", "title": "Machine Learning: ECML 2003", "abstract": null, "year": 2003, "referenceCount": 19, "citationCount": 95, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1730104", "name": "N. Lavrac"}, {"authorId": "1767339", "name": "D. Gamberger"}, {"authorId": "1755851", "name": "H. Blockeel"}, {"authorId": "1735220", "name": "L. Todorovski"}]}, {"paperId": "fb144a1d31aec3b2bece6a59bd11a876a9fafb34", "url": "https://www.semanticscholar.org/paper/fb144a1d31aec3b2bece6a59bd11a876a9fafb34", "title": "Exploiting Open-Endedness to Solve Problems Through the Search for Novelty", "abstract": "This paper establishes a link between the challenge of solving highly ambitious problems in machine learning and the goal of reproducing the dynamics of open-ended evolution in artificial life. A major problem with the objective function in machine learning is that through deception it may actually prevent the objective from being reached. In a similar way, selection in evolution may sometimes act to discourage increasing complexity. This paper proposes a single idea that both overcomes the obstacle of deception and suggests a simple new approach to open-ended evolution: Instead of either explicitly seeking an objective or modeling a domain to capture the open-endedness of natural evolution, the idea is to simply search for novelty. Even in an objective-based problem, such novelty search ignores the objective and searches for behavioral novelty. Yet because many points in the search space collapse to the same point in behavior space, it turns out that the search for novelty is computationally feasible. Furthermore, because there are only so many simple behaviors, the search for novelty leads to increasing complexity. In fact, on the way up the ladder of complexity, the search is likely to encounter at least one solution. In this way, by decoupling the idea of open-ended search from only artificial life worlds, the raw search for novelty can be applied to real world problems. Counterintuitively, in the deceptive maze navigation task in this paper, novelty search significantly outperforms objective-based search, suggesting a surprising new approach to machine learning.", "year": 2008, "referenceCount": 39, "citationCount": 494, "influentialCitationCount": 90, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39799304", "name": "J. Lehman"}, {"authorId": "1846883", "name": "Kenneth O. Stanley"}]}, {"paperId": "5396e9858d70f09bf42f251621b5f2816a9cb660", "url": "https://www.semanticscholar.org/paper/5396e9858d70f09bf42f251621b5f2816a9cb660", "title": "Cover trees for nearest neighbor", "abstract": "We present a tree data structure for fast nearest neighbor operations in general n-point metric spaces (where the data set consists of n points). The data structure requires O(n) space regardless of the metric's structure yet maintains all performance properties of a navigating net (Krauthgamer & Lee, 2004b). If the point set has a bounded expansion constant c, which is a measure of the intrinsic dimensionality, as defined in (Karger & Ruhl, 2002), the cover tree data structure can be constructed in O (c6n log n) time. Furthermore, nearest neighbor queries require time only logarithmic in n, in particular O (c12 log n) time. Our experimental results show speedups over the brute force search varying between one and several orders of magnitude on natural machine learning datasets.", "year": 2006, "referenceCount": 29, "citationCount": 845, "influentialCitationCount": 94, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2624289", "name": "A. Beygelzimer"}, {"authorId": "144695232", "name": "S. Kakade"}, {"authorId": "144162125", "name": "J. Langford"}]}, {"paperId": "4ec06dda674cd5f93351f0ab5871391671f9b974", "url": "https://www.semanticscholar.org/paper/4ec06dda674cd5f93351f0ab5871391671f9b974", "title": "Medical Image Analysis using Convolutional Neural Networks: A Review", "abstract": null, "year": 2017, "referenceCount": 142, "citationCount": 509, "influentialCitationCount": 11, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "9514351", "name": "A. Qayyum"}, {"authorId": "144608640", "name": "S. Anwar"}, {"authorId": "144974259", "name": "Muhammad Majid"}, {"authorId": "1387433697", "name": "M. Awais"}, {"authorId": "3436574", "name": "M. Alnowami"}]}, {"paperId": "3558dc0a0139d6a3dd48458d207c497db027ab1b", "url": "https://www.semanticscholar.org/paper/3558dc0a0139d6a3dd48458d207c497db027ab1b", "title": "Thirty Years of Machine Learning: The Road to Pareto-Optimal Wireless Networks", "abstract": "Future wireless networks have a substantial potential in terms of supporting a broad range of complex compelling applications both in military and civilian fields, where the users are able to enjoy high-rate, low-latency, low-cost and reliable information services. Achieving this ambitious goal requires new radio techniques for adaptive learning and intelligent decision making because of the complex heterogeneous nature of the network structures and wireless services. Machine learning (ML) algorithms have great success in supporting big data analytics, efficient parameter estimation and interactive decision making. Hence, in this article, we review the thirty-year history of ML by elaborating on supervised learning, unsupervised learning, reinforcement learning and deep learning. Furthermore, we investigate their employment in the compelling applications of wireless networks, including heterogeneous networks (HetNets), cognitive radios (CR), Internet of Things (IoT), machine to machine networks (M2M), and so on. This article aims for assisting the readers in clarifying the motivation and methodology of the various ML algorithms, so as to invoke them for hitherto unexplored services as well as scenarios of future wireless networks.", "year": 2019, "referenceCount": 380, "citationCount": 230, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145210497", "name": "JingJing Wang"}, {"authorId": "1750017", "name": "Chunxiao Jiang"}, {"authorId": "50024622", "name": "Haijun Zhang"}, {"authorId": "145659296", "name": "Yong Ren"}, {"authorId": "66073306", "name": "Kwang-Cheng Chen"}, {"authorId": "1730180", "name": "L. Hanzo"}]}, {"paperId": "c0e5a471179d2d8c7025febe77a90c3a99c7c9fa", "url": "https://www.semanticscholar.org/paper/c0e5a471179d2d8c7025febe77a90c3a99c7c9fa", "title": "Learning With $\\ell ^{1}$-Graph for Image Analysis", "abstract": "The graph construction procedure essentially determines the potentials of those graph-oriented learning algorithms for image analysis. In this paper, we propose a process to build the so-called directed \u00bf1-graph, in which the vertices involve all the samples and the ingoing edge weights to each vertex describe its \u00bf1-norm driven reconstruction from the remaining samples and the noise. Then, a series of new algorithms for various machine learning tasks, e.g., data clustering, subspace learning, and semi-supervised learning, are derived upon the \u00bf1-graphs. Compared with the conventional k -nearest-neighbor graph and \u00bf-ball graph, the \u00bf1-graph possesses the advantages: (1) greater robustness to data noise, (2) automatic sparsity, and (3) adaptive neighborhood for individual datum. Extensive experiments on three real-world datasets show the consistent superiority of \u00bf1-graph over those classic graphs in data clustering, subspace learning, and semi-supervised learning tasks.", "year": 2010, "referenceCount": 25, "citationCount": 567, "influentialCitationCount": 51, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science", "Medicine"], "authors": [{"authorId": "2055922779", "name": "Bin Cheng"}, {"authorId": "1706007", "name": "Jianchao Yang"}, {"authorId": "143653681", "name": "Shuicheng Yan"}, {"authorId": "145692782", "name": "Yun Fu"}, {"authorId": "153652752", "name": "Thomas S. Huang"}]}, {"paperId": "07a04fa73ae666e730b6f50e2c9c96d0a11cff94", "url": "https://www.semanticscholar.org/paper/07a04fa73ae666e730b6f50e2c9c96d0a11cff94", "title": "Advances in Machine Learning", "abstract": null, "year": 2021, "referenceCount": 16, "citationCount": 44, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1733088", "name": "M. Someren"}]}, {"paperId": "4603c2513605d18d8f7318c455aae32f20e0ecfc", "url": "https://www.semanticscholar.org/paper/4603c2513605d18d8f7318c455aae32f20e0ecfc", "title": "Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP", "abstract": null, "year": 2018, "referenceCount": 56, "citationCount": 84, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145147566", "name": "S. Muggleton"}, {"authorId": "1727734", "name": "Ute Schmid"}, {"authorId": "26755201", "name": "Christina Zeller"}, {"authorId": "1403549560", "name": "Alireza Tamaddoni-Nezhad"}, {"authorId": "143862012", "name": "Tarek R. Besold"}]}, {"paperId": "0d24a0695c9fc669e643bad51d4e14f056329dec", "url": "https://www.semanticscholar.org/paper/0d24a0695c9fc669e643bad51d4e14f056329dec", "title": "An Actor-Critic Algorithm for Sequence Prediction", "abstract": "We present an approach to training neural networks to generate sequences using actor-critic methods from reinforcement learning (RL). Current log-likelihood training methods are limited by the discrepancy between their training and testing modes, as models must generate tokens conditioned on their previous guesses rather than the ground-truth tokens. We address this problem by introducing a \\textit{critic} network that is trained to predict the value of an output token, given the policy of an \\textit{actor} network. This results in a training procedure that is much closer to the test phase, and allows us to directly optimize for a task-specific score such as BLEU. Crucially, since we leverage these techniques in the supervised learning setting rather than the traditional RL setting, we condition the critic network on the ground-truth output. We show that our method leads to improved performance on both a synthetic task, and for German-English machine translation. Our analysis paves the way for such methods to be applied in natural language generation tasks, such as machine translation, caption generation, and dialogue modelling.", "year": 2016, "referenceCount": 49, "citationCount": 505, "influentialCitationCount": 68, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3335364", "name": "Dzmitry Bahdanau"}, {"authorId": "2616163", "name": "Philemon Brakel"}, {"authorId": "36303818", "name": "Kelvin Xu"}, {"authorId": "1996705", "name": "Anirudh Goyal"}, {"authorId": "2054294", "name": "Ryan Lowe"}, {"authorId": "145134886", "name": "Joelle Pineau"}, {"authorId": "1760871", "name": "Aaron C. Courville"}, {"authorId": "1751762", "name": "Yoshua Bengio"}]}, {"paperId": "b4b1cbd74029f46ef9b462290a46111217552761", "url": "https://www.semanticscholar.org/paper/b4b1cbd74029f46ef9b462290a46111217552761", "title": "Understanding the Effect of Accuracy on Trust in Machine Learning Models", "abstract": "We address a relatively under-explored aspect of human-computer interaction: people's abilities to understand the relationship between a machine learning model's stated performance on held-out data and its expected performance post deployment. We conduct large-scale, randomized human-subject experiments to examine whether laypeople's trust in a model, measured in terms of both the frequency with which they revise their predictions to match those of the model and their self-reported levels of trust in the model, varies depending on the model's stated accuracy on held-out data and on its observed accuracy in practice. We find that people's trust in a model is affected by both its stated accuracy and its observed accuracy, and that the effect of stated accuracy can change depending on the observed accuracy. Our work relates to recent research on interpretable machine learning, but moves beyond the typical focus on model internals, exploring a different component of the machine learning pipeline.", "year": 2019, "referenceCount": 31, "citationCount": 197, "influentialCitationCount": 12, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2053888438", "name": "M. Yin"}, {"authorId": "4006636", "name": "Jennifer Wortman Vaughan"}, {"authorId": "1831395", "name": "H. Wallach"}]}, {"paperId": "9b2e7d31d5d90f6962d4d512834cf25555ee8f05", "url": "https://www.semanticscholar.org/paper/9b2e7d31d5d90f6962d4d512834cf25555ee8f05", "title": "Improved Rooftop Detection in Aerial Images with Machine Learning", "abstract": null, "year": 2003, "referenceCount": 102, "citationCount": 78, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "1715421", "name": "M. Maloof"}, {"authorId": "1713919", "name": "P. Langley"}, {"authorId": "1738591", "name": "T. Binford"}, {"authorId": "144862593", "name": "R. Nevatia"}, {"authorId": "47829236", "name": "S. Sage"}]}, {"paperId": "fee8f63972906214b77f16cfeca0b93ee8f36ba2", "url": "https://www.semanticscholar.org/paper/fee8f63972906214b77f16cfeca0b93ee8f36ba2", "title": "Fairness in Machine Learning: A Survey", "abstract": "As Machine Learning technologies become increasingly used in contexts that affect citizens, companies as well as researchers need to be confident that their application of these methods will not have unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches to mitigating (social) biases and increase fairness in the Machine Learning literature. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, unsupervised learning, and natural language processing is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as four dilemmas for fairness research.", "year": 2020, "referenceCount": 263, "citationCount": 172, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2452441", "name": "Simon Caton"}, {"authorId": "152864672", "name": "C. Haas"}]}, {"paperId": "961d0f3d0c852055a202ef315b02cd69863098e9", "url": "https://www.semanticscholar.org/paper/961d0f3d0c852055a202ef315b02cd69863098e9", "title": "Artificial intelligence, machine learning and deep learning", "abstract": "It is increasingly recognized that artificial intelligence has been touted as a new mobile. Because of the high volume of data that being generated by devices, sensors and social media users, the machine can learn to distinguish the pattern and makes a reasonably good prediction. This article will explore the use of machine learning and its methodologies. Furthermore, the field of deep learning which is being exploited in many leading IT providers will be clarified and discussed.", "year": 2017, "referenceCount": 49, "citationCount": 94, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "31807503", "name": "Pariwat Ongsulee"}]}, {"paperId": "ec01eea9da194367ee762651922256e1e130f948", "url": "https://www.semanticscholar.org/paper/ec01eea9da194367ee762651922256e1e130f948", "title": "Mahout in Action", "abstract": "SummaryMahout in Action is a hands-on introduction to machine learning with Apache Mahout. Following real-world examples, the book presents practical use cases and then illustrates how Mahout can be applied to solve them. Includes a free audio- and video-enhanced ebook. About the TechnologyA computer system that learns and adapts as it collects data can be really powerful. Mahout, Apache's open source machine learning project, captures the core algorithms of recommendation systems, classification, and clustering in ready-to-use, scalable libraries. With Mahout, you can immediately apply to your own projects the machine learning techniques that drive Amazon, Netflix, and others. About this BookThis book covers machine learning using Apache Mahout. Based on experience with real-world applications, it introduces practical use cases and illustrates how Mahout can be applied to solve them. It places particular focus on issues of scalability and how to apply these techniques against large data sets using the Apache Hadoop framework.This book is written for developers familiar with Java - no prior experience with Mahout is assumed. What's InsideUse group data to make individual recommendations Find logical clusters within your data Filter and refine with on-the-fly classification Free audio and video extrasTable of ContentsMeet Apache Mahout PART 1 RECOMMENDATIONS Introducing recommenders Representing recommender data Making recommendations Taking recommenders to production Distributing recommendation computations PART 2 CLUSTERING Introduction to clustering Representing data Clustering algorithms in Mahout Evaluating and improving clustering quality Taking clustering to production Real-world applications of clustering PART 3 CLASSIFICATION Introduction to classification Training a classifier Evaluating and tuning a classifier Deploying a classifier Case study: Shop It To Me", "year": 2011, "referenceCount": 8, "citationCount": 422, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2060860697", "name": "Sean Owen"}, {"authorId": "2066132164", "name": "Robin Anil"}, {"authorId": "115377844", "name": "T. Dunning"}, {"authorId": "98009014", "name": "Ellen Friedman"}]}, {"paperId": "7715bb1070691455d1fcfc6346ff458dbca77b2c", "url": "https://www.semanticscholar.org/paper/7715bb1070691455d1fcfc6346ff458dbca77b2c", "title": "An Efficient Explanation of Individual Classifications using Game Theory", "abstract": "We present a general method for explaining individual predictions of classification models. The method is based on fundamental concepts from coalitional game theory and predictions are explained with contributions of individual feature values. We overcome the method's initial exponential time complexity with a sampling-based approximation. In the experimental part of the paper we use the developed method on models generated by several well-known machine learning algorithms on both synthetic and real-world data sets. The results demonstrate that the method is efficient and that the explanations are intuitive and useful.", "year": 2010, "referenceCount": 30, "citationCount": 404, "influentialCitationCount": 45, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1738288", "name": "E. \u0160trumbelj"}, {"authorId": "143986204", "name": "I. Kononenko"}]}, {"paperId": "f63487b3fda2d96d8b3e97391448c76e00f2353c", "url": "https://www.semanticscholar.org/paper/f63487b3fda2d96d8b3e97391448c76e00f2353c", "title": "Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers", "abstract": "Machine Learning (ML) algorithms are used to train computers to perform a variety of complex tasks and improve with experience. Computers learn how to recognize patterns, make unintended decisions, or react to a dynamic environment. Certain trained machines may be more effective than others because they are based on more suitable ML algorithms or because they were trained through superior training sets. Although ML algorithms are known and publicly released, training sets may not be reasonably ascertainable and, indeed, may be guarded as trade secrets. While much research has been performed about the privacy of the elements of training sets, in this paper we focus our attention on ML classifiers and on the statistical information that can be unconsciously or maliciously revealed from them. We show that it is possible to infer unexpected but useful information from ML classifiers. In particular, we build a novel meta-classifier and train it to hack other classifiers, obtaining meaningful information about their training sets. This kind of information leakage can be exploited, for example, by a vendor to build more effective classifiers or to simply acquire trade secrets from a competitor's apparatus, potentially violating its intellectual property rights.", "year": 2013, "referenceCount": 88, "citationCount": 316, "influentialCitationCount": 34, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1700850", "name": "G. Ateniese"}, {"authorId": "1686516", "name": "L. Mancini"}, {"authorId": "2549445", "name": "A. Spognardi"}, {"authorId": "31637585", "name": "Antonio Villani"}, {"authorId": "2776964", "name": "Domenico Vitali"}, {"authorId": "152837742", "name": "G. Felici"}]}, {"paperId": "1a57fe1d21957c500cdf9abc19eaf97c54d294aa", "url": "https://www.semanticscholar.org/paper/1a57fe1d21957c500cdf9abc19eaf97c54d294aa", "title": "Applications of Artificial Intelligence in Machine Learning: Review and Prospect", "abstract": "Machine learning is one of the most exciting recent technologies in Artificial Intelligence. Learning algorithms in many applications that\u2019s we make use of daily. Every time a web search engine like Google or Bing is used to search the internet, one of the reasons that works so well is because a learning algorithm, one implemented by Google or Microsoft, has learned how to rank web pages. Every time Facebook is used and it recognizes friends' photos, that's also machine learning. Spam filters in email saves the user from having to wade through tons of spam email, that's also a learning algorithm. In this paper, a brief review and future prospect of the vast applications of machine learning has been made.", "year": 2015, "referenceCount": 61, "citationCount": 133, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48144656", "name": "S. Das"}, {"authorId": "51053725", "name": "Aritra Dey"}, {"authorId": "23707007", "name": "A. Pal"}, {"authorId": "7695838", "name": "N. Roy"}]}, {"paperId": "f9613c28ffed1ed9f3a9b5ce45a87192904f05f7", "url": "https://www.semanticscholar.org/paper/f9613c28ffed1ed9f3a9b5ce45a87192904f05f7", "title": "Introduction to Deep Learning", "abstract": "Modeling data via artificial neural networks (ANN) is not a new concept. Most of the underlying techniques have been known since the 1940s. It has to be pointed out though that a series of recent advances in how the networks are trained and utilized form the foundation of today's Deep Learning ecosystem. It is a fair statement that the recent advances brought forward by Deep Learning reflect a new era in Machine Learning (ML) that revolutionized many domains of signal and information processing. This holds true beyond the commonly discussed speech and object recognition science but also branches into computer vision, natural language processing, or information retrieval related fields.", "year": 2018, "referenceCount": 29, "citationCount": 147, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2066657497", "name": "M. Pantoja"}, {"authorId": "65806742", "name": "A. Behrouzi"}, {"authorId": "96710633", "name": "D. Fabris"}]}, {"paperId": "9815ac82b583ea50c03d4791a88520515b22e024", "url": "https://www.semanticscholar.org/paper/9815ac82b583ea50c03d4791a88520515b22e024", "title": "Mapping Species Distributions: Spatial Inference and Prediction", "abstract": "Part I. History and Ecological Basis of Species' Distribution Modeling: 1. Species distribution modeling 2. Why do we need species' distribution models? 3. Ecological understanding of species' distributions Part II. The Data Needed for Modeling Species' Distributions 4. Data for species' distribution models: the biological data 5. Data for species' distribution models: the environmental data Part III. An Overview of the Modeling Methods: 6. Statistical models - modern regression 7. Machine learning methods 8. Classification, similarity and other methods for presence-only data Part IV. Model Evaluation and Implementation: 9. Model evaluation 10. Implementation of species' distribution models.", "year": 2010, "referenceCount": 0, "citationCount": 1937, "influentialCitationCount": 248, "isOpenAccess": false, "fieldsOfStudy": ["Biology"], "authors": [{"authorId": "144735103", "name": "J. Franklin"}, {"authorId": "2109506351", "name": "Jennifer A. Miller"}]}, {"paperId": "7a0ae56fe2f5434fc1de4969867881a65680ab82", "url": "https://www.semanticscholar.org/paper/7a0ae56fe2f5434fc1de4969867881a65680ab82", "title": "Learning to see", "abstract": "The mind cannot make sense of the visual world out of raw image data alone. In an approach to visual processing known as learning from examples, computational neural networks and physiological studies suggest how neurons and machines adapt to novel images on the basis of past experience.", "year": 1996, "referenceCount": 6, "citationCount": 318, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1685292", "name": "T. Poggio"}, {"authorId": "1740300", "name": "D. Beymer"}]}, {"paperId": "c9742f2fcf29f759f02ec0446c019eec5372a771", "url": "https://www.semanticscholar.org/paper/c9742f2fcf29f759f02ec0446c019eec5372a771", "title": "On Kernel Methods for Relational Learning", "abstract": "Kernel methods have gained a great deal of popularity in the machine learning community as a method to learn indirectly in high-dimensional feature spaces. Those interested in relational learning have recently begun to cast learning from structured and relational data in terms of kernel operations. \n \nWe describe a general family of kernel functions built up from a description language of limited expressivity and use it to study the benefits and drawbacks of kernel learning in relational domains. Learning with kernels in this family directly models learning over an expanded feature space constructed using the same description language. This allows us to examine issues of time complexity in terms of learning with these and other relational kernels, and how these relate to generalization ability. The tradeoffs between using kernels in a very high dimensional implicit space versus a restricted feature space, is highlighted through two experiments, in bioinformatics and in natural language processing.", "year": 2003, "referenceCount": 28, "citationCount": 151, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2019029", "name": "Chad M. Cumby"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "9ae9d2b060e50094be7e473e449f192403019225", "url": "https://www.semanticscholar.org/paper/9ae9d2b060e50094be7e473e449f192403019225", "title": "Multitask learning and benchmarking with clinical time series data", "abstract": null, "year": 2017, "referenceCount": 94, "citationCount": 502, "influentialCitationCount": 86, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science", "Biology", "Medicine"], "authors": [{"authorId": "144787340", "name": "Hrayr Harutyunyan"}, {"authorId": "1729527", "name": "Hrant Khachatrian"}, {"authorId": "2107807", "name": "David C. Kale"}, {"authorId": "143728483", "name": "A. Galstyan"}]}, {"paperId": "033f7570be9877c5a4bcbb71f6aec8f95cee3608", "url": "https://www.semanticscholar.org/paper/033f7570be9877c5a4bcbb71f6aec8f95cee3608", "title": "To understand deep learning we need to understand kernel learning", "abstract": "Generalization performance of classifiers in deep learning has recently become a subject of intense study. Deep models, typically over-parametrized, tend to fit the training data exactly. Despite this \"overfitting\", they perform well on test data, a phenomenon not yet fully understood. \nThe first point of our paper is that strong performance of overfitted classifiers is not a unique feature of deep learning. Using six real-world and two synthetic datasets, we establish experimentally that kernel machines trained to have zero classification or near zero regression error perform very well on test data, even when the labels are corrupted with a high level of noise. We proceed to give a lower bound on the norm of zero loss solutions for smooth kernels, showing that they increase nearly exponentially with data size. We point out that this is difficult to reconcile with the existing generalization bounds. Moreover, none of the bounds produce non-trivial results for interpolating solutions. \nSecond, we show experimentally that (non-smooth) Laplacian kernels easily fit random labels, a finding that parallels results for ReLU neural networks. In contrast, fitting noisy data requires many more epochs for smooth Gaussian kernels. Similar performance of overfitted Laplacian and Gaussian classifiers on test, suggests that generalization is tied to the properties of the kernel function rather than the optimization process. \nCertain key phenomena of deep learning are manifested similarly in kernel methods in the modern \"overfitted\" regime. The combination of the experimental and theoretical results presented in this paper indicates a need for new theoretical ideas for understanding properties of classical kernel methods. We argue that progress on understanding deep learning will be difficult until more tractable \"shallow\" kernel methods are better understood.", "year": 2018, "referenceCount": 57, "citationCount": 296, "influentialCitationCount": 27, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "145520115", "name": "Mikhail Belkin"}, {"authorId": "143791100", "name": "Siyuan Ma"}, {"authorId": "151213231", "name": "Soumik Mandal"}]}, {"paperId": "b96472233dbe7ec745bcb4304d5de86a2ca03778", "url": "https://www.semanticscholar.org/paper/b96472233dbe7ec745bcb4304d5de86a2ca03778", "title": "Combining linguistic and machine learning techniques for email summarization", "abstract": "This paper shows that linguistic techniques along with machine learning can extract high quality noun phrases for the purpose of providing the gist or summary of email messages. We describe a set of comparative experiments using several machine learning algorithms for the task of salient noun phrase extraction. Three main conclusions can be drawn from this study: (i) the modifiers of a noun phrase can be semantically as important as the head for the task of gisting, (ii) linguistic filtering improves the performance of machine learning algorithms, (iii) a combination of classifiers improves accuracy.", "year": 2001, "referenceCount": 40, "citationCount": 66, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2295928", "name": "S. Muresan"}, {"authorId": "2601166", "name": "E. Tzoukermann"}, {"authorId": "1761739", "name": "Judith L. Klavans"}]}, {"paperId": "475eb84d8e056aeec4b994e94df2be50fa38dfbc", "url": "https://www.semanticscholar.org/paper/475eb84d8e056aeec4b994e94df2be50fa38dfbc", "title": "GraphLab: A New Parallel Framework for Machine Learning", "abstract": null, "year": 2010, "referenceCount": 0, "citationCount": 469, "influentialCitationCount": 55, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1680638", "name": "Y. Low"}, {"authorId": "2119113835", "name": "Joseph Gonzalez"}, {"authorId": "1717990", "name": "Aapo Kyrola"}, {"authorId": "1741745", "name": "D. Bickson"}, {"authorId": "1730156", "name": "Carlos Guestrin"}, {"authorId": "1695576", "name": "J. Hellerstein"}]}, {"paperId": "892edbdb6e424cd8c1d6f23a2c9d77fffef7d45c", "url": "https://www.semanticscholar.org/paper/892edbdb6e424cd8c1d6f23a2c9d77fffef7d45c", "title": "Machine Learning Approach to Modeling Sediment Transport", "abstract": "Inaccuracies of sediment transport models largely originate from our limitation to describe the process in precise mathematical terms. Machine learning (ML) is an alternative approach to reduce the inaccuracies of sedimentation models. It utilizes available domain knowledge for selecting the input and output variables for the ML models and uses modern regression techniques to fit the measured data. Two ML methods, artificial neural networks and model trees, are adopted to model bed-load and total-load transport using the measured data. The bed-load transport models are compared with the models due to Bagnold, Einstein, Parker et al., and van Rijn. The total-load transport models are compared with the models due to Ackers and White, Bagnold, Engelund and Hansen, and van Rijn. With the chosen data sets on bed-load and total-load transport the ML models provided better accuracy than the existing ones.", "year": 2007, "referenceCount": 45, "citationCount": 177, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "95970828", "name": "B. Bhattacharya"}, {"authorId": "50784215", "name": "R. Price"}, {"authorId": "1895474", "name": "D. Solomatine"}]}, {"paperId": "09c5931307cba3f80d3ecc14d02eecfa46463cfe", "url": "https://www.semanticscholar.org/paper/09c5931307cba3f80d3ecc14d02eecfa46463cfe", "title": "MLPACK: a scalable C++ machine learning library", "abstract": "MLPACK is a state-of-the-art, scalable, multi-platform C++ machine learning library released in late 2011 offering both a simple, consistent API accessible to novice users and high performance and flexibility to expert users by leveraging modern features of C++. MLPACK provides cutting-edge algorithms whose benchmarks exhibit far better performance than other leading machine learning libraries. MLPACK version 1.0.3, licensed under the LGPL, is available at http://www.mlpack.org.", "year": 2012, "referenceCount": 17, "citationCount": 159, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "34658148", "name": "Ryan R. Curtin"}, {"authorId": "3264504", "name": "J. R. Cline"}, {"authorId": "2101842823", "name": "N. P. Slagle"}, {"authorId": "1769646", "name": "William B. March"}, {"authorId": "2944292", "name": "P. Ram"}, {"authorId": "144447714", "name": "Nishant A. Mehta"}, {"authorId": "1703070", "name": "Alexander G. Gray"}]}, {"paperId": "b21917c1980df0fa5a8a74bcd29136cdd9b4b993", "url": "https://www.semanticscholar.org/paper/b21917c1980df0fa5a8a74bcd29136cdd9b4b993", "title": "Active Learning with Support Vector Machine Applied to Gene Expression Data for Cancer Classification", "abstract": "There is growing interest in the application of machine learning techniques in bioinformatics. The supervised machine learning approach has been widely applied to bioinformatics and gained a lot of success in this research area. With this learning approach researchers first develop a large training set, which is a time-consuming and costly process. Moreover, the proportion of the positive examples and negative examples in the training set may not represent the real-world data distribution, which causes concept drift. Active learning avoids these problems. Unlike most conventional learning methods where the training set used to derive the model remains static, the classifier can actively choose the training data and the size of training set increases. We introduced an algorithm for performing active learning with support vector machine and applied the algorithm to gene expression profiles of colon cancer, lung cancer, and prostate cancer samples. We compared the classification performance of active learning with that of passive learning. The results showed that employing the active learning method can achieve high accuracy and significantly reduce the need for labeled training instances. For lung cancer classification, to achieve 96% of the total positives, only 31 labeled examples were needed in active learning whereas in passive learning 174 labeled examples were required. That meant over 82% reduction was realized by active learning. In active learning the areas under the receiver operating characteristic (ROC) curves were over 0.81, while in passive learning the areas under the ROC curves were below 0.50.", "year": 2004, "referenceCount": 32, "citationCount": 177, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "49421151", "name": "Y. Liu"}]}, {"paperId": "971766088dfaf63fb55e6f0190b14f28f2c98ad0", "url": "https://www.semanticscholar.org/paper/971766088dfaf63fb55e6f0190b14f28f2c98ad0", "title": "A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection", "abstract": "This survey paper describes a focused literature survey of machine learning (ML) and data mining (DM) methods for cyber analytics in support of intrusion detection. Short tutorial descriptions of each ML/DM method are provided. Based on the number of citations or the relevance of an emerging method, papers representing each method were identified, read, and summarized. Because data are so important in ML/DM approaches, some well-known cyber data sets used in ML/DM are described. The complexity of ML/DM algorithms is addressed, discussion of challenges for using ML/DM for cyber security is presented, and some recommendations on when to use a given method are provided.", "year": 2016, "referenceCount": 116, "citationCount": 862, "influentialCitationCount": 42, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2343019", "name": "A. Buczak"}, {"authorId": "1922657", "name": "Erhan Guven"}]}, {"paperId": "76cb5a926a789e1f28035fcc7fa01d5900df6d0e", "url": "https://www.semanticscholar.org/paper/76cb5a926a789e1f28035fcc7fa01d5900df6d0e", "title": "Perceptron-based learning algorithms", "abstract": "A key task for connectionist research is the development and analysis of learning algorithms. An examination is made of several supervised learning algorithms for single-cell and network models. The heart of these algorithms is the pocket algorithm, a modification of perceptron learning that makes perceptron learning well-behaved with nonseparable training data, even if the data are noisy and contradictory. Features of these algorithms include speed algorithms fast enough to handle large sets of training data; network scaling properties, i.e. network methods scale up almost as well as single-cell models when the number of inputs is increased; analytic tractability, i.e. upper bounds on classification error are derivable; online learning, i.e. some variants can learn continually, without referring to previous data; and winner-take-all groups or choice groups, i.e. algorithms can be adapted to select one out of a number of possible classifications. These learning algorithms are suitable for applications in machine learning, pattern recognition, and connectionist expert systems.", "year": 1990, "referenceCount": 26, "citationCount": 519, "influentialCitationCount": 33, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "2889692", "name": "S. I. Gallant"}]}, {"paperId": "22adb2413901b74128f2a02584dafa77afbd8d60", "url": "https://www.semanticscholar.org/paper/22adb2413901b74128f2a02584dafa77afbd8d60", "title": "Highly Accurate Machine Fault Diagnosis Using Deep Transfer Learning", "abstract": "We develop a novel deep learning framework to achieve highly accurate machine fault diagnosis using transfer learning to enable and accelerate the training of deep neural network. Compared with existing methods, the proposed method is faster to train and more accurate. First, original sensor data are converted to images by conducting a Wavelet transformation to obtain time-frequency distributions. Next, a pretrained network is used to extract lower level features. The labeled time-frequency images are then used to fine-tune the higher levels of the neural network architecture. This paper creates a machine fault diagnosis pipeline and experiments are carried out to verify the effectiveness and generalization of the pipeline on three main mechanical datasets including induction motors, gearboxes, and bearings with sizes of 6000, 9000, and 5000 time series samples, respectively. We achieve state-of-the-art results on each dataset, with most datasets showing test accuracy near 100%, and in the gearbox dataset, we achieve significant improvement from 94.8% to 99.64%. We created a repository including these datasets located at mlmechanics.ics.uci.edu.", "year": 2019, "referenceCount": 0, "citationCount": 422, "influentialCitationCount": 24, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9122533", "name": "Siyu Shao"}, {"authorId": "143953836", "name": "S. McAleer"}, {"authorId": "35374692", "name": "Ruqiang Yan"}, {"authorId": "144902513", "name": "P. Baldi"}]}, {"paperId": "799f927692a6c08c5e630bea78c087c5051528fc", "url": "https://www.semanticscholar.org/paper/799f927692a6c08c5e630bea78c087c5051528fc", "title": "Self-Directed Learning", "abstract": "A widely advocated idea in education is that people learn better when the flow of experience is under their control (i.e., learning is self-directed). However, the reasons why volitional control might result in superior acquisition and the limits to such advantages remain poorly understood. In this article, we review the issue from both a cognitive and computational perspective. On the cognitive side, self-directed learning allows individuals to focus effort on useful information they do not yet possess, can expose information that is inaccessible via passive observation, and may enhance the encoding and retention of materials. On the computational side, the development of efficient \u201cactive learning\u201d algorithms that can select their own training data is an emerging research topic in machine learning. This review argues that recent advances in these related fields may offer a fresh theoretical perspective on how people gather information to support their own learning.", "year": 2012, "referenceCount": 161, "citationCount": 567, "influentialCitationCount": 56, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "3013567", "name": "T. Gureckis"}, {"authorId": "3164019", "name": "D. Markant"}]}, {"paperId": "cb2c9086eb239132a49f695f5b93b5bdb391ad92", "url": "https://www.semanticscholar.org/paper/cb2c9086eb239132a49f695f5b93b5bdb391ad92", "title": "Yes, Machine Learning Can Be More Secure! A Case Study on Android Malware Detection", "abstract": "To cope with the increasing variability and sophistication of modern attacks, machine learning has been widely adopted as a statistically-sound tool for malware detection. However, its security against well-crafted attacks has not only been recently questioned, but it has been shown that machine learning exhibits inherent vulnerabilities that can be exploited to evade detection at test time. In other words, machine learning itself can be the weakest link in a security system. In this paper, we rely upon a previously-proposed attack framework to categorize potential attack scenarios against learning-based malware detection tools, by modeling attackers with different skills and capabilities. We then define and implement a set of corresponding evasion attacks to thoroughly assess the security of Drebin, an Android malware detector. The main contribution of this work is the proposal of a simple and scalable secure-learning paradigm that mitigates the impact of evasion attacks, while only slightly worsening the detection rate in the absence of attack. We finally argue that our secure-learning approach can also be readily applied to other malware detection tasks.", "year": 2017, "referenceCount": 50, "citationCount": 216, "influentialCitationCount": 24, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2272441", "name": "Ambra Demontis"}, {"authorId": "40631285", "name": "Marco Melis"}, {"authorId": "1684175", "name": "B. Biggio"}, {"authorId": "3248803", "name": "Davide Maiorca"}, {"authorId": "79405068", "name": "Dan Arp"}, {"authorId": "144825749", "name": "K. Rieck"}, {"authorId": "2338858", "name": "I. Corona"}, {"authorId": "1779484", "name": "G. Giacinto"}, {"authorId": "1710171", "name": "F. Roli"}]}, {"paperId": "1f2b459bb644fcaf61118dbf31db5856f9f991de", "url": "https://www.semanticscholar.org/paper/1f2b459bb644fcaf61118dbf31db5856f9f991de", "title": "Machine Learning: ECML 2002", "abstract": null, "year": 2002, "referenceCount": 0, "citationCount": 116, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1720466", "name": "Tapio Elomaa"}, {"authorId": "1712654", "name": "H. Mannila"}, {"authorId": "143785973", "name": "Hannu (TT) Toivonen"}]}, {"paperId": "96752dc3a8c3edff8ef69b9c0fdeff9bde2b48f1", "url": "https://www.semanticscholar.org/paper/96752dc3a8c3edff8ef69b9c0fdeff9bde2b48f1", "title": "Named Entity Recognition: A Maximum Entropy Approach Using Global Information", "abstract": "This paper presents a maximum entropy-based named entity recognizer (NER). It differs from previous machine learning-based NERs in that it uses information from the whole document to classify each word, with just one classifier. Previous work that involves the gathering of information from the whole document often uses a secondary classifier, which corrects the mistakes of a primary sentence-based classifier. In this paper, we show that the maximum entropy framework is able to make use of global information directly, and achieves performance that is comparable to the best previous machine learning-based NERs on MUC-6 and MUC-7 test data.", "year": 2002, "referenceCount": 13, "citationCount": 328, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3024630", "name": "Hai Leong Chieu"}, {"authorId": "34789794", "name": "H. Ng"}]}, {"paperId": "53a95417d35d9f11b00c39ecd1f1809a98b20707", "url": "https://www.semanticscholar.org/paper/53a95417d35d9f11b00c39ecd1f1809a98b20707", "title": "XCS and GALE: A Comparative Study of Two Learning Classifier Systems on Data Mining", "abstract": null, "year": 2001, "referenceCount": 46, "citationCount": 124, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1399458171", "name": "Ester Bernad\u00f3-Mansilla"}, {"authorId": "1822628", "name": "Xavier Llor\u00e0"}, {"authorId": "1690014", "name": "J. M. Guiu"}]}, {"paperId": "3ec6b1aeddce8c350a4f8ea3ba7ffcfd73b57ed2", "url": "https://www.semanticscholar.org/paper/3ec6b1aeddce8c350a4f8ea3ba7ffcfd73b57ed2", "title": "High-Dimensional Statistics", "abstract": "Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.", "year": 2019, "referenceCount": 410, "citationCount": 548, "influentialCitationCount": 93, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1721860", "name": "M. Wainwright"}]}, {"paperId": "e78a35a75dc10f366da57b490679bb38a46da531", "url": "https://www.semanticscholar.org/paper/e78a35a75dc10f366da57b490679bb38a46da531", "title": "Incremental Learning from Noisy Data", "abstract": null, "year": 1986, "referenceCount": 34, "citationCount": 244, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2620923", "name": "J. C. Schlimmer"}, {"authorId": "144297714", "name": "R. Granger"}]}, {"paperId": "2f178781b6e1a05df8840a90c56ef1e0ea3c961d", "url": "https://www.semanticscholar.org/paper/2f178781b6e1a05df8840a90c56ef1e0ea3c961d", "title": "Model learning", "abstract": "Model learning emerges as an effective method for black-box state machine models of hardware and software components.", "year": 2017, "referenceCount": 91, "citationCount": 176, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1678787", "name": "F. Vaandrager"}]}, {"paperId": "8cec80e8752952188dba7ec407366e0962e3d4a6", "url": "https://www.semanticscholar.org/paper/8cec80e8752952188dba7ec407366e0962e3d4a6", "title": "Co-EM support vector learning", "abstract": "Multi-view algorithms, such as co-training and co-EM, utilize unlabeled data when the available attributes can be split into independent and compatible subsets. Co-EM outperforms co-training for many problems, but it requires the underlying learner to estimate class probabilities, and to learn from probabilistically labeled data. Therefore, co-EM has so far only been studied with naive Bayesian learners. We cast linear classifiers into a probabilistic framework and develop a co-EM version of the Support Vector Machine. We conduct experiments on text classification problems and compare the family of semi-supervised support vector algorithms under different conditions, including violations of the assumptions underlying multi-view learning. For some problems, such as course web page classification, we observe the most accurate results reported so far.", "year": 2004, "referenceCount": 30, "citationCount": 201, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1765114", "name": "Ulf Brefeld"}, {"authorId": "1751348", "name": "T. Scheffer"}]}, {"paperId": "b89458a474fa27c2f399d93b08ba24855b75e4df", "url": "https://www.semanticscholar.org/paper/b89458a474fa27c2f399d93b08ba24855b75e4df", "title": "Domain Adaptation Extreme Learning Machines for Drift Compensation in E-Nose Systems", "abstract": "This paper addresses an important issue known as sensor drift, which exhibits a nonlinear dynamic property in electronic nose (E-nose), from the viewpoint of machine learning. Traditional methods for drift compensation are laborious and costly owing to the frequent acquisition and labeling process for gas samples' recalibration. Extreme learning machines (ELMs) have been confirmed to be efficient and effective learning techniques for pattern recognition and regression. However, ELMs primarily focus on the supervised, semisupervised, and unsupervised learning problems in single domain (i.e., source domain). To our best knowledge, ELM with cross-domain learning capability has never been studied. This paper proposes a unified framework called domain adaptation extreme learning machine (DAELM), which learns a robust classifier by leveraging a limited number of labeled data from target domain for drift compensation as well as gas recognition in E-nose systems, without losing the computational efficiency and learning ability of traditional ELM. In the unified framework, two algorithms called source DAELM (DAELM-S) and target DAELM (DAELM-T) are proposed in this paper. In order to perceive the differences among ELM, DAELM-S, and DAELM-T, two remarks are provided. Experiments on the popular sensor drift data with multiple batches collected using E-nose system clearly demonstrate that the proposed DAELM significantly outperforms existing drift-compensation methods without cumbersome measures, and also bring new perspectives for ELM.", "year": 2015, "referenceCount": 37, "citationCount": 237, "influentialCitationCount": 25, "isOpenAccess": true, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "36685537", "name": "Lei Zhang"}, {"authorId": "66396175", "name": "D. Zhang"}]}, {"paperId": "eec5eec17b214799ba1ca27c5917b28e92cd444c", "url": "https://www.semanticscholar.org/paper/eec5eec17b214799ba1ca27c5917b28e92cd444c", "title": "A statistical learning learning model of text classification for support vector machines", "abstract": "This paper develops a theoretical learning model of text classification for Support Vector Machines (SVMs). It connects the statistical properties of text-classification tasks with the generalization performance of a SVM in a quantitative way. Unlike conventional approaches to learning text classifiers, which rely primarily on empirical evidence, this model explains why and when SVMs perform well for text classification. In particular, it addresses the following questions: Why can support vector machines handle the large feature spaces in text classification effectively? How is this related to the statistical properties of text? What are sufficient conditions for applying SVMs to text-classification problems successfully?", "year": 2001, "referenceCount": 22, "citationCount": 229, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1680188", "name": "T. Joachims"}]}, {"paperId": "cabd6bbdd2be9996dd7473185c9eaf104980fe21", "url": "https://www.semanticscholar.org/paper/cabd6bbdd2be9996dd7473185c9eaf104980fe21", "title": "An Efficient Approach for Assessing Hyperparameter Importance", "abstract": "The performance of many machine learning methods depends critically on hyperparameter settings. Sophisticated Bayesian optimization methods have recently achieved considerable successes in optimizing these hyperparameters, in several cases surpassing the performance of human experts. However, blind reliance on such methods can leave end users without insight into the relative importance of different hyperparameters and their interactions. This paper describes efficient methods that can be used to gain such insight, leveraging random forest models fit on the data already gathered by Bayesian optimization. We first introduce a novel, linear-time algorithm for computing marginals of random forest predictions and then show how to leverage these predictions within a functional ANOVA framework, to quantify the importance of both single hyperparameters and of interactions between hyperparameters. We conducted experiments with prominent machine learning frameworks and state-of-the-art solvers for combinatorial problems. We show that our methods provide insight into the relationship between hyperparameter settings and performance, and demonstrate that--even in very highdimensional cases--most performance variation is attributable to just a few hyperparameters.", "year": 2014, "referenceCount": 55, "citationCount": 314, "influentialCitationCount": 23, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144661829", "name": "F. Hutter"}, {"authorId": "2470869", "name": "H. Hoos"}, {"authorId": "1388404060", "name": "Kevin Leyton-Brown"}]}, {"paperId": "d474299d7a51b89a1d7394d426cf881a89b8013d", "url": "https://www.semanticscholar.org/paper/d474299d7a51b89a1d7394d426cf881a89b8013d", "title": "Efficient distribution-free learning of probabilistic concepts", "abstract": "A model of machine learning in which the concept to be learned may exhibit uncertain or probabilistic behavior is investigated. Such probabilistic concepts (or p-concepts) may arise in situations such as weather prediction, where the measured variables and their accuracy are insufficient to determine the outcome with certainty. It is required that learning algorithms be both efficient and general in the sense that they perform well for a wide class of p-concepts and for any distribution over the domain. Many efficient algorithms for learning natural classes of p-concepts are given, and an underlying theory of learning p-concepts is developed in detail.<<ETX>>", "year": 1990, "referenceCount": 28, "citationCount": 433, "influentialCitationCount": 41, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2056642528", "name": "M. Kearns"}, {"authorId": "1716301", "name": "R. Schapire"}]}, {"paperId": "c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3", "url": "https://www.semanticscholar.org/paper/c8e4d8ded0624f13cd7763b8e7a62fe7e36da6d3", "title": "Generalizing from a Few Examples: A Survey on Few-Shot Learning", "abstract": "Machine learning has been highly successful in data-intensive applications but is often hampered when the data set is small. Recently, Few-Shot Learning (FSL) is proposed to tackle this problem. Using prior knowledge, FSL can rapidly generalize to new tasks containing only a few samples with supervised information. In this paper, we conduct a thorough survey to fully understand FSL. Starting from a formal definition of FSL, we distinguish FSL from several relevant machine learning problems. We then point out that the core issue in FSL is that the empirical risk minimized is unreliable. Based on how prior knowledge can be used to handle this core issue, we categorize FSL methods from three perspectives: (i) data, which uses prior knowledge to augment the supervised experience; (ii) model, which uses prior knowledge to reduce the size of the hypothesis space; and (iii) algorithm, which uses prior knowledge to alter the search for the best hypothesis in the given hypothesis space. With this taxonomy, we review and discuss the pros and cons of each category. Promising directions, in the aspects of the FSL problem setups, techniques, applications and theories, are also proposed to provide insights for future research.", "year": 2019, "referenceCount": 193, "citationCount": 838, "influentialCitationCount": 60, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2115793087", "name": "Yaqing Wang"}, {"authorId": "3259992", "name": "Quanming Yao"}, {"authorId": "145193332", "name": "J. Kwok"}, {"authorId": "1726587", "name": "L. Ni"}]}, {"paperId": "a3046db7e02e302fe9661074e238be790f52c3b0", "url": "https://www.semanticscholar.org/paper/a3046db7e02e302fe9661074e238be790f52c3b0", "title": "Feature Extraction, Construction and Selection: A Data Mining Perspective", "abstract": "From the Publisher: \nThe book can be used by researchers and graduate students in machine learning, data mining, and knowledge discovery, who wish to understand techniques of feature extraction, construction and selection for data pre-processing and to solve large size, real-world problems. The book can also serve as a reference book for those who are conducting research about feature extraction, construction and selection, and are ready to meet the exciting challenges ahead of us.", "year": 1998, "referenceCount": 0, "citationCount": 1025, "influentialCitationCount": 26, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2146397025", "name": "Huan Liu"}, {"authorId": "1748072", "name": "H. Motoda"}]}, {"paperId": "ca4b40a6a28d057e74882b16f7b84520e347e37b", "url": "https://www.semanticscholar.org/paper/ca4b40a6a28d057e74882b16f7b84520e347e37b", "title": "Machine Learning", "abstract": null, "year": 2017, "referenceCount": 42, "citationCount": 64, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2571273", "name": "I. E. Naqa"}, {"authorId": "30436861", "name": "M. Murphy"}]}, {"paperId": "d050a486dfaf0d0de7af14a5af5ff5a29a151b67", "url": "https://www.semanticscholar.org/paper/d050a486dfaf0d0de7af14a5af5ff5a29a151b67", "title": "Alleviating naive Bayes attribute independence assumption by attribute weighting", "abstract": "Despite the simplicity of the Naive Bayes classifier, it has continued to perform well against more sophisticated newcomers and has remained, therefore, of great interest to the machine learning community. Of numerous approaches to refining the naive Bayes classifier, attribute weighting has received less attention than it warrants. Most approaches, perhaps influenced by attribute weighting in other machine learning algorithms, use weighting to place more emphasis on highly predictive attributes than those that are less predictive. In this paper, we argue that for naive Bayes attribute weighting should instead be used to alleviate the conditional independence assumption. Based on this premise, we propose a weighted naive Bayes algorithm, called WANBIA, that selects weights to minimize either the negative conditional log likelihood or the mean squared error objective functions. We perform extensive evaluations and find that WANBIA is a competitive alternative to state of the art classifiers like Random Forest, Logistic Regression and A1DE.", "year": 2013, "referenceCount": 49, "citationCount": 157, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "33899189", "name": "Nayyar Zaidi"}, {"authorId": "1838956", "name": "J. Cerquides"}, {"authorId": "2170598", "name": "Mark James Carman"}, {"authorId": "1726660", "name": "Geoffrey I. Webb"}]}, {"paperId": "a9c8124b18c0e843878f2df4d49da6f250dc06e7", "url": "https://www.semanticscholar.org/paper/a9c8124b18c0e843878f2df4d49da6f250dc06e7", "title": "Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning", "abstract": "\u00a9 2018 35th International Conference on Machine Learning, ICML 2018. All rights reserved. Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the : data. Using these models we show how to per- \u2217 form and utilize a decomposition of uncertainty in \u2022 aleatoric and epistemic components for decision making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. ' t Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.", "year": 2017, "referenceCount": 42, "citationCount": 200, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3407086", "name": "Stefan Depeweg"}, {"authorId": "1388574431", "name": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"}, {"authorId": "1388372395", "name": "Finale Doshi-Velez"}, {"authorId": "1699265", "name": "S. Udluft"}]}, {"paperId": "78989616eeeac55b202e3e4205225e7135054185", "url": "https://www.semanticscholar.org/paper/78989616eeeac55b202e3e4205225e7135054185", "title": "An Introduction to Deep Learning for the Physical Layer", "abstract": "We present and discuss several novel applications of deep learning for the physical layer. By interpreting a communications system as an autoencoder, we develop a fundamental new way to think about communications system design as an end-to-end reconstruction task that seeks to jointly optimize transmitter and receiver components in a single process. We show how this idea can be extended to networks of multiple transmitters and receivers and present the concept of radio transformer networks as a means to incorporate expert domain knowledge in the machine learning model. Lastly, we demonstrate the application of convolutional neural networks on raw IQ samples for modulation classification which achieves competitive accuracy with respect to traditional schemes relying on expert features. This paper is concluded with a discussion of open challenges and areas for future investigation.", "year": 2017, "referenceCount": 75, "citationCount": 1446, "influentialCitationCount": 140, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1388350203", "name": "Tim O'Shea"}, {"authorId": "1749686", "name": "J. Hoydis"}]}, {"paperId": "1b570ed4b58908444465823880cb88fbb812b4fc", "url": "https://www.semanticscholar.org/paper/1b570ed4b58908444465823880cb88fbb812b4fc", "title": "Evading Machine Learning Malware Detection", "abstract": "Machine learning is a popular approach to signatureless malware detection because it can generalize to never-beforeseen malware families and polymorphic strains. This has resulted in its practical use for either primary detection engines or supplementary heuristic detections by anti-malware vendors. Recent work in adversarial machine learning has shown that models are susceptible to gradient-based and other attacks. In this whitepaper, we summarize the various attacks that have been proposed for machine learning models in information security, each which require the adversary to have some degree of knowledge about the model under attack. Importantly, even when applied to attacking machine learning malware classifier based on static features for Windows portable executable (PE) files, these attacks, previous attack methodologies may break the format or functionality of the malware. We investigate a more general framework for attacking static PE anti-malware engines based on reinforcement learning, which models more realistic attacker conditions, and subsequently has provides much more modest evasion rates. A reinforcement learning (RL) agent is equipped with a set of functionality-preserving operations that it may perform on the PE file. It learns through a series of games played against the anti-malware engine which sequence of operations is most likely to result in evasion for a given malware sample. Given the general framework, it is not surprising that the evasion rates are modest. However, the resulting RL agent can succinctly summarize blind spots of the anti-malware model. Additionally, evasive variants generated by the agent may be used to harden machine learning anti-malware engine via adversarial training.", "year": 2017, "referenceCount": 25, "citationCount": 111, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": null, "authors": [{"authorId": "2639880", "name": "H. Anderson"}]}, {"paperId": "5fea87373f3e21ffd6d277474f5d707f5f4f422a", "url": "https://www.semanticscholar.org/paper/5fea87373f3e21ffd6d277474f5d707f5f4f422a", "title": "Machine learning approach for text and document mining", "abstract": "Text Categorization (TC), also known as Text Classification, is the task of automatically classifying a set of text documents into different categories from a predefined set. If a document belongs to exactly one of the categories, it is a single-label classification task; otherwise, it is a multi-label classification task. TC uses several tools from Information Retrieval (IR) and Machine Learning (ML) and has received much attention in the last years from both researchers in the academia and industry developers. In this paper, we first categorize the documents using KNN based machine learning approach and then return the most relevant documents.", "year": 2014, "referenceCount": 24, "citationCount": 227, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2859448", "name": "Vishwanath Bijalwan"}, {"authorId": "2132235638", "name": "P. Kumari"}, {"authorId": "34970250", "name": "Jord\u00e1n Pascual Espada"}, {"authorId": "2408385", "name": "V. B. Semwal"}]}, {"paperId": "58532d01140efd38c4d911e984e0845539d2db39", "url": "https://www.semanticscholar.org/paper/58532d01140efd38c4d911e984e0845539d2db39", "title": "SFO: A Toolbox for Submodular Function Optimization", "abstract": "In recent years, a fundamental problem structure has emerged as very useful in a variety of machine learning applications: Submodularity is an intuitive diminishing returns property, stating that adding an element to a smaller set helps more than adding it to a larger set. Similarly to convexity, submodularity allows one to efficiently find provably (near-) optimal solutions for large problems. We present SFO, a toolbox for use in MATLAB or Octave that implements algorithms for minimization and maximization of submodular functions. A tutorial script illustrates the application of submodularity to machine learning and AI problems such as feature selection, clustering, inference and optimized information gathering.", "year": 2010, "referenceCount": 16, "citationCount": 103, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "145343838", "name": "Andreas Krause"}]}, {"paperId": "34fa2515a259b4ff2aecb100a35e56019d62cd66", "url": "https://www.semanticscholar.org/paper/34fa2515a259b4ff2aecb100a35e56019d62cd66", "title": "Hyperparameter Optimization", "abstract": null, "year": 2019, "referenceCount": 170, "citationCount": 84, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2868444", "name": "Matthias Feurer"}, {"authorId": "144661829", "name": "F. Hutter"}]}, {"paperId": "d4849d29b27a8e5230edafe4cd4cdda395fb24d4", "url": "https://www.semanticscholar.org/paper/d4849d29b27a8e5230edafe4cd4cdda395fb24d4", "title": "Meta optimization: improving compiler heuristics with machine learning", "abstract": "Compiler writers have crafted many heuristics over the years to approximately solve NP-hard problems efficiently. Finding a heuristic that performs well on a broad range of applications is a tedious and difficult process. This paper introduces Meta Optimization, a methodology for automatically fine-tuning compiler heuristics. Meta Optimization uses machine-learning techniques to automatically search the space of compiler heuristics. Our techniques reduce compiler design complexity by relieving compiler writers of the tedium of heuristic tuning. Our machine-learning system uses an evolutionary algorithm to automatically find effective compiler heuristics. We present promising experimental results. In one mode of operation Meta Optimization creates application-specific heuristics which often result in impressive speedups. For hyperblock formation, one optimization we present in this paper, we obtain an average speedup of 23% (up to 73%) for the applications in our suite. Furthermore, by evolving a compiler's heuristic over several benchmarks, we can create effective, general-purpose heuristics. The best general-purpose heuristic our system found for hyperblock formation improved performance by an average of 25% on our training set, and 9% on a completely unrelated test set. We demonstrate the efficacy of our techniques on three different optimizations in this paper: hyperblock formation, register allocation, and data prefetching.", "year": 2003, "referenceCount": 25, "citationCount": 306, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1652011517", "name": "M. Stephenson"}, {"authorId": "1709150", "name": "Saman P. Amarasinghe"}, {"authorId": "150220413", "name": "M. Martin"}, {"authorId": "1401846156", "name": "U. O'Reilly"}]}, {"paperId": "819167ace2f0caae7745d2f25a803979be5fbfae", "url": "https://www.semanticscholar.org/paper/819167ace2f0caae7745d2f25a803979be5fbfae", "title": "The Limitations of Deep Learning in Adversarial Settings", "abstract": "Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97% adversarial success rate while only modifying on average 4.02% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.", "year": 2015, "referenceCount": 40, "citationCount": 2879, "influentialCitationCount": 398, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1967156", "name": "Nicolas Papernot"}, {"authorId": "144061974", "name": "P. Mcdaniel"}, {"authorId": "1680133", "name": "S. Jha"}, {"authorId": "2623167", "name": "Matt Fredrikson"}, {"authorId": "144643812", "name": "Z. B. Celik"}, {"authorId": "144231976", "name": "A. Swami"}]}, {"paperId": "18dd17656c42322e943deaedc4eee6400debe7c2", "url": "https://www.semanticscholar.org/paper/18dd17656c42322e943deaedc4eee6400debe7c2", "title": "FedML: A Research Library and Benchmark for Federated Machine Learning", "abstract": "Federated learning is a rapidly growing research field in the machine learning domain. Although considerable research efforts have been made, existing libraries cannot adequately support diverse algorithmic development (e.g., diverse topology and flexible message exchange), and inconsistent dataset and model usage in experiments make fair comparisons difficult. In this work, we introduce FedML, an open research library and benchmark that facilitates the development of new federated learning algorithms and fair performance comparisons. FedML supports three computing paradigms (distributed training, mobile on-device training, and standalone simulation) for users to conduct experiments in different system environments. FedML also promotes diverse algorithmic research with flexible and generic API design and reference baseline implementations. A curated and comprehensive benchmark dataset for the non-I.I.D setting aims at making a fair comparison. We believe FedML can provide an efficient and reproducible means of developing and evaluating algorithms for the federated learning research community. We maintain the source code, documents, and user community at this https URL.", "year": 2020, "referenceCount": 161, "citationCount": 207, "influentialCitationCount": 33, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "31927890", "name": "Chaoyang He"}, {"authorId": "2239461", "name": "Songze Li"}, {"authorId": "144491689", "name": "Jinhyun So"}, {"authorId": "2116031156", "name": "Mi Zhang"}, {"authorId": "2109798334", "name": "Hongyi Wang"}, {"authorId": "2118775509", "name": "Xiaoyang Wang"}, {"authorId": "2927870", "name": "Praneeth Vepakomma"}, {"authorId": "2034349211", "name": "Abhishek Singh"}, {"authorId": "49660254", "name": "Han Qiu"}, {"authorId": "2144035454", "name": "Li Shen"}, {"authorId": "144259957", "name": "P. Zhao"}, {"authorId": "1505828520", "name": "Yan Kang"}, {"authorId": "1614034792", "name": "Yang Liu"}, {"authorId": "145711633", "name": "R. Raskar"}, {"authorId": "153096457", "name": "Qiang Yang"}, {"authorId": "145599558", "name": "M. Annavaram"}, {"authorId": "121011351", "name": "S. Avestimehr"}]}, {"paperId": "154887a2a15e6f6ea2a26c2bfdf1d755a434f472", "url": "https://www.semanticscholar.org/paper/154887a2a15e6f6ea2a26c2bfdf1d755a434f472", "title": "Prototype vector machine for large scale semi-supervised learning", "abstract": "Practical data mining rarely falls exactly into the supervised learning scenario. Rather, the growing amount of unlabeled data poses a big challenge to large-scale semi-supervised learning (SSL). We note that the computational intensiveness of graph-based SSL arises largely from the manifold or graph regularization, which in turn lead to large models that are difficult to handle. To alleviate this, we proposed the prototype vector machine (PVM), a highly scalable, graph-based algorithm for large-scale SSL. Our key innovation is the use of \"prototypes vectors\" for efficient approximation on both the graph-based regularizer and model representation. The choice of prototypes are grounded upon two important criteria: they not only perform effective low-rank approximation of the kernel matrix, but also span a model suffering the minimum information loss compared with the complete model. We demonstrate encouraging performance and appealing scaling properties of the PVM on a number of machine learning benchmark data sets.", "year": 2009, "referenceCount": 24, "citationCount": 95, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2152986588", "name": "Kai Zhang"}, {"authorId": "145193332", "name": "J. Kwok"}, {"authorId": "35170576", "name": "B. Parvin"}]}, {"paperId": "d411a4a89f6648198dc171eadcad654921426ddc", "url": "https://www.semanticscholar.org/paper/d411a4a89f6648198dc171eadcad654921426ddc", "title": "Robust extreme learning machine", "abstract": null, "year": 2013, "referenceCount": 36, "citationCount": 133, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2858250", "name": "Punyaphol Horata"}, {"authorId": "2376043", "name": "S. Chiewchanwattana"}, {"authorId": "1888267", "name": "K. Sunat"}]}, {"paperId": "3ced109e0c16014cf4cdd677aeb1b95acf1c35d6", "url": "https://www.semanticscholar.org/paper/3ced109e0c16014cf4cdd677aeb1b95acf1c35d6", "title": "How to represent crystal structures for machine learning: Towards fast prediction of electronic properties", "abstract": "High-throughput density functional calculations of solids are highly time-consuming. As an alternative, we propose a machine learning approach for the fast prediction of solid-state properties. To achieve this, local spin-density approximation calculations are used as a training set. We focus on predicting the value of the density of electronic states at the Fermi energy. We find that conventional representations of the input data, such as the Coulomb matrix, are not suitable for the training of learning machines in the case of periodic solids. We propose a novel crystal structure representation for which learning and competitive prediction accuracies become possible within an unrestricted class of spd systems of arbitrary unit-cell size.", "year": 2013, "referenceCount": 42, "citationCount": 291, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics"], "authors": [{"authorId": "51257580", "name": "K. T. Schutt"}, {"authorId": "50359998", "name": "H. Glawe"}, {"authorId": "3468686", "name": "F. Brockherde"}, {"authorId": "50750682", "name": "A. Sanna"}, {"authorId": "116099820", "name": "K. Muller"}, {"authorId": "145191157", "name": "E. Gross"}]}, {"paperId": "724a2430522c7c398c01f90330f41e187c6a7243", "url": "https://www.semanticscholar.org/paper/724a2430522c7c398c01f90330f41e187c6a7243", "title": "Gaussian Processes for Ordinal Regression", "abstract": "We present a probabilistic kernel approach to ordinal regression based on Gaussian processes. A threshold model that generalizes the probit function is used as the likelihood function for ordinal variables. Two inference techniques, based on the Laplace approximation and the expectation propagation algorithm respectively, are derived for hyperparameter learning and model selection. We compare these two Gaussian process approaches with a previous ordinal regression method based on support vector machines on some benchmark and real-world data sets, including applications of ordinal regression to collaborative filtering and gene expression analysis. Experimental results on these data sets verify the usefulness of our approach.", "year": 2005, "referenceCount": 55, "citationCount": 448, "influentialCitationCount": 59, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2057046890", "name": "Wei Chu"}, {"authorId": "1744700", "name": "Zoubin Ghahramani"}]}, {"paperId": "8cdaef7f852bf5c4b87a5f9f740d15f221be0a5a", "url": "https://www.semanticscholar.org/paper/8cdaef7f852bf5c4b87a5f9f740d15f221be0a5a", "title": "Machine Learning and Portfolio Optimization", "abstract": "The portfolio optimization model has limited impact in practice because of estimation issues when applied to real data. To address this, we adapt two machine learning methods, regularization and cross-validation, for portfolio optimization. First, we introduce performance-based regularization PBR, where the idea is to constrain the sample variances of the estimated portfolio risk and return, which steers the solution toward one associated with less estimation error in the performance. We consider PBR for both mean-variance and mean-conditional value-at-risk CVaR problems. For the mean-variance problem, PBR introduces a quartic polynomial constraint, for which we make two convex approximations: one based on rank-1 approximation and another based on a convex quadratic approximation. The rank-1 approximation PBR adds a bias to the optimal allocation, and the convex quadratic approximation PBR shrinks the sample covariance matrix. For the mean-CVaR problem, the PBR model is a combinatorial optimization problem, but we prove its convex relaxation, a quadratically constrained quadratic program, is essentially tight. We show that the PBR models can be cast as robust optimization problems with novel uncertainty sets and establish asymptotic optimality of both sample average approximation SAA and PBR solutions and the corresponding efficient frontiers. To calibrate the right-hand sides of the PBR constraints, we develop new, performance-based k-fold cross-validation algorithms. Using these algorithms, we carry out an extensive empirical investigation of PBR against SAA, as well as L1 and L2 regularizations and the equally weighted portfolio. We find that PBR dominates all other benchmarks for two out of three Fama-French data sets. \n \nThis paper was accepted by Yinyu Ye, optimization.", "year": 2018, "referenceCount": 72, "citationCount": 125, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "103243641", "name": "Gah-Yi Ban"}, {"authorId": "22702756", "name": "N. E. Karoui"}, {"authorId": "40447874", "name": "Andrew E. B. Lim"}]}, {"paperId": "bd9ecc05a12563445a2ef4fe758a39d7f2bcda0d", "url": "https://www.semanticscholar.org/paper/bd9ecc05a12563445a2ef4fe758a39d7f2bcda0d", "title": "DeltaGrad: Rapid retraining of machine learning models", "abstract": "Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.", "year": 2020, "referenceCount": 68, "citationCount": 57, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "3037003", "name": "Yinjun Wu"}, {"authorId": "2694895", "name": "Edgar Dobriban"}, {"authorId": "49702692", "name": "S. Davidson"}]}, {"paperId": "ffb9404d53b5cfcecf8fc7dbf85257c560f135a4", "url": "https://www.semanticscholar.org/paper/ffb9404d53b5cfcecf8fc7dbf85257c560f135a4", "title": "Machine learning in materials science", "abstract": null, "year": 2019, "referenceCount": 145, "citationCount": 152, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "2141564635", "name": "Jing Wei"}, {"authorId": "2048654183", "name": "Xuan Chu"}, {"authorId": "93425239", "name": "Xiangyu Sun"}, {"authorId": "2113451966", "name": "Kun Xu"}, {"authorId": "48470691", "name": "H. Deng"}, {"authorId": "1940653397", "name": "Jigen Chen"}, {"authorId": "6095903", "name": "Zhongming Wei"}, {"authorId": "47805251", "name": "M. Lei"}]}, {"paperId": "48fc5cf4b5e3517d246e39f3d9e53181bdcb6687", "url": "https://www.semanticscholar.org/paper/48fc5cf4b5e3517d246e39f3d9e53181bdcb6687", "title": "Validating Machine Learning Algorithms for Twitter Data Against Established Measures of Suicidality", "abstract": "Background One of the leading causes of death in the United States (US) is suicide and new methods of assessment are needed to track its risk in real time. Objective Our objective is to validate the use of machine learning algorithms for Twitter data against empirically validated measures of suicidality in the US population. Methods Using a machine learning algorithm, the Twitter feeds of 135 Mechanical Turk (MTurk) participants were compared with validated, self-report measures of suicide risk. Results Our findings show that people who are at high suicidal risk can be easily differentiated from those who are not by machine learning algorithms, which accurately identify the clinically significant suicidal rate in 92% of cases (sensitivity: 53%, specificity: 97%, positive predictive value: 75%, negative predictive value: 93%). Conclusions Machine learning algorithms are efficient in differentiating people who are at a suicidal risk from those who are not. Evidence for suicidality can be measured in nonclinical populations using social media data.", "year": 2016, "referenceCount": 63, "citationCount": 136, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Psychology"], "authors": [{"authorId": "35415670", "name": "Scott R. Braithwaite"}, {"authorId": "1398517004", "name": "C. Giraud-Carrier"}, {"authorId": "3864835", "name": "J. West"}, {"authorId": "40048363", "name": "M. Barnes"}, {"authorId": "3209733", "name": "C. Hanson"}]}, {"paperId": "4e4303ac23684034f0ba7f2aefa75d18fc660e41", "url": "https://www.semanticscholar.org/paper/4e4303ac23684034f0ba7f2aefa75d18fc660e41", "title": "IoT Security Techniques Based on Machine Learning: How Do IoT Devices Use AI to Enhance Security?", "abstract": "The Internet of things (IoT), which integrates a variety of devices into networks to provide advanced and intelligent services, has to protect user privacy and address attacks such as spoofing attacks, denial of service (DoS) attacks, jamming, and eavesdropping. We investigate the attack model for IoT systems and review the IoT security solutions based on machine-learning (ML) techniques including supervised learning, unsupervised learning, and reinforcement learning (RL). ML-based IoT authentication, access control, secure offloading, and malware detection schemes to protect data privacy are the focus of this article. We also discuss the challenges that need to be addressed to implement these ML-based security schemes in practical IoT systems.", "year": 2018, "referenceCount": 30, "citationCount": 283, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "33821409", "name": "Liang Xiao"}, {"authorId": "14423947", "name": "Xiaoyue Wan"}, {"authorId": "47062683", "name": "Xiaozhen Lu"}, {"authorId": "38200671", "name": "Yanyong Zhang"}, {"authorId": "48198096", "name": "Di Wu"}]}, {"paperId": "b455ce189b359085861bebc25746db0e25947679", "url": "https://www.semanticscholar.org/paper/b455ce189b359085861bebc25746db0e25947679", "title": "Learned protein embeddings for machine learning", "abstract": "Motivation: Machine\u2010learning models trained on protein sequences and their measured functions can infer biological properties of unseen sequences without requiring an understanding of the underlying physical or biological mechanisms. Such models enable the prediction and discovery of sequences with optimal properties. Machine\u2010learning models generally require that their inputs be vectors, and the conversion from a protein sequence to a vector representation affects the model's ability to learn. We propose to learn embedded representations of protein sequences that take advantage of the vast quantity of unmeasured protein sequence data available. These embeddings are low\u2010dimensional and can greatly simplify downstream modeling. Results: The predictive power of Gaussian process models trained using embeddings is comparable to those trained on existing representations, which suggests that embeddings enable accurate predictions despite having orders of magnitude fewer dimensions. Moreover, embeddings are simpler to obtain because they do not require alignments, structural data, or selection of informative amino\u2010acid properties. Visualizing the embedding vectors shows meaningful relationships between the embedded proteins are captured. Availability and implementation: The embedding vectors and code to reproduce the results are available at https://github.com/fhalab/embeddings_reproduction/. Supplementary information: Supplementary data are available at Bioinformatics online.", "year": 2018, "referenceCount": 34, "citationCount": 166, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "47271615", "name": "Kevin Kaichuang Yang"}, {"authorId": "51125091", "name": "Zachary Wu"}, {"authorId": "5808395", "name": "C. N. Bedbrook"}, {"authorId": "2795724", "name": "F. Arnold"}]}, {"paperId": "92e536a02f6024ac3216b4845f44a5ad40558e7e", "url": "https://www.semanticscholar.org/paper/92e536a02f6024ac3216b4845f44a5ad40558e7e", "title": "Automated Machine Learning", "abstract": null, "year": 2021, "referenceCount": 48, "citationCount": 115, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2150496014", "name": "Chen Lei"}]}, {"paperId": "e9e645aaf799dcd43cb9715ec8732621c737131f", "url": "https://www.semanticscholar.org/paper/e9e645aaf799dcd43cb9715ec8732621c737131f", "title": "Conformal Prediction for Reliable Machine Learning: Theory, Adaptations and Applications", "abstract": "The conformal predictions framework is a recent development in machine learning that can associate a reliable measure of confidence with a prediction in any real-world pattern recognition application, including risk-sensitive applications such as medical diagnosis, face recognition, and financial risk prediction. Conformal Predictions for Reliable Machine Learning: Theory, Adaptations and Applications captures the basic theory of the framework, demonstrates how to apply it to real-world problems, and presents several adaptations, including active learning, change detection, and anomaly detection. As practitioners and researchers around the world apply and adapt the framework, this edited volume brings together these bodies of work, providing a springboard for further research as well as a handbook for application in real-world problems. Understand the theoretical foundations of this important framework that can provide a reliable measure of confidence with predictions in machine learning Be able to apply this framework to real-world problems in different machine learning settings, including classification, regression, and clustering Learn effective ways of adapting the framework to newer problem settings, such as active learning, model selection, or change detection", "year": 2014, "referenceCount": 328, "citationCount": 160, "influentialCitationCount": 28, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1699429", "name": "V. Balasubramanian"}, {"authorId": "37030905", "name": "S. Ho"}, {"authorId": "145675281", "name": "V. Vovk"}]}, {"paperId": "8533ce781a01ad6bbf3df91b12314ef3b479117c", "url": "https://www.semanticscholar.org/paper/8533ce781a01ad6bbf3df91b12314ef3b479117c", "title": "Machine Learning Strategy for Accelerated Design of Polymer Dielectrics", "abstract": null, "year": 2016, "referenceCount": 63, "citationCount": 248, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1398852745", "name": "A. Mannodi-Kanakkithodi"}, {"authorId": "49542803", "name": "G. Pilania"}, {"authorId": "6940285", "name": "T. D. Huan"}, {"authorId": "2891010", "name": "T. Lookman"}, {"authorId": "4063724", "name": "R. Ramprasad"}]}, {"paperId": "a0390b8d4a82daa1d24bba341b317aa710e4ce4d", "url": "https://www.semanticscholar.org/paper/a0390b8d4a82daa1d24bba341b317aa710e4ce4d", "title": "Super-resolution reconstruction of turbulent flows with machine learning", "abstract": "We use machine learning to perform super-resolution analysis of grossly under-resolved turbulent flow field data to reconstruct the high-resolution flow field. Two machine learning models are developed, namely, the convolutional neural network (CNN) and the hybrid downsampled skip-connection/multi-scale (DSC/MS) models. These machine learning models are applied to a two-dimensional cylinder wake as a preliminary test and show remarkable ability to reconstruct laminar flow from low-resolution flow field data. We further assess the performance of these models for two-dimensional homogeneous turbulence. The CNN and DSC/MS models are found to reconstruct turbulent flows from extremely coarse flow field images with remarkable accuracy. For the turbulent flow problem, the machine-leaning-based super-resolution analysis can greatly enhance the spatial resolution with as little as 50 training snapshot data, holding great potential to reveal subgrid-scale physics of complex turbulent flows. With the growing availability of flow field data from high-fidelity simulations and experiments, the present approach motivates the development of effective super-resolution models for a variety of fluid flows.", "year": 2018, "referenceCount": 26, "citationCount": 245, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Physics"], "authors": [{"authorId": "102460658", "name": "Kai Fukami"}, {"authorId": "14873147", "name": "K. Fukagata"}, {"authorId": "2780687", "name": "K. Taira"}]}, {"paperId": "9cbfbf6508f591555a898f6ebca4d4608fd02bea", "url": "https://www.semanticscholar.org/paper/9cbfbf6508f591555a898f6ebca4d4608fd02bea", "title": "An instance level analysis of data complexity", "abstract": null, "year": 2014, "referenceCount": 52, "citationCount": 262, "influentialCitationCount": 32, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2116645391", "name": "Michael R. Smith"}, {"authorId": "90716724", "name": "T. Martinez"}, {"authorId": "1398517004", "name": "C. Giraud-Carrier"}]}, {"paperId": "1384d42b8df88bab0ce38c21144e154d1afed238", "url": "https://www.semanticscholar.org/paper/1384d42b8df88bab0ce38c21144e154d1afed238", "title": "Statistical Mechanics of Learning", "abstract": "From the Publisher: \nThe effort to build machines that are able to learn and undertake tasks such as datamining, image processing and pattern recognition has led to the development of artificial neural networks in which learning from examples may be described and understood. The contribution to this subject made over the past decade by researchers applying the techniques of statistical mechanics is the subject of this book. The authors provide a coherent account of various important concepts and techniques that are currently only found scattered in papers, supplement this with background material in mathematics and physics, and include many examples and exercises.", "year": 2001, "referenceCount": 0, "citationCount": 463, "influentialCitationCount": 53, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "48488024", "name": "A. Engel"}, {"authorId": "120489430", "name": "C. Broeck"}]}, {"paperId": "374313c2fc407655ae05eed61005a29cfaa7460b", "url": "https://www.semanticscholar.org/paper/374313c2fc407655ae05eed61005a29cfaa7460b", "title": "Towards exact molecular dynamics simulations with machine-learned force fields", "abstract": null, "year": 2018, "referenceCount": 87, "citationCount": 368, "influentialCitationCount": 13, "isOpenAccess": true, "fieldsOfStudy": ["Materials Science", "Physics", "Medicine"], "authors": [{"authorId": "7631063", "name": "Stefan Chmiela"}, {"authorId": "10667063", "name": "H. E. Sauceda"}, {"authorId": "145034054", "name": "K. M\u00fcller"}, {"authorId": "2462983", "name": "A. Tkatchenko"}]}, {"paperId": "51006f395255a3c5bed1f418a1b838b2f24b7b38", "url": "https://www.semanticscholar.org/paper/51006f395255a3c5bed1f418a1b838b2f24b7b38", "title": "Malicious URL Detection using Machine Learning: A Survey", "abstract": "Malicious URL, a.k.a. malicious website, is a common and serious threat to cybersecurity. Malicious URLs host unsolicited content (spam, phishing, drive-by exploits, etc.) and lure unsuspecting users to become victims of scams (monetary loss, theft of private information, and malware installation), and cause losses of billions of dollars every year. It is imperative to detect and act on such threats in a timely manner. Traditionally, this detection is done mostly through the usage of blacklists. However, blacklists cannot be exhaustive, and lack the ability to detect newly generated malicious URLs. To improve the generality of malicious URL detectors, machine learning techniques have been explored with increasing attention in recent years. This article aims to provide a comprehensive survey and a structural understanding of Malicious URL Detection techniques using machine learning. We present the formal formulation of Malicious URL Detection as a machine learning task, and categorize and review the contributions of literature studies that addresses different dimensions of this problem (feature representation, algorithm design, etc.). Further, this article provides a timely and comprehensive survey for a range of different audiences, not only for machine learning researchers and engineers in academia, but also for professionals and practitioners in cybersecurity industry, to help them understand the state of the art and facilitate their own research and practical applications. We also discuss practical issues in system design, open research challenges, and point out some important directions for future research.", "year": 2017, "referenceCount": 209, "citationCount": 196, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "36187119", "name": "Doyen Sahoo"}, {"authorId": "2039481", "name": "Chenghao Liu"}, {"authorId": "1741126", "name": "S. Hoi"}]}, {"paperId": "d2d7049b752b4666120a370ebb968351a4040e58", "url": "https://www.semanticscholar.org/paper/d2d7049b752b4666120a370ebb968351a4040e58", "title": "Assessing Credit Card Applications Using Machine Learning", "abstract": null, "year": 1987, "referenceCount": 1, "citationCount": 227, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2082443290", "name": "Chris Carter"}, {"authorId": "144356748", "name": "J. Catlett"}]}, {"paperId": "2aadb938af2f77a6ad9321ff873c1c9b9a579fcb", "url": "https://www.semanticscholar.org/paper/2aadb938af2f77a6ad9321ff873c1c9b9a579fcb", "title": "A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection", "abstract": "Cyber security is that the body of technologies, processes and practices designed to safeguard networks, computers, programs and knowledge from attack, harm or unauthorized access. During a computing context, the term security implies cyber security. This survey paper describes a targeted literature survey of machine learning (ML) and data processing (DM) strategies for cyber analytics in support of intrusion detection. This paper focuses totally on cyber intrusion detection as it applies to wired networks. With a wired network, associate oppose must experience many layers of defense at firewalls and operative systems, or gain physical access to the network. The quality of ML/DM algorithms is addressed, discussion of challenges for victimization ML/DM for cyber security is conferred, and some recommendations on once to use a given methodology area unit provided.", "year": 2017, "referenceCount": 13, "citationCount": 861, "influentialCitationCount": 70, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "100818765", "name": "Lalu Banoth"}, {"authorId": "70392354", "name": "M. S. Teja"}, {"authorId": "2102149819", "name": "M. Saicharan"}, {"authorId": "143612469", "name": "N. Chandra"}]}, {"paperId": "17a63f10b425c6db4b5a877d11e92f3bb24d1904", "url": "https://www.semanticscholar.org/paper/17a63f10b425c6db4b5a877d11e92f3bb24d1904", "title": "Probabilistic Matrix Factorization for Automated Machine Learning", "abstract": "In order to achieve state-of-the-art performance, modern machine learning techniques require careful data pre-processing and hyperparameter tuning. Moreover, given the ever increasing number of machine learning models being developed, model selection is becoming increasingly important. Automating the selection and tuning of machine learning pipelines consisting of data pre-processing methods and machine learning models, has long been one of the goals of the machine learning community. In this paper, we tackle this meta-learning task by combining ideas from collaborative filtering and Bayesian optimization. Using probabilistic matrix factorization techniques and acquisition functions from Bayesian optimization, we exploit experiments performed in hundreds of different datasets to guide the exploration of the space of possible pipelines. In our experiments, we show that our approach quickly identifies high-performing pipelines across a wide range of datasets, significantly outperforming the current state-of-the-art.", "year": 2017, "referenceCount": 33, "citationCount": 96, "influentialCitationCount": 13, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "2723245", "name": "Nicol\u00f3 Fusi"}, {"authorId": "2607233", "name": "Rishit Sheth"}, {"authorId": "31829362", "name": "Melih Elibol"}]}, {"paperId": "3a6447361b20c249f5306ae17dee43f645430e31", "url": "https://www.semanticscholar.org/paper/3a6447361b20c249f5306ae17dee43f645430e31", "title": "Neural Logic Machines", "abstract": "We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks---as function approximators, and logic programming---as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone.", "year": 2019, "referenceCount": 81, "citationCount": 145, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "88880715", "name": "Honghua Dong"}, {"authorId": "13589371", "name": "Jiayuan Mao"}, {"authorId": "2143608361", "name": "Tian Lin"}, {"authorId": "2146308808", "name": "Chong Wang"}, {"authorId": "47681372", "name": "Lihong Li"}, {"authorId": "65855107", "name": "Denny Zhou"}]}, {"paperId": "0811597b0851b7ebe21aadce7cb4daac4664b44f", "url": "https://www.semanticscholar.org/paper/0811597b0851b7ebe21aadce7cb4daac4664b44f", "title": "One-Shot Generalization in Deep Generative Models", "abstract": "Humans have an impressive ability to reason about new concepts and experiences from just a single example. In particular, humans have an ability for one-shot generalization: an ability to encounter a new concept, understand its structure, and then be able to generate compelling alternative variations of the concept. We develop machine learning systems with this important capacity by developing new deep generative models, models that combine the representational power of deep learning with the inferential power of Bayesian reasoning. We develop a class of sequential generative models that are built on the principles of feedback and attention. These two characteristics lead to generative models that are among the state-of-the art in density estimation and image generation. We demonstrate the one-shot generalization ability of our models using three tasks: unconditional sampling, generating new exemplars of a given concept, and generating new exemplars of a family of concepts. In all cases our models are able to generate compelling and diverse samples-- having seen new examples just once--providing an important class of general-purpose models for one-shot machine learning.", "year": 2016, "referenceCount": 33, "citationCount": 210, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "1748523", "name": "Danilo Jimenez Rezende"}, {"authorId": "14594344", "name": "S. Mohamed"}, {"authorId": "1841008", "name": "Ivo Danihelka"}, {"authorId": "144717963", "name": "Karol Gregor"}, {"authorId": "1688276", "name": "Daan Wierstra"}]}, {"paperId": "2f2ade8c4944a96a44e6f70ef403b80b058d1725", "url": "https://www.semanticscholar.org/paper/2f2ade8c4944a96a44e6f70ef403b80b058d1725", "title": "Towards Making Systems Forget with Machine Unlearning", "abstract": "Today's systems produce a rapidly exploding amount of data, and the data further derives more data, forming a complex data propagation network that we call the data's lineage. There are many reasons that users want systems to forget certain data including its lineage. From a privacy perspective, users who become concerned with new privacy risks of a system often want the system to forget their data and lineage. From a security perspective, if an attacker pollutes an anomaly detector by injecting manually crafted data into the training data set, the detector must forget the injected data to regain security. From a usability perspective, a user can remove noise and incorrect entries so that a recommendation engine gives useful recommendations. Therefore, we envision forgetting systems, capable of forgetting certain data and their lineages, completely and quickly. This paper focuses on making learning systems forget, the process of which we call machine unlearning, or simply unlearning. We present a general, efficient unlearning approach by transforming learning algorithms used by a system into a summation form. To forget a training data sample, our approach simply updates a small number of summations -- asymptotically faster than retraining from scratch. Our approach is general, because the summation form is from the statistical query learning in which many machine learning algorithms can be implemented. Our approach also applies to all stages of machine learning, including feature selection and modeling. Our evaluation, on four diverse learning systems and real-world workloads, shows that our approach is general, effective, fast, and easy to use.", "year": 2015, "referenceCount": 75, "citationCount": 170, "influentialCitationCount": 25, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3139121", "name": "Yinzhi Cao"}, {"authorId": "152211006", "name": "Junfeng Yang"}]}, {"paperId": "e1eef97a5e8a79542c3c93e059c585ed71598e71", "url": "https://www.semanticscholar.org/paper/e1eef97a5e8a79542c3c93e059c585ed71598e71", "title": "Representative Sampling for Text Classification Using Support Vector Machines", "abstract": null, "year": 2003, "referenceCount": 18, "citationCount": 244, "influentialCitationCount": 16, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145160047", "name": "Zhao Xu"}, {"authorId": "2114075909", "name": "Kai Yu"}, {"authorId": "1700754", "name": "Volker Tresp"}, {"authorId": "2145783722", "name": "Xiaowei Xu"}, {"authorId": "3127002", "name": "Jizhi Wang"}]}, {"paperId": "e0535dedb8607d83cd2614317c99913378e89e26", "url": "https://www.semanticscholar.org/paper/e0535dedb8607d83cd2614317c99913378e89e26", "title": "Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations", "abstract": "A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.", "year": 2002, "referenceCount": 34, "citationCount": 3000, "influentialCitationCount": 253, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "145247053", "name": "W. Maass"}, {"authorId": "1792142", "name": "T. Natschl\u00e4ger"}, {"authorId": "1754307", "name": "H. Markram"}]}, {"paperId": "e3c221ee33f9082d8d47a363ed763d62044b60f6", "url": "https://www.semanticscholar.org/paper/e3c221ee33f9082d8d47a363ed763d62044b60f6", "title": "Introduction to Machine Learning with Python", "abstract": null, "year": 2020, "referenceCount": 0, "citationCount": 137, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "50821287", "name": "Josh Cutler"}, {"authorId": "145785478", "name": "Matt Dickenson"}]}, {"paperId": "84dc3395065ea458997f1422f2ff731846cd7673", "url": "https://www.semanticscholar.org/paper/84dc3395065ea458997f1422f2ff731846cd7673", "title": "Machine Learning Classifiers", "abstract": null, "year": 2020, "referenceCount": 2, "citationCount": 49, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "73069942", "name": "Rachna Behl"}, {"authorId": "72972858", "name": "Indu Kashyap"}]}, {"paperId": "9fe0ec4855136007fa73d306f1ff59365002bc58", "url": "https://www.semanticscholar.org/paper/9fe0ec4855136007fa73d306f1ff59365002bc58", "title": "Machine Learning Force Fields: Construction, Validation, and Outlook", "abstract": "Force fields developed with machine learning methods in tandem with quantum mechanics are beginning to find merit, given their (i) low cost, (ii) accuracy, and (iii) versatility. Recently, we proposed one such approach, wherein, the vectorial force on an atom is computed directly from its environment. Here, we discuss the multistep workflow required for their construction, which begins with generating diverse reference atomic environments and force data, choosing a numerical representation for the atomic environments, down selecting a representative training set, and lastly the learning method itself, for the case of Al. The constructed force field is then validated by simulating complex materials phenomena such as surface melting and stress\u2013strain behavior, that truly go beyond the realm of ab initio methods, both in length and time scales. To make such force fields truly versatile an attempt to estimate the uncertainty in force predictions is put forth, allowing one to identify areas of poor performance...", "year": 2016, "referenceCount": 40, "citationCount": 309, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Materials Science", "Physics", "Chemistry"], "authors": [{"authorId": "8352724", "name": "V. Botu"}, {"authorId": "143832304", "name": "R. Batra"}, {"authorId": "2086719652", "name": "James Chapman"}, {"authorId": "4063724", "name": "R. Ramprasad"}]}, {"paperId": "4cc7eab0973edddb9e910af8a90055cd0815e8f7", "url": "https://www.semanticscholar.org/paper/4cc7eab0973edddb9e910af8a90055cd0815e8f7", "title": "Data Mining Using MLC a Machine Learning Library in C++", "abstract": "Data mining algorithms including machine learning, statistical analysis, and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread. In this paper, we focus on classification algorithms and review the need for multiple classification algorithms. We describe a system called MLC++, which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest. MLC++ not only provides a workbench for such comparisons, but also provides a library of C++ classes to aid in the development of new algorithms, especially hybrid algorithms and multi-strategy algorithms. Such algorithms are generally hard to code from scratch. We discuss design issues, interfaces to other programs, and visualization of the resulting classifiers.", "year": 1996, "referenceCount": 97, "citationCount": 315, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1726733", "name": "Ron Kohavi"}, {"authorId": "40307399", "name": "D. Sommerfield"}, {"authorId": "2065335246", "name": "James Dougherty"}]}, {"paperId": "f91d1db0fa2ac12e0262a8f5e6a371da29a3c100", "url": "https://www.semanticscholar.org/paper/f91d1db0fa2ac12e0262a8f5e6a371da29a3c100", "title": "Multi-Agent Cooperation and the Emergence of (Natural) Language", "abstract": "The current mainstream approach to train natural language systems is to expose them to large amounts of text. This passive learning is problematic if we are interested in developing interactive machines, such as conversational agents. We propose a framework for language learning that relies on multi-agent communication. We study this learning in the context of referential games. In these games, a sender and a receiver see a pair of images. The sender is told one of them is the target and is allowed to send a message from a fixed, arbitrary vocabulary to the receiver. The receiver must rely on this message to identify the target. Thus, the agents develop their own language interactively out of the need to communicate. We show that two networks with simple configurations are able to learn to coordinate in the referential game. We further explore how to make changes to the game environment to cause the \"word meanings\" induced in the game to better reflect intuitive semantic properties of the images. In addition, we present a simple strategy for grounding the agents' code into natural language. Both of these are necessary steps towards developing machines that are able to communicate with humans productively.", "year": 2016, "referenceCount": 42, "citationCount": 313, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2672644", "name": "Angeliki Lazaridou"}, {"authorId": "1970752", "name": "A. Peysakhovich"}, {"authorId": "145283199", "name": "Marco Baroni"}]}, {"paperId": "063db66bc5b7af642fecd5b891ada6eb18828ee0", "url": "https://www.semanticscholar.org/paper/063db66bc5b7af642fecd5b891ada6eb18828ee0", "title": "On the Use of Machine Learning to Predict the Time and Resources Consumed by Applications", "abstract": "Most data centers, clouds and grids consist of multiple generations of computing systems, each with different performance profiles, posing a challenge to job schedulers in achieving the best usage of the infrastructure. A useful piece of information for scheduling jobs, typically not available, is the extent to which applications will use available resources once they are executed. This paper comparatively assesses the suitability of several machine learning techniques for predicting spatio temporal utilization of resources by applications. Modern machine learning techniques able to handle large number of attributes are used, taking into account application- and system-specific attributes (e.g., CPU micro architecture, size and speed of memory and storage, input data characteristics and input parameters). The work also extends an existing classification tree algorithm, called Predicting Query Runtime (PQR), to the regression problem by allowing the leaves of the tree to select the best regression method for each collection of data on leaves. The new method (PQR2) yields the best average percentage error, predicting execution time, memory and disk consumption for two bioinformatics applications, BLAST and RAxML, deployed on scenarios that differ in system and usage. In specific scenarios where usage is a non-linear function of system and application attributes, certain configurations of two other machine learning algorithms, Support Vector Machine and k-nearest neighbors, also yield competitive results. In addition, experiments show that the inclusion of system performance and application-specific attributes also improves the performance of machine learning algorithms investigated.", "year": 2010, "referenceCount": 25, "citationCount": 179, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1735571", "name": "Andr\u00e9a M. Matsunaga"}, {"authorId": "144371682", "name": "J. Fortes"}]}, {"paperId": "9bbf8ca712bca3e93c898aea56d021a0317a30c1", "url": "https://www.semanticscholar.org/paper/9bbf8ca712bca3e93c898aea56d021a0317a30c1", "title": "Machine-learning approaches in drug discovery: methods and applications.", "abstract": null, "year": 2015, "referenceCount": 146, "citationCount": 459, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "4280352", "name": "A. Lavecchia"}]}, {"paperId": "1aa955560242f32983cacb6c7f1ea9d9b36aa96a", "url": "https://www.semanticscholar.org/paper/1aa955560242f32983cacb6c7f1ea9d9b36aa96a", "title": "Comparing and experimenting machine learning techniques for code smell detection", "abstract": null, "year": 2016, "referenceCount": 83, "citationCount": 239, "influentialCitationCount": 33, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1695555", "name": "F. Fontana"}, {"authorId": "144811716", "name": "M. M\u00e4ntyl\u00e4"}, {"authorId": "145844143", "name": "Marco Zanoni"}, {"authorId": "2052737310", "name": "A. Marino"}]}, {"paperId": "1531178873510075121efe26dc377f63b02b3d5d", "url": "https://www.semanticscholar.org/paper/1531178873510075121efe26dc377f63b02b3d5d", "title": "Adversarial Examples for Malware Detection", "abstract": null, "year": 2017, "referenceCount": 43, "citationCount": 409, "influentialCitationCount": 43, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "39221858", "name": "Kathrin Grosse"}, {"authorId": "1967156", "name": "Nicolas Papernot"}, {"authorId": "2134700604", "name": "Praveen Manoharan"}, {"authorId": "144588806", "name": "M. Backes"}, {"authorId": "144061974", "name": "P. Mcdaniel"}]}, {"paperId": "5a05c543d533b9e7dfd7dd10ca59c2f6147b2900", "url": "https://www.semanticscholar.org/paper/5a05c543d533b9e7dfd7dd10ca59c2f6147b2900", "title": "Evaluation of unsupervised semantic mapping of natural language with Leximancer concept mapping", "abstract": null, "year": 2006, "referenceCount": 32, "citationCount": 948, "influentialCitationCount": 76, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "47450615", "name": "Andrew E. Smith"}, {"authorId": "2232163", "name": "M. Humphreys"}]}, {"paperId": "82a5a84528a7ca0409f75e2211a3b33a217e9bac", "url": "https://www.semanticscholar.org/paper/82a5a84528a7ca0409f75e2211a3b33a217e9bac", "title": "Ensuring Fairness in Machine Learning to Advance Health Equity", "abstract": "Machine learning can identify the statistical patterns of data generated by tens of thousands of physicians and billions of patients to train computers to perform specific tasks with sometimes superhuman ability, such as detecting diabetic eye disease better than retinal specialists (1). However, historical data also capture patterns of health care disparities, and machine-learning models trained on these data may perpetuate these inequities. This concern is not just academic. In a model used to predict future crime on the basis of historical arrest records, African American defendants who did not reoffend were classified as high risk at a substantially higher rate than white defendants who did not reoffend (2, 3). Similar biases have been observed in predictive policing (4) and identifying which calls to a child protective services agency required an in-person investigation (5, 6). The implications for health care led the American Medical Association to pass policy recommendations to promote development of thoughtfully designed, high-quality, clinically validated health care AI [artificial or augmented intelligence, such as machine learning] that . . . identifies and takes steps to address bias and avoids introducing or exacerbating health care disparities including when testing or deploying new AI tools on vulnerable populations (7). We argue that health care organizations and policymakers should go beyond the American Medical Association's position of doing no harm and instead proactively design and use machine-learning systems to advance health equity. Whereas much health disparities work has focused on discriminatory decision making and implicit biases by clinicians, policymakers, organizational leaders, and researchers are increasingly focusing on the ill health effects of structural racism and classismhow systems are shaped in ways that harm the health of disempowered, marginalized populations (8). For example, the United States has a shameful history of purposive decisions by government and private businesses to segregate housing. Zoning laws, discrimination in mortgage lending, prejudicial practices by real estate agents, and the ghettoization of public housing all contributed to the concentration of urban African Americans in inferior housing that has led to poor health (9, 10). Even when the goal of decision makers is not outright discrimination against disadvantaged groups, actions may lead to inequities. For example, if the goal of a machine-learning system is to maximize efficiency, that might come at the expense of disadvantaged populations. As a society, we value health equity. For example, the Healthy People 2020 vision statement aims for a society in which all people live long, healthy lives, and one of the mission's goals is to achieve health equity, eliminate disparities, and improve the health of all groups (11). The 4 classic principles of Western clinical medical ethics are justice, autonomy, beneficence, and nonmaleficence. However, health equity will not be attained unless we purposely design our health and social systems, which increasingly will be infused with machine learning (12), to achieve this goal. To ensure fairness in machine learning, we recommend a participatory process that involves key stakeholders, including frequently marginalized populations, and considers distributive justice within specific clinical and organizational contexts. Different technical approaches can configure the mathematical properties of machine-learning models to render predictions that are equitable in various ways. The existence of mathematical levers must be supplemented with criteria for when and why they should be usedeach tool comes with tradeoffs that require ethical reasoning to decide what is best for a given application. We propose incorporating fairness into the design, deployment, and evaluation of machine-learning models. We discuss 2 clinical applications in which machine learning might harm protected groups by being inaccurate, diverting resources, or worsening outcomes, especially if the models are built without consideration for these patients. We then describe the mechanisms by which a model's design, data, and deployment may lead to disparities; explain how different approaches to distributive justice in machine learning can advance health equity; and explore what contexts are more appropriate for different equity approaches in machine learning. Case Study 1: Intensive Care Unit Monitoring A common area of predictive modeling research focuses on creating a monitoring systemfor example, to warn a rapid response team about inpatients at high risk for deterioration (1315), requiring their transfer to an intensive care unit within 6 hours. How might such a system inadvertently result in harm to a protected group? In this thought experiment, we consider African Americans as a protected group. To build the model, our hypothetical researchers collected historical records of patients who had clinical deterioration and those who did not. The model acts like a diagnostic test of risk for intensive care unit transfer. However, if too few African American patients were included in the training datathe data used to construct the modelthe model might be inaccurate for them. For example, it might have a lower sensitivity and miss more patients at risk for deterioration. African American patients might be harmed if clinical teams started relying on alerts to identify at-risk patients without realizing that the prediction system underdetects patients in that group (automation bias) (16). If the model had a lower positive predictive value for African Americans, it might also disproportionately harm them through dismissal biasa generalization of alert fatigue in which clinicians may learn to discount or dismiss alerts for African Americans because they are more likely to be false-positive (17). Case Study 2: Reducing Length of Stay Imagine that a hospital created a model with clinical and social variables to predict which inpatients might be discharged earliest so that it could direct limited case management resources to them to prevent delays. If residence in ZIP codes of socioeconomically depressed or predominantly African American neighborhoods predicted greater lengths of stay (18), this model might disproportionately allocate case management resources to patients from richer, predominantly white neighborhoods and away from African Americans in poorer ones. What Is Machine Learning? Traditionally, computer systems map inputs to outputs according to manually specified ifthen rules. With increasingly complex tasks, such as language translation, manually specifying rules becomes infeasible, and instead the mapping (or model) is learned by the system given only input examples represented through a set of features together with their desired output, referred to as labels. The quality of a model is assessed by computing evaluation metrics on data not used to build the model, such as sensitivity, specificity, or the c-statistic, which measures the ability of a model to distinguish patients with a condition from those without it (19, 20). Once the model's quality is deemed satisfactory, it can be deployed to make predictions on new examples for which the label is unknown when the prediction is made. The quality of the models on retrospective data must be followed with tests of clinical effectiveness, safety, and comparison with current practice, which may require clinical trials (21). Traditionally, statistical models for prediction, such as the pooled-cohort equation (22), have used few variables to predict clinical outcomes, such as cardiovascular risk (23). Modern machine-learning techniques, however, can consider many more features. For example, a recent model to predict hospital readmissions examined hundreds of thousands of pieces of information, including the free text of clinical notes (24). Complex data and models can drive more personalized and accurate predictions but may also make algorithms hard to understand and trust (25). What Can Cause a Machine-Learning System to Be Unfair? The Glossary lists key biases in the design, data, and deployment of a machine-learning model that may perpetuate or exacerbate health care disparities if left unchecked. The Figure reveals how the various biases relate to one another and how the interactions of model predictions with clinicians and patients may exacerbate health care disparities. Biases may arise during the design of a model. For example, if the label is marred by health care disparities, such as predicting the onset of clinical depression in environments where protected groups have been systematically misdiagnosed, then the model will learn to perpetuate this disparity. This represents a generalization of test-referral bias (26) that we refer to as label bias. Moreover, the data on which the model is developed may be biased. Data on patients in the protected group might be distributed differently from those in the nonprotected group because of biological or nonbiological variation (9, 27). For example, the data may not contain enough examples from a group to properly tailor the predictions to them (minority bias) (28), or the data set of the protected group may be less informative because features are missing not at random as a result of more fragmented care (29, 30). Glossary Figure. Conceptual framework of how various biases relate to one another. During model development, differences in the distribution of features used to predict a label between the protected and nonprotected groups may bias a model to be less accurate for protected groups. Moreover, the data used to develop a model may not generalize to the data used during model deployment (trainingserving skew). Biases in model design and data affect patient outcomes through the model's interaction with clinicians and patients. The immediate effect of these differences is that the model may ", "year": 2018, "referenceCount": 52, "citationCount": 296, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "8638650", "name": "A. Rajkomar"}, {"authorId": "40356669", "name": "Michaela Hardt"}, {"authorId": "145310670", "name": "M. Howell"}, {"authorId": "2084098271", "name": "Greg S. Corrado"}, {"authorId": "2186184", "name": "M. Chin"}]}, {"paperId": "ae341ad66824e1f30a2675fd50742b97794c8f57", "url": "https://www.semanticscholar.org/paper/ae341ad66824e1f30a2675fd50742b97794c8f57", "title": "Learning from imbalanced data sets with boosting and data generation: the DataBoost-IM approach", "abstract": "Learning from imbalanced data sets, where the number of examples of one (majority) class is much higher than the others, presents an important challenge to the machine learning community. Traditional machine learning algorithms may be biased towards the majority class, thus producing poor predictive accuracy over the minority class. In this paper, we describe a new approach that combines boosting, an ensemble-based learning algorithm, with data generation to improve the predictive power of classifiers against imbalanced data sets consisting of two classes. In the DataBoost-IM method, hard examples from both the majority and minority classes are identified during execution of the boosting algorithm. Subsequently, the hard examples are used to separately generate synthetic examples for the majority and minority classes. The synthetic data are then added to the original training set, and the class distribution and the total weights of the different classes in the new training set are rebalanced. The DataBoost-IM method was evaluated, in terms of the F-measures, G-mean and overall accuracy, against seventeen highly and moderately imbalanced data sets using decision trees as base classifiers. Our results are promising and show that the DataBoost-IM method compares well in comparison with a base classifier, a standard benchmarking boosting algorithm and three advanced boosting-based algorithms for imbalanced data set. Results indicate that our approach does not sacrifice one class in favor of the other, but produces high predictions against both minority and majority classes.", "year": 2004, "referenceCount": 26, "citationCount": 597, "influentialCitationCount": 32, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1694050", "name": "Hongyu Guo"}, {"authorId": "1727256", "name": "H. Viktor"}]}, {"paperId": "23bae60509630acda34cb1811125de3a0cb86cd7", "url": "https://www.semanticscholar.org/paper/23bae60509630acda34cb1811125de3a0cb86cd7", "title": "Machine Learning and Deep Learning frameworks and libraries for large-scale data mining: a survey", "abstract": null, "year": 2019, "referenceCount": 128, "citationCount": 320, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1697308", "name": "Giang T. Nguyen"}, {"authorId": "2160638", "name": "S. Dlugolinsky"}, {"authorId": "2105227365", "name": "Martin Bob\u00e1k"}, {"authorId": "143824490", "name": "V. Tran"}, {"authorId": "2830481", "name": "\u00c1. Garc\u00eda"}, {"authorId": "145154247", "name": "Ignacio Heredia"}, {"authorId": "2057232086", "name": "Peter Mal\u00edk"}, {"authorId": "1744707", "name": "L. Hluch\u00fd"}]}, {"paperId": "8dcbcaaf337d7bd22e580f1bb7a795ed4bb604fd", "url": "https://www.semanticscholar.org/paper/8dcbcaaf337d7bd22e580f1bb7a795ed4bb604fd", "title": "LEAF: A Benchmark for Federated Settings", "abstract": "Modern federated networks, such as those comprised of wearable devices, mobile phones, or autonomous vehicles, generate massive amounts of data each day. This wealth of data can help to learn models that can improve the user experience on each device. However, the scale and heterogeneity of federated data presents new challenges in research areas such as federated learning, meta-learning, and multi-task learning. As the machine learning community begins to tackle these challenges, we are at a critical time to ensure that developments made in these areas are grounded with realistic benchmarks. To this end, we propose LEAF, a modular benchmarking framework for learning in federated settings. LEAF includes a suite of open-source federated datasets, a rigorous evaluation framework, and a set of reference implementations, all geared towards capturing the obstacles and intricacies of practical federated environments.", "year": 2018, "referenceCount": 41, "citationCount": 614, "influentialCitationCount": 210, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "52138923", "name": "S. Caldas"}, {"authorId": "2111194238", "name": "Peter Wu"}, {"authorId": "145530218", "name": "Tian Li"}, {"authorId": "32139366", "name": "Jakub Konecn\u00fd"}, {"authorId": "145057514", "name": "H. B. McMahan"}, {"authorId": "145260024", "name": "Virginia Smith"}, {"authorId": "145532827", "name": "Ameet S. Talwalkar"}]}, {"paperId": "6a1e5f1ff1b3106d160cb1e486fb9611252c393d", "url": "https://www.semanticscholar.org/paper/6a1e5f1ff1b3106d160cb1e486fb9611252c393d", "title": "Automated detection and differentiation of drusen, exudates, and cotton-wool spots in digital color fundus photographs for diabetic retinopathy diagnosis.", "abstract": "PURPOSE\nTo describe and evaluate a machine learning-based, automated system to detect exudates and cotton-wool spots in digital color fundus photographs and differentiate them from drusen, for early diagnosis of diabetic retinopathy.\n\n\nMETHODS\nThree hundred retinal images from one eye of 300 patients with diabetes were selected from a diabetic retinopathy telediagnosis database (nonmydriatic camera, two-field photography): 100 with previously diagnosed bright lesions and 200 without. A machine learning computer program was developed that can identify and differentiate among drusen, (hard) exudates, and cotton-wool spots. A human expert standard for the 300 images was obtained by consensus annotation by two retinal specialists. Sensitivities and specificities of the annotations on the 300 images by the automated system and a third retinal specialist were determined.\n\n\nRESULTS\nThe system achieved an area under the receiver operating characteristic (ROC) curve of 0.95 and sensitivity/specificity pairs of 0.95/0.88 for the detection of bright lesions of any type, and 0.95/0.86, 0.70/0.93, and 0.77/0.88 for the detection of exudates, cotton-wool spots, and drusen, respectively. The third retinal specialist achieved pairs of 0.95/0.74 for bright lesions and 0.90/0.98, 0.87/0.98, and 0.92/0.79 per lesion type.\n\n\nCONCLUSIONS\nA machine learning-based, automated system capable of detecting exudates and cotton-wool spots and differentiating them from drusen in color images obtained in community based diabetic patients has been developed and approaches the performance level of retinal experts. If the machine learning can be improved with additional training data sets, it may be useful for detecting clinically important bright lesions, enhancing early diagnosis, and reducing visual loss in patients with diabetes.", "year": 2007, "referenceCount": 29, "citationCount": 400, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3142710", "name": "M. Niemeijer"}, {"authorId": "123637526", "name": "B. van Ginneken"}, {"authorId": "1985675", "name": "S. Russell"}, {"authorId": "1397807242", "name": "M. Suttorp-Schulten"}, {"authorId": "143650563", "name": "M. Abr\u00e0moff"}]}]}