{"total": 5120627, "offset": 2400, "next": 2500, "data": [{"paperId": "6cff959a9dc84733200ad7fcf7089ec4bb9fd1bb", "url": "https://www.semanticscholar.org/paper/6cff959a9dc84733200ad7fcf7089ec4bb9fd1bb", "title": "INDUCTION OF DECISION TREES USING RELIEFF", "abstract": null, "year": 1995, "referenceCount": 12, "citationCount": 200, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Engineering"], "authors": [{"authorId": "143986204", "name": "I. Kononenko"}, {"authorId": "2649889", "name": "E. Simec"}]}, {"paperId": "4f48d20824b18e1cf151eabe0128a79e4cf47bb8", "url": "https://www.semanticscholar.org/paper/4f48d20824b18e1cf151eabe0128a79e4cf47bb8", "title": "Learning invariants using decision trees and implication counterexamples", "abstract": "Inductive invariants can be robustly synthesized using a learning model where the teacher is a program verifier who instructs the learner through concrete program configurations, classified as positive, negative, and implications. We propose the first learning algorithms in this model with implication counter-examples that are based on machine learning techniques. In particular, we extend classical decision-tree learning algorithms in machine learning to handle implication samples, building new scalable ways to construct small decision trees using statistical measures. We also develop a decision-tree learning algorithm in this model that is guaranteed to converge to the right concept (invariant) if one exists. We implement the learners and an appropriate teacher, and show that the resulting invariant synthesis is efficient and convergent for a large suite of programs.", "year": 2016, "referenceCount": 85, "citationCount": 161, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1802463", "name": "P. Garg"}, {"authorId": "1779795", "name": "D. Neider"}, {"authorId": "145104529", "name": "P. Madhusudan"}, {"authorId": "144590225", "name": "D. Roth"}]}, {"paperId": "29b5bdabb6f0cf99523b3d142bbe2a01c5f903b5", "url": "https://www.semanticscholar.org/paper/29b5bdabb6f0cf99523b3d142bbe2a01c5f903b5", "title": "Dropout prediction in e-learning courses through the combination of machine learning techniques", "abstract": null, "year": 2009, "referenceCount": 57, "citationCount": 311, "influentialCitationCount": 15, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3229142", "name": "I. Lykourentzou"}, {"authorId": "2502166", "name": "I. Giannoukos"}, {"authorId": "34951142", "name": "V. Nikolopoulos"}, {"authorId": "2865161", "name": "G. Mpardis"}, {"authorId": "2692971", "name": "V. Loumos"}]}, {"paperId": "0c8c500cec9b74ebc7be44c52b79d2bd78234605", "url": "https://www.semanticscholar.org/paper/0c8c500cec9b74ebc7be44c52b79d2bd78234605", "title": "Best practices in machine learning for chemistry", "abstract": null, "year": 2021, "referenceCount": 30, "citationCount": 81, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "11413392", "name": "Nongnuch Artrith"}, {"authorId": "3300287", "name": "K. Butler"}, {"authorId": "144646737", "name": "Fran\u00e7ois-Xavier Coudert"}, {"authorId": "1964642", "name": "Seungwu Han"}, {"authorId": "2385206", "name": "O. Isayev"}, {"authorId": "2541031", "name": "Anubhav Jain"}, {"authorId": "2143601680", "name": "Aron Walsh"}]}, {"paperId": "0fd357e4e4d84629bdb0b49c83a5a71cd6c43359", "url": "https://www.semanticscholar.org/paper/0fd357e4e4d84629bdb0b49c83a5a71cd6c43359", "title": "Machine Learning Methods for Demand Estimation", "abstract": "We survey and apply several techniques from the statistical and computer science literature to the problem of demand estimation. To improve out-of-sample prediction accuracy, we propose a method of combining the underlying models via linear regression. Our method is robust to a large number of regressors; scales easily to very large data sets; combines model selection and estimation; and can flexibly approximate arbitrary non-linear functions. We illustrate our method using a standard scanner panel data set and find that our estimates are considerably more accurate in out-of-sample predictions of demand than some commonly used alternatives.", "year": 2015, "referenceCount": 14, "citationCount": 142, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Economics"], "authors": [{"authorId": "52624594", "name": "Patrick Bajari"}, {"authorId": "2307080", "name": "Denis Nekipelov"}, {"authorId": "3288177", "name": "Stephen P. Ryan"}, {"authorId": "152790172", "name": "Miaoyu Yang"}]}, {"paperId": "34bed85f10510753a39baf728fddb3c5e2116ae6", "url": "https://www.semanticscholar.org/paper/34bed85f10510753a39baf728fddb3c5e2116ae6", "title": "Applied machine learning and artificial intelligence in rheumatology", "abstract": "Abstract Machine learning as a field of artificial intelligence is increasingly applied in medicine to assist patients and physicians. Growing datasets provide a sound basis with which to apply machine learning methods that learn from previous experiences. This review explains the basics of machine learning and its subfields of supervised learning, unsupervised learning, reinforcement learning and deep learning. We provide an overview of current machine learning applications in rheumatology, mainly supervised learning methods for e-diagnosis, disease detection and medical image analysis. In the future, machine learning will be likely to assist rheumatologists in predicting the course of the disease and identifying important disease factors. Even more interestingly, machine learning will probably be able to make treatment propositions and estimate their expected benefit (e.g. by reinforcement learning). Thus, in future, shared decision-making will not only include the patient\u2019s opinion and the rheumatologist\u2019s empirical and evidence-based experience, but it will also be influenced by machine-learned evidence.", "year": 2020, "referenceCount": 55, "citationCount": 53, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "114651311", "name": "M. H\u00fcgle"}, {"authorId": "6229929", "name": "P. Omoumi"}, {"authorId": "144517223", "name": "J. V. van Laar"}, {"authorId": "145581493", "name": "J. Boedecker"}, {"authorId": "3746534", "name": "T. H\u00fcgle"}]}, {"paperId": "4e1f35169edb0debd734e7e5834b5672ca7911e1", "url": "https://www.semanticscholar.org/paper/4e1f35169edb0debd734e7e5834b5672ca7911e1", "title": "Low Data Drug Discovery with One-Shot Learning", "abstract": "Recent advances in machine learning have made significant contributions to drug discovery. Deep neural networks in particular have been demonstrated to provide significant boosts in predictive power when inferring the properties and activities of small-molecule compounds (Ma, J. et al. J. Chem. Inf. Model.2015, 55, 263\u201327425635324). However, the applicability of these techniques has been limited by the requirement for large amounts of training data. In this work, we demonstrate how one-shot learning can be used to significantly lower the amounts of data required to make meaningful predictions in drug discovery applications. We introduce a new architecture, the iterative refinement long short-term memory, that, when combined with graph convolutional neural networks, significantly improves learning of meaningful distance metrics over small-molecules. We open source all models introduced in this work as part of DeepChem, an open-source framework for deep-learning in drug discovery (Ramsundar, B. deepchem.io. https://github.com/deepchem/deepchem, 2016).", "year": 2016, "referenceCount": 43, "citationCount": 457, "influentialCitationCount": 24, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics", "Medicine"], "authors": [{"authorId": "1400903276", "name": "H. Altae-Tran"}, {"authorId": "2378027", "name": "Bharath Ramsundar"}, {"authorId": "5929246", "name": "Aneesh S. Pappu"}, {"authorId": "1806271", "name": "V. Pande"}]}, {"paperId": "6566d324f4834c73284049420a2c99c3063a1fef", "url": "https://www.semanticscholar.org/paper/6566d324f4834c73284049420a2c99c3063a1fef", "title": "Democratic co-learning", "abstract": "For many machine learning applications it is important to develop algorithms that use both labeled and unlabeled data. We present democratic colearning in which multiple algorithms instead of multiple views enable learners to label data for each other. Our technique leverages off the fact that different learning algorithms have different inductive biases and that better predictions can be made by the voted majority. We also present democratic priority sampling, a new example selection method for active learning.", "year": 2004, "referenceCount": 20, "citationCount": 207, "influentialCitationCount": 22, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2150921406", "name": "Yan Zhou"}, {"authorId": "32150954", "name": "S. Goldman"}]}, {"paperId": "8a8e4fa580a81ba2fcb86965f323709cafcab275", "url": "https://www.semanticscholar.org/paper/8a8e4fa580a81ba2fcb86965f323709cafcab275", "title": "Representation Learning for Dynamic Graphs: A Survey", "abstract": "Graphs arise naturally in many real-world applications including social networks, recommender systems, ontologies, biology, and computational finance. Traditionally, machine learning models for graphs have been mostly designed for static graphs. However, many applications involve evolving graphs. This introduces important challenges for learning and inference since nodes, attributes, and edges change over time. In this survey, we review the recent advances in representation learning for dynamic graphs, including dynamic knowledge graphs. We describe existing models from an encoder-decoder perspective, categorize these encoders and decoders based on the techniques they employ, and analyze the approaches in each category. We also review several prominent applications and widely used datasets and highlight directions for future research.", "year": 2019, "referenceCount": 286, "citationCount": 143, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2470890", "name": "Seyed Mehran Kazemi"}, {"authorId": "46186660", "name": "Rishab Goel"}, {"authorId": "40133859", "name": "Kshitij Jain"}, {"authorId": "66895761", "name": "I. Kobyzev"}, {"authorId": "40639989", "name": "Akshay Sethi"}, {"authorId": "123211350", "name": "Peter Forsyth"}, {"authorId": "1807041", "name": "P. Poupart"}, {"authorId": "1704422", "name": "K. Borgwardt"}]}, {"paperId": "6338670193f9fdcfd3b6e7cec414f15ca906a85f", "url": "https://www.semanticscholar.org/paper/6338670193f9fdcfd3b6e7cec414f15ca906a85f", "title": "SVDFeature: a toolkit for feature-based collaborative filtering", "abstract": "In this paper we introduce SVDFeature, a machine learning toolkit for feature-based collaborative filtering. SVDFeature is designed to efficiently solve the feature-based matrix factorization. The feature-based setting allows us to build factorization models incorporating side information such as temporal dynamics, neighborhood relationship, and hierarchical information. The toolkit is capable of both rate prediction and collaborative ranking, and is carefully designed for efficient training on large-scale data set. Using this toolkit, we built solutions to win KDD Cup for two consecutive years.", "year": 2012, "referenceCount": 3, "citationCount": 231, "influentialCitationCount": 33, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1913774", "name": "Tianqi Chen"}, {"authorId": "2108309275", "name": "Weinan Zhang"}, {"authorId": "2117522623", "name": "Qiuxia Lu"}, {"authorId": "1995670", "name": "Kailong Chen"}, {"authorId": "2115548759", "name": "Zhao Zheng"}, {"authorId": "1811427", "name": "Yong Yu"}]}, {"paperId": "096eece19728f5d05ff6938d12b6a6ee2a814d9f", "url": "https://www.semanticscholar.org/paper/096eece19728f5d05ff6938d12b6a6ee2a814d9f", "title": "Recent advances on support vector machines research", "abstract": "Abstract Support vector machines (SVMs), with their roots in Statistical Learning Theory (SLT) and optimization methods, have become powerful tools for problem solution in machine learning. SVMs reduce most machine learning problems to optimization problems and optimization lies at the heart of SVMs. Lots of SVM algorithms involve solving not only convex problems, such as linear programming, quadratic programming, second order cone programming, semi-definite programming, but also non-convex and more general optimization problems, such as integer programming, semi-infinite programming, bi-level programming and so on. The purpose of this paper is to understand SVM from the optimization point of view, review several representative optimization models in SVMs, their applications in economics, in order to promote the research interests in both optimization-based SVMs theory and economics applications. This paper starts with summarizing and explaining the nature of SVMs. It then proceeds to discuss optimization...", "year": 2012, "referenceCount": 174, "citationCount": 135, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "143790318", "name": "Ying-jie Tian"}, {"authorId": "48081578", "name": "Yong Shi"}, {"authorId": "87180246", "name": "Xiaohui Liu"}]}, {"paperId": "29f5ecc324e934d21fe8ddde814fca36cfe8eaea", "url": "https://www.semanticscholar.org/paper/29f5ecc324e934d21fe8ddde814fca36cfe8eaea", "title": "Using Three Machine Learning Techniques for Predicting Breast Cancer Recurrence", "abstract": "Objective: The number and size of medical databases are increasing rapidly but most of these data are not analyzed \nfor finding the valuable and hidden knowledge. Advanced data mining techniques can be used to discover hidden \npatterns and relationships. Models developed from these techniques are useful for medical practitioners to make right \ndecisions. The present research studied the application of data mining techniques to develop predictive models for \nbreast cancer recurrence in patients who were followed-up for two years. \nMethod: The patients were registered in the Iranian Center for Breast Cancer (ICBC) program from 1997 to 2008. \nThe dataset contained 1189 records, 22 predictor variables, and one outcome variable. We implemented machine \nlearning techniques, i.e., Decision Tree (C4.5), Support Vector Machine (SVM), and Artificial Neural Network (ANN) to \ndevelop the predictive models. The main goal of this paper is to compare the performance of these three well-known \nalgorithms on our data through sensitivity, specificity, and accuracy. \nResults and Conclusion: Our analysis shows that accuracy of DT, ANN and SVM are 0.936, 0.947 and 0.957 \nrespectively. The SVM classification model predicts breast cancer recurrence with least error rate and highest accuracy. \nThe predicted accuracy of the DT model is the lowest of all. The results are achieved using 10-fold cross-validation for \nmeasuring the unbiased prediction accuracy of each model.", "year": 2013, "referenceCount": 23, "citationCount": 198, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "71885275", "name": "Ahmad Lg"}, {"authorId": "2992866", "name": "A. T. Eshlaghy"}, {"authorId": "2096356", "name": "A. Poorebrahimi"}, {"authorId": "1380749467", "name": "M. Ebrahimi"}, {"authorId": "72860490", "name": "Razavi Ar"}]}, {"paperId": "0c95aac77da302d5c605028cdd275354fd7cef9b", "url": "https://www.semanticscholar.org/paper/0c95aac77da302d5c605028cdd275354fd7cef9b", "title": "Using Factor Oracles for Machine Improvisation", "abstract": null, "year": 2004, "referenceCount": 22, "citationCount": 164, "influentialCitationCount": 12, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2435162", "name": "G. Assayag"}, {"authorId": "2204186", "name": "S. Dubnov"}]}, {"paperId": "1e3154b10b872c100b86181ac2931c8e26f67912", "url": "https://www.semanticscholar.org/paper/1e3154b10b872c100b86181ac2931c8e26f67912", "title": "Theory of Disagreement-Based Active Learning", "abstract": "Active learning is a protocol for supervised machine learning, in which a learning algorithm sequentially requests the labels of selected data points from a large pool of unlabeled data. This contrasts with passive learning, where the labeled data are taken at random. The objective in active learning is to produce a highly-accurate classifier, ideally using fewer labels than the number of random labeled data sufficient for passive learning to achieve the same. This article describes recent advances in our understanding of the theoretical benefits of active learning, and implications for the design of effective active learning algorithms. Much of the article focuses on a particular technique, namely disagreement-based active learning, which by now has amassed a mature and coherent literature. It also briefly surveys several alternative approaches from the literature. The emphasis is on theorems regarding the performance of a few general algorithms, including rigorous proofs where appropriate. However, the presentation is intended to be pedagogical, focusing on results that illustrate fundamental ideas, rather than obtaining the strongest or most general known theorems. The intended audience includes researchers and advanced graduate students in machine learning and statistics, interested in gaining a deeper understanding of the recent and ongoing developments in the theory of active learning.", "year": 2014, "referenceCount": 86, "citationCount": 202, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1762565", "name": "Steve Hanneke"}]}, {"paperId": "790985a4bee821046992ff3d5322ff11dd1b4262", "url": "https://www.semanticscholar.org/paper/790985a4bee821046992ff3d5322ff11dd1b4262", "title": "Coresets for Data-efficient Training of Machine Learning Models", "abstract": "Incremental gradient (IG) methods, such as stochastic gradient descent and its variants are commonly used for large scale optimization in machine learning. Despite the sustained effort to make IG methods more data-efficient, it remains an open question how to select a training data subset that can theoretically and practically perform on par with the full dataset. Here we develop CRAIG, a method to select a weighted subset (or coreset) of training data that closely estimates the full gradient by maximizing a submodular function. We prove that applying IG to this subset is guaranteed to converge to the (near)optimal solution with the same convergence rate as that of IG for convex optimization. As a result, CRAIG achieves a speedup that is inversely proportional to the size of the subset. To our knowledge, this is the first rigorous method for data-efficient training of general machine learning models. Our extensive set of experiments show that CRAIG, while achieving practically the same solution, speeds up various IG methods by up to 6x for logistic regression and 3x for training deep neural networks.", "year": 2019, "referenceCount": 51, "citationCount": 67, "influentialCitationCount": 18, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2389094", "name": "Baharan Mirzasoleiman"}, {"authorId": "1748118", "name": "J. Bilmes"}, {"authorId": "1702139", "name": "J. Leskovec"}]}, {"paperId": "87038966ecc2708af4c5c1543b1e1a456d37f743", "url": "https://www.semanticscholar.org/paper/87038966ecc2708af4c5c1543b1e1a456d37f743", "title": "An Efficient Deep Learning Model to Predict Cloud Workload for Industry Informatics", "abstract": "Deep learning, as the most important architecture of current computational intelligence, achieves super performance to predict the cloud workload for industry informatics. However, it is a nontrivial task to train a deep learning model efficiently since the deep learning model often includes a great number of parameters. In this paper, an efficient deep learning model based on the canonical polyadic decomposition is proposed to predict the cloud workload for industry informatics. In the proposed model, the parameters are compressed significantly by converting the weight matrices to the canonical polyadic format. Furthermore, an efficient learning algorithm is designed to train the parameters. Finally, the proposed efficient deep learning model is applied to the workload prediction of virtual machines on cloud. Experiments are conducted on the datasets collected from PlanetLab to validate the performance of the proposed model by comparing with other machine-learning-based approaches for workload prediction of virtual machines. Results indicate that the proposed model achieves a higher training efficiency and workload prediction accuracy than state-of-the-art machine-learning-based approaches, proving the potential of the proposed model to provide predictive services for industry informatics.", "year": 2018, "referenceCount": 29, "citationCount": 129, "influentialCitationCount": 7, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2108076070", "name": "Qingchen Zhang"}, {"authorId": "1690341", "name": "L. Yang"}, {"authorId": "2152532057", "name": "Zheng Yan"}, {"authorId": "1803933", "name": "Zhikui Chen"}, {"authorId": "47471114", "name": "Peng Li"}]}, {"paperId": "e9b97f38a092b1056996defd7a129406decf06a0", "url": "https://www.semanticscholar.org/paper/e9b97f38a092b1056996defd7a129406decf06a0", "title": "Preference Learning", "abstract": null, "year": 2005, "referenceCount": 9, "citationCount": 272, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1747752", "name": "Johannes F\u00fcrnkranz"}, {"authorId": "1691955", "name": "E. H\u00fcllermeier"}]}, {"paperId": "dd5342086bf39633e32cf96ab1bf934987b6626f", "url": "https://www.semanticscholar.org/paper/dd5342086bf39633e32cf96ab1bf934987b6626f", "title": "Does Machine Learning Automate Moral Hazard and Error?", "abstract": "Machine learning tools are beginning to be deployed en masse in health care. While the statistical underpinnings of these techniques have been questioned with regard to causality and stability, we highlight a different concern here, relating to measurement issues. A characteristic feature of health data, unlike other applications of machine learning, is that neither y nor x is measured perfectly. Far from a minor nuance, this can undermine the power of machine learning algorithms to drive change in the health care system--and indeed, can cause them to reproduce and even magnify existing errors in human judgment.", "year": 2017, "referenceCount": 10, "citationCount": 92, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Economics", "Medicine"], "authors": [{"authorId": "2062143", "name": "S. Mullainathan"}, {"authorId": "3797258", "name": "Z. Obermeyer"}]}, {"paperId": "2c20e7220269b28fb1935a83d0e7f2db330aa691", "url": "https://www.semanticscholar.org/paper/2c20e7220269b28fb1935a83d0e7f2db330aa691", "title": "Wild Patterns: Ten Years After the Rise of Adversarial Machine Learning", "abstract": null, "year": 2017, "referenceCount": 127, "citationCount": 905, "influentialCitationCount": 64, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1684175", "name": "B. Biggio"}, {"authorId": "1710171", "name": "F. Roli"}]}, {"paperId": "2a4c98672f1fa529210109bffaf2b327c29676d9", "url": "https://www.semanticscholar.org/paper/2a4c98672f1fa529210109bffaf2b327c29676d9", "title": "Internet Traffic Identification using Machine Learning", "abstract": "We apply an unsupervised machine learning ap- proach for Internet traffic identification and compare the results with that of a previously applied supervised machine learning approach. Our unsupervised approach uses an Expectation Max- imization (EM) based clustering algorithm and the supervised approach uses the NaBayes classifier. We find the unsu- pervised clustering technique has an accuracy up to 91% and outperform the supervised technique by up to 9%. We also find that the unsupervised technique can be used to discover traffic from previously unknown applications and has the potential to become an excellent tool for exploring Internet traffic. I. INTRODUCTION Accurate classification of Internet traffic is important in many areas such as network design, network management, and network security. One key challenge in this area is to adapt to the dynamic nature of Internet traffic. Increasingly, new applications are being deployed on the Internet; some new applications such as peer-to-peer (P2P) file sharing and online gaming are becoming popular. With the evolution of Internet traffic, both in terms of number and type of applications, however, traditional classification techniques such as those based on well-known port numbers or packet payload analysis are either no longer effective for all types of network traffic or are otherwise unable to deploy because of privacy or security concerns for the data. A promising approach that has recently received some attention is traffic classification using machine learning tech- niques (1)-(4). These approaches assume that the applications typically send data in some sort of pattern; these patterns can be used as a means of identification which would allow the connections to be classified by traffic class. To find these patterns, flow statistics (such as mean packet size, flow length, and total number of packets) available using only TCP/IP headers are needed. This allows the classification technique to avoid the use of port numbers and packet payload information in the classification process. In this paper, we apply an unsupervised learning technique (EM clustering) for the Internet traffic classification problem and compare the results with that of a previously applied supervised machine learning approach. The unsupervised clus- tering approach uses an Expectation Maximization (EM) algo- rithm (5) that is different in that it classifies unlabeled training data into groups called \"clusters\" based on similarity. The NaBayes classifier has been previously shown to have high accuracy for Internet traffic classification (2). In parallel work, Zander et al. focus on using the EM clustering approach to build the classification model (4). We complement their work by using the EM clustering approach to build a classifier and show that this classifier outperforms the Na\u00a8 Bayes classifier in terms of classification accuracy. We also analyze the time required to build the classification models for both approaches as a function of the size of the training data set. We also explore the clusters found by the EM approach and find that the majority of the connections are in a subset of the total clusters. The rest of this paper is organized as follows. Section II presents related work. In Section III, the background on the algorithms used in the Na\u00a8ive Bayes and EM clustering approaches are covered. In Section IV, we introduce the data sets used in our work and present our experimental results. Section V discusses the advantages and disadvantages of the approaches. Section VI presents our conclusions and describes future work avenues.", "year": 2006, "referenceCount": 16, "citationCount": 114, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "49327521", "name": "Jeffrey Erman"}, {"authorId": "145801399", "name": "A. Mahanti"}, {"authorId": "2185810", "name": "M. Arlitt"}]}, {"paperId": "1457ab94d86444dd3efc3abab351c2ad1709e7aa", "url": "https://www.semanticscholar.org/paper/1457ab94d86444dd3efc3abab351c2ad1709e7aa", "title": "Addressing uncertainty in atomistic machine learning.", "abstract": "Machine-learning regression has been demonstrated to precisely emulate the potential energy and forces that are output from more expensive electronic-structure calculations. However, to predict new regions of the potential energy surface, an assessment must be made of the credibility of the predictions. In this perspective, we address the types of errors that might arise in atomistic machine learning, the unique aspects of atomistic simulations that make machine-learning challenging, and highlight how uncertainty analysis can be used to assess the validity of machine-learning predictions. We suggest this will allow researchers to more fully use machine learning for the routine acceleration of large, high-accuracy, or extended-time simulations. In our demonstrations, we use a bootstrap ensemble of neural network-based calculators, and show that the width of the ensemble can provide an estimate of the uncertainty when the width is comparable to that in the training data. Intriguingly, we also show that the uncertainty can be localized to specific atoms in the simulation, which may offer hints for the generation of training data to strategically improve the machine-learned representation.", "year": 2017, "referenceCount": 32, "citationCount": 90, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "34641730", "name": "A. Peterson"}, {"authorId": "50500281", "name": "R. Christensen"}, {"authorId": "3454273", "name": "A. Khorshidi"}]}, {"paperId": "b7c565e002471173b1b342dbee9c593b52e96f50", "url": "https://www.semanticscholar.org/paper/b7c565e002471173b1b342dbee9c593b52e96f50", "title": "A Machine Learning Approach for Non-blind Image Deconvolution", "abstract": "Image deconvolution is the ill-posed problem of recovering a sharp image, given a blurry one generated by a convolution. In this work, we deal with space-invariant non-blind deconvolution. Currently, the most successful methods involve a regularized inversion of the blur in Fourier domain as a first step. This step amplifies and colors the noise, and corrupts the image information. In a second (and arguably more difficult) step, one then needs to remove the colored noise, typically using a cleverly engineered algorithm. However, the methods based on this two-step approach do not properly address the fact that the image information has been corrupted. In this work, we also rely on a two-step procedure, but learn the second step on a large dataset of natural images, using a neural network. We will show that this approach outperforms the current state-of-the-art on a large dataset of artificially blurred images. We demonstrate the practical applicability of our method in a real-world example with photographic out-of-focus blur.", "year": 2013, "referenceCount": 33, "citationCount": 268, "influentialCitationCount": 30, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "1814533", "name": "Christian J. Schuler"}, {"authorId": "2068427959", "name": "Harold Christopher Burger"}, {"authorId": "1734990", "name": "S. Harmeling"}, {"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}]}, {"paperId": "d4f49717c9adb46137f49606ebbdf17e3598b5a5", "url": "https://www.semanticscholar.org/paper/d4f49717c9adb46137f49606ebbdf17e3598b5a5", "title": "Hyperopt: A Python Library for Optimizing the Hyperparameters of Machine Learning Algorithms", "abstract": "Sequential model-based optimization (also known as Bayesian optimization) is one of the most efficient methods (per function evaluation) of function minimization. This efficiency makes it appropriate for optimizing the hyperparameters of machine learning algorithms that are slow to train. The Hyperopt library provides algorithms and parallelization infrastructure for performing hyperparameter optimization (model selection) in Python. This paper presents an introductory tutorial on the usage of the Hyperopt library, including the description of search spaces, minimization (in serial and parallel), and the analysis of the results collected in the course of minimization. The paper closes with some discussion of ongoing and future work.", "year": 2013, "referenceCount": 5, "citationCount": 564, "influentialCitationCount": 60, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "32837403", "name": "J. Bergstra"}, {"authorId": "2292273", "name": "Daniel Yamins"}, {"authorId": "2042941", "name": "D. Cox"}]}, {"paperId": "4850442c8db48500ada13a060c4d1584a575de81", "url": "https://www.semanticscholar.org/paper/4850442c8db48500ada13a060c4d1584a575de81", "title": "Static prediction games for adversarial learning problems", "abstract": "The standard assumption of identically distributed training and test data is violated when the test data are generated in response to the presence of a predictive model. This becomes apparent, for example, in the context of email spam filtering. Here, email service providers employ spam filters, and spam senders engineer campaign templates to achieve a high rate of successful deliveries despite the filters. We model the interaction between the learner and the data generator as a static game in which the cost functions of the learner and the data generator are not necessarily antagonistic. We identify conditions under which this prediction game has a unique Nash equilibrium and derive algorithms that find the equilibrial prediction model. We derive two instances, the Nash logistic regression and the Nash support vector machine, and empirically explore their properties in a case study on email spam filtering.", "year": 2012, "referenceCount": 22, "citationCount": 187, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2057847091", "name": "Michael Br\u00fcckner"}, {"authorId": "1740448", "name": "C. Kanzow"}, {"authorId": "1751348", "name": "T. Scheffer"}]}, {"paperId": "586f5b3aaae4ff6e3a537e215f06a9333166baee", "url": "https://www.semanticscholar.org/paper/586f5b3aaae4ff6e3a537e215f06a9333166baee", "title": "Some experiments in machine learning using vector evaluated genetic algorithms (artificial intelligence, optimization, adaptation, pattern recognition)", "abstract": "This dissertation describes experiments conducted to explore the efficacy of using vector-valued feedback with a class of adaptive procedures called genetic algorithms. The software system developed was called VEGA for Vector Evaluated Genetic Algorithm and was first used on multiple objective optimization problems. The principle conclusion of these experiments was that VEGA provided a powerful and robust search technique for complex multiobjective optimization problems of high order when little or no a priori knowledge was available to guide the search. These results were similar to those found by previous researchers using scalar genetic algorithms for scalar optimization problems. \nThe VEGA technique was then applied to multiclass pattern discrimination tasks. The resulting software system was called LS-2 for Learning System - Two since it followed closely the lead of a scalar-valued learning system called LS-1 developed by Stephen Smith. The experiments revealed that LS-2 was able to evolve high performance production system programs to perform the pattern discrimination tasks it was given. In addition, experiments which varied several of the parameters of LS-2 revealed something of the sensitivity of vector-valued genetic search to the settings of these parameters. \nIn sum it may be said that the VEGA approach has demonstrated the efficacy of extending the previously demonstrated power of genetic algorithms to vector-valued problems and thereby provides a new approach to machine learning.", "year": 1984, "referenceCount": 0, "citationCount": 377, "influentialCitationCount": 37, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145738991", "name": "J. D. Schaffer"}]}, {"paperId": "5c42daf53192d625ce6549e56b6e0d11f6dfa18c", "url": "https://www.semanticscholar.org/paper/5c42daf53192d625ce6549e56b6e0d11f6dfa18c", "title": "Sample Selection Bias Correction Theory", "abstract": null, "year": 2008, "referenceCount": 27, "citationCount": 283, "influentialCitationCount": 37, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145115014", "name": "Corinna Cortes"}, {"authorId": "81080659", "name": "M. Mohri"}, {"authorId": "145428168", "name": "M. Riley"}, {"authorId": "2435268", "name": "Afshin Rostamizadeh"}]}, {"paperId": "e929cebf9d9a447f14e76b55fcb3cc504682d4df", "url": "https://www.semanticscholar.org/paper/e929cebf9d9a447f14e76b55fcb3cc504682d4df", "title": "Statistics for Machine Learning", "abstract": null, "year": 2017, "referenceCount": 0, "citationCount": 81, "influentialCitationCount": 8, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "66277033", "name": "Pratap Dangeti"}]}, {"paperId": "dafa29f1f0534448d205365796d68873a0068c6b", "url": "https://www.semanticscholar.org/paper/dafa29f1f0534448d205365796d68873a0068c6b", "title": "A Survey of Zero-Shot Learning", "abstract": "Most machine-learning methods focus on classifying instances whose classes have already been seen in training. In practice, many applications require classifying instances whose classes have not been seen previously. Zero-shot learning is a powerful and promising learning paradigm, in which the classes covered by training instances and the classes we aim to classify are disjoint. In this paper, we provide a comprehensive survey of zero-shot learning. First of all, we provide an overview of zero-shot learning. According to the data utilized in model optimization, we classify zero-shot learning into three learning settings. Second, we describe different semantic spaces adopted in existing zero-shot learning works. Third, we categorize existing zero-shot learning methods and introduce representative methods under each category. Fourth, we discuss different applications of zero-shot learning. Finally, we highlight promising future research directions of zero-shot learning.", "year": 2019, "referenceCount": 182, "citationCount": 261, "influentialCitationCount": 25, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2158627516", "name": "Wei Wang"}, {"authorId": "3113725", "name": "V. Zheng"}, {"authorId": "9380191", "name": "Han Yu"}, {"authorId": "1679209", "name": "C. Miao"}]}, {"paperId": "6f7ac41dc9321cb7478919de1da6da70009023da", "url": "https://www.semanticscholar.org/paper/6f7ac41dc9321cb7478919de1da6da70009023da", "title": "Evaluating Automated and Manual Acquisition of Anaphora Resolution Strategies", "abstract": "We describe one approach to build an automatically trainable anaphora resolution system. In this approach, we use Japanese newspaper articles tagged with discourse information as training examples for a machine learning algorithm which employs the C4.5 decision tree algorithm by Quinlan (Quinlan, 1993). Then, we evaluate and compare the results of several variants of the machine learning-based approach with those of our existing anaphora resolution system which uses manually-designed knowledge sources. Finally, we compare our algorithms with existing theories of anaphora, in particular, Japanese zero pronouns.", "year": 1995, "referenceCount": 15, "citationCount": 207, "influentialCitationCount": 22, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2939759", "name": "Chinatsu Aone"}, {"authorId": "48213604", "name": "Scott W. Bennett"}]}, {"paperId": "171a37d680daa11a85c99c76a9cf0d5a56e1568c", "url": "https://www.semanticscholar.org/paper/171a37d680daa11a85c99c76a9cf0d5a56e1568c", "title": "Quantum machine learning for electronic structure calculations", "abstract": null, "year": 2018, "referenceCount": 63, "citationCount": 89, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Physics", "Medicine"], "authors": [{"authorId": "27778453", "name": "Rongxin Xia"}, {"authorId": "1699217", "name": "S. Kais"}]}, {"paperId": "a4e2861c03a96d8827c5f0244e2ae71f4c17c1ec", "url": "https://www.semanticscholar.org/paper/a4e2861c03a96d8827c5f0244e2ae71f4c17c1ec", "title": "Machine Learning for Computer Vision", "abstract": null, "year": 2012, "referenceCount": 52, "citationCount": 97, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1745672", "name": "R. Cipolla"}, {"authorId": "1742452", "name": "S. Battiato"}, {"authorId": "1729739", "name": "G. Farinella"}]}, {"paperId": "ffa885d3b7a551f50e35ceb1cec37e8b1ede4aae", "url": "https://www.semanticscholar.org/paper/ffa885d3b7a551f50e35ceb1cec37e8b1ede4aae", "title": "Hierarchical Genetic Algorithms Operating on Populations of Computer Programs", "abstract": "Existing approaches to artificial intelligence problems such as sequence induction, automatic programming, machine learning, planning, and pattern recognition typically require specification in advance of the size and shape of the solution to the problem (often in a unnatural and difficult way). This paper reports on a new approach in which the size and shape of the solution to such problems is dynamically created using Darwinian principles of reproduction and survival of the fittest. Moreover, the resulting solution is inherently hierarchical. The paper describes computer experiments, using the author's 4341 line LISP program, in five areas of artifical intelligence, namely (1) sequence induction (e.g. inducing a computational procedure for the recursive Fibonacci sequence and inducing a computational procedure for a cubic polynomial sequence), (2) automatic programming (e.g. discovering a computational procedure for solving pairs of linear equations, solving quadratic equations for complex roots, and discovering trigonometric identities), (3) machine learning of functions (e.g. learning a Boolean multiplexer function previously studied in neural net and classifier system work and learning the exclusive-or and parity function), (4) planning (e.g. developing a robotic action sequence that can stack an arbitrary initial configuration of blocks into a specified order), and (5) pattern recognition (e.g. translation-invariant recognition of a simple one dimensional shape in a linear retina).", "year": 1989, "referenceCount": 20, "citationCount": 377, "influentialCitationCount": 19, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1732302", "name": "J. Koza"}]}, {"paperId": "72cda26eb077e2fb62d559af652988ca8a9abdcb", "url": "https://www.semanticscholar.org/paper/72cda26eb077e2fb62d559af652988ca8a9abdcb", "title": "Fall Classification by Machine Learning Using Mobile Phones", "abstract": "Fall prevention is a critical component of health care; falls are a common source of injury in the elderly and are associated with significant levels of mortality and morbidity. Automatically detecting falls can allow rapid response to potential emergencies; in addition, knowing the cause or manner of a fall can be beneficial for prevention studies or a more tailored emergency response. The purpose of this study is to demonstrate techniques to not only reliably detect a fall but also to automatically classify the type. We asked 15 subjects to simulate four different types of falls\u2013left and right lateral, forward trips, and backward slips\u2013while wearing mobile phones and previously validated, dedicated accelerometers. Nine subjects also wore the devices for ten days, to provide data for comparison with the simulated falls. We applied five machine learning classifiers to a large time-series feature set to detect falls. Support vector machines and regularized logistic regression were able to identify a fall with 98% accuracy and classify the type of fall with 99% accuracy. This work demonstrates how current machine learning approaches can simplify data collection for prevention in fall-related research as well as improve rapid response to potential injuries due to falls.", "year": 2012, "referenceCount": 37, "citationCount": 191, "influentialCitationCount": 9, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "3182510", "name": "Mark V. Albert"}, {"authorId": "3282030", "name": "Konrad Paul Kording"}, {"authorId": "36153569", "name": "Megan Herrmann"}, {"authorId": "2226306", "name": "A. Jayaraman"}]}, {"paperId": "542df9c07b06202e6df417d80e4a026cd567f30a", "url": "https://www.semanticscholar.org/paper/542df9c07b06202e6df417d80e4a026cd567f30a", "title": "Deep Learning with R", "abstract": null, "year": 2019, "referenceCount": 0, "citationCount": 103, "influentialCitationCount": 17, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Geology"], "authors": [{"authorId": "47380999", "name": "A. Ghatak"}]}, {"paperId": "7793e3f51f3c373bd75f9768e927ff6632fe8d69", "url": "https://www.semanticscholar.org/paper/7793e3f51f3c373bd75f9768e927ff6632fe8d69", "title": "Multilabel dimensionality reduction via dependence maximization", "abstract": "Multilabel learning deals with data associated with multiple labels simultaneously. Like other data mining and machine learning tasks, multilabel learning also suffers from the curse of dimensionality. Dimensionality reduction has been studied for many years, however, multilabel dimensionality reduction remains almost untouched. In this article, we propose a multilabel dimensionality reduction method, MDDM, with two kinds of projection strategies, attempting to project the original data into a lower-dimensional feature space maximizing the dependence between the original feature description and the associated class labels. Based on the Hilbert-Schmidt Independence Criterion, we derive a eigen-decomposition problem which enables the dimensionality reduction process to be efficient. Experiments validate the performance of MDDM.", "year": 2008, "referenceCount": 49, "citationCount": 328, "influentialCitationCount": 40, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2108506794", "name": "Yin Zhang"}, {"authorId": "145624000", "name": "Zhi-Hua Zhou"}]}, {"paperId": "fc4e87e1ff60cb80d50b3bd7a77d5b6672bbecbc", "url": "https://www.semanticscholar.org/paper/fc4e87e1ff60cb80d50b3bd7a77d5b6672bbecbc", "title": "Deep Belief Networks Based Voice Activity Detection", "abstract": "Fusing the advantages of multiple acoustic features is important for the robustness of voice activity detection (VAD). Recently, the machine-learning-based VADs have shown a superiority to traditional VADs on multiple feature fusion tasks. However, existing machine-learning-based VADs only utilize shallow models, which cannot explore the underlying manifold of the features. In this paper, we propose to fuse multiple features via a deep model, called deep belief network (DBN). DBN is a powerful hierarchical generative model for feature extraction. It can describe highly variant functions and discover the manifold of the features. We take the multiple serially-concatenated features as the input layer of DBN, and then extract a new feature by transferring these features through multiple nonlinear hidden layers. Finally, we predict the class of the new feature by a linear classifier. We further analyze that even a single-hidden-layer-based belief network is as powerful as the state-of-the-art models in the machine-learning-based VADs. In our empirical comparison, ten common features are used for performance analysis. Extensive experimental results on the AURORA2 corpus show that the DBN-based VAD not only outperforms eleven referenced VADs, but also can meet the real-time detection demand of VAD. The results also show that the DBN-based VAD can fuse the advantages of multiple features effectively.", "year": 2013, "referenceCount": 80, "citationCount": 295, "influentialCitationCount": 19, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2118044484", "name": "Xiao-Lei Zhang"}, {"authorId": "2130527774", "name": "Ji Wu"}]}, {"paperId": "43751304d64c9c56c00db6659f9d041ac4cd561c", "url": "https://www.semanticscholar.org/paper/43751304d64c9c56c00db6659f9d041ac4cd561c", "title": "Water quality prediction using machine learning methods", "abstract": "This study investigates the performance of artificial intelligence techniques including artificial neural network (ANN), group method of data handling (GMDH) and support vector machine (SVM) for predicting water quality components of Tireh River located in the southwest of Iran. To develop the ANN and SVM, different types of transfer and kernel functions were tested, respectively. Reviewing the results of ANN and SVM indicated that both models have suitable performance for predicting water quality components. During the process of development of ANN and SVM, it was found that tansig and RBF as transfer and kernel functions have the best performance among the tested functions. Comparison of outcomes of GMDH model with other applied models shows that although this model has acceptable performance for predicting the components of water quality, its accuracy is slightly less than ANN and SVM. The evaluation of the accuracy of the applied models according to the error indexes declared that SVM was the most accurate model. Examining the results of the models showed that all of them had some over-estimation properties. By evaluating the results of the models based on the DDR index, it was found that the lowest DDR value was related to the performance of the SVM model.", "year": 2018, "referenceCount": 55, "citationCount": 130, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "8517689", "name": "A. Haghiabi"}, {"authorId": "50304295", "name": "A. Nasrolahi"}, {"authorId": "31855805", "name": "A. Parsaie"}]}, {"paperId": "57a7260208da54b0abc9b9435bc40091553ca561", "url": "https://www.semanticscholar.org/paper/57a7260208da54b0abc9b9435bc40091553ca561", "title": "Convolutional Neural Support Vector Machines: Hybrid Visual Pattern Classifiers for Multi-robot Systems", "abstract": "We introduce Convolutional Neural Support Vector Machines (CNSVMs), a combination of two heterogeneous supervised classification techniques, Convolutional Neural Networks (CNNs) and Support Vector Machines (SVMs). CNSVMs are trained using a Stochastic Gradient Descent approach, that provides the computational capability of online incremental learning and is robust for typical learning scenarios in which training samples arrive in mini-batches. This is the case for visual learning and recognition in multi-robot systems, where each robot acquires a different image of the same sample. The experimental results indicate that the CNSVM can be successfully applied to visual learning and recognition of hand gestures as well as to measure learning progress.", "year": 2012, "referenceCount": 25, "citationCount": 53, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3022882", "name": "J. Nagi"}, {"authorId": "1744127", "name": "G. D. Caro"}, {"authorId": "33354551", "name": "A. Giusti"}, {"authorId": "2242640", "name": "F. Nagi"}, {"authorId": "6803671", "name": "L. Gambardella"}]}, {"paperId": "fc8db0fc1fece539f2af39d798efb0463928f8bb", "url": "https://www.semanticscholar.org/paper/fc8db0fc1fece539f2af39d798efb0463928f8bb", "title": "Learning from Examples as an Inverse Problem", "abstract": "Many works related learning from examples to regularization techniques for inverse problems, emphasizing the strong algorithmic and conceptual analogy of certain learning algorithms with regularization algorithms. In particular it is well known that regularization schemes such as Tikhonov regularization can be effectively used in the context of learning and are closely related to algorithms such as support vector machines. Nevertheless the connection with inverse problem was considered only for the discrete (finite sample) problem and the probabilistic aspects of learning from examples were not taken into account. In this paper we provide a natural extension of such analysis to the continuous (population) case and study the interplay between the discrete and continuous problems. From a theoretical point of view, this allows to draw a clear connection between the consistency approach in learning theory and the stability convergence property in ill-posed inverse problems. The main mathematical result of the paper is a new probabilistic bound for the regularized least-squares algorithm. By means of standard results on the approximation term, the consistency of the algorithm easily follows.", "year": 2005, "referenceCount": 61, "citationCount": 211, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "144305413", "name": "E. D. Vito"}, {"authorId": "1690976", "name": "L. Rosasco"}, {"authorId": "2277624", "name": "A. Caponnetto"}, {"authorId": "39872333", "name": "U. Giovannini"}, {"authorId": "1712692", "name": "F. Odone"}]}, {"paperId": "d9188c981b0a458a657d49453e2aac0c4a599e19", "url": "https://www.semanticscholar.org/paper/d9188c981b0a458a657d49453e2aac0c4a599e19", "title": "Structured large margin machines: sensitive to data distributions", "abstract": null, "year": 2007, "referenceCount": 29, "citationCount": 71, "influentialCitationCount": 10, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "143784081", "name": "D. Yeung"}, {"authorId": "2145350567", "name": "Defeng Wang"}, {"authorId": "38218996", "name": "Wing W. Y. Ng"}, {"authorId": "1830488", "name": "E. Tsang"}, {"authorId": "40162103", "name": "Xizhao Wang"}]}, {"paperId": "10e9936c02bd79a0df49df329d6ae04042ad684a", "url": "https://www.semanticscholar.org/paper/10e9936c02bd79a0df49df329d6ae04042ad684a", "title": "Is Extreme Learning Machine Feasible? A Theoretical Assessment (Part II)", "abstract": "An extreme learning machine (ELM) can be regarded as a two-stage feed-forward neural network (FNN) learning system that randomly assigns the connections with and within hidden neurons in the first stage and tunes the connections with output neurons in the second stage. Therefore, ELM training is essentially a linear learning problem, which significantly reduces the computational burden. Numerous applications show that such a computation burden reduction does not degrade the generalization capability. It has, however, been open that whether this is true in theory. The aim of this paper is to study the theoretical feasibility of ELM by analyzing the pros and cons of ELM. In the previous part of this topic, we pointed out that via appropriately selected activation functions, ELM does not degrade the generalization capability in the sense of expectation. In this paper, we launch the study in a different direction and show that the randomness of ELM also leads to certain negative consequences. On one hand, we find that the randomness causes an additional uncertainty problem of ELM, both in approximation and learning. On the other hand, we theoretically justify that there also exist activation functions such that the corresponding ELM degrades the generalization capability. In particular, we prove that the generalization capability of ELM with Gaussian kernel is essentially worse than that of FNN with Gaussian kernel. To facilitate the use of ELM, we also provide a remedy to such a degradation. We find that the well-developed coefficient regularization technique can essentially improve the generalization capability. The obtained results reveal the essential characteristic of ELM in a certain sense and give theoretical guidance concerning how to use ELM.", "year": 2014, "referenceCount": 67, "citationCount": 104, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"], "authors": [{"authorId": "2432506", "name": "Shaobo Lin"}, {"authorId": "2110880625", "name": "Xia Liu"}, {"authorId": "2115378715", "name": "Jian Fang"}, {"authorId": "98220533", "name": "Zongben Xu"}]}, {"paperId": "3d80107be8c081cb0d9a5488e9d259783c407d01", "url": "https://www.semanticscholar.org/paper/3d80107be8c081cb0d9a5488e9d259783c407d01", "title": "Musical genre classification using support vector machines", "abstract": "Automatic musical genre classification is very useful for music indexing and retrieval. In this paper, an efficient and effective automatic musical genre classification approach is presented. A set of features is extracted and used to characterize music content. A multi-layer classifier based on support vector machines is applied to musical genre classification. Support vector machines are used to obtain the optimal class boundaries between different genres of music by learning from training data. Experimental results of multi-layer support vector machines illustrate good performance in musical genre classification and are more advantageous than traditional Euclidean distance based method and other statistic learning methods.", "year": 2003, "referenceCount": 10, "citationCount": 155, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145194969", "name": "Changsheng Xu"}, {"authorId": "2256869", "name": "N. Maddage"}, {"authorId": "2056132398", "name": "Xi Shao"}, {"authorId": "2061271818", "name": "Fang Cao"}, {"authorId": "2056267488", "name": "Q. Tian"}]}, {"paperId": "aa9b433daef865e7b3b1e3704b2b96d64bfaf78b", "url": "https://www.semanticscholar.org/paper/aa9b433daef865e7b3b1e3704b2b96d64bfaf78b", "title": "A structural approach to relaxation in glassy liquids", "abstract": null, "year": 2015, "referenceCount": 34, "citationCount": 263, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Materials Science"], "authors": [{"authorId": "2601641", "name": "S. Schoenholz"}, {"authorId": "8132903", "name": "E. D. Cubuk"}, {"authorId": "11644159", "name": "D. Sussman"}, {"authorId": "144996542", "name": "E. Kaxiras"}, {"authorId": "3307247", "name": "Andrea J. Liu"}]}, {"paperId": "07f229cc6e5b80fc8ffbbb3e4db85142466f55a9", "url": "https://www.semanticscholar.org/paper/07f229cc6e5b80fc8ffbbb3e4db85142466f55a9", "title": "DeepChain: Auditable and Privacy-Preserving Deep Learning with Blockchain-Based Incentive", "abstract": "Deep learning can achieve higher accuracy than traditional machine learning algorithms in a variety of machine learning tasks. Recently, privacy-preserving deep learning has drawn tremendous attention from information security community, in which neither training data nor the training model is expected to be exposed. Federated learning is a popular learning mechanism, where multiple parties upload local gradients to a server and the server updates model parameters with the collected gradients. However, there are many security problems neglected in federated learning, for example, the participants may behave incorrectly in gradient collecting or parameter updating, and the server may be malicious as well. In this article, we present a distributed, secure, and fair deep learning framework named DeepChain to solve these problems. DeepChain provides a value-driven incentive mechanism based on Blockchain to force the participants to behave correctly. Meanwhile, DeepChain guarantees data privacy for each participant and provides auditability for the whole training process. We implement a prototype of DeepChain and conduct experiments on a real dataset for different settings, and the results show that our DeepChain is promising.", "year": 2019, "referenceCount": 76, "citationCount": 234, "influentialCitationCount": 20, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "51148392", "name": "Jiasi Weng"}, {"authorId": "145369053", "name": "J. Weng"}, {"authorId": "35834541", "name": "Ming Li"}, {"authorId": "48378565", "name": "Yue Zhang"}, {"authorId": "144644709", "name": "Weiqi Luo"}]}, {"paperId": "0594c62529549fae2c91909539c4df8682fce442", "url": "https://www.semanticscholar.org/paper/0594c62529549fae2c91909539c4df8682fce442", "title": "Multinomial Naive Bayes for Text Categorization Revisited", "abstract": null, "year": 2004, "referenceCount": 17, "citationCount": 392, "influentialCitationCount": 37, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1879932", "name": "A. M. Kibriya"}, {"authorId": "143713826", "name": "Eibe Frank"}, {"authorId": "1737420", "name": "B. Pfahringer"}, {"authorId": "144282963", "name": "G. Holmes"}]}, {"paperId": "15e18a801c0016766d4544e4440e615c4e5d6952", "url": "https://www.semanticscholar.org/paper/15e18a801c0016766d4544e4440e615c4e5d6952", "title": "Identification of COVID-19 can be quicker through artificial intelligence framework using a mobile phone\u2013based survey when cities and towns are under quarantine", "abstract": "Abstract We propose the use of a machine learning algorithm to improve possible COVID-19 case identification more quickly using a mobile phone\u2013based web survey. This method could reduce the spread of the virus in susceptible populations under quarantine.", "year": 2020, "referenceCount": 24, "citationCount": 260, "influentialCitationCount": 2, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "7923294", "name": "Arni S. R. Srinivasa Rao"}, {"authorId": "122753891", "name": "J. Vazquez"}]}, {"paperId": "4df2176e5310ec09bbc27b6a88001eec9669c8ec", "url": "https://www.semanticscholar.org/paper/4df2176e5310ec09bbc27b6a88001eec9669c8ec", "title": "Prediction of Student Dropout in E-Learning Program Through the Use of Machine Learning Method", "abstract": "The high rate of dropout is a serious problem in E-learning program. Thus it has received extensive concern from the education administrators and researchers. Predicting the potential dropout students is a workable solution to prevent dropout. Based on the analysis of related literature, this study selected student\u2019s personal characteristic and academic performance as input attributions. Prediction models were developed using Artificial Neural Network (ANN), Decision Tree (DT) and Bayesian Networks (BNs). A large sample of 62375 students was utilized in the procedures of model training and testing. The results of each model were presented in confusion matrix, and analyzed by calculating the rates of accuracy, precision, recall, and F-measure. The results suggested all of the three machine learning methods were effective in student dropout prediction, and DT presented a better performance. Finally, some suggestions were made for considerable future research.", "year": 2015, "referenceCount": 44, "citationCount": 91, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2095389", "name": "Mingjie Tan"}, {"authorId": "37782184", "name": "Peiji Shao"}]}, {"paperId": "77122542dfb5772b394cfc95acef885583f6414d", "url": "https://www.semanticscholar.org/paper/77122542dfb5772b394cfc95acef885583f6414d", "title": "Statistical Learning and Kernel Methods", "abstract": null, "year": 2001, "referenceCount": 40, "citationCount": 154, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1707625", "name": "B. Sch\u00f6lkopf"}]}, {"paperId": "17fb6c82f9725cd781c91fb8ece15733ea82f66e", "url": "https://www.semanticscholar.org/paper/17fb6c82f9725cd781c91fb8ece15733ea82f66e", "title": "Photonic delay systems as machine learning implementations", "abstract": "Nonlinear photonic delay systems present interesting implementation platforms for machine learning models. They can be extremely fast, offer great degrees of parallelism and potentially consume far less power than digital processors. So far they have been successfully employed for signal processing using the Reservoir Computing paradigm. In this paper we show that their range of applicability can be greatly extended if we use gradient descent with backpropagation through time on a model of the system to optimize the input encoding of such systems. We perform physical experiments that demonstrate that the obtained input encodings work well in reality, and we show that optimized systems perform significantly better than the common Reservoir Computing approach. The results presented here demonstrate that common gradient descent techniques from machine learning may well be applicable on physical neuro-inspired analog computers.", "year": 2015, "referenceCount": 36, "citationCount": 43, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2179062", "name": "Michiel Hermans"}, {"authorId": "144524298", "name": "M. C. Soriano"}, {"authorId": "2309489", "name": "J. Dambre"}, {"authorId": "3275560", "name": "P. Bienstman"}, {"authorId": "1795232", "name": "Ingo Fischer"}]}, {"paperId": "c0ec50a3a933e9d55efa1a304a76c4cdf9367442", "url": "https://www.semanticscholar.org/paper/c0ec50a3a933e9d55efa1a304a76c4cdf9367442", "title": "Proceedings of the European Conference on Machine Learning", "abstract": null, "year": 1993, "referenceCount": 0, "citationCount": 286, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1712908", "name": "P. Brazdil"}]}, {"paperId": "22c2c3020c6379e3d72e541783b99ff9355a0625", "url": "https://www.semanticscholar.org/paper/22c2c3020c6379e3d72e541783b99ff9355a0625", "title": "Encog: library of interchangeable machine learning models for Java and C#", "abstract": "This paper introduces the Encog library for Java and C#, a scalable, adaptable, multiplatform machine learning framework that was 1st released in 2008. Encog allows a variety of machine learning models to be applied to datasets using regression, classification, and clustering. Various supported machine learning models can be used interchangeably with minimal recoding. Encog uses efficient multithreaded code to reduce training time by exploiting modern multicore processors. The current version of Encog can be downloaded from this http URL", "year": 2015, "referenceCount": 22, "citationCount": 78, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3150244", "name": "Jeff Heaton"}]}, {"paperId": "593338fbbb79065891beaf985a95860bd39f52bf", "url": "https://www.semanticscholar.org/paper/593338fbbb79065891beaf985a95860bd39f52bf", "title": "Semi-Supervised Learning for Neural Machine Translation", "abstract": "While end-to-end neural machine translation (NMT) has made remarkable progress recently, NMT systems only rely on parallel corpora for parameter estimation. Since parallel corpora are usually limited in quantity, quality, and coverage, especially for low-resource languages, it is appealing to exploit monolingual corpora to improve NMT. We propose a semi-supervised approach for training NMT models on the concatenation of labeled (parallel corpora) and unlabeled (monolingual corpora) data. The central idea is to reconstruct the monolingual corpora using an autoencoder, in which the source-to-target and target-to-source translation models serve as the encoder and decoder, respectively. Our approach can not only exploit the monolingual corpora of the target language, but also of the source language. Experiments on the Chinese-English dataset show that our approach achieves significant improvements over state-of-the-art SMT and NMT systems.", "year": 2016, "referenceCount": 34, "citationCount": 161, "influentialCitationCount": 6, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145161801", "name": "Yong Cheng"}, {"authorId": "47210460", "name": "W. Xu"}, {"authorId": "37985966", "name": "Zhongjun He"}, {"authorId": "48692318", "name": "W. He"}, {"authorId": "40354707", "name": "Hua Wu"}, {"authorId": "1753344", "name": "Maosong Sun"}, {"authorId": "2152798100", "name": "Yang Liu"}]}, {"paperId": "a844595c3b561b00325c75d00e5cc7f639e8ed9a", "url": "https://www.semanticscholar.org/paper/a844595c3b561b00325c75d00e5cc7f639e8ed9a", "title": "Event-Driven Random Back-Propagation: Enabling Neuromorphic Deep Learning Machines", "abstract": "An ongoing challenge in neuromorphic computing is to devise general and computationally efficient models of inference and learning which are compatible with the spatial and temporal constraints of the brain. The gradient descent back-propagation rule is a powerful algorithm that is ubiquitous in deep learning, but it relies on the immediate availability of network-wide information stored with high-precision memory. However, recent work shows that exact backpropagated weights are not essential for learning deep representations. Here, we demonstrate an event-driven random backpropagation (eRBP) rule that uses an error-modulated synaptic plasticity rule for learning deep representations in neuromorphic computing hardware. The rule is very suitable for implementation in neuromorphic hardware using a two-compartment leaky integrate & fire neuron and a membrane-voltage modulated, spike-driven plasticity rule. Our results show that using eRBP, deep representations are rapidly learned without using backpropagated gradients, achieving nearly identical classification accuracies compared to artificial neural network simulations on GPUs, while being robust to neural and synaptic state quantizations during learning.", "year": 2016, "referenceCount": 99, "citationCount": 207, "influentialCitationCount": 8, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "1734355", "name": "E. Neftci"}, {"authorId": "145696267", "name": "C. Augustine"}, {"authorId": "7908929", "name": "Somnath Paul"}, {"authorId": "8119742", "name": "Georgios Detorakis"}]}, {"paperId": "3df911e7c59258ef385d770f55f4d2c3fe044a38", "url": "https://www.semanticscholar.org/paper/3df911e7c59258ef385d770f55f4d2c3fe044a38", "title": "Spatiotemporal Learning via Infinite-Dimensional Bayesian Filtering and Smoothing: A Look at Gaussian Process Regression Through Kalman Filtering", "abstract": "Gaussian process-based machine learning is a powerful Bayesian paradigm for nonparametric nonlinear regression and classification. In this article, we discuss connections of Gaussian process regression with Kalman filtering and present methods for converting spatiotemporal Gaussian process regression problems into infinite-dimensional state-space models. This formulation allows for use of computationally efficient infinite-dimensional Kalman filtering and smoothing methods, or more general Bayesian filtering and smoothing methods, which reduces the problematic cubic complexity of Gaussian process regression in the number of time steps into linear time complexity. The implication of this is that the use of machine-learning models in signal processing becomes computationally feasible, and it opens the possibility to combine machine-learning techniques with signal processing methods.", "year": 2013, "referenceCount": 29, "citationCount": 165, "influentialCitationCount": 10, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "30443320", "name": "S. S\u00e4rkk\u00e4"}, {"authorId": "145410662", "name": "A. Solin"}, {"authorId": "34153908", "name": "Jouni Hartikainen"}]}, {"paperId": "bc7dce62e404d9d3f313bcc7bdb21129be21e6e4", "url": "https://www.semanticscholar.org/paper/bc7dce62e404d9d3f313bcc7bdb21129be21e6e4", "title": "A weighted nearest neighbor algorithm for learning with symbolic features", "abstract": null, "year": 2004, "referenceCount": 39, "citationCount": 217, "influentialCitationCount": 15, "isOpenAccess": true, "fieldsOfStudy": null, "authors": [{"authorId": "12725437", "name": "S. Cost"}, {"authorId": "1744109", "name": "S. Salzberg"}]}, {"paperId": "0e18db82467f100589f1a285ce34698536b0cd7a", "url": "https://www.semanticscholar.org/paper/0e18db82467f100589f1a285ce34698536b0cd7a", "title": "Applying machine learning to anaphora resolution", "abstract": null, "year": 1995, "referenceCount": 10, "citationCount": 47, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2939759", "name": "Chinatsu Aone"}, {"authorId": "48213604", "name": "Scott W. Bennett"}]}, {"paperId": "24636a3ee6b9fdfed2c4eb1643d8e1b39665f06d", "url": "https://www.semanticscholar.org/paper/24636a3ee6b9fdfed2c4eb1643d8e1b39665f06d", "title": "Predicting the Band Gaps of Inorganic Solids by Machine Learning.", "abstract": "A machine-learning model is developed that can accurately predict the band gap of inorganic solids based only on composition. This method uses support vector classification to first separate metals from nonmetals, followed by quantitatively predicting the band gap of the nonmetals using support vector regression. The superb accuracy of the regression model is obtained by using a training set composed entirely of experimentally measured band gaps and utilizing only compositional descriptors. In fact, because of the unique training set of experimental data, the machine learning predicted band gaps are significantly closer to the experimentally reported values than DFT (PBE-level) calculated band gaps. Not only does this resulting tool provide the ability to accurately predict the band gap for any composition but also the versatility and speed of the prediction based only on composition will make this a great resource to screen inorganic phase space and direct the development of functional inorganic materials.", "year": 2018, "referenceCount": 0, "citationCount": 189, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "49501947", "name": "Ya Zhuo"}, {"authorId": "14646016", "name": "Aria Mansouri Tehrani"}, {"authorId": "13191312", "name": "Jakoah Brgoch"}]}, {"paperId": "0620c2e510f5dfc1c2b717b3ac8ac132a653c1b0", "url": "https://www.semanticscholar.org/paper/0620c2e510f5dfc1c2b717b3ac8ac132a653c1b0", "title": "Network Intrusion Detection using Supervised Machine Learning Technique with Feature Selection", "abstract": "A novel supervised machine learning system is developed to classify network traffic whether it is malicious or benign. To find the best model considering detection success rate, combination of supervised learning algorithm and feature selection method have been used. Through this study, it is found that Artificial Neural Network (ANN) based machine learning with wrapper feature selection outperform support vector machine (SVM) technique while classifying network traffic. To evaluate the performance, NSL-KDD dataset is used to classify network traffic using SVM and ANN supervised machine learning techniques. Comparative study shows that the proposed model is efficient than other existing models with respect to intrusion detection success rate.", "year": 2019, "referenceCount": 20, "citationCount": 75, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "5102294", "name": "K. A. Taher"}, {"authorId": "73771648", "name": "Billal Mohammed Yasin Jisan"}, {"authorId": "9449574", "name": "M. Rahman"}]}, {"paperId": "082f4323a6631741a448526d6de4444712ce8539", "url": "https://www.semanticscholar.org/paper/082f4323a6631741a448526d6de4444712ce8539", "title": "Self-adaptive extreme learning machine", "abstract": null, "year": 2016, "referenceCount": 61, "citationCount": 94, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1781320", "name": "Gaige Wang"}, {"authorId": "2111220743", "name": "Mei Lu"}, {"authorId": "2115750823", "name": "Yong-Quan Dong"}, {"authorId": "2302428", "name": "Xiangjun Zhao"}]}, {"paperId": "9a12ac1e3b51f30042abdadc5636c88d41bf0ca7", "url": "https://www.semanticscholar.org/paper/9a12ac1e3b51f30042abdadc5636c88d41bf0ca7", "title": "KEA: practical automatic keyphrase extraction", "abstract": "Keyphrases provide semantic metadata that summarize and characterize documents. This paper describes Kea, an algorithm for automatically extracting keyphrases from text. Kea identifies candidate keyphrases using lexical methods, calculates feature values for each candidate, and uses a machinelearning algorithm to predict which candidates are good keyphrases. The machine learning scheme first builds a prediction model using training documents with known keyphrases, and then uses the model to find keyphrases in new documents. We use a large test corpus to evaluate Kea\u2019s effectiveness in terms of how many author-assigned keyphrases are correctly identified. The system is simple, robust, and publicly available.", "year": 1999, "referenceCount": 23, "citationCount": 1023, "influentialCitationCount": 139, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "9419406", "name": "I. Witten"}, {"authorId": "2171407", "name": "G. Paynter"}, {"authorId": "143713826", "name": "Eibe Frank"}, {"authorId": "1693768", "name": "C. Gutwin"}, {"authorId": "1389909404", "name": "C. Nevill-Manning"}]}, {"paperId": "ef6efab98ec19f1be937a21f9a98c4c3686b1122", "url": "https://www.semanticscholar.org/paper/ef6efab98ec19f1be937a21f9a98c4c3686b1122", "title": "Optimized Scoring Systems: Toward Trust in Machine Learning for Healthcare and Criminal Justice", "abstract": "The authors developed and implemented transparent machine-learning models that call into question the use of black-box machine-learning models in healthcare and criminal justice applications.", "year": 2018, "referenceCount": 113, "citationCount": 74, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Business", "Computer Science"], "authors": [{"authorId": "48395540", "name": "C. Rudin"}, {"authorId": "3072590", "name": "Berk Ustun"}]}, {"paperId": "887a80f2b85c1737f8f882986d078017654f23e4", "url": "https://www.semanticscholar.org/paper/887a80f2b85c1737f8f882986d078017654f23e4", "title": "Fast rates for support vector machines using Gaussian kernels", "abstract": "For binary classification we establish learning rates up to the order of n \u22121 for support vector machines (SVMs) with hinge loss and Gaussian RBF kernels. These rates are in terms of two assumptions on the considered distributions: Tsybakov\u2019s noise assumption to establish a small estimation error, and a new geometric noise condition which is used to bound the approximation error. Unlike previously proposed concepts for bounding the approximation error, the geometric noise assumption does not employ any smoothness assumption. 1. Introduction. In recent years support vector machines (SVMs) have been the subject of many theoretical considerations. Despite this effort, their learning performance on restricted classes of distributions is still widely unknown. In particular, it is unknown under which nontrivial circumstances SVMs can guarantee fast learning rates. The aim of this work is to use concepts like Tsybakov\u2019s noise assumption and local Rademacher averages to establish learning rates up to the order of n \u22121 for nontrivial distributions. In addition to these concepts that are used to deal with the stochastic part of the analysis we also introduce a geometric assumption for distributions that allows us to estimate the approximation properties of Gaussian RBF kernels. Unlike many other concepts introduced for bounding the approximation error, our geometric assumption is not in terms of smoothness but describes the concentration and the noisiness of the data-generating distribution near the decision boundary. Let us formally introduce the statistical classification problem. To this end let us fix a subset X \u2282 R d . We write Y := {\u22121,1}. Given a finite training set", "year": 2007, "referenceCount": 50, "citationCount": 264, "influentialCitationCount": 38, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "1782193", "name": "Ingo Steinwart"}, {"authorId": "143790221", "name": "C. Scovel"}]}, {"paperId": "0f4cb4ca84cb23c1454d319005647ea47a3daa03", "url": "https://www.semanticscholar.org/paper/0f4cb4ca84cb23c1454d319005647ea47a3daa03", "title": "VCONF: a reinforcement learning approach to virtual machines auto-configuration", "abstract": "Virtual machine (VM) technology enables multiple VMs to share resources on the same host. Resources allocated to the VMs should be re-configured dynamically in response to the change of application demands or resource supply. Because VM execution involves privileged domain and VM monitor, this causes uncertainties in VMs' resource to performance mapping and poses challenges in online determination of appropriate VM configurations. In this paper, we propose a reinforcement learning (RL) based approach, namely VCONF, to automate the VM configuration process. VCONF employs model-based RL algorithms to address the scalability and adaptability issues in applying RL in systems management. Experimental results on both controlled environments and a testbed of clouds with Xen VMs and representative server workloads demonstrate the effectiveness of VCONF. The approach is able to find optimal (near optimal) configurations in small scale systems and shows good adaptability and scalability.", "year": 2009, "referenceCount": 29, "citationCount": 263, "influentialCitationCount": 16, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144711146", "name": "J. Rao"}, {"authorId": "1983401", "name": "Xiangping Bu"}, {"authorId": "71868840", "name": "Chengzhong Xu"}, {"authorId": "2108571638", "name": "L. Wang"}, {"authorId": "143634612", "name": "G. Yin"}]}, {"paperId": "8446830f3c05b97c4d12a0751c022d1ae6a5115b", "url": "https://www.semanticscholar.org/paper/8446830f3c05b97c4d12a0751c022d1ae6a5115b", "title": "Learning to Extract Symbolic Knowledge from the World Wide Web", "abstract": "The World Wide Web is a vast source of information accessible to computers, but understandable only to humans. The goal of the research described here is to automatically create a computer understandable world wide knowledge base whose content mirrors that of the World Wide Web. Such a knowledge base would enable much more effective retrieval of Web information, and promote new uses of the Web to support knowledge-based inference and problem solving. Our approach is to develop a trainable information extraction system that takes two inputs: an ontology defining the classes and relations of interest, and a set of training data consisting of labeled regions of hypertext representing instances of these classes and relations. Given these inputs, the system learns to extract information from other pages and hyperlinks on the Web. This paper describes our general approach, several machine learning algorithms for this task, and promising initial results with a prototype system.", "year": 1998, "referenceCount": 89, "citationCount": 814, "influentialCitationCount": 66, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144557047", "name": "M. Craven"}, {"authorId": "2922396", "name": "Dan DiPasquo"}, {"authorId": "1758106", "name": "D. Freitag"}, {"authorId": "143753639", "name": "A. McCallum"}, {"authorId": "40975594", "name": "Tom Michael Mitchell"}, {"authorId": "145172877", "name": "K. Nigam"}, {"authorId": "1682522", "name": "Se\u00e1n Slattery"}]}, {"paperId": "b95dda2f2e606c91b69f371c1a706248cb5f69bb", "url": "https://www.semanticscholar.org/paper/b95dda2f2e606c91b69f371c1a706248cb5f69bb", "title": "On the Feasibility of Deep Learning in Sensor Network Intrusion Detection", "abstract": "In this letter, we present a comprehensive analysis of the use of machine and deep learning (DL) solutions for IDS systems in wireless sensor networks (WSNs). To accomplish this, we introduce restricted Boltzmann machine-based clustered IDS (RBC-IDS), a potential DL-based IDS methodology for monitoring critical infrastructures by WSNs. We study the performance of RBC-IDS, and compare it to the previously proposed adaptive machine learning-based IDS: the adaptively supervised and clustered hybrid IDS (ASCH-IDS). Numerical results show that RBC-IDS and ASCH-IDS achieve the same detection and accuracy rates, though the detection time of RBC-IDS is approximately twice that of ASCH-IDS.", "year": 2019, "referenceCount": 16, "citationCount": 150, "influentialCitationCount": 3, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2823694", "name": "S. Otoum"}, {"authorId": "2497479", "name": "B. Kantarci"}, {"authorId": "144769367", "name": "H. Mouftah"}]}, {"paperId": "fbe44e3b52e7f2b481e7480a2507ffcc80a63ff2", "url": "https://www.semanticscholar.org/paper/fbe44e3b52e7f2b481e7480a2507ffcc80a63ff2", "title": "Machine Learning Applications for Mass Spectrometry-Based Metabolomics", "abstract": "The metabolome of an organism depends on environmental factors and intracellular regulation and provides information about the physiological conditions. Metabolomics helps to understand disease progression in clinical settings or estimate metabolite overproduction for metabolic engineering. The most popular analytical metabolomics platform is mass spectrometry (MS). However, MS metabolome data analysis is complicated, since metabolites interact nonlinearly, and the data structures themselves are complex. Machine learning methods have become immensely popular for statistical analysis due to the inherent nonlinear data representation and the ability to process large and heterogeneous data rapidly. In this review, we address recent developments in using machine learning for processing MS spectra and show how machine learning generates new biological insights. In particular, supervised machine learning has great potential in metabolomics research because of the ability to supply quantitative predictions. We review here commonly used tools, such as random forest, support vector machines, artificial neural networks, and genetic algorithms. During processing steps, the supervised machine learning methods help peak picking, normalization, and missing data imputation. For knowledge-driven analysis, machine learning contributes to biomarker detection, classification and regression, biochemical pathway identification, and carbon flux determination. Of important relevance is the combination of different omics data to identify the contributions of the various regulatory levels. Our overview of the recent publications also highlights that data quality determines analysis quality, but also adds to the challenge of choosing the right model for the data. Machine learning methods applied to MS-based metabolomics ease data analysis and can support clinical decisions, guide metabolic engineering, and stimulate fundamental biological discoveries.", "year": 2020, "referenceCount": 142, "citationCount": 82, "influentialCitationCount": 1, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "3230233", "name": "Ulf W. Liebal"}, {"authorId": "1703842397", "name": "An N T Phan"}, {"authorId": "1473597205", "name": "Malvika Sudhakar"}, {"authorId": "145874089", "name": "Karthik Raman"}, {"authorId": "3062034", "name": "L. Blank"}]}, {"paperId": "0fd8e04c9717409fa23e0d2fc995920a23c933be", "url": "https://www.semanticscholar.org/paper/0fd8e04c9717409fa23e0d2fc995920a23c933be", "title": "Machine learning in biomedical engineering", "abstract": null, "year": 2018, "referenceCount": 10, "citationCount": 58, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Engineering", "Medicine"], "authors": [{"authorId": "34331906", "name": "Cheolsoo Park"}, {"authorId": "7249907", "name": "C. C. Took"}, {"authorId": "47396772", "name": "J. Seong"}]}, {"paperId": "8294698a449b6f701aaf6d2d8f8af03ce948392b", "url": "https://www.semanticscholar.org/paper/8294698a449b6f701aaf6d2d8f8af03ce948392b", "title": "NSML: Meet the MLaaS platform with a real-world case study", "abstract": "The boom of deep learning induced many industries and academies to introduce machine learning based approaches into their concern, competitively. However, existing machine learning frameworks are limited to sufficiently fulfill the collaboration and management for both data and models. We proposed NSML, a machine learning as a service (MLaaS) platform, to meet these demands. NSML helps machine learning work be easily launched on a NSML cluster and provides a collaborative environment which can afford development at enterprise scale. Finally, NSML users can deploy their own commercial services with NSML cluster. In addition, NSML furnishes convenient visualization tools which assist the users in analyzing their work. To verify the usefulness and accessibility of NSML, we performed some experiments with common examples. Furthermore, we examined the collaborative advantages of NSML through three competitions with real-world use cases.", "year": 2018, "referenceCount": 35, "citationCount": 104, "influentialCitationCount": 4, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "2145498170", "name": "Hanjoo Kim"}, {"authorId": "1645213350", "name": "Minkyu Kim"}, {"authorId": "2061385477", "name": "Dongjoo Seo"}, {"authorId": "2144171578", "name": "Jinwoong Kim"}, {"authorId": "25104364", "name": "Heungseok Park"}, {"authorId": "2107879373", "name": "Soeun Park"}, {"authorId": "47878351", "name": "Hyunwoo Jo"}, {"authorId": "2109351321", "name": "KyungHyun Kim"}, {"authorId": "32076945", "name": "Youngil Yang"}, {"authorId": "7651979", "name": "Youngkwan Kim"}, {"authorId": "40188877", "name": "Nako Sung"}, {"authorId": "2577039", "name": "Jung-Woo Ha"}]}, {"paperId": "ee2d8a15569df3c3413715e8ce682d1fc4820d0c", "url": "https://www.semanticscholar.org/paper/ee2d8a15569df3c3413715e8ce682d1fc4820d0c", "title": "A Structure-Based Drug Discovery Paradigm", "abstract": "Structure-based drug design is becoming an essential tool for faster and more cost-efficient lead discovery relative to the traditional method. Genomic, proteomic, and structural studies have provided hundreds of new targets and opportunities for future drug discovery. This situation poses a major problem: the necessity to handle the \u201cbig data\u201d generated by combinatorial chemistry. Artificial intelligence (AI) and deep learning play a pivotal role in the analysis and systemization of larger data sets by statistical machine learning methods. Advanced AI-based sophisticated machine learning tools have a significant impact on the drug discovery process including medicinal chemistry. In this review, we focus on the currently available methods and algorithms for structure-based drug design including virtual screening and de novo drug design, with a special emphasis on AI- and deep-learning-based methods used for drug discovery.", "year": 2019, "referenceCount": 142, "citationCount": 194, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "152250994", "name": "Maria Batool"}, {"authorId": "46599804", "name": "Bilal Ahmad"}, {"authorId": "3314704", "name": "Sangdun Choi"}]}, {"paperId": "b646ba235ba49ed545c7903dd033b50e3fa99f34", "url": "https://www.semanticscholar.org/paper/b646ba235ba49ed545c7903dd033b50e3fa99f34", "title": "Machine learning for identifying Randomized Controlled Trials: An evaluation and practitioner's guide", "abstract": "Machine learning (ML) algorithms have proven highly accurate for identifying Randomized Controlled Trials (RCTs) but are not used much in practice, in part because the best way to make use of the technology in a typical workflow is unclear. In this work, we evaluate ML models for RCT classification (support vector machines, convolutional neural networks, and ensemble approaches). We trained and optimized support vector machine and convolutional neural network models on the titles and abstracts of the Cochrane Crowd RCT set. We evaluated the models on an external dataset (Clinical Hedges), allowing direct comparison with traditional database search filters. We estimated area under receiver operating characteristics (AUROC) using the Clinical Hedges dataset.", "year": 2018, "referenceCount": 42, "citationCount": 195, "influentialCitationCount": 5, "isOpenAccess": true, "fieldsOfStudy": ["Medicine"], "authors": [{"authorId": "1808775", "name": "I. Marshall"}, {"authorId": "1401080041", "name": "A. Noel-Storr"}, {"authorId": "37912398", "name": "J. Kuiper"}, {"authorId": "2110226596", "name": "James Thomas"}, {"authorId": "1912476", "name": "Byron C. Wallace"}]}, {"paperId": "df3c030d05967b737153536bee8ff04b1b3edb68", "url": "https://www.semanticscholar.org/paper/df3c030d05967b737153536bee8ff04b1b3edb68", "title": "Probably Approximately Correct Learning", "abstract": null, "year": 1990, "referenceCount": 54, "citationCount": 69, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1733689", "name": "D. Haussler"}]}, {"paperId": "d49ac711d04f5c445c6e14e5e8b8d034b4e7a93d", "url": "https://www.semanticscholar.org/paper/d49ac711d04f5c445c6e14e5e8b8d034b4e7a93d", "title": "Chapter 1 (Introduction) What do you mean by 'collaborative learning'?", "abstract": "This book arises from a series of workshops on collaborative learning, that gathered together 20 scholars from the disciplines of psychology, education and computer science. The series was part of a research program entitled 'Learning in Humans and Machines' (LHM), launched by Peter Reimann and Hans Spada, and funded by the European Science Foundation. This program aimed to develop a multidisciplinary dialogue on learning, involving mainly scholars from cognitive psychology, educational science, and artificial intelligence (including machine learning). During the preparation of the program, Agnes Blaye, Claire O'Malley, Michael Baker and I developed a theme on collaborative learning. When the program officially began, 12 members were selected to work on this theme and formed the so-called 'task force 5'. I became the coordinator of the group. This group organised two workshops, in Sitges (Spain, 1994) and Aix-en-Provence (France, 1995). In 1996, the group was enriched with new members to reach its final size. Around 20 members met in the subsequent workshops, at Samoens (France, 1996), Houthalen (Belgium, 1996) and Mannheim (Germany, 1997). Several individuals joined the group for some time but have not written a chapter. I would nevertheless like to acknowledge their contributions to our activities: George Bilchev, Stevan Harnad, Calle Jansson and Claire O'Malley.", "year": 1999, "referenceCount": 39, "citationCount": 315, "influentialCitationCount": 34, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1799133", "name": "P. Dillenbourg"}]}, {"paperId": "02eed03bdf0c5f9c8ad1f2a9dfc9fc8d8e6c817e", "url": "https://www.semanticscholar.org/paper/02eed03bdf0c5f9c8ad1f2a9dfc9fc8d8e6c817e", "title": "Adaptation Regularization: A General Framework for Transfer Learning", "abstract": "Domain transfer learning, which learns a target classifier using labeled data from a different distribution, has shown promising value in knowledge discovery yet still been a challenging problem. Most previous works designed adaptive classifiers by exploring two learning strategies independently: distribution adaptation and label propagation. In this paper, we propose a novel transfer learning framework, referred to as Adaptation Regularization based Transfer Learning (ARTL), to model them in a unified way based on the structural risk minimization principle and the regularization theory. Specifically, ARTL learns the adaptive classifier by simultaneously optimizing the structural risk functional, the joint distribution matching between domains, and the manifold consistency underlying marginal distribution. Based on the framework, we propose two novel methods using Regularized Least Squares (RLS) and Support Vector Machines (SVMs), respectively, and use the Representer theorem in reproducing kernel Hilbert space to derive corresponding solutions. Comprehensive experiments verify that ARTL can significantly outperform state-of-the-art learning methods on several public text and image datasets.", "year": 2014, "referenceCount": 46, "citationCount": 417, "influentialCitationCount": 66, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "35776445", "name": "Mingsheng Long"}, {"authorId": "2144499343", "name": "Jianmin Wang"}, {"authorId": "38329336", "name": "Guiguang Ding"}, {"authorId": "1746914", "name": "Sinno Jialin Pan"}, {"authorId": "144019071", "name": "Philip S. Yu"}]}, {"paperId": "a66d4cb2ca4a59d6d39c00f7d4fcf0b2175a1ddf", "url": "https://www.semanticscholar.org/paper/a66d4cb2ca4a59d6d39c00f7d4fcf0b2175a1ddf", "title": "Differentiable Learning of Quantum Circuit Born Machine", "abstract": "Quantum circuit Born machines are generative models which represent the probability distribution of classical dataset as quantum pure states. Computational complexity considerations of the quantum sampling problem suggest that the quantum circuits exhibit stronger expressibility compared to classical neural networks. One can efficiently draw samples from the quantum circuits via projective measurements on qubits. However, similar to the leading implicit generative models in deep learning, such as the generative adversarial networks, the quantum circuits cannot provide the likelihood of the generated samples, which poses a challenge to the training. We devise an efficient gradient-based learning algorithm for the quantum circuit Born machine by minimizing the kerneled maximum mean discrepancy loss. We simulated generative modeling of the BARS-AND-STRIPES dataset and Gaussian mixture distributions using deep quantum circuits. Our experiments show the importance of circuit depth and the gradient-based optimization algorithm. The proposed learning algorithm is runnable on near-term quantum device and can exhibit quantum advantages for probabilistic generative modeling.", "year": 2018, "referenceCount": 69, "citationCount": 134, "influentialCitationCount": 18, "isOpenAccess": true, "fieldsOfStudy": ["Physics", "Computer Science", "Mathematics"], "authors": [{"authorId": "9184128", "name": "Jin-Guo Liu"}, {"authorId": "2152508799", "name": "Lei Wang"}]}, {"paperId": "a7df954318c505aaf979da92223bde402b223510", "url": "https://www.semanticscholar.org/paper/a7df954318c505aaf979da92223bde402b223510", "title": "Time to automate identification", "abstract": null, "year": 2010, "referenceCount": 11, "citationCount": 205, "influentialCitationCount": 5, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "48848111", "name": "N. Macleod"}, {"authorId": "2587455", "name": "M. Benfield"}, {"authorId": "2316003", "name": "P. Culverhouse"}]}, {"paperId": "090a3e91629759fb66dd7206c4c3ed4c1fce45c9", "url": "https://www.semanticscholar.org/paper/090a3e91629759fb66dd7206c4c3ed4c1fce45c9", "title": "Experimental realization of a quantum support vector machine.", "abstract": "The fundamental principle of artificial intelligence is the ability of machines to learn from previous experience and do future work accordingly. In the age of big data, classical learning machines often require huge computational resources in many practical cases. Quantum machine learning algorithms, on the other hand, could be exponentially faster than their classical counterparts by utilizing quantum parallelism. Here, we demonstrate a quantum machine learning algorithm to implement handwriting recognition on a four-qubit NMR test bench. The quantum machine learns standard character fonts and then recognizes handwritten characters from a set with two candidates. Because of the wide spread importance of artificial intelligence and its tremendous consumption of computational resources, quantum speedup would be extremely attractive against the challenges of big data.", "year": 2015, "referenceCount": 0, "citationCount": 110, "influentialCitationCount": 3, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine"], "authors": [{"authorId": "5312421", "name": "Zhaokai Li"}, {"authorId": "48032382", "name": "Xiaomei Liu"}, {"authorId": "145203231", "name": "Nanyang Xu"}, {"authorId": "145084594", "name": "Jiangfeng Du"}]}, {"paperId": "75e50717070e82cdf3945265a75def6960b55a9d", "url": "https://www.semanticscholar.org/paper/75e50717070e82cdf3945265a75def6960b55a9d", "title": "Explanation-based neural network learning a lifelong learning approach", "abstract": "From the Publisher: \nLifelong learning addresses situations in which a learner faces a series of different learning tasks providing the opportunity for synergy among them. Explanation-based neural network learning (EBNN) is a machine learning algorithm that transfers knowledge across multiple learning tasks. When faced with a new learning task, EBNN exploits domain knowledge accumulated in previous learning tasks to guide generalization in the new one. As a result, EBNN generalizes more accurately from less data than comparable methods. Explanation-Based Neural Network Learning: A Lifelong Learning Approach describes the basic EBNN paradigm and investigates it in the context of supervised learning, reinforcement learning, robotics, and chess.", "year": 1995, "referenceCount": 0, "citationCount": 175, "influentialCitationCount": 11, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144867807", "name": "S. Thrun"}]}, {"paperId": "c10b6ba92e63d9056a81c7bff5f499536e9ea73f", "url": "https://www.semanticscholar.org/paper/c10b6ba92e63d9056a81c7bff5f499536e9ea73f", "title": "Machine Learning Models and Algorithms for Big Data Classification", "abstract": null, "year": 2016, "referenceCount": 0, "citationCount": 283, "influentialCitationCount": 14, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "3339259", "name": "S. Suthaharan"}]}, {"paperId": "ecc030a9108ddafab5c23ef6b4018ab05f050918", "url": "https://www.semanticscholar.org/paper/ecc030a9108ddafab5c23ef6b4018ab05f050918", "title": "ModelDB: a system for machine learning model management", "abstract": "Building a machine learning model is an iterative process. A data scientist will build many tens to hundreds of models before arriving at one that meets some acceptance criteria (e.g. AUC cutoff, accuracy threshold). However, the current style of model building is ad-hoc and there is no practical way for a data scientist to manage models that are built over time. As a result, the data scientist must attempt to \"remember\" previously constructed models and insights obtained from them. This task is challenging for more than a handful of models and can hamper the process of sensemaking. Without a means to manage models, there is no easy way for a data scientist to answer questions such as \"Which models were built using an incorrect feature?\", \"Which model performed best on American customers?\" or \"How did the two top models compare?\" In this paper, we describe our ongoing work on ModelDB, a novel end-to-end system for the management of machine learning models. ModelDB clients automatically track machine learning models in their native environments (e.g. scikit-learn, spark.ml), the ModelDB backend introduces a common layer of abstractions to represent models and pipelines, and the ModelDB frontend allows visual exploration and analyses of models via a web-based interface.", "year": 2016, "referenceCount": 12, "citationCount": 183, "influentialCitationCount": 17, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145845948", "name": "Manasi Vartak"}]}, {"paperId": "5040d7244bd5c25477d790cfb6e6f53292513585", "url": "https://www.semanticscholar.org/paper/5040d7244bd5c25477d790cfb6e6f53292513585", "title": "Handbook of categorization in cognitive science", "abstract": "Categorization in Cognitive Science. Semantic Categories. Syntactic Categories. Acquisition of Categories. Neuroscience of Categorization and Category Learning. Categories in Perception and Inference. Grounding, Recognition, and Reasoning in Categorization. Machine Category Learning. Data Mining for Categories and Ontologies. The Naturalization of Categories.", "year": 2005, "referenceCount": 0, "citationCount": 257, "influentialCitationCount": 1, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144383263", "name": "H. Cohen"}, {"authorId": "23616776", "name": "C. Lefebvre"}]}, {"paperId": "df013a17ab84d5403361da4538a04d574f58be83", "url": "https://www.semanticscholar.org/paper/df013a17ab84d5403361da4538a04d574f58be83", "title": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning", "abstract": "There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies.", "year": 2018, "referenceCount": 48, "citationCount": 845, "influentialCitationCount": 215, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1913774", "name": "Tianqi Chen"}, {"authorId": "47108160", "name": "T. Moreau"}, {"authorId": "1732910", "name": "Ziheng Jiang"}, {"authorId": "2149970173", "name": "Lianmin Zheng"}, {"authorId": "2621619", "name": "Eddie Q. Yan"}, {"authorId": "3050154", "name": "Haichen Shen"}, {"authorId": "37270394", "name": "M. Cowan"}, {"authorId": "2185540", "name": "Leyuan Wang"}, {"authorId": "49994783", "name": "Yuwei Hu"}, {"authorId": "1717411", "name": "L. Ceze"}, {"authorId": "1730156", "name": "Carlos Guestrin"}, {"authorId": "144695691", "name": "A. Krishnamurthy"}]}, {"paperId": "e14609a3a6c6f8ef3269d3e0728f88da57826698", "url": "https://www.semanticscholar.org/paper/e14609a3a6c6f8ef3269d3e0728f88da57826698", "title": "SENTIMENT CLASSIFICATION of MOVIE REVIEWS USING CONTEXTUAL VALENCE SHIFTERS", "abstract": "We present two methods for determining the sentiment expressed by a movie review. The semantic orientation of a review can be positive, negative, or neutral. We examine the effect of valence shifters on classifying the reviews. We examine three types of valence shifters: negations, intensifiers, and diminishers. Negations are used to reverse the semantic polarity of a particular term, while intensifiers and diminishers are used to increase and decrease, respectively, the degree to which a term is positive or negative. The first method classifies reviews based on the number of positive and negative terms they contain. We use the General Inquirer to identify positive and negative terms, as well as negation terms, intensifiers, and diminishers. We also use positive and negative terms from other sources, including a dictionary of synonym differences and a very large Web corpus. To compute corpus\u2010based semantic orientation values of terms, we use their association scores with a small group of positive and negative terms. We show that extending the term\u2010counting method with contextual valence shifters improves the accuracy of the classification. The second method uses a Machine Learning algorithm, Support Vector Machines. We start with unigram features and then add bigrams that consist of a valence shifter and another word. The accuracy of classification is very high, and the valence shifter bigrams slightly improve it. The features that contribute to the high accuracy are the words in the lists of positive and negative terms. Previous work focused on either the term\u2010counting method or the Machine Learning method. We show that combining the two methods achieves better results than either method alone.", "year": 2006, "referenceCount": 30, "citationCount": 795, "influentialCitationCount": 54, "isOpenAccess": true, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "34827767", "name": "Alistair Kennedy"}, {"authorId": "1697366", "name": "D. Inkpen"}]}, {"paperId": "3798281a1fcd1f8c9fa0bbacef60e3231e51460a", "url": "https://www.semanticscholar.org/paper/3798281a1fcd1f8c9fa0bbacef60e3231e51460a", "title": "Machine learning in games: a survey", "abstract": "This paper provides a survey of previously published work on machine learning in game playing. The material is organized around a variety of problems that typically arise in game playing and that can be solved with machine learning methods. This approach, we believe, allows both, researchers in game playing to find appropriate learning techniques for helping to solve their problems as well as machine learning researchers to identify rewarding topics for further research in game-playing domains. The chapter covers learning techniques that range from neural networks to decision tree learning in games that range from poker to chess. However, space constraints prevent us from giving detailed introductions to the used learning techniques or games. Overall, we aimed at striking a fair balance between being exhaustive and being exhausting.", "year": 2001, "referenceCount": 331, "citationCount": 75, "influentialCitationCount": 0, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1747752", "name": "Johannes F\u00fcrnkranz"}]}, {"paperId": "355b86dafd852e4df905f6ad9402c7d03831d618", "url": "https://www.semanticscholar.org/paper/355b86dafd852e4df905f6ad9402c7d03831d618", "title": "Probabilistic Latent Semantic Analysis", "abstract": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.", "year": 1999, "referenceCount": 14, "citationCount": 2705, "influentialCitationCount": 334, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "143936663", "name": "Thomas Hofmann"}]}, {"paperId": "0b6624f39c549f99422b49e1e7a004492ed3d11c", "url": "https://www.semanticscholar.org/paper/0b6624f39c549f99422b49e1e7a004492ed3d11c", "title": "bartMachine: Machine Learning with Bayesian Additive Regression Trees", "abstract": "We present a new package in R implementing Bayesian additive regression trees (BART). The package introduces many new features for data analysis using BART such as variable selection, interaction detection, model diagnostic plots, incorporation of missing data and the ability to save trees for future prediction. It is significantly faster than the current R implementation, parallelized, and capable of handling both large sample sizes and high-dimensional data.", "year": 2013, "referenceCount": 30, "citationCount": 250, "influentialCitationCount": 20, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "50645721", "name": "A. Kapelner"}, {"authorId": "1576051500", "name": "J. Bleich"}]}, {"paperId": "e3c9b317f7eb5730e3137f42472e014e0043d245", "url": "https://www.semanticscholar.org/paper/e3c9b317f7eb5730e3137f42472e014e0043d245", "title": "Scaling Up Crowd-Sourcing to Very Large Datasets: A Case for Active Learning", "abstract": "Crowd-sourcing has become a popular means of acquiring labeled data for many tasks where humans are more accurate than computers, such as image tagging, entity resolution, and sentiment analysis. However, due to the time and cost of human labor, solutions that rely solely on crowd-sourcing are often limited to small datasets (i.e., a few thousand items). This paper proposes algorithms for integrating machine learning into crowd-sourced databases in order to combine the accuracy of human labeling with the speed and cost-effectiveness of machine learning classifiers. By using active learning as our optimization strategy for labeling tasks in crowd-sourced databases, we can minimize the number of questions asked to the crowd, allowing crowd-sourced applications to scale (i.e., label much larger datasets at lower costs). \n \nDesigning active learning algorithms for a crowd-sourced database poses many practical challenges: such algorithms need to be generic, scalable, and easy to use, even for practitioners who are not machine learning experts. We draw on the theory of nonparametric bootstrap to design, to the best of our knowledge, the first active learning algorithms that meet all these requirements. \n \nOur results, on 3 real-world datasets collected with Amazons Mechanical Turk, and on 15 UCI datasets, show that our methods on average ask 1--2 orders of magnitude fewer questions than the baseline, and 4.5--44\u00d7 fewer than existing active learning algorithms.", "year": 2014, "referenceCount": 46, "citationCount": 162, "influentialCitationCount": 15, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2198667", "name": "Barzan Mozafari"}, {"authorId": "2507519", "name": "Purnamrita Sarkar"}, {"authorId": "143666627", "name": "M. Franklin"}, {"authorId": "1694621", "name": "Michael I. Jordan"}, {"authorId": "144478906", "name": "S. Madden"}]}, {"paperId": "e319212cc6b1c1c8ffeb4e42be4e0a319dffcc90", "url": "https://www.semanticscholar.org/paper/e319212cc6b1c1c8ffeb4e42be4e0a319dffcc90", "title": "Algebraic geometry and statistical learning theory", "abstract": "Preface 1. Introduction 2. Singularity theory 3. Algebraic geometry 4. Zeta functions and singular integral 5. Empirical processes 6. Singular learning theory 7. Singular learning machines 8. Singular information science Bibliography Index.", "year": 2009, "referenceCount": 0, "citationCount": 186, "influentialCitationCount": 39, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics"], "authors": [{"authorId": "102653910", "name": "\u6e21\u908a \u6f84\u592b"}]}, {"paperId": "57a06dad26d5a4692289aec30d2ae77af7a0f7a1", "url": "https://www.semanticscholar.org/paper/57a06dad26d5a4692289aec30d2ae77af7a0f7a1", "title": "How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation", "abstract": "Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.", "year": 2018, "referenceCount": 66, "citationCount": 246, "influentialCitationCount": 9, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "35807269", "name": "M. Narayanan"}, {"authorId": "2113754344", "name": "Emily Chen"}, {"authorId": "2109576017", "name": "Jeffrey He"}, {"authorId": "3351164", "name": "Been Kim"}, {"authorId": "1831199", "name": "S. Gershman"}, {"authorId": "1388372395", "name": "Finale Doshi-Velez"}]}, {"paperId": "ecb9f14cb1c96447bfd730063744176f8d9eb971", "url": "https://www.semanticscholar.org/paper/ecb9f14cb1c96447bfd730063744176f8d9eb971", "title": "A Kernel Classification Framework for Metric Learning", "abstract": "Learning a distance metric from the given training samples plays a crucial role in many machine learning tasks, and various models and optimization algorithms have been proposed in the past decade. In this paper, we generalize several state-of-the-art metric learning methods, such as large margin nearest neighbor (LMNN) and information theoretic metric learning (ITML), into a kernel classification framework. First, doublets and triplets are constructed from the training samples, and a family of degree-2 polynomial kernel functions is proposed for pairs of doublets or triplets. Then, a kernel classification framework is established to generalize many popular metric learning methods such as LMNN and ITML. The proposed framework can also suggest new metric learning methods, which can be efficiently implemented, interestingly, using the standard support vector machine (SVM) solvers. Two novel metric learning methods, namely, doublet-SVM and triplet-SVM, are then developed under the proposed framework. Experimental results show that doublet-SVM and triplet-SVM achieve competitive classification accuracies with state-of-the-art metric learning methods but with significantly less training time.", "year": 2013, "referenceCount": 73, "citationCount": 97, "influentialCitationCount": 7, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science", "Medicine", "Mathematics"], "authors": [{"authorId": "46429294", "name": "Faqiang Wang"}, {"authorId": "1724520", "name": "W. Zuo"}, {"authorId": "36685537", "name": "Lei Zhang"}, {"authorId": "1803714", "name": "Deyu Meng"}, {"authorId": "2109588613", "name": "David Zhang"}]}, {"paperId": "f1b6338335cae6f0de1ee1a30c8cdd661b69ed55", "url": "https://www.semanticscholar.org/paper/f1b6338335cae6f0de1ee1a30c8cdd661b69ed55", "title": "Learnability can be undecidable", "abstract": null, "year": 2019, "referenceCount": 33, "citationCount": 55, "influentialCitationCount": 4, "isOpenAccess": true, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "1401829700", "name": "Shai Ben-David"}, {"authorId": "2417054", "name": "P. Hrubes"}, {"authorId": "36415145", "name": "S. Moran"}, {"authorId": "1787363", "name": "Amir Shpilka"}, {"authorId": "1699039", "name": "A. Yehudayoff"}]}, {"paperId": "8a842c54db3c4e4e2e221abb03811eaf0409191f", "url": "https://www.semanticscholar.org/paper/8a842c54db3c4e4e2e221abb03811eaf0409191f", "title": "Comparing Multiple Machine Learning Algorithms and Metrics for Estrogen Receptor Binding Prediction.", "abstract": "Many chemicals that disrupt endocrine function have been linked to a variety of adverse biological outcomes. However, screening for endocrine disruption using in vitro or in vivo approaches is costly and time-consuming. Computational methods, e.g., quantitative structure-activity relationship models, have become more reliable due to bigger training sets, increased computing power, and advanced machine learning algorithms, such as multilayered artificial neural networks. Machine learning models can be used to predict compounds for endocrine disrupting capabilities, such as binding to the estrogen receptor (ER), and allow for prioritization and further testing. In this work, an exhaustive comparison of multiple machine learning algorithms, chemical spaces, and evaluation metrics for ER binding was performed on public data sets curated using in-house cheminformatics software (Assay Central). Chemical features utilized in modeling consisted of binary fingerprints (ECFP6, FCFP6, ToxPrint, or MACCS keys) and continuous molecular descriptors from RDKit. Each feature set was subjected to classic machine learning algorithms (Bernoulli Naive Bayes, AdaBoost Decision Tree, Random Forest, Support Vector Machine) and Deep Neural Networks (DNN). Models were evaluated using a variety of metrics: recall, precision, F1-score, accuracy, area under the receiver operating characteristic curve, Cohen's Kappa, and Matthews correlation coefficient. For predicting compounds within the training set, DNN has an accuracy higher than that of other methods; however, in 5-fold cross validation and external test set predictions, DNN and most classic machine learning models perform similarly regardless of the data set or molecular descriptors used. We have also used the rank normalized scores as a performance-criteria for each machine learning method, and Random Forest performed best on the validation set when ranked by metric or by data sets. These results suggest classic machine learning algorithms may be sufficient to develop high quality predictive models of ER activity.", "year": 2018, "referenceCount": 79, "citationCount": 87, "influentialCitationCount": 0, "isOpenAccess": true, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "8903363", "name": "Daniel P. Russo"}, {"authorId": "67164018", "name": "Kimberley M. Zorn"}, {"authorId": "143961624", "name": "A. Clark"}, {"authorId": "145153297", "name": "Hao Zhu"}, {"authorId": "1887610", "name": "S. Ekins"}]}, {"paperId": "36b8c88c8b3dd4048ad7583ca41c1b21f6a71ef7", "url": "https://www.semanticscholar.org/paper/36b8c88c8b3dd4048ad7583ca41c1b21f6a71ef7", "title": "Deep Learning: The Good, the Bad, and the Ugly.", "abstract": "Artificial vision has often been described as one of the key remaining challenges to be solved before machines can act intelligently. Recent developments in a branch of machine learning known as deep learning have catalyzed impressive gains in machine vision-giving a sense that the problem of vision is getting closer to being solved. The goal of this review is to provide a comprehensive overview of recent deep learning developments and to critically assess actual progress toward achieving human-level visual intelligence. I discuss the implications of the successes and limitations of modern machine vision algorithms for biological vision and the prospect for neuroscience to inform the design of future artificial vision systems. Expected final online publication date for the Annual Review of Vision Science, Volume 5 is September 16, 2019. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.", "year": 2019, "referenceCount": 37, "citationCount": 139, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Medicine", "Computer Science"], "authors": [{"authorId": "152684324", "name": "T. Serre"}]}, {"paperId": "5c5e69387020d7ca7d49487ca841958dc5e08ce6", "url": "https://www.semanticscholar.org/paper/5c5e69387020d7ca7d49487ca841958dc5e08ce6", "title": "The Cross-Entropy Method: A Unified Approach to Combinatorial Optimization, Monte-Carlo Simulation, and Machine Learning", "abstract": "Furthermore, if i and j are neighboring locations, then the correlation of observations at those points conditional on all other observations is Qij/ \u221a QiiQjj , and the conditional mean and precision of any single observation can also be expressed as simple functions of nonzero elements of Q. Since GMRF\u2019s can be specified through Normal conditionals defined by sparse matrices, this allows certain computations to run much more quickly than they would in the case of methods based on the dense matrix . Throughout the book, Normal distributions are specified by precisions, rather than by variances. The authors assume that the reader has a good working knowledge of conditional probability and of the multivariate Normal distribution at the level of Hogg, McKean, and Craig (2005). Beyond this, they outline or discuss in detail the technical aspects of fitting GMRF\u2019s, with a strong emphasis on only those details that are needed to make practical use of the methods. More formal and mathematical results are referenced in the endnotes. Their style is terse and if some detail seems to be missing, it can often be found upon rereading. If not, then the references and endnotes will point to where those details can be found. Theory is usually presented first, followed by a detailed investigation of one or more complex examples. After a brief and very useful introduction, Chapter 2 contains most of the general results about GMRF\u2019s. It begins with a presentation of notation necessary to describe conditional distributions relative to graphical dependence networks. The use of the precision matrix in model specification, inference, and simulation is then discussed, and a section is dedicated to a general review of solvers for sparse systems of linear equations. Since the speed of precisionmatrix-based methods depends on the speed of these solvers, this discussion is central to the practical use of these methods, and any text on inference that requires these methods should include a section of this kind. The use of toroidal boundary conditions on the data is discussed, showing how they can lead to further speed increases through use of cyclic precision matrices. Finally, several methods for ensuring that Q is positive definite are compared. The three remaining chapters apply the basic techniques to increasingly complex models. Whereas the third chapter is accessible and self-contained, the authors admit that the last two chapters cover areas where research is ongoing and suggest that they are best used by those with prior knowledge of those areas. Chapter 3 presents GMRF\u2019s in which Q is not of full rank. This can result from a linear constraint on conditional means, but also arises in cases where the data are first or second differences between observations at neighboring sites. For example, GMRF methods can be used to estimate parameters for random walks, based upon their paths. After fitting a random walk on the line, the authors go on to random walks on lattices, to random walks in continuous time observed at discrete random times, and to second order random walks. Very nice graphics are used to describe sums with a simple geometrical structure that would be hard to express with formal notation. The fourth chapter discusses hierarchical models, in which GMRF\u2019s are used to model dependencies between parameters. As an example, one could observe a sequence Yi of Bernoulli(pi) random variables that are not independent. To model the dependence, one could assume that Xi = logit(Yi) arises from an autoregressive model and further assume that the variance of the random terms in that autoregressive process is random with some suitable distribution. Fitting a model of this kind requires use of Markov chain Monte Carlo (MCMC) techniques, which are briefly discussed. The focus of these discussions is not on convergence (which is referenced elsewhere), but to the blocking of parameters to increase the efficiency of the MCMC algorithms. Other examples include the fitting of a model in which monthly numbers of driver deaths on British roads are modeled as", "year": 2006, "referenceCount": 3, "citationCount": 819, "influentialCitationCount": 111, "isOpenAccess": false, "fieldsOfStudy": ["Mathematics", "Computer Science"], "authors": [{"authorId": "40618053", "name": "L. Deng"}]}, {"paperId": "f886ed01f185072669bf3bf6d6bc110e126ac0f3", "url": "https://www.semanticscholar.org/paper/f886ed01f185072669bf3bf6d6bc110e126ac0f3", "title": "Machine Learning: A Historical and Methodological Analysis", "abstract": "Machine learning has always been an integral part of artificial intelligence, and its methodology has evolved in concert with the major concerns of the field. In response to the difficulties of encoding ever-increasing volumes of knowledge in modern AI systems, many researchers have recently turned their attention to machine learning as a means to overcome the knowledge acquisition bottleneck. This article presents a taxonomic analysis of machine learning organized primarily by learning strategies and secondarily by knowledge representation and application areas. A historical survey outlining the development of various approaches to machine learning is presented from early neural networks to present knowledge-intensive techniques.", "year": 1983, "referenceCount": 28, "citationCount": 64, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Engineering"], "authors": [{"authorId": "143712374", "name": "J. Carbonell"}, {"authorId": "2421006", "name": "R. Michalski"}, {"authorId": "40975594", "name": "Tom Michael Mitchell"}]}, {"paperId": "328899dd523c907f502d2eee97c568db2883e5c8", "url": "https://www.semanticscholar.org/paper/328899dd523c907f502d2eee97c568db2883e5c8", "title": "Explainable AI for Trees: From Local Explanations to Global Understanding", "abstract": "Tree-based machine learning models such as random forests, decision trees, and gradient boosted trees are the most popular non-linear predictive models used in practice today, yet comparatively little attention has been paid to explaining their predictions. Here we significantly improve the interpretability of tree-based models through three main contributions: 1) The first polynomial time algorithm to compute optimal explanations based on game theory. 2) A new type of explanation that directly measures local feature interaction effects. 3) A new set of tools for understanding global model structure based on combining many local explanations of each prediction. We apply these tools to three medical machine learning problems and show how combining many high-quality local explanations allows us to represent global structure while retaining local faithfulness to the original model. These tools enable us to i) identify high magnitude but low frequency non-linear mortality risk factors in the general US population, ii) highlight distinct population sub-groups with shared risk characteristics, iii) identify non-linear interaction effects among risk factors for chronic kidney disease, and iv) monitor a machine learning model deployed in a hospital by identifying which features are degrading the model's performance over time. Given the popularity of tree-based machine learning models, these improvements to their interpretability have implications across a broad set of domains.", "year": 2019, "referenceCount": 79, "citationCount": 173, "influentialCitationCount": 21, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Mathematics"], "authors": [{"authorId": "23451726", "name": "Scott M. Lundberg"}, {"authorId": "6579861", "name": "G. Erion"}, {"authorId": "2616600", "name": "Hugh Chen"}, {"authorId": "50731529", "name": "A. DeGrave"}, {"authorId": "3869000", "name": "J. Prutkin"}, {"authorId": "2447936", "name": "B. Nair"}, {"authorId": "2740105", "name": "R. Katz"}, {"authorId": "3538555", "name": "J. Himmelfarb"}, {"authorId": "34451449", "name": "N. Bansal"}, {"authorId": "2180463", "name": "Su-In Lee"}]}, {"paperId": "1d92cf3f0c86bfde10238fcaf31182a245bc920b", "url": "https://www.semanticscholar.org/paper/1d92cf3f0c86bfde10238fcaf31182a245bc920b", "title": "Learning with Opponent-Learning Awareness", "abstract": "Multi-agent settings are quickly gathering importance in machine learning. This includes a plethora of recent work on deep multi-agent reinforcement learning, but also can be extended to hierarchical RL, generative adversarial networks and decentralised optimisation. In all these settings the presence of multiple learning agents renders the training problem non-stationary and often leads to unstable training or undesired final results. We present Learning with Opponent-Learning Awareness (LOLA), a method in which each agent shapes the anticipated learning of the other agents in the environment. The LOLA learning rule includes a term that accounts for the impact of one agent's policy on the anticipated parameter update of the other agents. Results show that the encounter of two LOLA agents leads to the emergence of tit-for-tat and therefore cooperation in the iterated prisoners' dilemma, while independent learning does not. In this domain, LOLA also receives higher payouts compared to a naive learner, and is robust against exploitation by higher order gradient-based methods. Applied to repeated matching pennies, LOLA agents converge to the Nash equilibrium. In a round robin tournament we show that LOLA agents successfully shape the learning of a range of multi-agent learning algorithms from literature, resulting in the highest average returns on the IPD. We also show that the LOLA update rule can be efficiently calculated using an extension of the policy gradient estimator, making the method suitable for model-free RL. The method thus scales to large parameter and input spaces and nonlinear function approximators. We apply LOLA to a grid world task with an embedded social dilemma using recurrent policies and opponent modelling. By explicitly considering the learning of the other agent, LOLA agents learn to cooperate out of self-interest. The code is at this http URL.", "year": 2017, "referenceCount": 58, "citationCount": 359, "influentialCitationCount": 71, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "145356667", "name": "Jakob N. Foerster"}, {"authorId": "2896187", "name": "Richard Y. Chen"}, {"authorId": "1401178735", "name": "Maruan Al-Shedivat"}, {"authorId": "1766767", "name": "Shimon Whiteson"}, {"authorId": "1689992", "name": "P. Abbeel"}, {"authorId": "2080746", "name": "Igor Mordatch"}]}, {"paperId": "e9c22584df6369dc443712aa02b6749c025996cf", "url": "https://www.semanticscholar.org/paper/e9c22584df6369dc443712aa02b6749c025996cf", "title": "Multi-View Clustering and Feature Learning via Structured Sparsity", "abstract": "Combining information from various data sources has become an important research topic in machine learning with many scientific applications. Most previous studies employ kernels or graphs to integrate different types of features, which routinely assume one weight for one type of features. However, for many problems, the importance of features in one source to an individual cluster of data can be varied, which makes the previous approaches ineffective. In this paper, we propose a novel multi-view learning model to integrate all features and learn the weight for every feature with respect to each cluster individually via new joint structured sparsity-inducing norms. The proposed multi-view learning framework allows us not only to perform clustering tasks, but also to deal with classification tasks by an extension when the labeling knowledge is available. A new efficient algorithm is derived to solve the formulated objective with rigorous theoretical proof on its convergence. We applied our new data fusion method to five broadly used multi-view data sets for both clustering and classification. In all experimental results, our method clearly outperforms other related state-of-the-art methods.", "year": 2013, "referenceCount": 30, "citationCount": 271, "influentialCitationCount": 23, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "2113253801", "name": "Hua Wang"}, {"authorId": "144962210", "name": "F. Nie"}, {"authorId": "1748032", "name": "Heng Huang"}]}, {"paperId": "0dae6ca675f44ce3c556a4f56ea35d2d3a23c3de", "url": "https://www.semanticscholar.org/paper/0dae6ca675f44ce3c556a4f56ea35d2d3a23c3de", "title": "Intelligent Hybrid Vehicle Power Control\u2014Part II: Online Intelligent Energy Management", "abstract": "This is the second paper in a series of two that describe our research in intelligent energy management in a hybrid electric vehicle (HEV). In the first paper, we presented the machine-learning framework ML_EMO_HEV, which was developed for learning the knowledge about energy optimization in an HEV. The framework consists of machine-learning algorithms for predicting driving environments and generating the optimal power split of the HEV system for a given driving environment. In this paper, we present the following three online intelligent energy controllers: 1) IEC_HEV_SISE; 2) IEC_HEV_MISE ; and 3) IEC_HEV_MIME. All three online intelligent energy controllers were trained within the machine-learning framework ML_EMO_HEV to generate the best combination of engine power and battery power in real time such that the total fuel consumption over the whole driving cycle is minimized while still meeting the driver's demand and the system constraints, including engine, motor, battery, and generator operation limits. The three online controllers were integrated into the Ford Escape hybrid vehicle model for online performance evaluation. Based on their performances on ten test drive cycles provided by the Powertrain Systems Analysis Toolkit library, we can conclude that the roadway type and traffic congestion level specific machine learning of optimal energy management is effective for in-vehicle energy control. The best controller, IEC_HEV_MISE, trained with the optimal power split generated by the DP optimization algorithm with multiple initial SOC points and single ending point, can provide fuel savings ranging from 5% to 19%. Together, these two papers cover the innovative technologies for modeling power flow, mathematical background of optimization in energy management, and machine-learning algorithms for generating intelligent energy controllers for quasioptimal energy flow in a power-split HEV.", "year": 2013, "referenceCount": 24, "citationCount": 155, "influentialCitationCount": 2, "isOpenAccess": false, "fieldsOfStudy": ["Engineering", "Computer Science"], "authors": [{"authorId": "9491649", "name": "Y. Murphey"}, {"authorId": "2087950", "name": "Jungme Park"}, {"authorId": "2773844", "name": "Leonidas Kiliaris"}, {"authorId": "2157249", "name": "M. Kuang"}, {"authorId": "144347027", "name": "M. Masrur"}, {"authorId": "31639858", "name": "A. Phillips"}, {"authorId": "2145464452", "name": "Qing Wang"}]}, {"paperId": "957309ccfdb9d1263cd778c046c807ebee7aab1b", "url": "https://www.semanticscholar.org/paper/957309ccfdb9d1263cd778c046c807ebee7aab1b", "title": "Federated Learning via Over-the-Air Computation", "abstract": "The stringent requirements for low-latency and privacy of the emerging high-stake applications with intelligent devices such as drones and smart vehicles make the cloud computing inapplicable in these scenarios. Instead, edge machine learning becomes increasingly attractive for performing training and inference directly at network edges without sending data to a centralized data center. This stimulates a nascent field termed as federated learning for training a machine learning model on computation, storage, energy and bandwidth limited mobile devices in a distributed manner. To preserve data privacy and address the issues of unbalanced and non-IID data points across different devices, the federated averaging algorithm has been proposed for global model aggregation by computing the weighted average of locally updated model at each selected device. However, the limited communication bandwidth becomes the main bottleneck for aggregating the locally computed updates. We thus propose a novel over-the-air computation based approach for fast global model aggregation via exploring the superposition property of a wireless multiple-access channel. This is achieved by joint device selection and beamforming design, which is modeled as a sparse and low-rank optimization problem to support efficient algorithms design. To achieve this goal, we provide a difference-of-convex-functions (DC) representation for the sparse and low-rank function to enhance sparsity and accurately detect the fixed-rank constraint in the procedure of device selection. A DC algorithm is further developed to solve the resulting DC program with global convergence guarantees. The algorithmic advantages and admirable performance of the proposed methodologies are demonstrated through extensive numerical results.", "year": 2018, "referenceCount": 54, "citationCount": 451, "influentialCitationCount": 45, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science", "Engineering", "Mathematics"], "authors": [{"authorId": "1637401447", "name": "Kai Yang"}, {"authorId": "143872550", "name": "Tao Jiang"}, {"authorId": "1754997", "name": "Yuanming Shi"}, {"authorId": "145283850", "name": "Z. Ding"}]}, {"paperId": "6e2f3011f35969b63626e1c36b8fafd21a206ec0", "url": "https://www.semanticscholar.org/paper/6e2f3011f35969b63626e1c36b8fafd21a206ec0", "title": "Using machine learning to secure IoT systems", "abstract": "The Internet of Things (IoT) is a massive group of devices containing sensors or actuators connected together over wired or wireless networks. With an estimate of over 25 billion devices connected together by 2020, IoT has been rapidly growing over the past decade. During the growth, security has been identified as one of the weakest areas in IoT. When implementing security within an IoT network, there are several challenges including heterogeneity within the system as well as the quantity of devices that need to be addressed. To approach the challenges in securing IoT devices, we propose using machine learning within an IoT gateway to help secure the system. We investigate using Artificial Neural Networks in a gateway to detect anomalies in the data sent from the edge devices. We are convinced that this approach can improve the security of IoT systems.", "year": 2016, "referenceCount": 14, "citationCount": 114, "influentialCitationCount": 6, "isOpenAccess": false, "fieldsOfStudy": ["Computer Science"], "authors": [{"authorId": "144737250", "name": "J. Canedo"}, {"authorId": "1734586", "name": "A. Skjellum"}]}]}